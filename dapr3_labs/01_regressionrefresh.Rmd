---
title: "Regression Refresh and Clustered Data"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: FALSE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(patchwork)
```


:::lo
**Preliminaries**  
 
1. Open Rstudio, and **create a new project for this course!!** 
2. Create a new RMarkdown document or R script (whichever you like) for this week. 

:::

# New Packages!  
:::rtip

These are the main packages we're going to use in this block. It might make sense to install them now if you do not have them already (note, the rstudio.ppls.ed.ac.uk server already has `lme4` and `tidyverse` installed for you).  

+ __tidyverse__ : for organising data  
+ __ICC__ : for quickly calculating intraclass correlation coefficient
+ __lme4__ : for fitting generalised linear mixed effects models
+ __lmeresampler__ : for bootstrapping!
+ __effects__ : for tabulating and graphing effects in linear models
+ __broom.mixed__ : tidying methods for mixed models
+ __sjPlot__ : for plotting models
+ __DHARMa__ : for simulating residuals to assess assumptions
+ __hlmDiag__ : for examining case diagnostics at multiple levels

:::

```{r include=FALSE}
library(tidyverse)
library(lme4)
library(broom.mixed)
library(effects)
```

:::frame
Let's consider a little toy example in which we might use linear regression to determine how practice (in hours per week) influences the reading age of different toy figurines:
```{r echo=FALSE, fig.cap = "[Image and example from USMR Week 8 Lecture](https://uoepsy.github.io/usmr/lectures/lecture_7.html#29)", out.width="300px",fig.align="center"}
knitr::include_graphics("images/intro/reading.png")
```
  
Let's suppose we have data on various types of toys, from Playmobil (pictured above) to Powerrangers, to farm animals. You can find a dataset at https://uoepsy.github.io/data/toyexample.csv, and read it into your R environment using the code below: 
```{r eval=FALSE}
toys_read <- read_csv("https://uoepsy.github.io/data/toyexample.csv")
```
  
  
The dataset contains information on 132 different toy figures. You can see the variables in the table below^[Image sources:<br>http://tophatsasquatch.com/2012-tmnt-classics-action-figures/<br>https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/<br>https://www.wish.com/product/5da9bc544ab36314cfa7f70c<br>https://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asp<br>https://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.html<br>https://tvtropes.org/pmwiki/pmwiki.php/Toys/Furby<br>https://www.fun.com/toy-story-4-figure-4-pack.html<br>https://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461].
<br>
<div style="display:inline-block; width: 45%;vertical-align: middle;">
```{r echo=FALSE, out.width="300px",fig.align="center"}
knitr::include_graphics("images/intro/toys.png")
```
</div>
<div style="display:inline-block; width: 45%;vertical-align: middle;">
```{r echo=FALSE, message=FALSE,warning=FALSE}
library(gt)
toys_read <- read_csv("https://uoepsy.github.io/data/toyexample.csv")
tibble(variable=names(toys_read),
       description=c("Type of Toy","Character","Hours of practice per week","Age (in years)","Reading Age")
) %>% gt()

```
</div>

:::



# Linear model refresh   

:::statbox

Recall that in the DAPR2 course last year we learned all about the linear regression model, which took the form:

$$
\begin{align}\\
& \text{for observation }i \\
& \color{red}{Y_i} = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} X_{1i} \ + \ ... \ + \ \beta_p \cdot{} X_{pi}} + \varepsilon_i \\ 
\end{align}
$$

And if we wanted to write this more simply, we can express $X_1$ to $X_p$ as an $n \times p$ matrix (samplesize $\times$ parameters), and $\beta_0$ to $\beta_p$ as a vector of coefficients:

$$
\mathbf{y} = \boldsymbol{X\beta} + \boldsymbol{\varepsilon}
\quad \\
\text{where} \quad \varepsilon \sim N(0, \sigma) \text{ independently}
$$

:::

`r qbegin("A1")`
Read in the toy data from https://uoepsy.github.io/data/toyexample.csv and plot the bivariate relationship between Reading Age and Hrs per Week practice, and then fit the simple linear model: 
$$
\text{Reading Age}_i = \beta_0 + \beta_1 \cdot \text{Hours per week practice}_i + \varepsilon_i
$$
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
toys_read <- read_csv("https://uoepsy.github.io/data/toyexample.csv") 

ggplot(data = toys_read, aes(x = hrs_week, y = R_AGE))+
  geom_point()+
  geom_smooth(method = "lm")

simplemod <- lm(R_AGE ~ hrs_week, data = toys_read)
summary(simplemod)
```

`r solend()`

`r qbegin("A2")`
Think about the assumptions we make about our model:
$$
\text{where} \quad \varepsilon_i \sim N(0, \sigma) \text{ independently}
$$
Have we satisfied this assumption (specifically, the assumption of *independence* of errors)? 
`r qend()`

`r solbegin(show=TRUE, toggle=params$TOGGLE)`
Our model from the previous question will assume that the residuals for all toys are independent of one another. But is this a reasonable assumption that we can make? Might we not think that the Playmobil characters could be generally better at reading than the Power Rangers? Or even that ScoobyDoo figurines might be more receptive to practice than the Sock Puppets are?   

The natural grouping of the toys into their respective type introduces a level of *dependence* which we would be best to account for.  
`r solend()`

`r qbegin("A3")`
Try running the code below.  
```{r eval=FALSE}
ggplot(data = toys_read, aes(x=hrs_week, y=R_AGE))+
  geom_point()+
  geom_smooth(method="lm",se=FALSE)
```
Then try editing the code to include an aesthetic mapping from the type of toy to the color in the plot.  
How do your thoughts about the relationship between Reading Age and Practice change?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
ggplot(data = toys_read, aes(x=hrs_week, y=R_AGE))+
  geom_point()+
  geom_smooth(method="lm",se=FALSE)
```

```{r}
ggplot(data = toys_read, aes(x=hrs_week, y=R_AGE, col=toy_type))+
  geom_point()+
  geom_smooth(method="lm",se=FALSE)
```
  
From the second plot, we see a lot of the toy types appear to have a positive relationship (practice increases reading age). There seem to be differences between toy types in both the general reading level (Scooby Doo characters can read very well), and in how practice influences reading age (for instance, the Farm Animals don't seem to improve at all with practice!). 

`r solend()`

:::frame
**Complete Pooling**  

We can consider the simple regression model (`lm(R_AGE ~ hrs_week, data = toys_read)`) to "pool" the information from all observations together. In this 'Complete Pooling' approach, we simply ignore the natural clustering of the toys, as if we were unaware of it. The problem is that this assumes the same regression line for all toy types, which might not be that appropriate:  

```{r echo=FALSE, out.width="350px", fig.align="center", fig.cap="Complete pooling can lead to bad fit for certain groups"}
ggplot(toys_read, aes(x=hrs_week, y=R_AGE))+
  geom_point(size=3, alpha=.1)+
  geom_abline(intercept = coef(simplemod)[1], slope = coef(simplemod)[2], lwd=2)+
  geom_text(inherit.aes=F,x=4.5,y=8, label="Complete Pooling Line")+
  theme(text=element_text(size=21))+
  geom_point(data = filter(toys_read, str_detect(toy_type, "Scooby|Farm")), size=3, aes(col=toy_type))
```

  
**No Pooling**  

There are various ways we could attempt to deal with the problem that our data are in groups (or "clusters"). With the tools you have learned in DAPR2, you may be tempted to try including toy type in the model as another predictor, to allow for some toy types being generally better than others:
```{r eval=FALSE}
lm(R_AGE ~ hrs_week + toy_type, data = toys_read)
```
Or even to include an interaction to allow for toy types to respond differently to practice:
```{r eval=FALSE}
lm(R_AGE ~ hrs_week * toy_type, data = toys_read)
```

This approach gets termed the "No Pooling" method, because the information from each cluster contributes *only* to an estimated parameter for that cluster, and there is no pooling of information across clusters. This is a good start, but it means that a) we are estimating *a lot* of parameters, and b) we are not necessarily estimating the parameter of interest (the *overall* effect of practice on reading age). Furthermore, we'll probably end up having high variance in the estimates at each group.  

:::

`r qbegin("A4")`
Fit a linear model which accounts for the grouping of toys into their different types, but holds the effect of practice-hours-per-week on reading age as constant across types:
```{r}
mod1 <- lm(R_AGE ~ hrs_week + toy_type, data = toys_read)
```

Can you construct a plot of the **fitted** values from this model, coloured by toy_type?  
(Hint: you might want to use the `augment()` function from the **broom** package)

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
broom::augment(mod1) %>%
  ggplot(.,aes(x=hrs_week, y=.fitted, col=toy_type))+
  geom_line()
```
`r solend()`

`r qbegin("A5")`
What happens (to the plot, and to your parameter estimates) when you include the interaction between `toy_type` and `hrs_week`?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
mod2 <- lm(R_AGE ~ hrs_week * toy_type, data = toys_read)

broom::augment(mod2) %>%
  ggplot(.,aes(x=hrs_week, y=.fitted, col=toy_type))+
  geom_line()
```
We can see now that our model is fitting a different relationship between reading age and practice for each toy type. This is good - we're going to get better estimates for different types of toy (e.g. scooby doo's reading age increases with practice, farm animals don't).  

We can see that this model provides a better fit - it results in a significant reduction in the residual sums of squares:
```{r}
anova(mod1, mod2)
```

But accounting for this heterogeneity over clusters in the effect of interest comes at the expense of not pooling information across groups to get one estimate for "the effect of practice on reading age". Additionally, these models will tend to have low statistical power because they are using fewer observations (only those within each cluster) to estimate parameters which only represent within-cluster effects.  
`r solend()`


# Exploring Clustering

:::frame
**Data: Raising the stakes**

30 volunteers from an amateur basketball league participated in a study on stress induced by size and type of potential reward for successfully completing a throw. Each participant completed 20 trials in which they were tasked with throwing a basketball and scoring a goal in order to win a wager. The size of the wager varied between trials, ranging from 1 to 20 points, with the order randomised for each participant. If a participant successfully threw the ball in the basket, then their score increased accordingly. If they missed, their score decreased accordingly. Participants were informed of the size of the potential reward/loss prior to each throw.  

To examine the influence of the *type* of reward/loss on stress-levels, the study consisted of two conditions. In the monetary condition, (n = 15) participants were informed at the start of the study that the points corresponded to a monetary reward, and that they would be given their total score in £ at the end of the study. In the reputation condition, (n = 15) participants were informed that the points would be inputted on to a scoreboard and distributed around the local basketball clubs and in the league newsletter. 

Throughout each trial, participants' heart rate variability (HRV) was measured via a chest strap. HRV is considered to be indirectly related to levels of stress (i.e., higher HRV = less stress).

The data is in **.xlsx** format, and can be downloaded from 
https://uoepsy.github.io/data/basketballhrv.xlsx 
:::


`r qbegin()`
what are the units of observations
what are the groups/clusters?
what varies *within* these clusters?
what varies *between* these clusters?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
todo
`r solend()`


`r qbegin()`
Get the data into your R session. 

__Note:__ This is a bit different to how we have given you data in previous exercises. We used to give you data as a __.csv__ file, which you could read directly into R from the link using, `read_csv("https://uoepsy.......)`.  

However, in reality you are likely to be confronted with data in all sorts of weird formats. 
Have a look around the internet to try and find any packages/functions/techniques for getting the data in to R. 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

- Step 1: download the data to your computer  
- Step 2: load the __readxl__ package.  
- Step 3: use the `read_xlsx()` function to read in the data, pointing it to the relevant place on your computer. 

You can actually do all these steps from within R.
```{r}
# Step 1
download.file(url = "https://uoepsy.github.io/data/basketballhrv.xlsx", 
              destfile = "baskeballdata.xlsx")
# Step 2
library(readxl)
# Step 3
bball <- read_xlsx("baskeballdata.xlsx")
bball <- bball %>% mutate(sub = factor(sub))
```
```{r}
head(bball)
```


`r solend()`

`r qbegin()`
Plot the relationship between size of reward and HRV, ignoring the fact that there are repeated observations for each subject.  
Can you make a separate plot for each of the experimental conditions? (Hint: `facet_wrap()`)
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ggplot(bball, aes(x=stakes,y=hrv))+geom_smooth(method="lm", alpha=.2)+
  geom_point() +
  facet_wrap(~condition)
```
`r solend()`

`r qbegin()`
> How are stress levels (measured via HRV) influenced by the size of potential reward/loss?  

Fit a simple linear regression estimating how heart rate variability is influenced by how high the stakes are (i.e. how big the reward is) for a given throw.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
simple_mod <- lm(hrv ~ stakes, data = bball)
summary(simple_mod)
```


`r solend()`




`r qbegin()`
Plot the relationship between size of reward and HRV, with a separate line for each subject. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ggplot(bball, aes(x=stakes, y=hrv, group=sub, col=condition))+
  geom_smooth(method="lm",se=F, alpha=.2)+
  geom_point()+NULL
```
`r solend()`

`r qbegin()`
Calculate the ICC, using the `ICCbare()` function from the **ICC** package.  

Remember, you can look up the help for a function by typing a `?` followed by the function name in the console. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
library(ICC)
ICCbare(x = sub, y = hrv, data = bball)
```
`r solend()`

`r optbegin("Optional - Extra difficult. Calculate ICC manually", olabel=F)`
We have equal group sizes here (there are 2 $\times$ 15 participants, each with 20 observations), which makes calculating ICC by hand a lot easier, but it's still a bit tricky.  

Let's take a look at the formula for ICC

$$
\begin{align}
ICC \; (\rho) = & \frac{\sigma^2_{b}}{\sigma^2_{b} + \sigma^2_e} \\
\qquad \\
= & \frac{\frac{MS_b - MS_e}{k}}{\frac{MS_b - MS_e}{k} + MS_e} \\
\qquad \\
= & \frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\
\qquad \\
\qquad \\
\text{Where:} & \\ 
k = & \textrm{number of observations in each group} \\
MS_b = & \textrm{Mean Squares between groups} = \frac{\text{Sums Squares between groups}}{df_\text{groups}}
= \frac{\sum\limits_{i=1}(\bar{y}_i - \bar{y})^2}{\textrm{n groups}-1}\\
MS_e = & \textrm{Mean Squares within groups} \frac{\text{Sums Squares within groups}}{df_\text{within groups}} 
= \frac{\sum\limits_{i=1}\sum\limits_{j=1}(y_{ij} - \bar{y_i})^2}{\textrm{n obs}-\textrm{n groups}}\\
\end{align}
$$
So we're going to need to calculate the grand mean of $y$, the group means of $y$, and then the various squared differences between group means and grand mean, and between observations and their respective group means.  

The code below will give us a new column which is the overall mean of y. This bit is fairly straightforward. 
```{r eval=F}
bball %>% mutate(
  grand_mean = mean(hrv)
)
```


:::rtip
We have seen a lot of the combination of `group_by() %>% summarise()`, but we can also combine `group_by()` with `mutate()`!
:::

Try the following:
```{r eval=F}
bball %>% mutate(
    grand_mean = mean(hrv)
  ) %>% 
  group_by(sub) %>%
  mutate(
    group_mean = mean(hrv)
  )
```


:::rtip
**The grouping gets carried forward.**  

Using `group_by()` can quite easily land you in trouble if you forget that you have grouped the dataframe. 

Look at the output of `class()` when we have grouped the data. It still mentions something about the grouping. 
```{r}
bball <- bball %>% mutate(
    grand_mean = mean(hrv)
  ) %>% 
  group_by(sub) %>%
  mutate(
    group_mean = mean(hrv)
  )

class(bball)
```

To remove the grouping, we can use `ungroup()` (we could also just add this to the end of our code sequence above and re-run it):

```{r}
bball <- ungroup(bball)
class(bball)
```

:::

Now we need to create a column which is the squared differences between the observations $y_{ij}$ and the group means $\bar{y_i}$.  
We also want a column which is the squared differences between the group means $\bar{y_i}$ and the overall mean $\bar{y}$.  
```{r}
bball <- bball %>% 
  mutate(
    within = (hrv-group_mean)^2,
    between = (group_mean-grand_mean)^2
  )
```

And then we want to sum them:
```{r}
ssbetween = sum(bball$between)
sswithin = sum(bball$within)
```

Finally, we divide them by the degrees of freedom. 
```{r}
# Mean Squares between
msb = ssbetween / (30-1)
# Mean Squares within 
mse = sswithin / (600-30)
```

And calculate the ICC!!!
```{r}
# ICC
(msb-mse) /(msb + (19*mse))
```

`r optend()`

:::statbox

**Understanding ICC a bit better**  
  
Think about what ICC represents - the ratio of the variance between the groups to the total variance.  
You can think of the "variance between the groups" as the group means varying around the overall mean (the black dots around the black line), and the total variance as that plus the addition of the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):
```{r}
ggplot(bball, aes(x=sub, y=hrv))+
  geom_point(aes(col=sub),alpha=.3)+
  stat_summary(geom = "pointrange")+
  geom_hline(yintercept = mean(bball$hrv))+
  guides(col=FALSE)
```

You can also think of the ICC as the correlation between two randomly drawn observations from the same group. 
This is a bit of a tricky thing to get your head round if you try to relate it to the type of "correlation" that you are familiar with. Pearson's correlation (e.g think about a typical scatterplot) operates on *pairs of observations* (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on *data which is structured in groups*. 
`r optbegin("Optional - ICC as the expected correlation between two observations from same group", olabel=F)`

Let's suppose we had only 2 observations in each group.  
```{r echo=FALSE}
tempdat <- read.csv("../../data/iccexplainer.csv")
head(tempdat) %>% rbind(.,rep("...", 3))
```

```{r include=F}
library(nlme)
res <- lme(y ~ 1, random = ~ 1 | cluster, data=tempdat, method="ML")
ic <- getVarCov(res)[1] / (getVarCov(res)[1] + res$sigma^2)
```
The ICC for this data is `r round(ic,2)`:

Now suppose we *reshape* our data so that we have one row per group, and one column for each observation to look like this:
```{r echo=F}
tempdat_wide <- tempdat %>% 
  pivot_wider(names_from=observation, values_from=y, names_prefix = "obs") 
tempdat_wide %>% head %>% rbind(.,rep("...", 3))
```
Calculating Pearson's correlation on those two columns yields `r cor(tempdat_wide$obs1, tempdat_wide$obs2) %>% round(.,2)`, which isn't quite right. It's close, but not quite.. 

:::imp 
The crucial thing here is that it is completely arbitrary which observations get called "obs1" and which get called "obs2".  
The data aren't paired, but grouped. 
:::

Essentially, there are lots of different combinations of "pairs" here. 
There are the ones we have shown above:
```{r echo=F}
head(tempdat_wide) %>% rbind(., rep("...",3))
```
But we might have equally chosen these:
```{r echo=F}
sample_n(tempdat, n()) %>% arrange(cluster) %>% group_by(cluster) %>% 
  mutate(observation = 1:n()) %>% ungroup %>%
  pivot_wider(names_from=observation, values_from=y, names_prefix = "obs") %>% head() %>% rbind(., rep("...",3))
```
or these:
```{r echo=F}
sample_n(tempdat, n()) %>% arrange(cluster) %>% group_by(cluster) %>% 
  mutate(observation = 1:n()) %>% ungroup %>%
  pivot_wider(names_from=observation, values_from=y, names_prefix = "obs") %>% head() %>% rbind(., rep("...",3))
```

If we take the correlation of all these combinations of pairings, then we get our ICC of `r round(ic, 2)`!

__ICC = the expected correlation of a *randomly drawn pair* of observations from the same group.__

<!-- We could even do this via simulation, and write our own customised function! -->
<!-- The code below creates a function for us to use. Can you figure out how it works?  -->
<!-- ```{r} -->
<!-- get_random_pair <- function(){ -->
<!--   my_sub = sample(unique(bball$sub), 1) -->
<!--   my_obs = sample(bball$hrv[bball$sub == my_sub], size=2) -->
<!--   my_obs -->
<!-- } -->
<!-- ``` -->
<!-- Try it out, by running it several times.  -->
<!-- ```{r} -->
<!-- get_random_pair() -->
<!-- ``` -->

<!-- Now let's make our computer do it loads and loads of times: -->
<!-- ```{r} -->
<!-- # replicate is a way of making R execute the same code repeatedly, n times. -->
<!-- sims <- replicate(1e6, get_random_pair()) -->
<!-- # t() is short for "transpose" and simple rotates the object 90 degrees (so rows become columns and columns become rows) -->
<!-- sims <- t(sims) -->
<!-- cor(sims[,1], sims[,2]) -->

<!-- ``` -->

`r optend()`

:::


<!-- # GEE -->

<!-- if we're just interested in the effect of x on y and want to adjust for the clustering of the data, generalising estimation equations (GEE) can sometimes be useful. They are in some ways more limited than multilevel models, but simplicity can often be a good thing. -->

<!-- GEEs allow fitting of various correlation structures:  -->

<!-- - "independence": the observations within the groups are uncorrelated. -->
<!-- - "exchangeable": each pair of observations in a group has the same correlation. -->
<!-- - "unstructured": each pair of observations in a group is allowed to have a different correlation. -->
<!-- - "ar1": used to fit an autoregressive structure.  -->

<!-- For repeated measures, "exchangeable" is recommended, but if there is some expected temporal correlation (e.g. participants are influenced by their previous trials) then you might consider an autoregressive structure.  -->

<!-- Subjects are already arranged in order, but they need to be numeric. -->
<!-- ```{r} -->
<!-- ## make group id numeric an ordered -->
<!-- bball %>% mutate( -->
<!--   sub = as.numeric(sub) -->
<!--   ) %>% arrange(sub) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(geepack) -->
<!-- geemod <- geeglm(hrv ~ stakes, -->
<!--        id=sub, data = bball, -->
<!--        corstr="exchangeable") -->
<!-- ``` -->

<!-- `r qbegin()` -->
<!-- Fit the GEE above, and examine the summary output. The package is designed to be interpreted similarly to `lm()` and `glm()`. -->
<!-- `r qend()` -->
<!-- `r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)` -->
<!-- ```{r} -->
<!-- summary(geemod) -->
<!-- ``` -->
<!-- Note that the point estimate is the same (`coef(geemod)[2]`) as that from our simple model `lm(hrv ~ stakes, data = bball)` but the standard errors are wider (and the relationship is no longer significant).   -->
<!-- `r solend()` -->

# Revisiting interactions


`r qbegin()`
Let's suppose we want to account for the by-participant clustering in our data with the "No pooling" method (i.e., include participant as a fixed effect along with its interaction with explanatory variable of interest).  

Fit the model, and use the `plot_model()` function (with `type = "int"`) to plot the interaction terms between `stakes` and each participant. 
   
__Note:__ When examining parameter values, remember to think about how HRV is considered to relate to stress, and whether the direction of any effect you see makes theoretical sense. 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
nopool_mod <- lm(hrv ~ stakes*sub, data=bball)
sjPlot::plot_model(nopool_mod, type = "int")
```
`r solend()`


`r qbegin()`
We have fitted two models so far:  

1. The complete pooling model: `lm(hrv ~ stakes, data = bball)`, which ignores the fact that our data has some inherent grouping (multiple datapoints per participant)
2. The no pooling model: `lm(hrv ~ stakes*sub, data = bball)`, which estimates only participant-specific effects. 

Compare the two models using `anova()`. Which model provides the best fit?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
pool_mod <- lm(hrv ~ stakes, data=bball)
nopool_mod <- lm(hrv ~ stakes*sub, data=bball)
anova(pool_mod, nopool_mod)
```
We can see that the no pooling model provides a significantly better fit to the data.
`r solend()`


`r qbegin()`
Now let us suppose that we are interested in this question: 

> Does the influence of the size of reward/loss on stress levels differ depending upon whether it is money vs reputation at stake? 


Extend your complete pooling model to include the interaction between stakes and experimental condition and examine the parameter values.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
simple_mod <- lm(hrv ~ condition*stakes, data = bball)
anova(simple_mod)
summary(simple_mod)
```
```{r include=F}
res = summary(simple_mod)$coefficients %>% round(2)
```
:::int
Heart Rate Variability (HRV) was found to be influenced by both the size of the potential reward/loss of a given trial, whether whether participants were playing for money or for a place on the scoreboard, and the interaction between the two. 
For a 1 point increase in stakes, HRV decreased by `r res[4,1]` ($SE=`r res[4,2]`,t(`r simple_mod[["df.residual"]]`)=`r res[4,3]`,p=`r res[4,4]`$)**** in the condition in which participants played for money relative to that in which participants played for kudos, suggesting that the size of the reward has a greater effect on stress levels when playing for money compared to playing for reputation.  
   
**** __Caveat:__ Our model did __not__ account for by-participant clustering of data, thereby violating the assumption that errors are iid (independent and identically distributed). 
:::
`r solend()`


`r qbegin()`
Let's suppose we want to examine the interaction here using the "no pooling" method (i.e., including participant as a fixed effect).  

We have the variable `stakes`, that varies __within__ each participant, and another variable `condition` that varies __between__ participants.  

This becomes difficult because the `sub` variable (the participant id variable) uniquely identifies the two conditions. Note that if we fit the following model, some coefficients are not defined.
Try it and see:  
```{r eval=FALSE}
lm(hrv ~ stakes*sub + stakes*condition, data=bball)
```
`r qend()`

:::statbox

This sort of perfectly balanced design has traditionally been approached with extensions of ANOVA ("repeated measures ANOVA","mixed ANOVA"). These methods can partition out variance due to one level of clustering (e.g. subjects), and can examine factorial designs when one factor is within cluster, and the other is between. You can see an example below should you be interested.  
However, these techniques have a lot of constraints  ANOVA has a lot of constraints - it can't handle multiple levels of clustering (e.g. children in classes in schools), it will likely require treating time as a factor, and it's not great with missing data. 
The multi-level model (MLM) provides a more flexible framework, and this is what we will begin to look at next week.  

:::

# Optional Extra: ANOVA

:::imp
This section is optional for this course, but may be useful for your dissertations should your field/supervisor prefer the ANOVA framework to that of the linear model. 

This walks briefly through these models with the __ez__ package. There are many other packages available, and many good tutorials online should you desire extra resources in the future:

- https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r
- https://www.r-bloggers.com/2021/04/repeated-measures-of-anova-in-r-complete-tutorial/
- https://stats.idre.ucla.edu/r/seminars/repeated-measures-analysis-with-r/ 
- https://www.datanovia.com/en/lessons/mixed-anova-in-r/  

:::

## Repeated Measures ANOVA

For a repeated measures ANOVA, our independent variables are _within_ groups.  

Following from the example study above, we might consider using it to answer the question below.  
 
> __Question:__ What is the effect of the size of reward on stress levels (as measured by HRV)?

The easiest way to conduct a repeated measures ANOVA in R is to use the __ez__ package.  
It comes with some handy functions to visualise the experimental design.  
We can see from below that every participant completed a trial for each value of reward-size (1-20 points):
```{r}
library(ez)
ezDesign(data = bball, x = sub, y = stakes)
```
`r qbegin()`
The `ezANOVA()` function takes a few arguments. 

The ones you will need for this are:

- *data* the name of the dataframe
- *dv* the column name for the dependent variable
- *wid* the column name for the participant id variable
- *within* the column name(s) for the predictor variable(s) that vary within participants 
- *between* the column name(s) for any predictor variable(s) that vary between participants

Fit a repeated measures ANOVA to examine the effect of the size of reward on HRV. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ezANOVA(data=bball, dv=hrv, wid = sub, within = stakes)
```
`r solend()`

## Mixed ANOVA 
Mixed ANOVA can be used to investigate effects of independent variables that are at two different levels,  i.e. some are *within* clusters and some are *between*.  

> __Question:__ Does the influence of the size of reward/loss on stress levels differ depending upon whether it is money vs reputation at stake? 

`r qbegin()`
Look at the two lines below. Can you work out what the plots will look like _before_ you run them? 

```{r eval=F}
ezDesign(data = bball, x = condition, y = sub)
ezDesign(data = bball, x = condition, y = stakes)
```

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Participants 1-15 are in one condition, and 16-30 are in another.  
This should look like a two big blocks on the diagonal. 
```{r}
ezDesign(data = bball, x = condition, y = sub)
```
In each condition, the full set of stakes (1-20 points) were observed in the same number of trials.
This should be a full grid:
```{r}
ezDesign(data = bball, x = condition, y = stakes)
```
`r solend()`


`r qbegin()`
Fit a mixed ANOVA to examine the interaction between size and type of reward on HRV. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ezANOVA(data=bball, dv=hrv, wid = sub, within = stakes, between = condition)
```
`r solend()`


`r qbegin()`
The __ez__ package also contains some easy plotting functions for factorial experiments, such as `ezPlot()`. It takes similar arguments to the `ezANOVA()` function. 

- look up the help documentation for `ezPlot()`. 
- try to recreate the plot below using `ezPlot()` (__Note:__ you may need to make sure that the `stakes` variable is a factor).

```{r echo=FALSE}
bball <- bball %>% mutate(stakes = factor(stakes))
ezPlot(data=bball, dv=hrv, wid = sub, within = stakes, between = condition, x = stakes, split = condition)
```


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
bball <- bball %>% mutate(stakes = factor(stakes))
ezPlot(data=bball, dv=hrv, wid = sub, within = stakes, between = condition, x = stakes, split = condition)
```
`r solend()`





<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>