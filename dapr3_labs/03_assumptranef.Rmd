---
title: "Assumptions, Diagnostics, and Random Effect Structures"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(patchwork)
library(effects)
library(knitr)
library(kableExtra)
```

:::green
__Information about solutions__

Solutions for these exercises are available immediately below each question.  
We would like to emphasise that much evidence suggests that testing enhances learning, and we __strongly__ encourage you to make a concerted attempt at answering each question *before* looking at the solutions. Immediately looking at the solutions and then copying the code into your work will lead to poorer learning.  
We would also like to note that there are always many different ways to achieve the same thing in R, and the solutions provided are simply _one_ approach.  

:::


:::lo
**Preliminaries**  
 
1. Create a new RMarkdown document or R script (whichever you like) for this week. 

:::

# Assumptions

## Toy data

:::frame
<div style="display:inline-block; width: 45%;vertical-align: middle;">
Recall our toy example data in which we fitted a multilevel model to model how practice (in hours per week) influences the reading age of toy figurines which are grouped by toy-type (Playmobil, Powerrangers, Farm Animals etc).
</div>
<div style="display:inline-block; width: 45%;vertical-align: middle;">
```{r echo=FALSE, out.width="300px",fig.align="center"}
knitr::include_graphics("images/intro/toys.png")
```
</div>
```{r message=FALSE,warning=FALSE}
toys_read <- read_csv("https://uoepsy.github.io/data/toyexample.csv")
```
:::

`r qbegin("A1")`
Run the code below to fit the model with by-toy-type random intercepts and slopes of practice. 
```{r message=F,warning=F}
library(tidyverse)
library(lme4)
toys_read <- read_csv("https://uoepsy.github.io/data/toyexample.csv")
rs_model <- lmer(R_AGE ~ 1 + hrs_week + (1 + hrs_week | toy_type), data = toys_read)
```

Plot the residuals vs fitted model, and assess the extend to which the assumption holds that the residuals are zero mean.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
plot(rs_model, type=c("p","smooth"))
```
`r solend()`

`r qbegin("A2")`
Construct a scale-location plot. This is where the square-root of the absolute value of the standardised residuals is plotted against the fitted values, and allows you to more easily assess the assumption of constant variance. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
plot(rs_model,
     form = sqrt(abs(resid(.))) ~ fitted(.),
     type = c("p","smooth"))
```
`r solend()`

`r qbegin("A3")`
Using the `augment()` function from the __broom.mixed__ package, construct the same plot using ggplot.  

_(constructing plots like this manually is a really useful way to help understand **what** exactly is being plotted)_
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
library(broom.mixed)
augment(rs_model) %>%
  mutate(
    sqrtr = sqrt(abs(.resid))
  ) %>%
  ggplot(aes(x=.fitted, y=sqrtr)) + 
  geom_point() +
  geom_smooth()
```
`r solend()`

`r qbegin("A4")`
Examine the normality of both the level 1 and level 2 residuals. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

__Level 1__  

```{r}
hist(resid(rs_model))
library(lattice)
qqmath(rs_model, id=0.05)
```
__Level 2__  
```{r}
qqmath(ranef(rs_model))
hist(ranef(rs_model)$toy_type[,1])
hist(ranef(rs_model)$toy_type[,2])
```

`r solend()`

# Diagnostics

`r qbegin("B1")`
Which toy in the dataset has the greatest influence on our model?  

**Hint:** as well as `hlm_influence()` in the __HLMdiag__ package there is another nice function, `hlm_augment()`

:::imp
We can often end up in confusion because the $i^{th}$ observation inputted to our model (and therefore the $i^{th}$ observation of `hlm_influence()` output) **might not be** the $i^{th}$ observation in our original dataset - there may be missing data! 
:::

(Luckily, we have no missing data in the Toy dataset). 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
library(HLMdiag)
l1_inf <- hlm_influence(rs_model,level=1)
dotplot_diag(l1_inf$cooksd, cutoff="internal")+
  ylim(0,.15)
hlm_augment(rs_model, level=1) %>% arrange(desc(cooksd))
```
Greatest influence:
```{r}

```
```{r echo=FALSE, out.width="100px",fig.align="center"}
knitr::include_graphics("images/ace.png")
```

`r solend()`

`r qbegin("B2")`
For which toy is the model fit the worst (i.e., who has the highest residual?)
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
hlm_augment(rs_model, level=1) %>% arrange(desc(abs(.resid)))
toys_read[93, ]
```
```{r echo=FALSE, out.width="100px",fig.align="center"}
knitr::include_graphics("images/coldfront.png")
```
`r solend()`


`r qbegin("B3")`
Which _type_ of toy has the greatest influence on our model?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Either this way:
```{r}
hlm_augment(rs_model, level="toy_type") %>% arrange(desc(cooksd))
```

Or the plot. Note, the __only__ reason we using the `cutoff = .15` is to make the labels appear. 
```{r}
inftoytype <- hlm_influence(rs_model,level="toy_type")
dotplot_diag(inftoytype$cooksd, index=inftoytype$toy_type, cutoff=.15) +
  ylim(0,.2)
```
`r solend()`

`r qbegin("B4")`
Looking at the random effects, which toy type shows the least improvement in reading age as practice increases, and which shows the greatest improvement?
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
It looks like the Farm Animals have the least improvement, and Scooby Doo shows the most improvement
```{r}
ranef(rs_model)
```
`r solend()`

`r qbegin("B5")`
What is the estimated reading age for sock puppets with zero hours of practice per week, and what is their estimated change in reading age for every hour per week increase in practice? 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can get these either by adding the `fixef()` and `ranef()` together, or:
```{r}
coef(rs_model)
```
```{r}
coef(rs_model)$toy_type["Sock Puppets",]
```

```{r echo=FALSE, out.width="100px",fig.align="center"}
knitr::include_graphics("images/sockpuppet.gif")
```

`r solend()`

# Convergence Issues 

## Singular fits
:::statbox

You may have noticed that a lot of our models over the last few weeks have been giving a warning: `boundary (singular) fit: see ?isSingular`.   
Up to now, we've been largely ignoring these warnings. However, this week we're going to look at how to deal with this issue.

The warning is telling us that our model has resulted in a 'singular fit'. Singular fits often indicate that the model is 'overfitted' - that is, the random effects structure which we have specified is too complex to be supported by the data.  

Perhaps the most intuitive advice would be remove the most complex part of the random effects structure (i.e. random slopes). This leads to a simpler model that is not over-fitted. In other words, start simplying from the top (where the most complexity is) to the bottom (where the lowest complexity is).
Additionally, when variance estimates are very low for a specific random effect term, this indicates that the model is not estimating this parameter to differ much between the levels of your grouping variable. It might, in some experimental designs, be perfectly acceptable to remove this or simply include it as a fixed effect.

A key point here is that when fitting a mixed model, we should think about how the data are generated. Asking yourself questions such as "do we have good reason to assume subjects might vary over time, or to assume that they will have different starting points (i.e., different intercepts)?" can help you in specifying your random effect structure

You can read in depth about what this means by reading the help documentation for `?isSingular`. For our purposes, a relevant section is copied below:  

*... intercept-only models, or 2-dimensional random effects such as intercept + slope models, singularity is relatively easy to detect because it leads to random-effect variance estimates of (nearly) zero, or estimates of correlations that are (almost) exactly -1 or 1.*

:::

## Convergence warnings  
:::rtip 

Issues of non-convergence can be caused by many things. If you're model doesn't converge, it does *not necessarily* mean the fit is incorrect, however it is **is cause for concern**, and should be addressed, else you may end up reporting inferences which do not hold.

There are lots of different things which you could do which *might* help your model to converge. A select few are detailed below:  

- double-check the model specification and the data  

- adjust stopping (convergence) tolerances for the nonlinear optimizer, using the optCtrl argument to [g]lmerControl. (see `?convergence` for convergence controls). 
    - What is "tolerance"? Remember that our optimizer is the the method by which the computer finds the best fitting model, by iteratively assessing and trying to maximise the likelihood (or minimise the loss). 
    ```{r echo=FALSE, fig.cap="An optimizer will stop after a certain number of iterations, or when it meets a tolerance threshold"}
    knitr::include_graphics("../dapr3_lectures/mlm_lectures/jk_img_sandbox/tolerance.png")
    ```

- center and scale continuous predictor variables (e.g. with `scale`)  

- Change the optimization method (for example, here we change it to `bobyqa`):
    `lmer(..., control = lmerControl(optimizer="bobyqa"))`  
    `glmer(..., control = glmerControl(optimizer="bobyqa"))`  

- Increase the number of optimization steps:
    `lmer(..., control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=50000))`  
    `glmer(..., control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=50000))`  

- Use `allFit()` to try the fit with all available optimizers. This will of course be slow, but is considered 'the gold standard'; *"if all optimizers converge to values that are practically equivalent, then we would consider the convergence warnings to be false positives."*  

:::


`r qbegin("C1")`
Recall Questions C1-C6 in last week's exercises, we used the `WeightMaintain3` dataset and fitted some models:

```{r}
load(url("https://uoepsy.github.io/data/WeightMaintain3.rda"))
m.base <- lmer(WeightChange ~ Assessment + (1 + Assessment | ID), data=WeightMaintain3)
m.int <- lmer(WeightChange ~ Assessment + Condition + (1 + Assessment | ID), data=WeightMaintain3)
m.full <- lmer(WeightChange ~ Assessment * Condition + (1 + Assessment | ID), data=WeightMaintain3)
```

Many of these were singular fits, and we ignored them, e.g:
```{r}
isSingular(m.base)
```
Think carefully about how you might simplify the random effect structure for these models, and pay close attention to the study design. What do we think the baseline weight change should be? Should it be the same for everyone? If so, might we want to remove the random intercept, which we can do by setting it to 0.  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Think about what these models are fitting. Why (in this very specific research design) might it make sense to not model by-participant variation around the intercept?  
The "baseline" visit is the starting point for the study, and (based on the description we were given) it doesn't make sense to think of participants' weights as having changed by their baseline visit - changed since when? As baseline is at the very start of the weight maintenance, it makes sense that we wouldn't have very much (if any) participant variation in change at this point. 
Note that by removing the estimation of this parameter, our models now converge!
```{r}
m.base <- lmer(WeightChange ~ 1 + Assessment + (0 + Assessment | ID), data=WeightMaintain3)
m.int <- lmer(WeightChange ~ 1 + Assessment + Condition + (0 + Assessment | ID), data=WeightMaintain3)
m.full <- lmer(WeightChange ~ 1 + Assessment * Condition + (0 + Assessment | ID), data=WeightMaintain3)
```

```{r}
isSingular(m.base)
```


`r solend()`




# Random effects

:::statbox

When specifying a random effects model, think about the data you have and how they fit in the following table:

| Criterion: | Repetition: <br> _If the experiment were repeated:_ | Desired inference: <br> _The conclusions refer to:_                   |
|----------------|--------------------------------------------------|--------------------------------------------------------------------|
| Fixed effects  | Same levels would be used                        | The levels used                                                    |
| Random effects | Different levels would be used                   | A population from which the levels used are just a (random) sample |

For example, applying the criteria to the following questions:

- Do dogs learn faster with higher rewards? 

  FIXED: reward
  
  RANDOM: dog

- Do students read faster at higher temperatures?

  FIXED: temperature
  
  RANDOM: student

- Does people speaking one language speak faster than another?

  FIXED: the language
  
  RANDOM: the people speaking that language

<br>

Sometimes, after simplifying the model, you find that there isn't much variability in a specific random effect and, if it still leads to singular fits or convergence warnings, it is common to just model that variable as a fixed effect. 

Other times, you don't have sufficient data or levels to estimate the random effect variance, and you are forced to model it as a fixed effect.
This is similar to trying to find the "best-fit" line passing through a single point... You can't because you need two points!

:::


## Random effects in lme4  
:::rtip

Below are a selection of different formulas for specifying different random effect structures, taken from the [lme4 vignette](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf). This might look like a lot, but over time and repeated use of multilevel models you will get used to reading these in a similar way to getting used to reading the formula structure of `y ~ x1 + x2` in all our linear models. 
<br>

|  Formula|  Alternative|  Meaning|
|--------:|------------:|--------:|
|  $\text{(1 | g)}$|  $\text{1 + (1 | g)}$|  Random intercept with fixed mean|
|  $\text{0 + offset(o) + (1 | g)}$|  $\text{-1 + offset(o) + (1 | g)}$|  Random intercept with *a priori* means|
|  $\text{(1 | g1/g2)}$|  $\text{(1 | g1) + (1 | g1:g2)}$|  Intercept varying among $g1$ and $g2$ within $g1$|
|  $\text{(1 | g1) + (1 | g2)}$|  $\text{1 + (1 | g1) + (1 | g2)}$|  Intercept varying among $g1$ and $g2$|
|  $\text{x + (x | g)}$|  $\text{1 + x + (1 + x | g)}$|  Correlated random intercept and slope|
|  $\text{x + (x || g)}$|  $\text{1 + x + (x | g) + (0 + x | g)}$|  Uncorrelated random intercept and slope|
**Table 1:** Examples of the right-hand-sides of mixed effects model formulas. $g$, $g1$, $g2$ are grouping factors, covariates and *a priori* known offsets are $x$ and $o$.

:::

## Three-level nesting

:::frame
__Data: Treatment Effects__
  
Synthetic data from a RCT treatment study: 5 therapists randomly assigned participants to control or treatment group and monitored the participants' performance over time. There was a baseline test, then 6 weeks of treatment, with test sessions every week (7 total sessions).

The following code will load in your R session an object already called `tx` with the data:  

```{r}
load(url("https://uoepsy.github.io/msmr/data/tx.Rdata"))
```

You can see the head of the data below: 
```{r, echo=FALSE}
head(tx) %>%
  kable(digits = 2, align = 'c') %>%
  kable_styling(full_width = FALSE)
```

:::

`r qbegin("D1")`
Load and visualise the data. Does it look like the treatment had an effect on the performance score?
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r message=FALSE}
ggplot(tx, aes(session, Score, color=group)) +
  stat_summary(fun.data = mean_se, geom="pointrange") +
  stat_smooth() +
  theme_classic()
```

Just for fun, let's add on the individual participant scores, and also make a plot for each therapist. 
```{r message=FALSE}
ggplot(tx, aes(session, Score, color=group)) +
  stat_summary(fun = mean_se, geom = "pointrange") +
  stat_smooth() +
  theme_classic() +
  geom_line(aes(group = PID), alpha = .2) + 
  facet_wrap(~ therapist)
```
`r solend()` 



`r qbegin("D2")`
Consider these questions when you're designing your model(s) and use your answers to motivate your model design and interpretation of results:  

a. What are the levels of nesting? How should that be reflected in the random effect structure?
a. What is the shape of change over time? Do you need polynomials to model this shape? If yes, what order polynomials?
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
a. There are repeated measures of participants (session). There are also repeated measures of therapists (each one treated many participants).
a. Looks like linear change, don't need polynomials. Good to know that there is no difference at baseline, so no need for orthogonal time.
`r solend()` 



`r qbegin("D3")`
Test whether the treatment had an effect using mixed-effects modelling.  

Try to fit the **maximal** model.  
Does it converge? Is it singular?

`r optbegin("Hint: What *is* the maximal model?", olabel=FALSE, toggle=params$TOGGLE)`

There's an interaction between session and group. Each participant has several sessions, so we want `(1 + session | PID)` and each therapist has several groups and sessions, so we want `(1 + ? * ? | ?)`

`r optend()`

`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
library(lme4)

# start with maximal model
m1 <- lmer(Score ~ session * group + 
             (1 + session | PID) + 
             (1 + session * group | therapist),
           data=tx)

isSingular(m1)
```

`r solend()` 



`r qbegin("D4")`
Try adjusting your model by removing random effects or correlations, examine the model again, and so on..  
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
VarCorr(m1)
```
There's a correlation of exactly -1 between the random intercepts and slopes for therapists, and the standard deviation estimate for `session|therapist` is pretty small. Let's remove it. 
```{r}
m2 <- lmer(Score ~ session * group + 
             (1 + session | PID) + 
             (1 | therapist),
           data=tx)
VarCorr(m2)
```
It now looks like estimates for random intercepts for therapists is now 0. If we remove this, our model finally is non-singular:
```{r}
m3 <- lmer(Score ~ session * group + 
             (1 + session | PID),
           data=tx)
summary(m3)
```
Lastly, it's then a good idea to check that the parameter estimates and SE are not radically different across these models (they are virtually identical)
```{r}
summary(m1)$coefficients
summary(m2)$coefficients
summary(m3)$coefficients
```
`r solend()` 



`r qbegin('D5: Optional')`

Try the code below to use the `allFit()` function to fit your final model with all the available optimizers.^[If you have an older version of `lme4`, then `allFit()` might not be directly available, and you will need to run the following: `source(system.file("utils", "allFit.R", package="lme4"))`.]  
  
+ You might need to install the `dfoptim` package to get one of the optimizers  


```{r eval=FALSE}
sumfits <- allFit(yourmodel)
summary(sumfits)
```
`r qend()` 





## Crossed random effects

:::frame
__Data: Test-enhanced learning__  

An experiment was run to conceptually replicate "test-enhanced learning" (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (`StudyStudy`), the other group studied the material once then did a test (`StudyTest`). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (`Test_word`). One of the researchers' questions concerned how test-enhanced learning influences time-to-recall. 

The critical (replication) prediction is that the `StudyStudy` group should perform somewhat better on the immediate recall test, but the `StudyTest` group will retain the material better and thus perform better on the 1-week follow-up test.

The following code loads the data into your R environment by creating a variable called `tel`:
```{r eval=F}
load(url("https://uoepsy.github.io/data/testenhancedlearning.RData"))
```
```{r include=F} 
# load(url("https://uoepsy.github.io/msmr/data/TestEnhancedLearning.RData"))
# tel$Rtime <- 750-((predict(m3) + rnorm(nrow(tel),0,10))*30)
# hist(tel$Rtime)
# tel %>% mutate(
# Rtime = ifelse(Group=="StudyStudy", Rtime+25,Rtime)
# ) -> tel
# save(tel,file="../../data/testenhancedlearning.RData")
load("../../data/testenhancedlearning.RData")
```




The head of the dataset can be seen below:
```{r echo=FALSE}
head(tel) %>%
  kable(digits = 2, align = 'c') %>%
  kable_styling(full_width = FALSE)
```

:::


`r qbegin("E1")`
Load and plot the data. Does it look like the effect was replicated?
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
You can make use of `stat_summary()` again!
```{r, eval=FALSE}
ggplot(tel, aes(Delay, Rtime, col=Group)) + 
  stat_summary(fun.data=mean_se, geom="pointrange")+
  theme_light()
```
It's more work, but some people might rather calculate the numbers and then plot them directly. It does just the same thing: 
```{r}
tel %>% 
  group_by(Delay, Group) %>%
  summarise(
    mean = mean(Rtime),
    se = sd(Rtime)/sqrt(n())
  ) %>%
  ggplot(., aes(x=Delay, col = Group)) +
  geom_pointrange(aes(y=mean, ymin=mean-se, ymax=mean+se))+
  theme_light() +
  labs(y = "Response Time (ms)")
```
  
That looks like test-enhanced learning to me!  
`r solend()` 



`r qbegin("E2")`
Test the critical hypothesis using a mixed-effects model. Fit the maximal random effect structure supported by the experimental design.  

Some questions to consider:  
  
+ Item accuracy is a binary variable. What kind of model will you use?  
+ We can expect variability across subjects (some people are better at learning than others) and across items (some of the recall items are harder than others). How should this be represented in the random effects?

+ If a model takes ages to fit, you might want to cancel it by pressing the escape key. It is normal for complex models to take time, but for the purposes of this task, give up after a couple of minutes, and try simplifying your model.  

`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
This one will probably take a little bit of time:
```{r}
m <- lmer(Rtime ~ Delay*Group +
             (1 + Delay | Subject_ID) +
             (1 + Delay * Group | Test_word),
           data=tel, control=lmerControl(optimizer = "bobyqa"))
```
`r solend()` 

`r qbegin("E3")`
The model with maximal random effects will probably not converge, or will obtain a singular fit. Simplify the model until you achieve convergence.  
<br>
What we're aiming to do here is to follow [Barr et al.'s](https://doi.org/10.1016/j.jml.2012.11.001) advice of defining our maximal model and then removing only the terms to allow a non-singular fit.  
<br>
**Note:** This strategy - starting with the maximal random effects structure and removing terms until obtaining model convergence, is just *one* approach, and there are drawbacks (see [Matuschek et al., 2017](https://doi.org/10.1016/j.jml.2017.01.001)). There is no consensus on what approach is best (see `?isSingular`).  
<br>
<br>
*Tip:* you can look at the variance estimates and correlations easily by using the `VarCorr()` function. What jumps out?  
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
There's a correlation of .999 for some of the random effects by-item!
```{r}
VarCorr(m)
```

lets remove the interaction in the by-word random effects:
```{r}
m1 <- lmer(Rtime ~ Delay*Group +
             (1 + Delay | Subject_ID) +
             (1 + Delay + Group | Test_word),
           data=tel, control=lmerControl(optimizer = "bobyqa"))
VarCorr(m1)
isSingular(m1)
```
We still have a singular fit here, and the correlation is just slightly different (0.998). Thinking about the study, if we are going to remove __one__ of the by-testword random effects (`Delay` or `Group`), which one do we consider to be more theoretically justified? Is the effect of Delay likely to vary by test-words? More so than the effect of group is likely to vary by test-words? Quite possibly - there's no obvious reason for _certain_ words to be more memorable for people in one group vs another. But there is reason for words to vary in the effect that delay of one week has - how familiar a word is will likely influence the amount to which a week's delay has on recall.   

Let's remove the by-testword random effect of group. 
```{r}
m2 <- lmer(Rtime ~ Delay*Group +
             (1 + Delay | Subject_ID) +
             (1 + Delay | Test_word),
           data=tel, control=lmerControl(optimizer = "bobyqa"))
isSingular(m2)
VarCorr(m2)
```
Hooray, the model converged! 
```{r}
summary(m2)
```
`r solend()` 

`r qbegin("E4")`
Load the **effects** package, and try running this code:
```{r}
library(effects)
ef <- as.data.frame(effect("Delay:Group", m2))
```

What is `ef`? and how can you use it to plot the model-estimated condition means and variability?

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
ggplot(ef, aes(Delay, fit, color=Group)) + 
  geom_pointrange(aes(ymax=upper, ymin=lower), position=position_dodge(width = 0.2))+
  theme_classic() # just for a change :)
```

`r solend()`

`r qbegin("E5")`
Can we get a similar plot using `plot_model()` from the __sjPlot__ package? 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
library(sjPlot)
plot_model(m2, type="int")
```

`r solend()`
 

`r qbegin("E6")`
What should we do with this information? How can we apply test-enhanced learning to learning R and statistics?
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
You'll get the benefits of test-enhanced learning if you try yourself before looking at the solutions! If you don't test yourself, you're more likely to forget it in the long run. 
`r solend()` 


<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

# Less-Guided Exercises

:::frame
__Data: Naming__ 

72 children from 10 schools were administered the full Boston Naming Test (BNT-60) on a yearly basis for 5 years to examine development of word retrieval. Five of the schools taught lessons in a bilingual setting with English as one of the languages, and the remaining five schools taught in monolingual English.  

The data is available at [https://uoepsy.github.io/data/bntmono.csv](https://uoepsy.github.io/data/bntmono.csv).  

```{r echo=FALSE}
bnt <- read_csv("../../data/bntmono.csv")
tibble(variable = names(bnt),
       description = c("unique child identifier","unique school identifier","score on the Boston Naming Test-60. Scores range from 0 to 60","Year of school","Mono/Bi-lingual School. 0 = Bilingual, 1 = Monolingual")
) %>% pander::pander()
```

:::

`r qbegin("E7")`
Fit a model examining the interaction between the effects of school year and mono/bilingual teaching on word retrieval, with random intercepts only for children and schools.  
**tip:** make sure your variables are of the right type first - e.g. numeric, factor etc  
<br>
Examine the fit and consider your model assumptions, and assess what might be done to improve the model in order to make better statistical inferences. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
bnt <- bnt %>% mutate(across(c(mlhome, school_id, child_id), factor))
bntm0 <- lmer(BNT60 ~ schoolyear * mlhome + (1 | school_id/child_id), data = bnt)
```

Residuals don't look zero mean:
```{r}
plot(bntm0, type=c("p","smooth"))
```

It looks a little like, compared to our model (black lines below) the children's scores (coloured lines) are more closely clustered together when they start school, and then they are more spread out by the end of the study. 
The fact that we're fitting the same slope for each child is restricting us here, so we should try fitting random effects of schoolyear. 
```{r}
augment(bntm0) %>%
  ggplot(aes(x=schoolyear, col=child_id)) + 
  geom_point(aes(y = BNT60))+
  geom_path(aes(y = BNT60))+
  geom_path(aes(y = .fitted), col="black", alpha=.3)+
  guides(col="none")+
  facet_wrap(~school_id)
```


```{r}
bntm1 <- lmer(BNT60 ~ schoolyear * mlhome + (1 + schoolyear | school_id/child_id), data = bnt)
plot(bntm1, type=c("p","smooth"))
```
Much better!  

Let's do some quick diagnostic checks for influence:
```{r}
inf1 <- hlm_influence(bntm1, level=1)
dotplot_diag(inf1$cooksd, cutoff = "internal")
```

If you check in the help for `dotplot_diag()`, it tells you that

a) we can add an index for the labels, and 
b) the coordinates (x,y) are flipped. We're telling R to change the limits of the y axis, but actually it is the x axis. This is just because we want to see the label for that point out to the right. 

```{r}
infchild <- hlm_influence(bntm1, level="child_id:school_id")
dotplot_diag(infchild$cooksd, cutoff = "internal", index = infchild$`child_id:school_id`) + 
  scale_y_continuous(limits=c(0,.05))
```
And then we can examine the effects to the fixed effects and our standard errors when we remove this child:
```{r}
del94 <- case_delete(bntm1, level="child_id:school_id", delete = "ID94:SC9")
cbind(del94$fixef.original, del94$fixef.delete)
```


`r optbegin("Optional: Case deletion influence on standard errors", olabel=F)`

We can examine the influence that deleting a case has on the standard errors.
The standard errors are the square-root of the diagonal of the model-implied variance-covariance matrix:

```{r}
cbind( 
  sqrt(diag(del94$vcov.original)),
  sqrt(diag(del94$vcov.delete))
)
```
`r optend()`

```{r}
infschool <- hlm_influence(bntm1, level="school_id")
dotplot_diag(infschool$cooksd, cutoff = "internal", index = infschool$school_id)
```
`r solend()`

`r qbegin("E8")`
Using a method of your choosing, conduct inferences from your model and write up the results. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
This took quite a while to run: 
```{r eval=F}
library(lmeresampler)
bntm1BS <- bootstrap(bntm1, .f=fixef, type = "case", B = 2000, resample = c(FALSE,TRUE,FALSE))
confint(bntm1BS, type = "basic")
```
```{r echo=FALSE}
library(lmeresampler)
load("data/bntbs.rdata")
confint(bntm1BS, type = "basic")
```


TODO

`r solend()`

