---
title: "Path Analysis"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(lavaan)
library(semPlot)
# knitr::opts_chunk$set(cache = TRUE)
options(digits=3, scipen = 3)
```

:::green
__Information about solutions__

Solutions for these exercises are available immediately below each question.  
We would like to emphasise that much evidence suggests that testing enhances learning, and we __strongly__ encourage you to make a concerted attempt at answering each question *before* looking at the solutions. Immediately looking at the solutions and then copying the code into your work will lead to poorer learning.  
We would also like to note that there are always many different ways to achieve the same thing in R, and the solutions provided are simply _one_ approach.  

:::

:::lo
**Relevant packages**  

+ lavaan  
+ semPlot or tidySEM

::: 

# Motivation

By now, we are getting more comfortable with the regression world, and we can see how it is extended to lots of different types of outcome and data structures. So far in DAPR3 it's been all about the multiple levels. This has brought so many more potential study designs that we can now consider modelling - pretty much any study where we are interested in explaining some outcome variable, and where we have sampled clusters of observations (or clusters of clusters of clusters of ... etc.).  

But we are still restricted to thinking, similar to how we thought in DAPR2, about one single outcome variable. In fact, if we think about the structure of the fixed effects part of a model (i.e., the bit we're _specifically interested in_), then we're still limited to thinking of the world in terms of "this is my outcome variable, everything else predicts it". 

:::statbox
__Regression as a path diagram__    

1. Imagine writing the names of all your variables on a whiteboard
2. Specify which one is your dependent (or "outcome" or "response") variable. 
3. Sit back and relax, you're done!

In terms of a _theoretical_ model of the world, there's not really much to it. We have few choices in the model we construct beyond specifying which is our outcome variable.  
We can visualise our multiple regression model like this: 
```{r mregfig, out.width="250px", echo=FALSE,fig.cap="In multiple regression, we decide which variable is our outcome variable, and then everything else is done for us"}
knitr::include_graphics("images/path/mregpath.png")
```

Of course, there are a few other things that are included (an intercept term, the residual error, and the fact that our predictors can be correlated with one another), but the idea remains pretty much the same:

```{r mregfig2, out.width="250px", echo=FALSE,fig.cap="Multiple regression with intercept, error, predictor covariances"}
knitr::include_graphics("images/path/mregpath2.png")
```
:::
<br>

## A model reflects a theory

What if I my theoretical model of the world doesn't fit this structure?  

Let's suppose I have 5 variables: Age, Parental Income, Income, Autonomy, and Job Satisfaction. I draw them up on my whiteboard:
```{r path1nopath, out.width="350px", echo=FALSE,fig.cap="My variables"}
knitr::include_graphics("images/path/paths1nopaths.png")
```

My theoretical understanding of how these things fit together leads me to link my variables to end up with something like that in Figure \@ref(fig:path1path). 
```{r path1path, out.width="350px", echo=FALSE,fig.cap="My theory about my system of variables"}
knitr::include_graphics("images/path/paths1paths.png")
```
In this diagram, a persons income is influenced by their age, their parental income, and their level of autonomy, and in turn their income predicts their job satisfaction. Job satisfaction is also predicted by a persons age directly, and by their level of autonomy, which is also predicted by age. It's complicated to look at, but in isolation each bit of this makes theoretical sense. 

Take each arrow in turn and think about what it represents:
```{r path1pathdesc, out.width="550px", echo=FALSE}
knitr::include_graphics("images/path/paths1desc.png")
```

If we think about trying to fit this "model" with the tools that we have, then we might end up wanting to fit three separate regression models, which between them specify all the different arrows in the diagram:

$$
\begin{align}
\textrm{Job Satisfaction} & = \beta_0 + \beta_1(\textrm{Age}) + \beta_2(\textrm{Autonomy}) + \beta_3(\textrm{Income}) + \varepsilon \\
\textrm{Income} & = \beta_0 + \beta_1(\textrm{Age}) + \beta_2(\textrm{Autonomy}) + \beta_2(\textrm{Parental Income}) \varepsilon \\
\textrm{Autonomy} & = \beta_0 + \beta_1(\textrm{Age}) + \varepsilon \\
\end{align}
$$

This is all well and good, but what if I want to talk about how well my entire model (Figure \@ref(fig:path1path)) fits the data we observed?  

# Introducing Path Analysis

:::green

**Mountains cannot be surmounted except by winding paths**

:::

The starting point for Path Analysis is to think about our theories in terms of the connections between variables drawn on a whiteboard. By representing a theory as paths to and from different variables, we open up a whole new way of 'modelling' the world around us.  

There are a few conventions to help us understand this sort of diagrammatical way of thinking. By using combinations of rectangles, ovals, single- and double-headed arrows, we can draw all sorts of model structures. In Path Diagrams, we use specific shapes and arrows to represent different things in our model:

:::statbox
__Shapes and Arrows in Path Diagrams__  

- **Observed variables** are represented by squares or rectangles. These are the named variables of interest which exist in our dataset - i.e. the ones which we have measured directly. 
- **Variances/Covariances** are represented by double-headed arrows. In many diagrams these are curved. 
- **Regressions** are shown by single headed arrows (e.g., an arrow from $x$ to $y$ for the path $y~x$).  
<li style="opacity:.3">**Latent variables** are represented by ovals, and we will return to these in a few weeks time!</li>

```{r echo=FALSE, out.width="650px"}
knitr::include_graphics("images/path/semplots.png")
```

:::

:::lo
**Terminology refresher**  

- **Exogenous variables** are a bit like what we have been describing with words like "independent variable" or "predictor". In a path diagram, they have no paths coming from other variables in the system, but have paths *going to* other variables.  
- **Endogenous variables** are more like the "outcome"/"dependent"/"response" variables we are used to. They have some path coming from another variable in the system (and may also - but not necessarily - have paths going out from them).  

:::


## How does it work (in brief)? 

The logic behind path analysis is to estimate a system of equations that can repdroduce the covariance structure that we see in the data. 

Suppose we think that 



we collect data on these variables and observed the following correlation matrix:

we can evaluate how well our theoretical model (the system of equations) can reproduce the correlation matrix

this allows us to then compare this theoretical model with one which does NOT have a path from ... to .. 

`r optbegin("Optional: How does it work (not as brief)?", olabel=F,toggle=params$TOGGLE)`

__Path Diagram Tracing__

For Path Diagrams that meet a certain set of pre-requisites, we can use a cool technique called Path Tracing to estimate the different paths (i.e., the coefficients) from just the covariance matrix of the dataset. 
For us to be able to do this, a Path Diagram must meet these criteria:

- All our exogenous variables are correlated (unless we specifically assume that their correlation is zero)
- All models are 'recursive' (no two-way causal relations, no feedback loops)
- Residuals are uncorrelated with exogenous variables
- Endogenous variables are not connected by correlations (we would use correlations between residuals here, because the residuals are not endogenous)
- All 'causal' relations are linear and additive
- 'causes' are unitary (if A -> B and A -> C, then it is presumed that this is the same aspect of A resulting in a change in both B and C, and not two distinct aspects of A, which would be better represented by two correlated variables A1 and A2). 

:::statbox
**Causal??**

It is a slippery slope to start using the word 'cause', and personally I am not that comfortable using it here. However, you will likely hear it a lot in resources about path analysis and SEM, so it is best to be warned.  

Please keep in mind that we are using a very broad definition of 'causal', simply to reflect the one way nature of the relationship we are modeling. In Figure \@ref(fig:causes), a change in the variable **X1** is associated with a change in **Y**, but not vice versa. 

```{r causes, echo=FALSE, fig.cap="Paths are still just regressions."}
knitr::include_graphics("images/path/causes.png")
```
:::

__Tracing Rules__  

Thanks to [Sewal Wright](https://www.jstor.org/stable/pdf/2527551.pdf?casa_token=3QF0ad2ZoBcAAAAA:MbEkDNNdoLZr1SXE4LrnK--qrhhsTXLgsRtcWre1UvWxiQiGNUl5vWytGp34XIxhAYMZJe-MbIcBnEwXSfX6MAONevz04-sMXpEDI3IaYKk6mMX46QvX), we can express the correlation between any two variables in the system as the sum of all *compound paths* between the two variables. 

*compound paths* are any paths you can trace between A and B for which there are: 

- no loops
- no going forward then backward
- maximum of one curved arrow per path

:::frame
__An example__  

Let's consider the example below, for which the paths are all labelled with lower case letters $a, b, c, \text{and } d$. 

```{r patheq1, echo=FALSE, fig.cap="A multiple regression model as a path diagram"}
knitr::include_graphics("images/path/patheq1.png")
```

According to Wright's tracing rules above, write out the equations corresponding to the 3 correlations between our observed variables (remember that $r_{a,b} = r_{b,a}$, so it doesn't matter at which variable we start the paths). 

- $r_{x1,x2} =  c$  
- $r_{x1,y} = a + bc$  
- $r_{x2,y} =  b + ac$  

Now let's suppose we observed the following correlation matrix:
```{r}
egdat <- read_csv("https://uoepsy.github.io/data/patheg.csv")
egdat <- read_csv("../../data/patheg.csv")
round(cor(egdat),2)
```
We can plug these into our system of equations:

- $r_{x1,x2} = c = 0.36$  
- $r_{x1,y} = a + bc = 0.75$   
- $r_{x2,y} = b + ac = 0.60$

And with some substituting and rearranging, we can work out the values of $a$, $b$ and $c$. 

`r optbegin("Click for answers", olabel=FALSE, toggle=params$TOGGLE)`
This is what I get:

a = 0.61  
b = 0.38  
c = 0.36  

`r optend()`

We can even work out what the path labeled $d$ (the residual variance) is.  
First we sum up all the equations for the paths from Y to Y itself.  
These are:

- $a^2$ (from Y to X1 and back)  
- $b^2$ (from Y to X2 and back)  
- $acb$ (from Y to X1 to X2 to Y)
- $bca$ (from Y to X2 to X1 to Y)
  
Summing them all up and solving gives us:  
$$
\begin{align}
 r_{y \cdot x1, x2} & = a^2 + b^2 + acb + bca\\
 & = 0.61^2 + 0.38^2 + 2 \times(0.61 \times 0.38 \times 0.36)\\
 & = 0.68 \\
\end{align}
$$
We can think of this as the portion of the correlation of Y with itself that occurs *via the predictors*. Put another way, this is the amount of variance in Y explained jointly by X1 and X2, which sounds an awful lot like an $R^2$!  
The path labelled $d$ is simply all that is left in Y after taking out the variance explained by X1 and X2, meaning that the path $d = \sqrt{1-R^2}$ (i.e., the residual variance!).  

Hooray! We've just worked out regression coefficients when all we had was the correlation matrix of the variables! It's important to note that we have been using the correlation matrix, so, somewhat unsurprisingly, our estimates are *standardised* coefficients. 

Because we have the data itself, let's quickly find them with `lm()`
```{r}
# quickly scale all the columns in the data
egdat <- egdat %>% mutate_all(~scale(.)[,1])
# extract the coefs
coef(lm(y~x1+x2, egdat)) %>% round(.,2)
# extract the r^2
summary(lm(y~x1+x2, egdat))$r.squared
```
:::

`r optend()`

# Doing Path Analysis

:::rtip
__Introducing `lavaan`__  

For the remaining weeks of the course, we're going to rely heavily on the **lavaan** (**La**tent **Va**riable **An**alysis) package. This is the main package in R for fitting path diagrams (as well as more cool models like factor analysis sructures and structural equation mdoels). There is a huge scope of what this package can do.  
The first thing to get to grips with is the various new operators which it allows us to use.   

Our old multiple regression formula in R was specified as `y ~ x1 + x2 + x3 + ... `.  
In __lavaan__, we continue to fit regressions using the `~` symbol, but we can also specify the construction of latent variables using `=~` and residual variances & covariances using `~~`.  

|  formula type|  operator|  mnemonic|
|--:|--:|--:|
|  latent variable definition|  `=~`|  "is measured by"|
|  regression|  `~`|  "is regressed on"|
|  (residual) (co)variance |  `~~`|  "is correlated with"|
|  intercept |  `~1`|  "intercept"|

(from https://lavaan.ugent.be/tutorial/syntax1.html) 

In practice, fitting models in __lavaan__ tends to be a little different from things like `lm()` and `(g)lmer()`.  
Instead of including the model formula *inside* the fit function (e.g., `lm(y ~ x1 + x2, data = df)`), we tend to do it in a step-by-step process. 
This is because as our models become more complex, our formulas can pretty long!   

We write the model as a character string (e.g. `model <- "y ~ x1 + x2"`) and then we pass that formula along with the data to the relevant __lavaan__ function, which for our purposes will be the `sem()` function, `sem(model, data = mydata)`.   


:::

`r optbegin("Fitting a multiple regression model with lavaan",olabel=FALSE,toggle=params$TOGGLE)`
You can see a multiple regression fitted with __lavaan__ below.  

```{r message=FALSE, warning=FALSE}
library(lavaan)
scsdat <- read_csv("https://uoepsy.github.io/data/scs_study.csv")

# the lm() way
mreg_lm <- lm(dass ~ zo + zc + ze + za + zn + scs, scsdat)

# setting up the model for SEM
mreg_model <- "
    #regression
    dass ~ zo + zc + ze + za + zn + scs
"
mreg_sem <- sem(mreg_model, data=scsdat)
```

These are the coefficients from our `lm()` model:
```{r}
coefficients(mreg_lm)
```

And you can see the estimated parameters are the same for our `sem()` model!
```{r}
summary(mreg_sem)
```
`r optend()`  

## Specifying the model

```{r include=FALSE}
set.seed(123456)
m = "
jobsat ~ (-0.8)*age + .58*autonomy + .47*income + (-0.39)*x1
income ~ .22*autonomy + .57*age + 0.4*parentincome + .1*x1
autonomy ~ .28*age + 0.8*x1
"
df <- simulateData(m, sample.nobs=50, standardized = T)
df<-df[,c(1:4,6)]
df %>% mutate(
  jobsat = round(50+(jobsat*17)),
  income = round(35+(income*7)),
  autonomy = round(45+(autonomy*18)),
  age = round(50 +(age*7)),
  parentincome = round(45+(parentincome*4)),
) -> df
#write.csv(df, "../../data/jobsatpath.csv",row.names=F)

```

The first part of estimating a path model involves specifying the model. This means basically writing down the paths that are included in your theoretical model. 

Let's start by looking at the example about job satisfaction, income, autonomy and age.  
Recall we had this theoretical model:
```{r path1patha, out.width="350px", echo=FALSE}
knitr::include_graphics("images/path/paths1paths.png")
```
And now let's suppose that we collected data on these variables: 
```{r echo=T, eval=F}
# jobsatpath <- read_csv("https://uoepsy.github.io/data/jobsatpath.csv")
jobsatpath <- read_csv("../../data/jobsatpath.csv")
head(jobsatpath)
```
```{r echo=FALSE}
jobsatpath <- read_csv("../../data/jobsatpath.csv")
knitr::kable(rbind(head(jobsatpath),"..."))
```



Remember we said that we could specify all these paths using three regression models? Well, to specify our path model, we simply write these out like we would do in `lm()`, but this time we do so all in one character string.  
We still have to make sure that we use the correct variable names, as when we make R estimate the model, it will look in the data for things like "jobsat". 
```{r}
mymodel <- "
jobsat ~ age + autonomy + income
income ~ autonomy + age + parentincome
autonomy ~ age
"
```

There are some other things which we will automatically be estimated here: all our exogenous variables (the ones with arrows only going _from_ them) will be free to correlate with one another. We can write this explicitly in the model if we like, using the two tildes `~~` between our two exogenous variables `age` and `parentincome`. We will also get the variances of all our variables.  

We can see all the paths here:
```{r eval=F}
lavaanify(mymodel)
```
```{r echo=FALSE}
lavaanify(mymodel)[,2:4] %>% head(13L)
```
and even make a nice diagram:
```{r}
library(semPlot)
semPaths(lavaanify(mymodel))
```





## Estimating the model

Estimating the model is relatively straightforward. We pass the formula we have written to the `sem()` function, along with the data set in which we want it to look for the variables:
```{r}
mymodel.fit <- sem(mymodel, data = jobsatpath)
```

We can then examine the parameter estimates: 
```{r}
summary(mymodel.fit)
```

We can now, to "visualise" our model, add the estimates to the diagram:
```{r echo=FALSE, fig.cap="Model estimates"}
knitr::include_graphics("images/path/jobsatmod.png")
```

# Model Fit 

You'll have heard the term "model fit" many times since the start of DAPR2, when we began model-based thinking. However, there is a crucial difference in what it means when it is used in for path analysis.  

In things like multiple regression, we have been using "model fit" to be the measure of "how much variance can we explain in y with our set of predictors?". For a path model, examining "model fit" is more like asking "how well does our model reproduce the characteristics of the data that we observed?".  
We can represent the "characteristics of our data" in a covariance matrix, so one way of thinking of "model fit" is as "how well can our model reproduce our observed covariance matrix?".  

```{r}
cov(jobsatpath)
```

In this meaning of the term "model fit", the multiple regression model fits almost perfectly. 
```{r}
scsdat <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
cov(scsdat %>% select(dass,zo,zc,ze,za,zn,scs))

mreg_model <- "
    #regression
    dass ~ zo + zc + ze + za + zn + scs
"
mreg_sem <- sem(mreg_model, data=scsdat)
mreg_sem@implied$cov
```

## Degrees of freedom

When we think of "degrees of freedom" for a multiple regression model, in DAPR2 we learned that $df = n-k-1$ ($n$ is the number of observations, $k$ is the number of predictors). These degrees of freedom related to the corresponding $F$ and $t$-distributions with which we performed our hypothesis tests (e.g. $t$ tests for a null hypothesis that a coefficient is zero, or $F$-tests for a null that the reduction in residual sums of squares is zero).  

But in relation to it's ability to represent a $k \times k$ covariance matrix (i.e. the covariance matrix of our variables), the multiple regression model has **zero** degrees of freedom. 

"Degrees of freedom" in the this framework correspond to the number of *knowns* (observed covariances/variances) minus the number of *unknowns* (parameters to be estimated by the model). A model is only able to be estimated if it has at least 0 degrees of freedom (if there are as many knowns as unknowns). A model with 0 degrees of freedom is termed **just-identified**.  
**under-** and **over-** identified models correspond to those with $<0$ and $>0$ degrees of freedom respectively. 

So the multiple regression model is an example of a *just-identified* model! In multiple regression, everything is allowed to covary with everything else, which means that there is a unique solution for all of the model's parameters because there are *as many paths as there are observed covariances*. This means that in this path analysis world, a multiple regression model is "just-identified". 

`r optbegin("huh?", olabel=FALSE, toggle=params$TOGGLE)`

```{r echo=FALSE}
scsdat <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
scsdat %>% transmute(y=dass,x1=zo,x2=zn) -> somedata
```


If I have two predictors and one outcome variable, then there are 6 variances and covariances available. For instance: 
```{r}
cov(somedata)
```
The multiple regression model will estimate the two variances of the exogenous variables (the predictors), their covariance, the two paths from each exogenous to the endogenous (each predictor to the outcome), and the error variance. This makes up 6 estimated parameters - which is exactly how many known covariances there are.
```{r echo=FALSE, out.width="350px"}
mreg = "y~x1+x2"
library(tidySEM)
graph_sem(mreg_sem, 
          nodes = get_nodes(mreg_sem))
```
`r optend()`

:::statbox
**How many knowns are there?**   

The number of known covariances in a set of $k$ observed variables is equal to $\frac{k \cdot (k+1)}{2}$. 

:::

When learning about SEM, the visualisations can play a key part. It often helps to draw all our variables (both observed and latent) on the whiteboard, and connect them up according to your theoretical model. You can then count the number of paths (arrows) and determine whether the $\text{number of knowns} > \text{number of unknowns}$. We can reduce the number of unknowns by fixing parameters to be specific values. 

:::statbox

By constraining some estimated parameter to be some specific value, we free-up a degree of freedom! For instance "the correlation between x1 and x2 is equal to 0.7 ($r_{x_1x_2} = .07$)". This would turn a previously estimated parameter into a fixed parameter, and this gains us the prize of a lovely degree of freedom!  

**By removing a path altogether, we are constraining it to be zero.**  

:::

There are too many different indices of model fit for these types of models, and there's lots of controversy over the various merits and disadvantages and proposed cutoffs of each method. We will return to this more in coming weeks, but it is an important notion to remember - "model fit" and "degrees of freedom" have quite different meanings to those you are likely used to.  






<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>