---
title: "Exploratory Factor Analysis (EFA): Part 2"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
invisible(pacman::p_unload(pacman::p_loaded(), character.only = TRUE))
source('assets/setup.R')
library(tidyverse)
library(lavaan)
library(semPlot)
# knitr::opts_chunk$set(cache = TRUE)
options(digits=3, scipen=3)
```

:::green
__Information about solutions__

Solutions for these exercises are available immediately below each question.  
We would like to emphasise that much evidence suggests that testing enhances learning, and we __strongly__ encourage you to make a concerted attempt at answering each question *before* looking at the solutions. Immediately looking at the solutions and then copying the code into your work will lead to poorer learning.  
We would also like to note that there are always many different ways to achieve the same thing in R, and the solutions provided are simply _one_ approach.  

:::

:::lo
**Relevant packages**

+ tidyverse
+ psych

::: 

```{r eval=F, echo=FALSE}
library(lavaan)
set.seed(11)

# paste(paste0(abs(round(rnorm(9,0,.3),1)),"*",paste0("PHQ_",1:9), collapse=" + ")," + ",
# paste0(abs(round(rnorm(7,0,1),1)),"*",paste0("GAD_",1:7), collapse=" + "))

m = "
depression =~ 0.5*PHQ_1 + 0.6*PHQ_2 + 0.8*PHQ_3 + 1.2*PHQ_4 + 0.5*PHQ_5 + 1.4*PHQ_6 + 1*PHQ_7 + 0.7*PHQ_8 + 1.8*PHQ_9  +  0.1*GAD_1 + 0.1*GAD_3 + 0.1*GAD_5 + 0.1*GAD_6
anxiety =~ 0.8*PHQ_3 + 0.1*PHQ_4 + 0.1*PHQ_7 + 0.9*GAD_1 + 1.2*GAD_2 + 0.8*GAD_3 + 0.9*GAD_4 + 0.5*GAD_5 + 0.6*GAD_6 + 0.8*GAD_7
anxiety ~~ 0.3*depression
petowner ~ 0.3*anxiety
petowner ~~ 0.3*petowner
"
df <- simulateData(m, sample.nobs = 620)
petowner = 1-(rbinom(620, size=1,prob=plogis(scale(df$petowner)[,1])))
df <- df[,-17]
df$PHQ_7<-rnorm(620)
makelik <- function(x){
    round(pmax(1,pmin(7,(4+(x*1)))))
}
df <- apply(df, MARGIN=2, FUN=makelik)  
print(fa(cor(df),nfactors = 2)$loadings)

phqgad_items = c(
"Little interest or pleasure in doing things?",
"Feeling down, depressed, or hopeless?",
"Trouble falling or staying asleep, or sleeping too much?",
"Feeling tired or having little energy?",
"Poor appetite or overeating?",
"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?",
"Trouble concentrating on things, such as reading the newspaper or watching television?",
"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?",
"A lack of motivation to do anything at all?",
"Feeling nervous, anxious or on edge?",
"Not being able to stop or control worrying?",
"Worrying too much about different things?",
"Trouble relaxing?",
"Being so restless that it is hard to sit still?",
"Becoming easily annoyed or irritable?",
"Feeling afraid as if something awful might happen?")

df <- as_tibble(df)
names(df) <- phqgad_items
df <- janitor::clean_names(df)
df$do_you_own_a_pet <- petowner
df$ppt_id <- paste0("ppt",1:620)
write.csv(df, "../../data/pgpets.csv",row.names=F)

```


# Practical Issues with EFA

:::frame
__Data: pgpets.csv__  

A pet food company has conducted a questionnaire on the internet ($n = 620$) to examine whether owning a pet influences low mood. They asked 16 questions on a Likert scale (1-7, detailed below) followed by a simple Yes/No question concerning whether the repsondent owned a pet.   
There are lots of questions, and the researchers don't really know much about the theory of mood disorders, but they think that they are likely picking up on multiple different types of "low mood". They want to conduct a factor analysis to examine this, and then plan on investigating the group differences (pet owners vs not pet owners) on the factor scores.  

The data is available at https://uoepsy.github.io/data/pgpets.csv  

```{r echo=FALSE}
tibble(QuestionNumber = paste0("item",1:16),`Over the last 2 weeks, how much have you had/have you been...` = 
         c("Little interest or pleasure in doing things?",
"Feeling down, depressed, or hopeless?",
"Trouble falling or staying asleep, or sleeping too much?",
"Feeling tired or having little energy?",
"Poor appetite or overeating?",
"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?",
"Reading the newspaper or watching television?",
"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?",
"A lack of motivation to do anything at all?",
"Feeling nervous, anxious or on edge?",
"Not being able to stop or control worrying?",
"Worrying too much about different things?",
"Trouble relaxing?",
"Being so restless that it is hard to sit still?",
"Becoming easily annoyed or irritable?",
"Feeling afraid as if something awful might happen?")) %>% knitr::kable() %>% kableExtra::kable_styling(full_width = T)

```

:::

`r qbegin(1)`
Read the data into R.  
Create a new object in R that contains a subset the data. It should include all variables except for the participant ID and the variable corresponding to whether or not they have a pet (we're going to come back to these later on).  
In this new object, change the names of the columns to match the question number, rather than the question itself (see the data description above). This will be easier to work with.  

__Hint:__  
Check the output of the code `paste0("item", 1:10)`. Consider this code in combination with another function: `names(data)`. How could you combine the two codes to assign the new names to the current variable names?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
library(tidyverse)
library(psych)
```

```{r}
pgpets <- read_csv("https://uoepsy.github.io/data/pgpets.csv")
```

```{r}
names(pgpets)
```

```{r}
df <- pgpets %>% 
    select(-ppt_id, -do_you_own_a_pet)
```

```{r}
names(df) <- paste0("item", 1:ncol(df))
head(df)
```

`r solend()`

`r qbegin(2)`
Visualise the items (this might be the histograms of all marginal distributions, or a scatterplot matrix, or both). 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
library(psych)
multi.hist(df)
```

The data have 6 variables, so we will create two plots each focusing on 8 at a time:
```{r}
pairs.panels(df[, 1:8])
pairs.panels(df[, 9:16])
```

`r solend()`

`r qbegin(3)`
Compute the correlation matrix for the items, and assess the suitability for factor analysis, using the Bartlett test and the Kaiser-Meyer-Olkin factor adequacy. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
cordf = cor(df)
```


```{r}
cortest.bartlett(cordf, n = nrow(df))
```
:::int
Bartlett's test indicates that the correlation matrix is proportionally different from the identity matrix ($\chi^2 (120) = 3008, p<.001$), suggesting our correlations are significantly different from zero. 
:::

```{r}
KMO(cordf)
```

:::int
The KMO for individual items are mainly "meritorious" or "marvelous". The only item that may need further investigation is item 7, having an "unacceptable" KMO. 
The overall measure of sampling adequacy is 0.90, which suggests we have suitable data to perform a factor analysis
:::

`r solend()`

`r qbegin(4)`
Determine how many factors you will extract. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Using parallel analysis:

```{r}
fa.parallel(df, fa = 'fa')
```

Using Velicer's Minimum Average Partial:

```{r}
VSS(df)
```
:::int
Both parallel analysis and Velicerâ€™s Minimum Average Partial suggested retaining 2 factors.
:::
`r solend()`

`r qbegin(5)`
Choosing an appropriate estimation method and rotation, perform a factor analysis to extract the desired number of factors (based on your answer to the previous question).  

If you get an error, you may need to install the "GPArotation" package: `install.packages("GPArotation")`.
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We probably shouldn't use MLE as an estimation method here, because we're going to be treating what looks like some sort of Likert data (ordinal, responses of 1-7) as if it is continuous.  
Let's use `minres` here.  

Finally, do we have any reason to think that the factors we are going to extract are orthogonal? Given what the questions look to be evaluating, it's hard to see distinctly unrelated constructs within the 16 items. 

```{r}
pgmod <- fa(df, nfactors = 2, rotate = "oblimin", fm = "minres")
pgmod
```

`r solend()`


:::statbox
**DETAILS**

\newcommand{\item}[1]{ \text{Item}_{#1} }
\newcommand{\fact}[1]{ \text{Factor}_{#1} }
\newcommand{\e}[1]{ \epsilon_{#1} }
\newcommand{\LL}[2]{ {\lambda_{#1,#2}} }

If we think about a factor analysis being a set of regressions (convention in factor analysis is to use $\LL{}{}$ instead of $\beta$), then we can think of a given item being the manifestation of some latent factors, plus a bit of randomness:

\begin{aligned}
\item{1}  &= \LL{1}{1} \cdot \fact{1} + \LL{2}{1} \cdot \fact{2} + \e{1} \\
\item{2}  &= \LL{1}{2} \cdot \fact{1} + \LL{2}{2} \cdot \fact{2} + \e{2} \\
&\vdots \\
\item{16} &= \LL{1}{16} \cdot \fact{1} + \LL{2}{16} \cdot \fact{2} + \e{16}
\end{aligned}

As you can see from the above, the 16 different items all stem from the same two factors ($\fact{1}, \fact{2}$), plus some item-specific errors ($\e{1}, \dots, \e{16}$). The $\LL{}{}$ terms are called factor loadings, or loadings in short

__Communality__ is sum of the squared factor loadings for each item.  

Intuitively, for each row, the two $\LL{}{}$s tell us how much each item depends on the two factors shared by the 16 items. The sum of the squared loadings tells us how much of one item's information is due to the shared factors.

The communality is a bit like the $R^2$ (the proportion of variance of an item that is explained by the factor structure).  

The __uniqueness__ of each item is simply $1 - \text{communality}$.  
This is the leftover bit; the variance in each item that is left unexplained by the latent factors.  

_Side note: this is what sets Factor Analysis apart from PCA, which is the linear combination of total variance (including error) in all our items. FA allows some of the variance to be shared by the underlying factors, and considers the remainder to be unique to the individual items (or, in another, error in how each item measures the construct)._

The __complexity__ of an item corresponds to how well an item reflects a _single_ underlying construct. Specifically, it is ${(\sum \lambda_i^2)^2}/{\sum \lambda_i^4}$, where $\lambda_i$ is the loading on to the $i^{th}$ factor. It will be equal to 1 for an item which loads _only_ on one factor, and 2 if it loads evenly on to two factors, and so on. 

:::rtip
In R, we will often see these estimats under specific columns:  

+ __h2__ = item communality  
+ __u__  = item uniqueness  
+ __com__ = item complexity

:::

:::

`r qbegin(6)`
Using `fa.sort()`, examine the loadings of each item onto the factors, along with communalities, uniqueness and complexity scores.  


- Do all factors load on 3+ items at a salient level?
- Do all items have at least one loading above a salient cut off?
- Are there any Heywood cases (communalities $\geq1$)? 
- Should we perhaps remove some complex items? 
- Does the solution account for an acceptable level of variance?  
- Is the factor structure (items that load on to each factor) coherent, and does it make theoretical sense?  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
fa.sort(pgmod)
```

- Both factors load on 3+ items at a salient level. 
- All except item7 has loadings $>0.3$. 
- There are no Heywood cases.
- Item 3 loads quite highly on both factors (high complexity) 
- The solution explains 38% of the variance.  
- The coherence of the factor structure will require us to look back at the questions themselves. We'll not do this right now.  

`r solend()`


`r qbegin(7)`
If you think any items should be removed, do so now, and perform the factor analysis once more. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We're going to remove that problematic item 7 (which seems to be about "reading the newspaper and watching tv"), as well as item 3 (which seems to be about sleepiness, and relates to both factors quite highly).  
```{r}
df2 <- df %>% 
    select(-item7,-item3)
cortest.bartlett(cor(df2), n = nrow(df2))
KMO(cor(df2))
fa.parallel(df2, fa = 'fa')
VSS(df2)
pgmod2 <- fa(df2, nfactors = 2, rotate = "oblimin", fm = "minres")
pgmod2$loadings
fa.sort(pgmod2)
```

`r solend()`

# Reliability 

<!-- parallel tests -->

<!-- test-retest reliability   -->
<!-- split-half reliability   -->
<!-- cronbach's alpha   -->
<!-- mcdonald's omega   -->
<!-- inter-rater reliability   -->
<!-- intra-class correlations!    -->
<!-- - link back to MLM   -->

<!-- attenuation for measurement error -->

:::statbox
__Measurement Error & Reliability__  

You will often find research that foregoes the measurement model by taking a scale score (i.e., the sum or mean of a set of likert-type questions). For instance, think back to our exercises on path mediation, where "Health Locus of Control" was measured as the "average score on a set of items relating to perceived control over ones own health". In doing so, we make the assumption that these variables provide measurements of the underlying latent construct **without error**.  
In fact, if we think about it a little, in our simple regression framework $y = \beta_0 + \beta_1x_1 + ... + \beta_kx_k + \varepsilon$ all our predictor variables $x_1$ to $x_k$ are assumed to be measured without error - it is only our outcome $y$ that our model considers to have some randomness included (the $\varepsilon$). This seems less problematic if our variables are representing something that is quite easily and reliably measured (time, weight, height, age, etc.), but it seems inappropriate when we are concerned with something more abstract. For instance, two people both scoring 11 on the Generalised Anxiety Disorder 7 (GAD-7) scale does not necessarily mean that they have _identical_ levels of anxiety.  
  
The inconsistency with which an observed variable reflects the underlying construct that we consider it to be measuring is termed the *reliability*.  

:::frame
**Reliability: A silly example** 

Suppose I'm trying to weigh [my dog](https://photos.app.goo.gl/f26FrRDyJxTvXvGS9). I have a set of scales, and I put him on the scales. He weighs in at 13.53kg. 
I immediately do it again, and the scales this time say 13.41kg. I do it again. 13.51kg, and again, 13.60kg. 
What is happening? Is Dougal's weight (Dougal is the dog, by the way) randomly fluctuating by 100g? Or are my scales just a bit inconsistent, and my observations contain measurement error?  

```{r echo=FALSE}
tibble(
    measurement =1:4,
    dougal = c(13.53,13.41,13.51,13.6)
) %>% 
    ggplot(.,aes(x=measurement,y=dougal))+
    geom_point()+
    geom_hline(yintercept=13.483,lty="dashed")+
    geom_text(x=3,y=13.484,label="Dougal's true weight (unknown to us)",
              vjust=-.5)+
    labs(y="Dougal's Weight")
```

I take him to the vets, where they have a much better set of weighing scales, and I do the same thing (measure him 4 times). The weights are 13.47, 13.49, 13.48, 13.48.  
The scales at the vets are clearly *more __reliable__*. We still don't know Dougal's *true* weight, but we are better informed to estimate it if we go on the measurements from the scales at the vet.^[Of course this all assuming that the scales aren't completely miscalibrated] 

```{r echo=FALSE}
tibble(
    scales = rep(c("mine","vets"),each=4),
    measurement = c(1:4,1:4),
    dougal = c(13.53,13.41,13.51,13.6, 13.47, 13.49, 13.48, 13.48)
) %>% 
    ggplot(.,aes(x=measurement,y=dougal, col=scales))+
    geom_point()+
    geom_path()+
    geom_hline(yintercept=13.483,lty="dashed")+
    geom_text(inherit.aes=F, x=3,y=13.484,label="Dougal's true weight (unknown to us)",vjust=-.5)+
    labs(y="Dougal's Weight")
```
:::

Another way to think about reliability is to consider the idea that more error means less reliability: 
$$\text{observations = truth + error}$$  

:::


There are different types of reliability:  

- **test re-test reliability:** correlation between values over repeated measurements. 
- **alternate-form reliability:** correlation between scores on different forms/versions of a test (we might want different versions to avoid practice effects). 
- **Inter-rater reliability:** correlation between values obtained from different raters. 
- **split-half reliability:** correlation between scores of two equally sized subsets of items. 

The form of reliability we are going to be most concerned with here is known as **Internal Consistency**. This is the extent to which items within a scale are correlated with one another. There are two main measures of this:

:::yellow
#### alpha and omega

**Cronbach's $\alpha$** ranges from 0 to 1 (higher is better). You can get this using the `alpha()` function from the **psych** package. The formula is:

$$
\begin{aligned}
\text{Cronbach's } \alpha = \frac{n \cdot \overline{cov(ij)}}{\overline{\sigma^2_i} + (n-1) \cdot \overline{cov(ij)}}  \\
\\
\text{where}:  \qquad
& n = \text{number of items} \\
& \overline{cov(ij)} = \text{average covariance between item-pairs}  \\
& \overline{\sigma^2_i} = \text{average item variance}  \\
\end{aligned}
$$

**McDonald's Omega ($\omega$)** is substantially more complicated, but avoids the limitation that Cronbach's alpha which assumes that all items are equally related to the construct. You can get it using the `omega()` function from the **psych** package. If you want more info about it then the help docs (`?omega()`) are a good place to start. 

:::

`r qbegin(8)`
Using the relevant function, obtain alpha for the set of items in each factor from your final factor analysis model of low mood. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The first factor has high loadings for items 1 to 9 (we have excluded 3 and 7). 
```{r}
names(df2)
alpha(select(df2, item1:item9))
alpha(select(df2, item10:item16))
```
:::int
Cronbach's $\alpha$ of 0.82 and 0.81 for the two factors and suggests high internal consistency. 
:::

`r solend()`


<!-- :::frame -->
<!-- You can't test the structural model if the measurement model is bad -->

<!-- if you test the relationships between a set of latent factors, and they are not reliably measured by the observed items, then this error propagates up to influence the fit of the model.   -->
<!-- To test the measurement model, it is typical to *saturate* the structural model (i.e., allow all the latent variables to correlate with one another). This way any misfit is due to the measurement model.   -->

<!-- ::: -->

<!-- what can we __do__ with this knowledge?   -->
<!-- attenuation due to measurement error -->

# Replicability 

`r qbegin(9)`
Split the dataset in half, and assess the replicability of your factor structure of low mood by examining the factor congruence in scores on each subset of the data. 

__Hint:__ see the lectures! 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
library(tidyverse)
set1 <- df2 %>%
  sample_frac(.5) # randomly select one half
set2 <- anti_join(df2, set1) # select the non-matching cases

res1 <- fa(set1, nfactors = 2, rotate = "oblimin")
res2 <- fa(set2, nfactors = 2, rotate = "oblimin")
fa.congruence(res1, res2) 
```

We can see there is a high level of factor congruence across the two subsets of data.  
`r solend()`


# Factor Scores

`r qbegin(10)`
Extract the factor scores for each factor of low-mood from your model, and attach them to original dataset (the one which has information on pet ownership).  
Then, conduct a $t$-test to examine whether the pet-owners differ from non-pet-owners in their levels of each factor of low mood.  

As a bonus, can you come up with some description of the two factors? (you will have to look back to the question numbers and questions right at the start). 

```{r echo=FALSE}
tibble(QuestionNumber = paste0("item",1:16),`Over the last 2 weeks, how much have you had/have you been...` = 
         c("Little interest or pleasure in doing things?",
"Feeling down, depressed, or hopeless?",
"Trouble falling or staying asleep, or sleeping too much?",
"Feeling tired or having little energy?",
"Poor appetite or overeating?",
"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?",
"Reading the newspaper or watching television?",
"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?",
"A lack of motivation to do anything at all?",
"Feeling nervous, anxious or on edge?",
"Not being able to stop or control worrying?",
"Worrying too much about different things?",
"Trouble relaxing?",
"Being so restless that it is hard to sit still?",
"Becoming easily annoyed or irritable?",
"Feeling afraid as if something awful might happen?")) %>% knitr::kable() %>% kableExtra::kable_styling(full_width = T)

```


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`


```{r}
pgpets <- 
  pgpets %>% 
    mutate(
      depression = pgmod2$scores[,1],
      anxiety = pgmod2$scores[,2],
      pet = factor(do_you_own_a_pet)
    )

t.test(pgpets$depression ~ pgpets$pet)
ggplot(pgpets, aes(x=pet, y=depression))+
  geom_boxplot()

t.test(pgpets$anxiety~pgpets$pet)
ggplot(pgpets, aes(x=pet, y=anxiety))+
  geom_boxplot()
```


`r solend()`






<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>