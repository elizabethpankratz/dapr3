---
title: "Introducing Multilevel Models"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(patchwork)
library(effects)
library(lme4)
```

:::green
__Information about solutions__

Solutions for these exercises are available immediately below each question.  
We would like to emphasise that much evidence suggests that testing enhances learning, and we __strongly__ encourage you to make a concerted attempt at answering each question *before* looking at the solutions. Immediately looking at the solutions and then copying the code into your work will lead to poorer learning.  
We would also like to note that there are always many different ways to achieve the same thing in R, and the solutions provided are simply _one_ approach.  

:::

:::lo
**Preliminaries**  
 
1. Create a new RMarkdown document or R script (whichever you like) for this week. 

:::


# Thinking about research questions

One of the biggest challenges in conducting research comes in the translation of our broader research aims to the specification of a statistical model or test. Questions which appear clear-cut and straightforward can turn out to be somewhat confusing when we think in more depth about what exactly it is that we have measured.  

These first exercises are designed to help you think through the steps to get from a description of a research study to a model specification. 

:::frame
__The study: Hangry hypothesis__

The study is interested in evaluating whether hunger influences peoples' levels of irritability (i.e., "the hangry hypothesis"), and whether this is different for people following a diet that includes fasting. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet. 

```{r echo=FALSE}
hangry<-read_csv("https://uoepsy.github.io/data/hangry.csv") %>% mutate(fivetwo=factor(fivetwo))
tibble(
    variable = names(hangry),
    description = c("Score on irritability questionnaire (0:100)",
                    "Score on hunger questionnaire (0:100)",
                    "Participant",
                    "Whether the participant follows the five-two diet")
) %>% knitr::kable()
```

:::

`r qbegin("A1")`
What is our outcome variable?  

_hint: The research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet._
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Our outcome is irritability here, because it is the thing that we are trying to explain through peoples' hunger levels and diets. 
`r solend()`

`r qbegin("A2")`
What are our explanatory variables?

_hint: The research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet._
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The explanatory variables of interest here are a person's level of hunger, and whether they are on the five-two diet. 
`r solend()`

`r qbegin("A3")`
Is there any grouping (or "clustering") of our data that we consider to be a random sample? If so, what are the groups? 

_hint: We can split our data in to groups of each participant. We can also split it into groups of each diet. Which of these groups have we randomly sampled? Do we have a random sample of participants? Do we have a random sample of diets? Another way to think of this is "if i repeated the experiment, what these groups be different?"_
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
If we did this experiment again, would we have different participants?  
Yes.   

If we did this experiment again, would we have different diets?   
No, because we're interested in the specific differences between the five-two diet and no dieting.  

This means we will likely want to by-participant random deviations (e.g. the `( ... | participant)` bit in `lmer`). But we won't have by-diet random effects because they are the specific differences we wish to test. 
`r solend()`

`r qbegin("A4")`
Based on previous questions, we are starting to be able to think of how our model would look in lmer syntax:
```{}
outcome ~ explanatory variables + (???? | grouping)
```

In the explanatory variables, we are interested in the effect of hunger, and whether this effect is different for the five-two diet. So we are interested in the interaction:
```{r eval=F}
lmer(irritability ~ 1 + hunger + diet + hunger:diet  + (???? | participant))
```
(remember that `hunger + diet + hunger:diet` is just a more explicit way of writing `hunger*diet`). 

Let's try and think about the maximal random effect structure. 
What __can__ we model as randomly varying between participants?  
some options:

1. participants vary in how irritable they are on average   
(the intercept, `1 | participant`)
2. participants vary in how much hunger influences their irritability   
(the effect of hunger, `hunger | participant`)
3. participants vary in how much diet influences irritability   
(the effect of diet, `diet | participant`)
4. participants vary in how much diet effects hunger's influence on irritability   
(the interaction between diet and hunger, `diet:hunger | participant`)

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

We can vary 1 and 2, but not 3 and 4. 
This is because each participant is either following the five-two diet or they are not. So for a _single_ participant, we can't assess "the effect diet has" on anything, because we haven't seen that participant under different diets. if we try to plot a single participants' data, we can see that it is impossible for us to assess "the effect of diet":
```{r echo=FALSE, fig.asp=.5}
hangry %>% filter(ppt == "N1p2") %>%
    ggplot(., aes(x=fivetwo, y=q_irritability))+
    geom_point()+
    geom_smooth(method="lm",se=F)+
    scale_x_discrete(drop=FALSE)+
  labs(title="The 'effect of diet' for a single\nparticipant from the Hangry study",subtitle="cannot be defined")
```

By contrast, we __can__ vary the intercept and the effect of hunger, because each participant has multiple values of irritability, and multiple different observations of hunger. We can think about a single participant's "effect of hunger on irritability" and how we might fit a line to their data:
```{r echo=FALSE,fig.asp=.5}
hangry %>% filter(ppt == "N1p2") %>%
    ggplot(., aes(x=q_hunger, y=q_irritability))+
    geom_point()+
    geom_smooth(method="lm",se=F)+
  labs(title="The 'effect of hunger' for a single\nparticipant from the Hangry study")
```

`r solend()`

:::frame
__The data: hangry.csv__   

The data is available at: https://uoepsy.github.io/data/hangry.csv

:::

`r qbegin("A5")`
Read the data into R and fit the maximal model.  
If the model doesn't converge, try fitting it with a different optimiser before considering simplifying the model structure. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
library(tidyverse)
library(lme4)
hangry <- read_csv("https://uoepsy.github.io/data/hangry.csv")
hangry <- hangry %>% mutate(fivetwo=factor(fivetwo))

mmod <- lmer(q_irritability ~ q_hunger * fivetwo + 
         (1 + q_hunger | ppt), data = hangry,
         control = lmerControl(optimizer="bobyqa"))
summary(mmod)
```
`r solend()`



# Centering & Scaling revisited

:::frame

We're going to quickly return to the data used right back at the beginning of DAPR2. You can find it at: https://uoepsy.github.io/data/dapr2_report1.csv

A description is here:

_Researchers were interested in assessing the degree to which urban environments and personality traits contribute towards perceptions of health. They conducted a survey study (n = 750) assessing individual’s personality - as measured by a self report Big Five measure from the International Personality Item Pool (IPIP; 6point Likert scale ranging from 1 to 6), as well as individual’s age, location of residence (city, suburb, countryside) and multiple self-ratings of different aspects of health (sleep, diet, physical health, mental health - high scores = better health). These variables were combined into a single z-score composite variable._

:::



`r qbegin("B1")`
Read in the data, and fit a simple linear regression (one predictor) to evaluate how neuroticism (variable `N` in the dataset) influences self reported health (variable `srh` in the dataset).  

Provide an interpretation of the intercept and the coefficient for neuroticism. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
dap2 <- read_csv("https://uoepsy.github.io/data/dapr2_report1.csv")
m1 <- lm(srh ~ N, data = dap2)
summary(m1)$coefficients
```
:::int

- Someone scoring zero on the neuroticism domain of the Big Five measure from the International Personality Item Pool (IPIP) is estimated to self report thier health `r coefficients(m1)[1] %>% round(.,2)` standard deviations above average.  
- For every increase of one score on the neuroticism domain of the IPIP, self reported health is estimated to decrease by `r coefficients(m1)[2] %>% round(.,2)` standard deviations. 

:::
`r solend()`

`r qbegin("B2")`
Mean center your predictor and fit the model again. What (if anything) has changed, and why?

Provide an interpretation of the intercept and the coefficient for neuroticism. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
dap2 <- dap2 %>% mutate(Nmc = N - mean(N))
m2 <- lm(srh ~ Nmc, data = dap2)
summary(m2)$coefficients
```
:::int

- Someone scoring at the mean of the neuroticism domain of the Big Five measure from the International Personality Item Pool (IPIP) is estimated to self report their health at zero. Because the measure of self-reported health is standardised, zero represents the sample average self-reported health. 
- For every increase of one score on the neuroticism domain of the IPIP, self reported health is estimated to decrease by `r coefficients(m2)[2] %>% round(.,2)` standard deviations. 

:::
`r solend()`

`r qbegin("B3")`
Standardise your predictor and fit the model again. What (if anything) has changed, and why?  

Provide an interpretation of the intercept and the coefficient for neuroticism. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
dap2 <- dap2 %>% mutate(Nz = scale(N))
m3 <- lm(srh ~ Nz, data = dap2)
summary(m3)$coefficients
```
:::int

- Someone scoring at the mean of the neuroticism domain of the Big Five measure from the International Personality Item Pool (IPIP) is estimated to self report their health at zero. Because the measure of self-reported health is standardised, zero represents the sample average self-reported health. 
- For every increase of one __standard deviation__ on the neuroticism domain of the IPIP, self reported health is estimated to decrease by `r coefficients(m3)[2] %>% round(.,2)` standard deviations. 

:::

`r solend()`

`r qbegin("B4")`
Conduct a model comparison between the 3 models you have just fitted, examining whether any of them show a significant reduction in the residual sums of squares. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
anova(m1,m2,m3)
```

`r solend()`


`r qbegin("B5")`
Think about what changes when you center or scale a predictor in a standard regression model. 

- The variance explained by the predictor remains exactly the same
- The intercept will change to be the estimated mean outcome where that predictor is 0 (the significance test will therefore change because the intercept has a different meaning)
- The slope of the predictor will change according to any scaling (e.g. if you divide your predictor by 10, the slope will multiply by 10).
- The **test** of the slope of the predictor remains exactly the same


Consider the two models you have just fitted:
```{r}
dap2 <- dap2 %>% mutate(Nz = scale(N))
# raw N
m1 <- lm(srh ~ N, data = dap2)
# standardized N
m3 <- lm(srh ~ Nz, data = dap2)
```

What will you need to multiply the coefficient of **N** from model `m1` by in order to obtain the coefficient of **Nz** from model `m3`?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Remember that the `scale(N)` is subtracting the mean from each value, then dividing those by the standard deviation. 
The standard deviation of `dap2$N` is:
```{r}
sd(dap2$N)
```
so in our variable `dap2$Nz`, a change of `r sd(dap2$N) %>% round(.,2)` gets scaled to be a change of 1 (because we are dividing by `sd(dap2$N)`).  

```{r}
coef(m1)[2] * sd(dap2$N)
coef(m3)[2]
```
`r solend()`

# Group Centering

Returning to the __Hangry Hypothesis__ study, let's think about our continuous predictor: the level of hunger. It has a mean:
```{r}
mean(hangry$q_hunger)
```
But it also has a mean _for each participant_:
```{r}
hangry %>% group_by(ppt) %>%
    summarise(mean_hunger = mean(q_hunger))
```

`r qbegin("C1")`
Returning to the __hangry.csv__ dataset, add a column to the data which is the average hungriness score for each participant.  

_hint: `group_by() %>% mutate()`_
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
hangry <- 
    hangry %>% group_by(ppt) %>%
        mutate(
            avg_hunger = mean(q_hunger)
        )
```
`r solend()`

`r qbegin("C2")`
Using the new variable you created in the previous question, plot each participants' average hunger-score against all of their irritability scores. Can you use `stat_summary` to make it easier to interpret?  

Does it look like hungry people are more irritable than less hungry people?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can just plot all the points:
```{r fig.asp=.5}
ggplot(hangry,aes(x=avg_hunger,y=q_irritability))+
    geom_point()
```
But we might find it easier to look at a plot where each participant is represented as their mean plus an indication of their range of irritability scores:
```{r fig.asp=.5}
ggplot(hangry,aes(x=avg_hunger,y=q_irritability))+
    stat_summary(geom="pointrange")
```
There appears to be a slight positive relationship between a persons' average hunger and their irritability scores. 
`r solend()`

`r qbegin("C3")`
Adjust your plot so that you have some means of telling apart those people who are on the five-two diet vs those who are not. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can do things like colour, or facet wrap
```{r}
library(patchwork) # for arranging multiple ggplots
ggplot(hangry,aes(x=avg_hunger,y=q_irritability, col=fivetwo))+
    stat_summary(geom="pointrange") +
ggplot(hangry,aes(x=avg_hunger,y=q_irritability))+
    stat_summary(geom="pointrange") +
    facet_wrap(~fivetwo)
```

`r solend()`


`r qbegin("C4")`
Add a column to the data, which is the deviation from each person's hunger score tothat person's average hunger score. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
hangry <- 
    hangry %>% 
        mutate(
            hunger_gc = q_hunger - avg_hunger
        )
```

`r solend()`


`r qbegin("C5")`
Plot each participant's deviations from their average hunger scores against their irritability scores. 
Does it look like when people are more hungry than normal, they are more angry? 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt))+
    geom_point()+
    geom_line(alpha=.1)
```
This is harder to tell what the relationship is, because there are a lot of lines. Let's plot a sample of participants:
```{r}
# these are all our unique ppt IDs: unique(hangry$ppt)
# let's take a sample of them:
toplot <- sample(unique(hangry$ppt), size=12)
hangry %>% filter(ppt %in% toplot) %>%
ggplot(.,aes(x=hunger_gc,y=q_irritability, group=ppt))+
    geom_point()+
    geom_line(alpha=.1)+
    facet_wrap(~ppt)
```
I think there might be a positive trend in here, in that participants tend to be higher irritability when they are higher )for them) on the hunger score. 
`r solend()`

`r qbegin("C6")`
Adjust your plot so that you have some means of telling apart those people who are on the five-two diet vs those who are not. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt, col=fivetwo))+
    geom_point()+
    geom_line(alpha=.1) +
ggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt))+
    geom_point()+
    geom_line(alpha=.1) +
    facet_wrap(~fivetwo)
```
`r solend()`

# Centering in the MLM

__Recall our research aim:__   

*The study is interested in evaluating whether hunger influences peoples' levels of irritability (i.e., "the hangry hypothesis")*, and whether this is different for people following a diet that includes fasting.

Thinking just about the relationship between irritability and hunger (i.e., ignoring the aim of evaluating whether it is different for different diets), how did you initially interpreted what the research aimed to study?  
Was it:  

A: "Are people more irritable if they are, __on average__, more hungry __than other people__?"  
B: "Are people more irritable if they are, __for them__, more hungry __than they usually are__?"  
C: Both  
D: Unsure   

This is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data. 


`r qbegin("C8")`
We already have variables (created in the questions above) that capture the mean hunger score for each person, and the deviations from the mean hunger score for each person.  

We also have our model, which we wrote as something like:
```{r eval=FALSE}
mmod <- lmer(q_irritability ~ q_hunger * fivetwo + 
         (1 + q_hunger | ppt), data = hangry,
         control = lmerControl(optimizer="bobyqa"))
```

We know that the raw hunger scores we were working with are basically `hunger_average + hunger deviation`, and that these represent different features of a person that can influence the irritability. 

So maybe we simply separate out these two aspects of hunger in our model?  
Why won't this model work?
```{r eval=FALSE}
lmer(irritability ~ 1 + avg_hunger + hunger_gc + diet + 
                        avg_hunger:diet + hunger_gc:diet + 
         (1 + avg_hunger + hunger_gc  | ppt))
```
(this is a tricky question. hint: it's because of the random effects).
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Recall that we discussed how we cannot have a random effect of diet, because "an effect of diet" makes no sense for a single participant (they are either on the diet or they are not, so there is no 'effect').  

The same is true of a random effect of "average hungriness".  
For a single participant in our data, we can't really comprehend "an effect of being on average more hungry", because each participant has only one value for their average hungriness, which is their average hungriness.  

Consider the plot of `irritability ~ avg_hunger` for the first participant. How can we draw a slope?  
```{r fig.asp=.5}
hangry %>% filter(ppt == "N1p1") %>%
ggplot(., aes(x=avg_hunger, y=q_irritability))+
  geom_point()
```
`r solend()`

`r qbegin("C9")`
Fit the following model:
```{r}
hangrywb <- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + 
                (1 + hunger_gc | ppt), data = hangry,
            control = lmerControl(optimizer="bobyqa"))
```

Look at the confidence intervals for the fixed effect estimates from the model below. _We will not conduct a thorough model criticism here as we are focusing on the interpretation of the coefficients._  
```{r}
cbind(est = fixef(hangrywb), 
      confint(hangrywb, method="boot")[5:10,])
```

Try to provide an answer for each of these questions:

- For those following no diet, is there evidence to suggest that people who are _on average_ more hungry are more irritable?
- Is there evidence to suggest that this is different for those following the five-two diet? In what way?
- Do people following no diet tend to be more irritable when they are more hungry than they usually are? 
- Is there evidence to suggest that this is different for those following the five-two diet? In what way?

Trickier question:
- What does the fivetwo coefficient represent? 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`


**Q: For those following no diet, is there evidence to suggest that people who are _on average_ more hungry are more irritable?**  
A: No diet is the reference level of the five-two variable, and because we have an interaction, that means the `avg_hunger` coefficient will provide the relevant estimate. There is no evidence to suggest that when not dieting, hungrier people are more irritable than less hungry people. A value of 0 for this effect is within our 95% confidence intervals.   


**Q: Is there evidence to suggest that this is different for those following the five-two diet? In what way?**  
A: This is the interaction between `avg_hunger:fivetwo1`. We can see that, for every increase of 1 in average hunger, irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet.  
These units are still in terms of the original scale (i.e. 0 to 100). 


**Q: Do people following no diet tend to be more irritable when they are more hungry than they usually are?** 
A: This is the estimate for the coefficient of `hunger_gc`. For people following no diet, there is an estimated 0.19 increase in irritability for every 1 unit more hungry they become. 

**Q: Is there evidence to suggest that this is different for those following the five-two diet? In what way?**
A: This effect of a 1 unit change on within-person hunger increasing irritability is increased for those who are following the five-two diet by an additional 0.38

**Q: What does the `fivetwo1` coefficient represent?** 
A: This represents the group difference of irritability between those on the five-two diet vs those not dieting, for someone who has an average hunger score of 0. 

`r solend()`

`r qbegin("C10")`
Construct two plots showing the two model estimated interactions.  

_hint: this isn't as difficult as it sounds. the sjPlot package can do it in one line of code!_ 

Think about your answers to the previous question, and check that they match with what you are seeing in the plots (do not underestimate the utility of this activity for helping understanding!).  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
library(sjPlot)
plot_model(hangrywb, type = "int")
```

`r solend()`

# Logistic MLM

:::frame
**Don't forget to look back at other materials!** 

Back in DAPR2, we introduced logistic regression in semester 2, week 8. The lab contained some simulated data based on a hypothetical study [about inattentional blindness](https://uoepsy.github.io/wayback/dapr2labs/15_binary_logistic.html). 
That content will provide a lot of the groundwork for this week, so we recommend revisiting it if you feel like it might be useful. 
:::

:::rtip
__`lmer()` >> `glmer()`__

Remember how we simply used `glm()` and could specify the `family = "binomial"` in order to fit a logistic regression? Well it's much the same thing for multi-level models! 

+ Gaussian model: `lmer(y ~ x1 + x2 + (1 | g), data = data)`  
+ Binomial model: `glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial(link='logit'))`
    + or just `glmer(y ~ x1 + x2 + (1 | g), data = data, family = "binomial")`
    + or `glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial)`
    
:::
    
`r optbegin("Binary? Binomial?", olabel=F,show=T,toggle=params$TOGGLE)`
For binary regression, all the data in our outcome variable has to be a 0 or a 1.  
For example, the `correct` variable below:  
```{r echo=FALSE}
tibble(participant = c(1,1,1),question=c(1,2,3),correct=c(1,0,1)) %>% rbind(rep("...",3)) %>%
  gt::gt()
```

But we can re-express this information in a different way, when we know the total number of questions asked.
```{r echo=FALSE}
tibble(participant = c(1,2,3),questions_correct=c(2,1,3),questions_incorrect=c(1,2,0)) %>% rbind(rep("...",3)) %>% gt::gt()
```

To model data when it is in this form, we can express our outcome as `cbind(questions_correct, questions_incorrect)`
`r optend()`


:::frame  
__Data: Novel Word Learning__

```{r}
nwl <- read_csv("https://uoepsy.github.io/data/nwl2.csv")
```

In the `nwl2` data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. Participants had 9 experimental blocks. In each of these they completed 30 exercises. There is data on the whether participants responded correctly or incorrectly to each exercise. The first 7 blocks were learning blocks, immediately followed by a test (block 8). Finally, participants also completed a follow-up test (block 9). 
<br>
**Research Aims**  
_Compare the two groups (those with anterior vs. posterior lesions) with respect to their accuracy of responses over the course of the study_
<br>
Figure \@ref(fig:nwl-fig) shows the differences between groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up)

```{r nwl-fig, fig.asp=.6, echo=FALSE, fig.cap="Differences between groups in the average proportion of correct responses at each block"}
ggplot(nwl, aes(block, correct, color=lesion_location, 
                                shape=lesion_location)) +
  #geom_line(aes(group=ID),alpha=.2) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(data=filter(nwl,block <= 7), 
                           fun=mean, geom="line") + 
  geom_hline(yintercept=0.5, linetype="dashed") + 
  geom_vline(xintercept=c(7.5, 8.5), linetype="dashed") + 
  scale_x_continuous(breaks=1:9, labels=c(1:7, "Test", "Follow-Up")) + 
  theme_bw(base_size=10) + 
  labs(x="Block", y="Proportion Correct", shape="Lesion\nLocation", color="Lesion\nLocation")
```

:::

`r qbegin("D1: Initial thoughts")`

1. What is our outcome? 
2. Is it longitudinal? (simply, is there a temporal sequence to observations within a participant?) 
3. How many participants with brain lesions do we have data for? 

*Hint:* Think carefully about question 1. There might be several variables which either fully or partly express the information we are considering the "outcome" here. 
`r qend()`
`r solbegin(show=TRUE, toggle=params$TOGGLE)` 

1. The outcome here is (in words) "the proportion of correct answers in each block". This makes it tempting to look straight to the variable called `PropCorrect`. Unfortunately, this is encoded as a proportion (i.e., is bounded by 0 and 1), and of the models we have learned about so far, we don't definitely have the necessary tools to model this.  
    - linear regression = continuous *unbounded* outcome variable  
    - logistic regression = binomial, expressed as `0` or `1` if the number of trials per observation is 1, and expressed as `cbind(num_successes, num_failures)` if the number of trials per observation is >1. 
2. It is longitudinal in the sense that blocks are sequential. However, we probably wouldn't call it a "longitudinal study" as that tends to get reserved for when we follow-up participants over a fairly long time period (months or years). 
3. There are 12 patients
```{r}
nwl %>% count(id)
```

`r solend()`

`r qbegin("D2")`
> **Research Question 1:**  
> Is the learning rate (training blocks) different between these two groups?

**Hints**: 

- Make sure we're running models on only the data we are actually interested in.  
- Think back to what we have been doing with model comparison to examine the effect of including certain predictors.  We could use this approach for this question, to compare:
    - A model with just the change over the sequence of blocks
    - A model with the change over the sequence of blocks *and* an overall difference between groups
    - A model with groups differing with respect to their change over the sequence of blocks
- What about the random effects part?  
    1. What are our observations grouped by? 
    2. What variables can vary within these groups? 
    3. What do you want your model to allow to vary within these groups?

`r optbegin("Suggested answers to the hints if you don't know where to start", olabel=FALSE, toggle=params$TOGGLE)`

- Make sure we're running models on only the data we are actually interested in. 
    - We want only the learning blocks, not block 8 or 9.
    - You might want to store this data in a separate object, but in the code for the solution we will just use `filter()` *inside* the `glmer()`.   
  
- A model with just the change over the sequence of blocks:
    - **outcome ~ block**
- A model with the change over the sequence of blocks *and* an overall difference between groups:
    - **outcome ~ block + lesion_location**
- A model with groups differing with respect to their change *over the sequence of blocks:
    - **outcome ~ block * lesion_location**
    
- What are our observations grouped by? 
    - repeated measures by-participant. i.e., the `id` variable
- What variables can vary within these groups? 
    - only the outcome variable (`correct`), and the `block`.
    - The other variables (`lesion_location` and `group`) do **not** vary for each ID. Lesions don't suddenly change where they are located, nor do participants swap between being a patient vs a control (we don't need the group variable anyway as we are excluding the controls).  
What do you want your model to allow to vary within these groups?
    - Do you think the change over the course of the blocks is **the same** for everybody? Or do you think it varies? Is this variation important to think about in terms of your research question?   
    
`r optend()`

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
m.base <- glmer(correct ~ block + (block | id), 
                data = filter(nwl, block < 8),
                family=binomial)
m.loc0 <- glmer(correct ~ block + lesion_location + (block | id), 
                data=filter(nwl, block < 8),
                family=binomial)
m.loc1 <- glmer(correct ~ block * lesion_location + (block | id), 
                data=filter(nwl, block < 8),
                family=binomial)
#summary(m.loc1)
anova(m.base, m.loc0, m.loc1, test="Chisq")
```
:::int
No significant difference in learning rate between groups ($\chi^2(1)=2.2, p = 0.138$).
:::

`r solend()`

## Optional Extras

:::imp
This next bit gets a little complicated, and does not contain any unique information that will prove helpful in completing the assessment.  
However, it does provide an example of a logistic multi-level model, along with interpretation of the fixed effects. We hope that it may provide a useful learning tool. 
:::

> **Research Question 2**  
> In the testing phase, does performance on the immediate test differ between lesion location groups, and does their retention from immediate to follow-up test differ?

Let's try a different approach to this. Instead of fitting various models and comparing them via likelihood ratio tests, just fit the one model which could answer both parts of the question above.  

This line of code will create a variable specifying whether a block is "learning" (blocks 1-7), an "immediate test" (block 8), or a "followup test" (block 9). 

```{r}
nwl <- nwl %>% mutate(
    phase = ifelse(block <=7, "learning",
                   ifelse(block==8,"immediate","followup"))
)
```

We can now fit our model: 
```{r}
m.recall.loc <- glmer(correct ~ phase * lesion_location + (phase | id), 
                  nwl %>% filter(block>7), family="binomial")
summary(m.recall.loc)
```

:::frame

Take some time to remind yourself from DAPR2 of the [interpretation of logistic regression coefficients](https://uoepsy.github.io/wayback/dapr2labs/15_binary_logistic.html).  

In `family = binomial(link='logit')`, we are modelling the log-odds. 
We can obtain estimates on this scale using:  

- `fixef(model)`
- `summary(model)$coefficients`
- `tidy(model)` **from broom.mixed**  
- (there are probably more ways, but I can't think of them right now!)

We can use `exp()`, to get these back into odds and odds ratios.  
:::

`r qbegin("Extra 1")`
Make sure you pay attention to trying to interpret each fixed effect from your models.  
These can be difficult, especially when it's logistic, and especially when there are interactions.  

- What is the increase in the odds of answering correctly in the immediate test for someone with a posterior legion compared to someone with an anterior legion?  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
* `(Intercept)` ==> Anterior lesion group performance in immediate test. This is the log-odds of them answering correctly in the immediate test. 
* `PhaseFollow-up`  ==> Change in performance (for the anterior lesion group) from immediate to follow-up test. 
* `lesion_locationposterior` ==> Posterior lesion group performance in immediate test relative to anterior lesion group performance in immediate test
* `PhaseFollow-up:lesion_locationposterior` ==> Change in performance from immediate to follow-up test, posterior lesion group relative to anterior lesion group

```{r}
exp(fixef(m.recall.loc))[3]
```

:::int
Those with posterior lesions have `r round(exp(fixef(m.recall.loc))[3],2)` times the odds of answering correctly in the immediate test compared to someone with an anterior lesion. 
:::

`r solend()`

`r qbegin("Extra 2")`
Can you recreate the visualisation in Figure \@ref(fig:nwl-fig2)? (try without looking at the solution code!). 

```{r nwl-fig2, echo=FALSE, fig.cap="Differences between groups in the average proportion of correct responses at each block"}
ggplot(nwl, aes(block, correct, color=lesion_location, 
                                shape=lesion_location)) +
  #geom_line(aes(group=ID),alpha=.2) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(data=filter(nwl,block <= 7), 
                           fun=mean, geom="line") + 
  geom_hline(yintercept=0.5, linetype="dashed") + 
  geom_vline(xintercept=c(7.5, 8.5), linetype="dashed") + 
  scale_x_continuous(breaks=1:9, labels=c(1:7, "Test", "Follow-Up")) + 
  theme_bw(base_size=10) + 
  labs(x="Block", y="Proportion Correct", shape="Lesion\nLocation", color="Lesion\nLocation")
```

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ggplot(nwl, aes(block, correct, color=lesion_location, 
                                shape=lesion_location)) +
  #geom_line(aes(group=ID),alpha=.2) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(data=filter(nwl,block <= 7), 
                           fun=mean, geom="line") + 
  geom_hline(yintercept=0.5, linetype="dashed") + 
  geom_vline(xintercept=c(7.5, 8.5), linetype="dashed") + 
  scale_x_continuous(breaks=1:9, labels=c(1:7, "Test", "Follow-Up")) + 
  theme_bw(base_size=10) + 
  labs(x="Block", y="Proportion Correct", shape="Lesion\nLocation", color="Lesion\nLocation")
```
`r solend()`



<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>