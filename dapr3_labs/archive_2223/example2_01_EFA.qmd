---
title: 'Analysis Example: Exploratory Factor Analysis'
link-citations: yes
code-fold: true
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---
```{r setup, include=F}
knitr::opts_chunk$set(message = F, warning = F, fig.align = 'center')
```

:::frame

Please note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email [ug.ppls.stats@ed.ac.uk](mailto:ug.ppls.stats@ed.ac.uk). 

:::

```{r echo=FALSE}
set.seed(3)
```


# Intro

This is a quick demonstration of one way of dealing with these tasks. It is by no means the only correct way. There is a substantial level of subjectivity in _Exploratory Factor Analysis_ and the method involves repeated evaluation and re-evaluation of the model in light of the extracted factors and their conceptual relationships with the analysed items. In other words, a good EFA requires you to get your hands dirty.

:::frame
__Data: Work Pressures Survey__  

The Work Pressures Survey (WPS) data is available at the following link:
[https://uoepsy.github.io/data/WPS_data.csv](https://uoepsy.github.io/data/WPS_data.csv).  
The data contains responses from 946 workers from a variety of companies to the Work Pressures Survey. You can look at the survey taken by the study participants at the following [link: https://uoepsy.github.io/data/WPS_data_codebook.pdf](https://uoepsy.github.io/data/WPS_data_codebook.pdf)

We will be performing a factor analysis of the main section of this survey (Job1 to Job50).

:::

# Read in Data

Read the WPS data into R. Make sure to take a look at the variable names and data structure.
```{r}
library(tidyverse)
df <- read.csv("https://uoepsy.github.io/data/WPS_data.csv")
head(df)
```

Variable names - Option 1:
```{r}
names(df)
```

Variable names - Option 2:
```{r eval=FALSE}
colnames(df)
```

Data structure - Option 1:
```{r}
str(df)
```

Data structure - Option 2:
```{r}
glimpse(df)
```

# Sanity Checks  

Produce a table of summary statistics for the variables in the data.
```{r}
df %>%
    summarise(across(everything(), 
                     list(M = mean, SD = sd, MIN = min, MAX = max))) %>%
    pivot_longer(everything())
```

We can see that there are a few missing values in some variables.

If you were to analyse this data for a research project hopefully leading to a paper, you would probably want to perform sanity check on the variables, such as check if everyone is an adult (assuming this was a requirement for partaking of the study).

Check whether all participants in the study are adults.

```{r}
unique(df$doby)
```

The data look like a bit of a mess.. Some participants have the full year of birth, some only the last 2 digits. Let’s only extract the last 2 digits from all rows then.

Let's use the `str_sub()` function to take only the last 2 characters.
```{r}
df <- df %>%
    mutate(doby = str_sub(doby, -2, -1))
```
It seems like it's now a character rather than a number:
```{r}
class(df$doby)
```
Let's make it a number again:
```{r}
df$doby <- as.numeric(df$doby)
```


Visualise the distribution of birth year.
Do we notice anything strange?  
```{r}
ggplot(df, aes(x = doby)) + 
    geom_histogram(color = 'white')
```
Or a dotplot if you prefer:
```{r}
ggplot(df, aes(x = doby)) + 
    geom_dotplot(dotsize = 0.6, binwidth = 1, fill = 'dodgerblue', color = NA)
```

A year of birth equal to −1 doesn't make any sense and, since we only want to keep adults, we will remove the rows in the data set having a year of birth equal to -1.

In the meantime, we will also remove those participants who don't have a value for `doby`.
```{r}
df <- df %>%
    filter(!is.na(doby) | doby > 0)

hist(df$doby)
```
That looks much better!

Normally, you'd want to check other variables too.
For now, because we are focusing on EFA, we'll just assume that the other variables are okay.

# Subset

Remember that the only variables we are interested in for our EFA are the `job1` to `job50` variables. We need to subset the data set to only include those variables.

```{r}
df <- df %>%
    select(job1:job50)
```

Create a table of descriptive summary statistics for each variable.
```{r}
library(psych)
describe(df)
```

Some of the variables appear to have values of 0, −1, as well as values larger than 7, even though all the questionnaire items are on a 7-point Likert scale. 

Get rid of infeasible values:
```{r}
df[df < 1 | df > 7] <- NA
```

# Descriptives & Visualising  

Let's now look at the score distributions per item and the correlations between pairs of items.

Sometimes easier to use subsets of ten variables each time. For example, look at the pairwise plots of the first 10 variables, then the next 10, and so on. 

There's a lot here to look at. the bottom triangle shows the scatterplots for pairs of variables. Because we have lots of data, and everything is likert data (1-7), the points themselves don't tell us much, but we can eyeball the lines to look at the relationships. And we can look in the top triangle to see these too. It's also good to take a look at the diagonals, which show the distributions of each item. This will help inform us about what type of correlations and factor extraction we might use.  
```{r}
pairs.panels(df[, 1:10])
pairs.panels(df[, 11:20])
pairs.panels(df[, 21:30])
pairs.panels(df[, 31:40])
pairs.panels(df[, 41:50])
```

As you can see, while some of the items have pretty much bell-shaped distributions, some others are massively skewed (looking at you `job49`) or close to uniform (`job9`). 
At this stage, you'd want to have a closer look at the wording of these troublesome items and see if you can spot any methodological issues that might account for these distributions. 
If the items look fine, you might want to consider alternative correlation coefficients (e.g., polychoric correlations) that might be more suitable to items with weird distributions. 
For now, let’s stick to Pearson's correlation (r). Since we have NAs in the data, let's just use complete observations.


Compute the correlation matrix of the variables.

Instead of looking at the $50 \times 50$ matrix of correlations, look at the distribution of correlation coefficients from the lower triangular part of the matrix.

Option 1:
```{r}
R <- cor(df, use = "complete.obs")
```

Option 2:
```{r}
R <- cor(na.omit(df))
```

```{r}
hist(R[lower.tri(R)])
```

If you want to be a little fancier, you can categorise the coefficients into negligible, weak, moderate, and strong correlations and plot a bar plot like this:

```{r}
Rc <- cut(abs(R), 
          breaks = c(0, .2, .5, .7, 1), 
          labels = c("negligible", "weak", "moderate", "strong"))

barplot(table(Rc[lower.tri(Rc)]))
```

As we can see, most of the correlations are negligible and many are weak. There are some moderate and strong relationships in the data. This suggests that there might be multiple independent factors.

# Suitability for FA  

Check if the correlations are sufficient for EFA with Bartlett's test of sphericity and if the sample was adequate with KMO.
```{r}
cortest.bartlett(R, 
                 n = sum(complete.cases(df))) # we have 917 complete cases
```

```{r}
KMO(R)
```

A significant Bartlett's test of sphericity means that our correlation matrix is _not_ proportional to an identity matrix (a matrix with only 1s on the diagonal and 0s everywhere else). This is exactly what we want, so we're happy!

Likewise, the sampling adequacy is pretty good. All items have a measure of sampling adequacy (MSA) in the $>.7$ "middling" region and the overall KMO is bordering on the $>.9$ "marvellous" level (I kid you not).

Given these results, we can merrily factor-analyse!

# Number of Factors  

In order to decide how many factors to use, look at the suggestions given by parallel analysis and MAP.

```{r}
fa.parallel(df, fa = 'fa')
```

```{r}
VSS(df)
```

Since parallel analysis (suggesting 11 factor) tends to overextract and MAP (suggesting 7 factors) can sometimes underextract, it is reasonable to look at solutions with 7-10 factors.
However, looking at the scree plot, it might be reasonable to cast a glance on a 5- or 6-factor solution.

# Perform EFA

Fit a factor analysis model to the data using 10 factors.
Since there is no good reason to expect the factors to be uncorrelated (orthogonal), let's use the oblimin rotation.  

```{r}
m_10f <- fa(df, nfactors = 10, rotate = "oblimin", fm = "ml")
```

Print loadings sorted according to loadings
```{r}
fa.sort(m_10f)
```

OK, 10 factors looks like way too many as the last 2 have very few substantive loadings (>.33). Let’s look at a smaller solution, e.g. 9 or 8 factors and see if it’s still the case...

```{r}
m_9f <- fa(df, nfactors = 9, rotate = "oblimin", fm = "ml")
m_8f <- fa(df, nfactors = 8, rotate = "oblimin", fm = "ml")
```

It was the case and even the 9-factor solution has only 1 substantive loading on the last factor. These are however quite large so let’s look at this solution a little closer. We can see that a few items don’t have any loadings larger than our .33 cut-off:
We can see that a few items don’t have any loadings larger than our .33 cut-off:
```{r}
# get loadings
x <- m_9f$loadings
which(rowSums(x < .33) == 9)
```

It was the case and even the 8-factor solution has only 2 substantive loadings on the last factor. These are however quite large so let’s look at this solution a little closer. We can see that a few items don’t have any loadings larger than our .33 cut-off:

```{r}
# get loadings
x <- m_8f$loadings
which(rowSums(x < .33) == 8)
```

Here is when we would go back to the item wordings and try to see why these items might not really correlate with any other items. For instance, `job11` ("I regularly discuss problems at work with my colleagues.") might be ambiguous: does it mean that there are often problems or that if there are problems, I discuss them regularly?

For argument's sake, let's say, all of these identified items are deemed problematic so we should remove them:

```{r}
cols_to_remove <- names(which(rowSums(x < .33) == 8))
df2 <- df[ , !names(df) %in% cols_to_remove]
```

Check parallel analysis and MAP again.

```{r}
fa.parallel(df2, fa = 'fa')
```

```{r}
VSS(df2)
```

Fit an 8-factor model to the new dataset.

```{r}
m_8f <- fa(df2, nfactors = 8, rotate = "oblimin", fm = "ml")
fa.sort(m_8f)
```

We still only get 2 substantive loadings on the last factor, while we want at least 3. This also happens with the 7- and 6-factor solutions. But once we hit the 5-factor solution, we find a better structure:

```{r}
m_5f <- fa(df2, nfactors = 5, rotate = "oblimin", fm = "ml")
fa.sort(m_5f)
```

Also notice that the 8-factor solution accounted for 49% of the variance and the 5-factor one explains 41%. That is not a huge drop considering that, by choosing the 5-factor over the 8-factor solution, we reduce the dimensionality of the data (number of variables we have to deal with) by further 3 dimensions!

Looking at the loadings, we can see that only 4 items have substantial cross-loadings (on exactly 2 factors), which is not terrible.

Glance at the factor correlations of this final model, we see that only factor 1 and 5 are weakly-to-moderately correlated, which is not too bad! It allows us to claim that the factors (except for one) are largely independent of each other. This model accounts for about 41% of the variance.

At this stage, we would go to the individual items, look at which factors load on which items, and try to figure out what is the common theme linking these items. For instance, let’s look at the factor ML5. I would start by looking at the items with the highest loadings, i.e., items 44, 4, 33, and 24. They all have three things in common: they address fairness, openness, and promotions/pay rises. Since we have multiple themes going on here, let’s look at the items with loadings in the .4-.6 range. A stronger theme of fair acknowledgement of performance emerges. Not all of the lower-loading items chime with this theme terribly well, but those that do not tend to have cross-loadings with other factor. I would therefore be reasonable confident that the factor taps into something that could be called "Fair recognition" (apparently this is referred to in the OrgPsych jargon as "Procedural justice").


<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
