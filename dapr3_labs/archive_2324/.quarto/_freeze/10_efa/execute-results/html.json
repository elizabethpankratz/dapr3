{
  "hash": "0c3769fd280587b0895afc76d61dc7e1",
  "result": {
    "markdown": "---\ntitle: \"10. EFA 1\"\nparams: \n    SHOW_SOLS: TRUE\n    TOGGLE: TRUE\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n:::lo\n**Relevant packages**\n\n+ psych\n+ GPArotation\n\n::: \n\n# PCA vs FA  \n\nWhere **PCA** aims to summarise a set of measured variables into a set of orthogonal (uncorrelated) components as linear combinations (a weighted average) of the measured variables, **Factor Analysis (FA)** assumes that the relationships between a set of measured variables can be explained by a number of underlying *latent factors*.   \n  \nNote how the directions of the arrows in @fig-pcafa are different between PCA and FA - in PCA, each component $C_i$ is the weighted combination of the observed variables $y_1, ...,y_n$, whereas in FA, each measured variable $y_i$ is seen as *generated by* some latent factor(s) $F_i$ plus some unexplained variance $u_i$.   \n\nIt might help to read the $\\lambda$s as beta-weights ($b$, or $\\beta$), because that's all they really are. \nThe equation $y_i = \\lambda_{1i} F_1 + \\lambda_{2i} F_2 + u_i$ is just our way of saying that the variable $y_i$ is the manifestation of some amount ($\\lambda_{1i}$) of an underlying factor $F_1$, some amount ($\\lambda_{2i}$) of some other underlying factor $F_2$, and some error ($u_i$). \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Path diagrams for PCA and FA](images/pcaefa/pca_efa.png){#fig-pcafa fig-align='center' width=1000px}\n:::\n:::\n\n\nIn **_Exploratory_ Factor Analysis (EFA)**, we are starting with no hypothesis about either the number of latent factors or about the specific relationships between latent factors and measured variables (known as the *factor structure*). Typically, all variables will load on all factors, and a transformation method such as a rotation (we'll cover this in more detail below) is used to help make the results more easily interpretable.^[When we have some clear hypothesis about relationships between measured variables and latent factors, we might want to impose a specific factor structure on the data (e.g., items 1 to 10 all measure social anxiety, items 11 to 15 measure health anxiety, and so on). When we impose a specific factor structure, we are doing **Confirmatory Factor Analysis (CFA)**. This is not covered in this course, but it's important to note that in practice EFA is not wholly \"exploratory\" (your theory *will* influence the decisions you make) nor is CFA wholly \"confirmatory\" (in which you will inevitably get tempted to explore how changing your factor structure might improve fit).] \n\n\n# Suitability of items for EFA\n\nThere are various ways of assessing the suitability of our items for exploratory factor analysis, and most of them rely on examining the observed correlations between items.  \n\n:::statbox\n__Look at the correlation matrix__  \n\nUse a function such as `cor` or `corr.test(data)` (from the **psych** package) to create the correlation matrix.  \n\n:::\n\n:::statbox\n__Bartlett's Test__  \n\nThe function `cortest.bartlett(cor(data), n = nrow(data))` conducts \"Bartlett's test\". This tests against the null that the correlation matrix is proportional to the identity matrix (a matrix of all 0s except for 1s on the diagonal).  \n\n  - Null hypothesis: observed correlation matrix is equivalent to the identity matrix  \n  - Alternative hypothesis: observed correlation matrix is not equivalent to the identity matrix.  \n  \n\n\n::: {.callout-note collapse=\"true\"}\n#### What is the identity matrix?\n\nThe \"Identity matrix\" is a matrix of all 0s except for 1s on the diagonal.  \ne.g. for a 3x3 matrix:  \n$$\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\end{bmatrix}\n$$\nIf a correlation matrix looks like this, then it means there is __no__ shared variance between the items, so it is not suitable for factor analysis\n:::\n\n:::\n\n:::statbox\n__Kaiser, Meyer, Olkin Measure of Sampling Adequacy__  \n\nYou can check the \"factorability\" of the correlation matrix using `KMO(data)` (also from **psych**!).  \n\n- Rules of thumb: \n    - $0.8 < MSA < 1$: the sampling is adequate\n    - $MSA <0.6$: sampling is not adequate \n    - $MSA \\sim 0$: large partial correlations compared to the sum of correlations. Not good for FA  \n    \n    \n::: {.callout-note collapse=\"true\"}\n#### Kaiser's suggested cuts  \n\nThese are Kaiser's corresponding adjectives suggested for each level of the KMO:  \n\n- 0.00 to 0.49 \"unacceptable\"  \n- 0.50 to 0.59 \"miserable\"  \n- 0.60 to 0.69 \"mediocre\"  \n- 0.70 to 0.79 \"middling\"  \n- 0.80 to 0.89 \"meritorious\"  \n- 0.90 to 1.00 \"marvelous\"  \n\n:::\n\n:::\n\n:::statbox\n__Check for linearity__  \n\nIt also makes sense to check for linearity of relationships prior to conducting EFA. EFA is all based on correlations, which assume the relations we are capturing are linear.  \n\nYou can check linearity of relations using `pairs.panels(data)` (also from **psych**), and you can view the histograms on the diagonals, allowing you to check univariate normality (which is usually a good enough proxy for multivariate normality). \n\n:::\n\n\n\n# Exercises: Conduct Problems  \n\n\n:::frame\n__Data: Conduct Problems__  \n\nA researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours:  \n\n1. Stealing\n1. Lying\n1. Skipping school\n1. Vandalism\n1. Breaking curfew\n1. Threatening others\n1. Bullying\n1. Spreading malicious rumours\n1. Using a weapon \n1. Fighting\n\nYour task is to use the dimension reduction techniques you learned about in the lecture to help inform how to organise the items she has developed into subscales.  \n\nThe data can be found at https://uoepsy.github.io/data/conduct_probs.csv \n\n:::\n\n## 1. Check Suitability\n\n\n\n<div class='question-begin'>Question 1</div><div class='question-body'>\n\n\nRead in the dataset from [https://uoepsy.github.io/data/conduct_probs.csv](https://uoepsy.github.io/data/conduct_probs.csv).  \nThe first column is clearly an ID column, and it is easiest just to discard this when we are doing factor analysis.  \n  \nCreate a correlation matrix for *the items*.  \nInspect the items to check their suitability for exploratory factor analysis.   \n\n\n\n\n</div><p class=\"question-end\"></p>\n\n \n\n\n<div class=\"solution-begin\"><button id='sol-start-1' class=\"jk-circle-right solution-icon clickable\" onclick=\"toggle_visibility('sol-body-1', 'sol-start-1')\">  Solution </button></div><div class=\"solution-body\" id = \"sol-body-1\" style=\"display: none;\">\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(psych)\ndf <- read.csv(\"https://uoepsy.github.io/data/conduct_probs.csv\")\n# discard the first column\ndf <- df[,-1]\n\ncorr.test(df)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:corr.test(x = df)\nCorrelation matrix \n       item1 item2 item3 item4 item5 item6 item7 item8 item9 item10\nitem1   1.00  0.59  0.49  0.48  0.60  0.17  0.30  0.32  0.26   0.20\nitem2   0.59  1.00  0.53  0.51  0.66  0.20  0.33  0.30  0.29   0.19\nitem3   0.49  0.53  1.00  0.49  0.55  0.15  0.25  0.24  0.25   0.15\nitem4   0.48  0.51  0.49  1.00  0.65  0.23  0.29  0.32  0.28   0.25\nitem5   0.60  0.66  0.55  0.65  1.00  0.21  0.30  0.29  0.27   0.21\nitem6   0.17  0.20  0.15  0.23  0.21  1.00  0.54  0.57  0.41   0.44\nitem7   0.30  0.33  0.25  0.29  0.30  0.54  1.00  0.83  0.61   0.58\nitem8   0.32  0.30  0.24  0.32  0.29  0.57  0.83  1.00  0.61   0.59\nitem9   0.26  0.29  0.25  0.28  0.27  0.41  0.61  0.61  1.00   0.44\nitem10  0.20  0.19  0.15  0.25  0.21  0.44  0.58  0.59  0.44   1.00\nSample Size \n[1] 450\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n       item1 item2 item3 item4 item5 item6 item7 item8 item9 item10\nitem1      0     0     0     0     0     0     0     0     0      0\nitem2      0     0     0     0     0     0     0     0     0      0\nitem3      0     0     0     0     0     0     0     0     0      0\nitem4      0     0     0     0     0     0     0     0     0      0\nitem5      0     0     0     0     0     0     0     0     0      0\nitem6      0     0     0     0     0     0     0     0     0      0\nitem7      0     0     0     0     0     0     0     0     0      0\nitem8      0     0     0     0     0     0     0     0     0      0\nitem9      0     0     0     0     0     0     0     0     0      0\nitem10     0     0     0     0     0     0     0     0     0      0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n```\n:::\n\n```{.r .cell-code}\ncortest.bartlett(cor(df), n=450)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$chisq\n[1] 2238\n\n$p.value\n[1] 0\n\n$df\n[1] 45\n```\n:::\n\n```{.r .cell-code}\nKMO(df)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = df)\nOverall MSA =  0.87\nMSA for each item = \n item1  item2  item3  item4  item5  item6  item7  item8  item9 item10 \n  0.90   0.88   0.92   0.88   0.84   0.94   0.82   0.81   0.95   0.94 \n```\n:::\n\n```{.r .cell-code}\npairs.panels(df)\n```\n\n::: {.cell-output-display}\n![](10_efa_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\nor alternatively, if you want a ggplot based approach:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(GGally)\nggpairs(data=df, diag=list(continuous=\"density\"), axisLabels=\"show\")\n```\n\n::: {.cell-output-display}\n![](10_efa_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n</div><p class=\"solution-end\"></p>\n\n\n\n## 2. How many factors?\n\n\n\n<div class='question-begin'>Question 2</div><div class='question-body'>\n\n\nHow many dimensions should be retained? This question can be answered in the same way as we did above for PCA. \n  \nUse a scree plot, parallel analysis, and MAP test to guide you.   \nYou can use `fa.parallel(data, fa = \"fa\")` to conduct both parallel analysis and view the scree plot!   \n\n\n</div><p class=\"question-end\"></p>\n\n \n\n\n<div class=\"solution-begin\"><button id='sol-start-2' class=\"jk-circle-right solution-icon clickable\" onclick=\"toggle_visibility('sol-body-2', 'sol-start-2')\">  Solution </button></div><div class=\"solution-body\" id = \"sol-body-2\" style=\"display: none;\">\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfa.parallel(df, fa = \"fa\")\n```\n\n::: {.cell-output-display}\n![](10_efa_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n```\n:::\n:::\n\nIn this case the scree plot has a kink at the third factor, so we probably want to retain 2 factors.  \n  \nWe can conduct the MAP test using `VSS(data)`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nVSS(df, plot = FALSE, n = ncol(df))$map\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.1058 0.0338 0.0576 0.1035 0.1494 0.2520 0.3974 0.4552 1.0000     NA\n```\n:::\n:::\n\nThe MAP test suggests retaining 2 factors.  \n\n\n</div><p class=\"solution-end\"></p>\n\n\n\n## 3. Perform EFA\n\nNow we need to perform the factor analysis. But there are two further things we need to consider, and they are:  \n\na) whether we want to apply a __rotation__ to our factor loadings, in order to make them easier to interpret, and  \nb) how do we want to extract our factors (it turns out there are loads of different approaches!). \n\n\n:::statbox\n**Rotations?** \n\nRotations are so called because they transform our loadings matrix in such a way that it can make it more easy to interpret. You can think of it as a transformation applied to our loadings in order to optimise interpretability, by maximising the loading of each item onto one factor, while minimising its loadings to others. We can do this by simple rotations, but maintaining our axes (the factors) as perpendicular (i.e., uncorrelated) as in @fig-rot2, or we can allow them to be transformed beyond a rotation to allow the factors to correlate (@fig-rot3). \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![No rotation](images/pcaefa/rot1.png){#fig-rot1 fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Orthogonal rotation](images/pcaefa/rot2.png){#fig-rot2 fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Oblique rotation](images/pcaefa/rot3.png){#fig-rot3 fig-align='center' width=80%}\n:::\n:::\n\nIn our path diagram of the model (@fig-efarot), all the factor loadings remain present, but some of them become negligible. We can also introduce the possible correlation between our factors, as indicated by the curved arrow between $F_1$ and $F_2$. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Path diagrams for EFA with rotation](images/pcaefa/efa_rot.png){#fig-efarot fig-align='center' width=1000px}\n:::\n:::\n\n\n:::\n\n:::frame\n**Factor Extraction**  \n\nPCA (using eigendecomposition) is itself a method of extracting the different dimensions from our data. However, there are lots more available for factor analysis. \n\nYou can find a lot of discussion about different methods both in the help documentation for the `fa()` function from the psych package: \n\n>Factoring method fm=\"minres\" will do a minimum residual as will fm=\"uls\". Both of these use a first derivative. fm=\"ols\" differs very slightly from \"minres\" in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative. This will be slower. fm=\"wls\" will do a weighted least squares (WLS) solution, fm=\"gls\" does a generalized weighted least squares (GLS), fm=\"pa\" will do the principal factor solution, fm=\"ml\" will do a maximum likelihood factor analysis. fm=\"minchi\" will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm =\"minrank\" will do a minimum rank factor analysis. \"old.min\" will do minimal residual the way it was done prior to April, 2017 (see discussion below). fm=\"alpha\" will do alpha factor analysis as described in Kaiser and Coffey (1965)\n\nAnd there are lots of discussions both in papers and on [forums](https://stats.stackexchange.com/questions/50745/best-factor-extraction-methods-in-factor-analysis). \n\nAs you can see, this is a complicated issue, but when you have a large sample size, a large number of variables, for which you have similar communalities, then the extraction methods tend to agree. For now, don't fret too much about the factor extraction method.^[(It's a *bit* like the optimiser issue in the multi-level model block)]\n\n:::\n\n\n\n\n<div class='question-begin'>Question 3</div><div class='question-body'>\n\n\nUse the function `fa()` from the **psych** package to conduct and EFA to extract 2 factors (this is what *we* suggest based on the various tests above, but *you* might feel differently - the ideal number of factors is subjective!). Use a suitable rotation (`rotate = ?`) and extraction method (`fm = ?`).  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconduct_efa <- fa(data, nfactors = ?, rotate = ?, fm = ?)\n```\n:::\n\n\n\n\n</div><p class=\"question-end\"></p>\n\n \n\n\n<div class=\"solution-begin\"><button id='sol-start-3' class=\"jk-circle-right solution-icon clickable\" onclick=\"toggle_visibility('sol-body-3', 'sol-start-3')\">  Solution </button></div><div class=\"solution-body\" id = \"sol-body-3\" style=\"display: none;\">\n\n\nFor example, you could choose an oblimin rotation to allow factors to correlate and use minres as the extraction method.  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconduct_efa <- fa(df, nfactors=2, rotate='oblimin', fm=\"minres\")\n```\n:::\n\n\n\n</div><p class=\"solution-end\"></p>\n\n\n\n## 4. Inspect\n\nWe can simply print the name of our model in order to see a lot of information. Let's go through it in pieces.   \n\n:::statbox\n\n__Loadings__  \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = df, nfactors = 2, rotate = \"oblimin\", fm = \"minres\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n         MR1   MR2   h2   u2 com\nitem1   0.03  0.71 0.52 0.48   1\nitem2   0.01  0.77 0.60 0.40   1\nitem3  -0.02  0.68 0.45 0.55   1\nitem4   0.06  0.68 0.50 0.50   1\nitem5  -0.04  0.87 0.73 0.27   1\nitem6   0.63 -0.02 0.39 0.61   1\nitem7   0.89  0.00 0.80 0.20   1\nitem8   0.92 -0.01 0.84 0.16   1\nitem9   0.63  0.09 0.45 0.55   1\nitem10  0.67 -0.03 0.43 0.57   1\n```\n:::\n:::\n\n\nFactor loading's, like PCA loading's, show the relationship of each measured variable to each factor. They range between -1.00 and 1.00\nLarger absolute values represent stronger relationship between measured variable and factor.  \n\n- The columns that (depending upon estimation method) might be called `MR`/`ML`/`PC` are the factors. The number assigned to is arbitrary, and they might not always be in a numeric order (this has to do with a rotated solution). Typically, the numbering maps to how much variance each factor account for. \n- __h2:__ This is the \"communality\", which is how much variance in the item is explained by the factors. It is calculated as the sum of the squared loadings.  \n- __u2:__ This is $1 - h2$. It is the residual variance, or the \"uniqueness\" for that item (i.e. the amount left unexplained).  \n- __com:__ This is the \"Item complexity\". It tells us how much a given item reflects a single factor (vs being \"more complex\" in that it represents multiple factors). It equals one if an item loads only on one factor, 2 if evenly loads on two factors, etc.  \n\nYou can get these on their own using\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconduct_efa$loadings\n```\n:::\n\n\n:::\n\n:::statbox\n__Variance Accounted For__  \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                       MR1  MR2\nSS loadings           2.92 2.80\nProportion Var        0.29 0.28\nCumulative Var        0.29 0.57\nProportion Explained  0.51 0.49\nCumulative Proportion 0.51 1.00\n```\n:::\n:::\n\n\nBelow the factor loadings, we have a familiar set of measures of the variance in the data accounted for by each factor. This is very similar to what we saw with PCA.  \n\n- SS loadings: The sum of the squared loadings. The eigenvalues.  \n- Proportion Var: how much of the overall variance the factor accounts for out of all the variables. \n- Cumulative Var: cumulative sum of Proportion Var.\n- Proportion Explained: relative amount of variance explained ($\\frac{\\text{Proportion Var}}{\\text{sum(Proportion Var)}}$.\n- Cumulative Proportion: cumulative sum of Proportion Explained.\n\nYou can get these on their own using\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconduct_efa$Vaccounted\n```\n:::\n\n\n:::\n\n:::statbox\n__Factor Correlations__  \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.43\nMR2 0.43 1.00\n\nMean item complexity =  1\n```\n:::\n:::\n\n\nWhether we see this section will depend if we have run a factor analysis with $\\geq 2$ factors and a rotation.  \n\n- `factor correlations`: shows the correlation matrix between the factors. \n- `mean item complexity`: shows the mean of the `com` column from the loadings above.  \n\nYou can get these on their own using\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconduct_efa$Phi\n```\n:::\n\n\n:::\n\n\n:::statbox\n__Tests, Fit Indices etc__  \n\nWe also get a whole load of other stuff that can sometimes be useful. These include: a test of an hypothesis that the 2 factors are sufficient; information on the number of observations; fit indices such as RMSEA, TLI RMSR etc; and measures of factor score adequacy (we'll get to talking about factor scores next week).\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  45  with the objective function =  5.03 with Chi Square =  2238\ndf of  the model are 26  and the objective function was  0.09 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  450 with the empirical chi square  13.7  with prob <  0.98 \nThe total n.obs was  450  with Likelihood Chi Square =  40  with prob <  0.039 \n\nTucker Lewis Index of factoring reliability =  0.989\nRMSEA index =  0.035  and the 90 % confidence intervals are  0.008 0.055\nBIC =  -119\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.96 0.94\nMultiple R square of scores with factors          0.92 0.88\nMinimum correlation of possible factor scores     0.84 0.76\n```\n:::\n:::\n\n\n\n\n:::\n\n\n\n<div class='question-begin'>Question 4</div><div class='question-body'>\n\n\nInspect the loadings (`conduct_efa$loadings`) and give the factors you extracted labels based on the patterns of loadings.  \n  \nLook back to the description of the items, and suggest a name for your factors  \n\n\n</div><p class=\"question-end\"></p>\n\n \n\n\n<div class=\"solution-begin\"><button id='sol-start-4' class=\"jk-circle-right solution-icon clickable\" onclick=\"toggle_visibility('sol-body-4', 'sol-start-4')\">  Solution </button></div><div class=\"solution-body\" id = \"sol-body-4\" style=\"display: none;\">\n\n\nYou can inspect the loadings using:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(conduct_efa$loadings, sort=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLoadings:\n       MR1    MR2   \nitem6   0.634       \nitem7   0.890       \nitem8   0.924       \nitem9   0.629       \nitem10  0.669       \nitem1          0.706\nitem2          0.772\nitem3          0.681\nitem4          0.676\nitem5          0.872\n\n                MR1   MR2\nSS loadings    2.90 2.784\nProportion Var 0.29 0.278\nCumulative Var 0.29 0.568\n```\n:::\n:::\n\nWe can see that the first five items have high loadings for one factor and the second five items have high loadings for the other.  \n  \nThe first five items all have in common that they are non-aggressive forms of conduct problems, while the last five items are all aggressive behaviours. We could, therefore, label our factors: ‘non-aggressive’ and ‘aggressive’ conduct problems.\n\n\n</div><p class=\"solution-end\"></p>\n\n\n\n\n\n<div class='question-begin'>Question 5</div><div class='question-body'>\n\n\nHow correlated are your factors?  \n\nWe can inspect the factor correlations (if we used an oblique rotation) using:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconduct_efa$Phi\n```\n:::\n\n\n\n</div><p class=\"question-end\"></p>\n\n \n\n\n<div class=\"solution-begin\"><button id='sol-start-5' class=\"jk-circle-right solution-icon clickable\" onclick=\"toggle_visibility('sol-body-5', 'sol-start-5')\">  Solution </button></div><div class=\"solution-body\" id = \"sol-body-5\" style=\"display: none;\">\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconduct_efa$Phi\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     MR1  MR2\nMR1 1.00 0.43\nMR2 0.43 1.00\n```\n:::\n:::\n\nWe can see here that there is a moderate correlation between the two factors. An oblique rotation would be appropriate here. \n\n\n</div><p class=\"solution-end\"></p>\n\n\n\n## 5. Write-up \n\n\n\n<div class='question-begin'>Question 6</div><div class='question-body'>\n\n\nDrawing on your previous answers and conducting any additional analyses you believe would be necessary to identify an optimal factor structure for the 10 conduct problems, write a brief text that summarises your method and the results from your chosen optimal model.\n\n\n</div><p class=\"question-end\"></p>\n\n\n\n\n<div class=\"solution-begin\"><button id='sol-start-6' class=\"jk-circle-right solution-icon clickable\" onclick=\"toggle_visibility('sol-body-6', 'sol-start-6')\">  Solution </button></div><div class=\"solution-body\" id = \"sol-body-6\" style=\"display: none;\">\n\n\nThe main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description).\n\nAn example summary would be:\n\n:::int \n\nFirst, the data were checked for their suitability for factor analysis. Normality was checked using visual inspection of histograms, linearity was checked through the inspection of the linear and lowess lines for the pairwise relations of the variables, and factorability was confirmed using a KMO test, which yielded an overall KMO of $.87$ with no variable KMOs $<.50$. \nAn exploratory factor analysis was conducted to inform the structure of a new conduct problems test. Inspection of a scree plot alongside parallel analysis (using principal components analysis; PA-PCA) and the MAP test were used to guide the number of factors to retain. All three methods suggested retaining two factors; however, a one-factor and three-factor solution were inspected to confirm that the two-factor solution was optimal from a substantive and practical perspective, e.g., that it neither blurred important factor distinctions nor included a minor factor that would be better combined with the other in a one-factor solution. These factor analyses were conducted using minres extraction and (for the two- and three-factor solutions) an oblimin rotation, because it was expected that the factors would correlate. Inspection of the factor loadings and correlations reinforced that the two-factor solution was optimal: both factors were well-determined, including 5 loadings $>|0.3|$ and the one-factor model blurred the distinction between different forms of conduct problems. \nThe factor loadings are provided in @tbl-loadingtab^[You should provide the table of factor loadings. It is conventional to omit factor loadings $<|0.3|$; however, be sure to ensure that you mention this in a table note.]. Based on the pattern of factor loadings, the two factors were labelled 'aggressive conduct problems' and 'non-aggressive conduct problems'. These factors had a  correlation of $r=.43$. Overall, they accounted for 57% of the variance in the items, suggesting that a two-factor solution effectively summarised the variation in the items.\n\n\n\n::: {#tbl-loadingtab .cell layout-align=\"center\" tbl-cap='Factor Loadings'}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> MR1 </th>\n   <th style=\"text-align:right;\"> MR2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> item1 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item2 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item3 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item4 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item5 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item6 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item7 </td>\n   <td style=\"text-align:right;\"> 0.89 </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item8 </td>\n   <td style=\"text-align:right;\"> 0.92 </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item9 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> item10 </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n:::\n\n\n\n\n</div><p class=\"solution-end\"></p>\n\n\n\n# PCA & EFA Comparison Exercise\n\n\n\n<div class='question-begin'>Question 7</div><div class='question-body'>\n\n\nUsing the same data, conduct a PCA using the `principal()` function.  \n  \nWhat differences do you notice compared to your EFA?  \n  \nDo you think a PCA or an EFA is more appropriate in this particular case?\n\n\n</div><p class=\"question-end\"></p>\n\n \n\n\n<div class=\"solution-begin\"><button id='sol-start-7' class=\"jk-circle-right solution-icon clickable\" onclick=\"toggle_visibility('sol-body-7', 'sol-start-7')\">  Solution </button></div><div class=\"solution-body\" id = \"sol-body-7\" style=\"display: none;\">\n\n\nWe can use:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprincipal(df, nfactors=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 2)\nStandardized loadings (pattern matrix) based upon correlation matrix\n        RC1  RC2   h2   u2 com\nitem1  0.17 0.77 0.62 0.38 1.1\nitem2  0.17 0.81 0.68 0.32 1.1\nitem3  0.11 0.75 0.58 0.42 1.0\nitem4  0.21 0.74 0.60 0.40 1.2\nitem5  0.16 0.85 0.75 0.25 1.1\nitem6  0.73 0.08 0.53 0.47 1.0\nitem7  0.87 0.20 0.80 0.20 1.1\nitem8  0.88 0.19 0.82 0.18 1.1\nitem9  0.72 0.21 0.56 0.44 1.2\nitem10 0.75 0.09 0.57 0.43 1.0\n\n                       RC1  RC2\nSS loadings           3.29 3.22\nProportion Var        0.33 0.32\nCumulative Var        0.33 0.65\nProportion Explained  0.51 0.49\nCumulative Proportion 0.51 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.06 \n with the empirical chi square  166  with prob <  1.9e-22 \n\nFit based upon off diagonal values = 0.98\n```\n:::\n:::\n\nWe can see that while the loadings differ somewhat between the EFA and the PCA, the overall pattern is quite similar. This is not always the case, especially when the item communalities are low.  \n  \nIn terms of which method is more appropriate, arguably EFA would be more appropriate in this case because our researcher wishes to measure a theoretical construct (conduct problems), rather than simply reduce the dimensions of her data.\n\n\n</div><p class=\"solution-end\"></p>\n\n\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>",
    "supporting": [
      "10_efa_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/panelset-0.2.6/panelset.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/panelset-0.2.6/panelset.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}