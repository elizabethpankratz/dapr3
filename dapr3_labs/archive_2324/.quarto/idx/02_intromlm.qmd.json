{"title":"2. Intro to Multilevel Models","markdown":{"yaml":{"title":"2. Intro to Multilevel Models","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"A Note on terminology","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n```\n\n\n:::{.callout-caution collapse=\"true\"}\n\nThe methods we're going to learn about in the first five weeks of this course are known by lots of different names: \"multilevel models\"; \"hierarchical linear models\"; \"mixed-effect models\"; \"mixed models\"; \"nested data models\"; \"random coefficient models\"; \"random-effects models\"; \"random parameter models\"... and so on).   \n\nWhat the idea boils down to is that **model parameters vary at more than one level.** This week, we're going to explore what that means.  \n\nThroughout this course, we will tend to use the terms \"mixed effect model\", \"linear mixed model (LMM)\" and \"multilevel model (MLM)\" interchangeably. \n\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Multilevel Model Notation  \n\nMultilevel Models (MLMs) (or \"Linear Mixed Models\" (LMMs)) take the approach of allowing the groups/clusters to vary around our $\\beta$ estimates. \n\nIn the lectures, we saw this as:\n\n$$\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n$$\n\nWe are now assuming $\\color{orange}{\\zeta_0}$, $\\color{orange}{\\zeta_1}$, and $\\varepsilon$ to be normally distributed with a mean of 0, and we denote their variances as $\\sigma_{\\color{orange}{\\zeta_0}}^2$, $\\sigma_{\\color{orange}{\\zeta_1}}^2$, $\\sigma_\\varepsilon^2$ respectively. \n\nThe $\\color{orange}{\\zeta}$ components also get termed the \"random effects\" part of the model, Hence names like \"random effects model\", etc. \n\n:::\n\n::: {.callout-note collapse=\"true\"}\n#### Alternative (\"mixed effect\") notation\n\nMany people use the symbol $u$ in place of $\\zeta$, and in various resources, you are likely to see $\\alpha$ used to denote the intercept instead of $\\beta_0$.  \n\nSometimes, you will see the levels collapsed into one equation, as it might make for more intuitive reading.  \n\nThis often fits with the name \"mixed effects\" for these models, as the \"effect\" of a predictor is a mix of both a fixed and a random part:  \n\n$$\\color{red}{y_{ij}} = \\underbrace{(\\gamma_{00} + \\color{orange}{\\zeta_{0i}})}_{\\color{blue}{\\beta_{0i}}} \\cdot 1 + \\underbrace{(\\gamma_{10} + \\color{orange}{\\zeta_{1i}})}_{\\color{blue}{\\beta_{1i}}} \\cdot x_{ij}  +  \\varepsilon_{ij} \\\\$$\n\nIn words, this equation is denoting: \n$$\n\\begin{align}\n\\color{red}{\\text{outcome}}\\color{black} = &(\\color{blue}{\\text{overall intercept}}\\color{black} + \\color{orange}{\\text{random adjustment to intercept by group}}\\color{black}) \\cdot 1 \\, + \\\\\n&( \\color{blue}{\\text{overall slope}}\\color{black} + \\color{orange}{\\text{random adjustment to slope by group}} \\color{black}) \\cdot \\text{predictor}\\, + \\\\\n& \\text{residual} \\\\\n\\end{align}\n$$\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Fitting Multilevel Models: Model Formula\n\nTo fit multilevel models, we're going to use the `lme4` package, and specifically the functions `lmer()` and `glmer()`.^[\"(g)lmer\" here stands for \"(generalised) linear mixed effects regression\".] \n\nYou will have seen some use of these functions in the lectures. The broad syntax is:  \n<br>\n<div style=\"margin-left:50px;\">\nlmer(**_formula_**,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = *logical*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(*options*) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\n\nWe write the first bit of our **formula** just the same as our old friend the normal linear model `y ~ 1 + x + x2 + ...`, where `y` is the name of our outcome variable, `1` is the intercept (which we don't have to explicitly state as it will be included anyway) and `x`, `x2` etc are the names of our explanatory variables.  \n\nWith **lme4**, we now have the addition of __random effect terms__, specified in parenthesis with the `|` operator (the vertical line | is often found to the left of the z key on QWERTY keyboards). We use the `|` operator to separate the parameters (intercept, slope etc.) on the LHS, from the grouping variable(s) on the RHS, by which we would like to model these parameters as varying.  \n\n:::statbox\n__Random Intercepts__  \nLet us suppose that we wish to model our intercept not as a fixed constant, but as varying randomly according to some grouping around a fixed center. \nWe can such a model by allowing the intercept to vary by our grouping variable (`g` below): \n\n<center>`lmer(y ~ 1 + x + (1|g), data = df)`</center>\n$$\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{Y_{ij}} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot X_{ij}} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\end{align}\n$$\n:::\n\n:::statbox\n__Random Intercepts and Slopes__  \nBy extension we can also allow the effect `y~x` to vary between groups, by including the `x` on the left hand side of `|` in the random effects part of the call to `lmer()`.\n\n<center>`lmer(y ~ 1 + x + (1 + x |g), data = df)`</center>\n$$\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\end{align}\n$$\n:::\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Fitting Multilevel Models: Model Estimation\n\nWe can choose whether to estimate our model parameters with ML (maximum likelihood) or REML (restricted maximum likelihood) with the `REML` argument of `lmer()`:  \n\n<br>\n<div style=\"margin-left:50px;\">\nlmer(*formula*,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = **_logical_**, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(*options*) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\n`lmer()` models are by default fitted with REML, which tends to be better for small samples.    \n\n### Maximum Likelihood (ML)  \n\nRemember back to DAPR2 when we introduced logistic regression, and we briefly discussed **Maximum likelihood estimation** in an explanation of how models are fitted.  \n\nThe key idea of maximum likelihood estimation (MLE) is that we (well, the computer) iteratively finds the set of estimates for our model which it considers to best reproduce our observed data. Recall our simple linear regression model of how time spent outdoors (hrs per week) is associated with mental wellbeing: \n$$\n\\color{red}{Wellbeing_i} = \\color{blue}{\\beta_0 \\cdot{} 1 + \\beta_1 \\cdot{} OutdoorTime_{i}} + \\varepsilon_i\n$$\nThere are values of $\\beta_0$ and $\\beta_1$ and $\\sigma_\\varepsilon$ which maximise the probability of observing the data that we have. For linear regression, these we obtained these same values a different way, via minimising the sums of squares. This approach is not possible for more complex models (e.g., logistic)  which is why we turn to MLE.  \n\n:::statbox\nTo read about the subtle difference between \"likelihood\" and \"probability\", you can find a short explanation [here](./lvp.html){target=\"_blank\"}\n:::\n\nIf we are estimating just one single parameter (e.g. a mean), then we can imagine the process of maximum likelihood estimation in a one-dimensional world - simply finding the top of the curve: \n```{r}\n#| label: fig-mle\n#| echo: false\n#| out.width: \"350px\"\n#| fig-cap: \"MLE\"\nknitr::include_graphics(\"images/intro/mle.png\")\n```\nHowever, our typical models estimate a whole bunch of parameters. The simple regression model above is already having to estimate $\\beta_0$, $\\beta_1$ and $\\sigma_\\varepsilon$, and our multi-level models have far more! With lots of parameters being estimated and all interacting to influence the likelihood, our nice curved line becomes a complex surface (see Left panel of @fig-multisurf). So what we (our computers) need to do is find the maximum, but avoid local maxima and singularities (see @fig-maxima). \n```{r}\n#| label: fig-multisurf\n#| echo: false\n#| out.width: \"49%\" \n#| fig-cap: \"MLE for a more complex model\"\n#| fig-align: 'center'\nknitr::include_graphics(\"images/multisurftb.png\")\n```\n\n### Restricted Maximum Likelihood (REML)\n\nWhen it comes to estimating multilevel models, maximum likelihood will consider the fixed effects as fixed, known values when it estimates the variance components (the random effect variances). This leads to biased estimates of the variance components, specifically biasing them toward being too small, especially if $n_\\textrm{clusters} - n_\\textrm{level 2 predictors} - 1 < 50$. This leads to the standard errors of the fixed effects being too small, thereby inflating our type 1 error rate (i.e. greater chance of incorrectly rejecting our null hypothesis).  \n\nRestricted Maximum Likelihood (REML) is a method that separates the estimation of fixed and random parts of the model, leading to unbiased estimates of the variance components.   \n\n\n:::\n<!-- :::sticky -->\n<!-- __Model Comparisons in MLM__ -->\n\n<!-- When we compare models that differ in their fixed effects via comparing model deviance (e.g. the likelihood ratio), REML should __not__ be used as only the variance components are included in the likelihood. Functions like `anova()` will automatically refit your models with `ML` for you, but it is worth checking.    -->\n\n<!-- We __cannot__ compare (either with ML or REML) models that differ in both the fixed and random parts.  -->\n\n<!-- ::: -->\n\n:::{.callout-note collapse=\"true\"}\n## Fitting Multilevel Models: Model Convergence\n\nAlongside the ML/REML choice for model estimation, we have some control over the underlying algorithm that is used to move around/search the likelihood surface for our estimates. We'll learn more about this next week.  \n\n<br>\n<div style=\"margin-left:50px;\">\nlmer(*formula*,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = *logical*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(**_options_**) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\n\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a *convergence warning*.  There are lots of different ways to [deal with these](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html) (to try to rule out hypotheses about what is causing them).  \n\nFor the time being, if `lmer()` gives you convergence errors, you could try changing the optimizer. Bobyqa is a good one: add `control = lmerControl(optimizer = \"bobyqa\")` when you run your model.  \n\n```{r eval=F}\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))\n```\n\n\n:::statbox\n__What *is* a convergence warning??__  \n\nThere are different techniques for maximum likelihood estimation, which we apply by using different 'optimisers'. Technical problems to do with **model convergence** and **'singular fit'** come into play when the optimiser we are using either can't find a suitable maximum, or gets stuck in a singularity (think of it like a black hole of likelihood, which signifies that there is not enough variation in our data to construct such a complex model).  \n\n```{r}\n#| label: fig-maxima\n#| echo: false\n#| out.width: \"49%\"\n#| fig-cap: \"local/global maxima and singularities\"\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/intro/mle2.png\")\n```\n\n:::\n:::\n<br>  \n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Exercises: Cross-Sectional\n\n:::frame\n__Data: Wellbeing Across Scotland__  \n\nRecall our dataset from last week, in which we used linear regression to determine how outdoor time (hours per week) is associated with wellbeing in different local authority areas (LAAs) of Scotland. We have data from various LAAs, from Glasgow City, to the Highlands.  \n\n```{r message=FALSE,warning=FALSE}\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n```\n```{r echo=FALSE, message=FALSE,warning=FALSE}\nlibrary(gt)\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\ntibble(variable=names(scotmw),\n       description=c(\"Participant ID\",\"Participant Name\",\"Local Authority Area\",\"Self report estimated number of hours per week spent outdoors\",\"Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\",\"LAA Population Density (people per square km)\")\n) %>% gt()\n```\n:::\n\n`r qbegin(\"1\")`\nUsing `lmer()` from the **lme4** package, fit a model predict `wellbeing` from `outdoor_time`, with by-LAA random intercepts.  \nPass the model to `summary()` to see the output. \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n#| include: false\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n```\n\n```{r}\nlibrary(lme4)\nri_model <- lmer(wellbeing ~ outdoor_time + (1 | laa), data = scotmw)\nsummary(ri_model)\n```\n`r solend()`\n\n`r qbegin(\"2\")`\nSometimes the easiest way to start understanding your model is to visualise it. \n \nLoad the package **broom.mixed**. Along with some handy functions `tidy()` and `glance()` which give us the information we see in `summary()`, there is a handy function called `augment()` which returns us the data in the model plus the fitted values, residuals, hat values, Cook's D etc..  \n\n:::{.callout-note collapse=true}\n#### broom.mixed  \n\n```{r}\n#| include: false\nmodel <- ri_model\n```\n\nthe broom.mixed package has some useful functions (just like those in the __broom__ package for `lm()`):  \n\n```{r}\nlibrary(broom.mixed)\nglance(model) # for overall model stats\ntidy(model) # for parameter stats\naugment(model) # observation level stuff (data + model)\n```\n:::\n\n\nAdd to the code below to plot the model fitted values, and color them according to LAA. \n(you will need to edit `ri_model` to be whatever name you assigned to your model).\n\n```{r eval=FALSE}\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = ...... \n```\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line()\n```\n`r solend()`\n\n\n`r qbegin(\"3\")`\nWe have just fitted the model:\n$$\n\\begin{align}\n& \\text{For person } j \\text{ in LAA } i \\\\\n& \\color{red}{\\textrm{Wellbeing}_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot \\textrm{Outdoor Time}_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\end{align}\n$$\n\nFor our estimates of $\\gamma_{00}$ (the fixed value around which LAA intercepts vary) and $\\beta_1$ (the fixed estimate of the relationship between wellbeing and outdoor time), we can use `fixef()`.  \n```{r}\nfixef(ri_model)\n```\nCan you add to the plot in the previous question, a thick black line with the intercept and slope given by `fixef()`?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n**Hint:** `geom_abline()`\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_abline(intercept = fixef(ri_model)[1], slope = fixef(ri_model)[2], lwd = 2)\n```\n`r solend()`\n\n`r qbegin(\"4\")`\nBy now, you should have a plot which looks more or less like the left-hand figure below (we have added on the raw data - the points).  \n<div style=\"display:inline-block; width: 55%;vertical-align: top;\">\n```{r}\n#| label: fig-modfit\n#| echo: false\n#| fig.asp: 1\n#| fig-cap: \"Model fitted values\"\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_abline(intercept = fixef(ri_model)[1], slope = fixef(ri_model)[2], lwd = 2)+\n  geom_point(aes(y=wellbeing), alpha=.4)\n```\n</div>\n<div style=\"display:inline-block; width: 40%;vertical-align: top;\">\n```{r}\n#| label: fig-lmersummap\n#| echo: false\n#| out.width: \"400px\"\n#| fig-cap: \"Summary model output<br>lmer(wellbeing~1 + outdoor_time + (1|laa),<br>data = scotmw)\"\nknitr::include_graphics(\"images/intro/summarylmer2.png\")\n```\n</div>\n<br>\n<br>\nMatch the coloured sections Yellow (W), Red (X), Blue (Y), and Orange (Z) in @fig-lmersummap to the descriptions below of @fig-modfit A through D. \n\nA) where the black line cuts the y axis\nB) the standard deviation of the distances from all the individual LAA lines to the black line\nC) the slope of the black line\nD) the standard deviation of the distances from all the individual observations to the line for the LAA to which it belongs.\n\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n+ A = Blue (Y)\n+ B = Yellow (W)\n+ C = Orange (Z)\n+ D = Red (X)\n\n`r solend()`\n\n\n`r qbegin(\"5\")`\nMatch the colours sections and descriptions from the previous question, to the mathematical terms in the model equation:  \n\n$$\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{Wellbeing_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot OutdoorTime_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\quad \\\\\n& \\text{where} \\\\\n& \\color{orange}{\\zeta_0}\\color{black} \\sim N(0, \\sigma_{\\color{orange}{\\zeta_{0}}}\\color{black})  \\text{ independently} \\\\\n& \\varepsilon \\sim N(0, \\sigma_{\\varepsilon}) \\text{ independently} \\\\\n\\end{align}\n$$\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n+ A = Blue (Y) = $\\gamma_{00}$  \n+ B = Yellow (W) = $\\sigma_{\\color{orange}{\\zeta_{0}}}$\n+ C = Orange (Z) = $\\beta_{1}$     \n+ D = Red (X) = $\\sigma_{\\varepsilon}$    \n\n\n\n`r solend()`\n\n\n`r qbegin(\"6\")`\nFit a model which allows *also* (along with the intercept) the effect of `outdoor_time` to vary by-LAA.   \n\nThen, using `augment()` again, plot the model fitted values. What do you think you will see?  \nDoes it look like this model better represents the individual LAAs? Take a look at, for instance, Glasgow City.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nrs_model <- lmer(wellbeing ~ 1 + outdoor_time + (1 + outdoor_time | laa), data = scotmw)\n\naugment(rs_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_point(aes(y=wellbeing), alpha=.4)\n```\n\n`r solend()`\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Exercises: Repeated Measures\n\nWhile the wellbeing example considers the groupings or 'clusters' of different LAAs, a more common grouping in psychological research is that of several observations belonging to the same individual. One obvious benefit of this is that we can collect many more observations with fewer participants but control for the resulting dependency of observations. \n\n:::frame\n__Data: REPLICATION Audio/SDMT study__  \n\n\nRecall the data from the previous week, from an experiment in which executive functioning was measured (via the SDMT) for people when listening to different types of audio, either via normal speakers or via noise-cancelling headphones.  \n\nThis week, we have data from a __replication__ of that study, in which the researchers managed to recruit 30 participants. Unfortunately, some participants did not complete all the trials, so we have an unbalanced design. The data is available at [https://uoepsy.github.io/data/ef_replication.csv](https://uoepsy.github.io/data/ef_replication.csv).  \n```{r}\n#| include: false\nset.seed(5)\nn_groups = 30\nN = n_groups*3*5\ng = rep(1:n_groups, e = N/n_groups)\n\nw = rep(rep(letters[1:3],5),n_groups)\nw1 = model.matrix(lm(rnorm(N)~w))[,2]\nw2 = model.matrix(lm(rnorm(N)~w))[,3]\n\nb = rep(0:1, e = N/2)\n\nre0 = rnorm(n_groups, sd = 2)[g]\nre_w1  = rnorm(n_groups, sd = 1)[g]\nre_w2  = rnorm(n_groups, sd = 1)[g]\n\nlp = (0 + re0) + \n  (3)*b + \n  (0 + re_w1)*w1 +\n  (-2 + re_w2)*w2 + \n  (2)*b*w1 +\n  (-1)*b*w2\n  \ny = rnorm(N, mean = lp, sd = 1.5) # create a continuous target variable\n\ndf <- data.frame(w, g=factor(g),b, y)\nhead(df)\nwith(df,boxplot(y~interaction(w,b)))\n\nlibrary(tidyverse)\ndf %>% transmute(\n  PID = paste0(\"PPT_\",formatC(g,width=2,flag=0)),\n  audio = fct_recode(factor(w),\n                     no_audio = \"a\",\n                     white_noise = \"b\",\n                     music = \"c\"),\n  headphones = fct_recode(factor(b),\n                          speakers = \"0\",\n                          anc_headphones = \"1\"),\n  SDMT = pmax(0,round(35 + scale(y)[,1]*12))\n) %>% arrange(PID,audio,headphones) -> ef_music\n\nef_music <- ef_music %>% group_by(PID) %>%\n  mutate(trial_n = paste0(\"Trial_\",formatC(sample(1:15),width=2,flag=0))) %>%\n  arrange(PID,trial_n) %>% ungroup()\n\nefrep <- slice_sample(ef_music, prop = .8) %>% select(PID,trial_n,audio,headphones,SDMT)\n\n# write_csv(efrep, file=\"../../data/ef_replication.csv\")\n```\n\n```{r}\n#| echo: false\ntibble(variable=names(efrep),\n       description = c(\n         \"Participant ID\",\n         \"Trial Number (1-15)\",\n         \"Audio heard during the test ('no_audio', 'white_noise','music')\",\n         \"Whether the participant listened via speakers in the room or via noise cancelling headphones\",\n         \"Symbol Digit Modalities Test (SDMT) score\")\n) %>% gt()\n```\n\n:::\n\n`r qbegin(\"7\")`\nHow many participants are there in the data?   \nHow many have complete data (15 trials)?  \nWhat is the average number of trials that participants completed? What is the minimum?   \nDoes every participant have _some_ data for each type of audio?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThe `count()` function will likely be useful here. \n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| eval: false\nefrep <- read_csv(\"https://uoepsy.github.io/data/ef_replication.csv\")\n```\n\nHere are the counts of trials for each participant. We can see that no participant completed all 15 trials. Everyone completed at least 10, and the median was 12. \n```{r}\nefrep %>% \n  count(PID) %>%\n  summary()\n```\n\nWe can add in `audio` to our counting to see that everyone has data from $\\geq 2$ trials for a given audio type.\n```{r}\nefrep %>% \n  count(PID,audio) %>%\n  summary()\n```\n\n`r solend()`\n\n\n`r qbegin(\"8\")`\nThe model below is sometimes referred to as the \"null model\" (or \"intercept only model\"). The grouping structure of the data is specified in the model, but nothing more.  \n\n```{r}\nnullmod <- lmer(SDMT ~ 1 + (1 | PID), data = efrep)\n```\n\nFit the model and examine the summary.  \nHow much of the variation in SDMT scores is down to participant grouping?  \n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nnullmod <- lmer(SDMT ~ 1 + (1 | PID), data = efrep)\nsummary(nullmod)\n```\n```{r}\n#| echo: false\nrr = as.data.frame(VarCorr(nullmod))[,4]\n```\n\n$\\frac{`r round(rr[1],2)`}{`r round(rr[1],2)`+`r round(rr[2],2)`} = `r round(rr[1]/sum(rr),2)`$, or `r round(rr[1]/sum(rr),2)*100`% of the variance in SDMT scores is explained by participant differences.  \n\nWe can check this matches (closely enough) with the `ICCbare()` function from last week:  \n```{r}\nlibrary(ICC)\nICCbare(x = PID, y = SDMT, data = efrep)\n```\n\n\n`r solend()`\n\n`r qbegin(\"9\")`\nSet the reference levels of the `audio` and `headphones` variables to \"no audio\" and \"speakers\" respectively.    \nFit a multilevel model to address the research question below.  \n\n> How do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used? \n \n\n::: {.callout-tip collapse=\"true\"}\n#### things to think about:  \n\n- what is our outcome variable of interest?\n- what are our predictor variables (and interactions?) that we are interested in?\n    - these should be in the fixed effects part.    \n- what is the clustering?\n    - this should be the random effects `(1 | cluster)` part\n- does audio type (`audio`) vary within clusters, or between?\n    - if so, we might be able to fit a random slope of `audio | cluster`. if not, then it doesn't make sense to do so.  \n- does delivery mode (`headphones`) vary within clusters, or between?\n      - if so, we might be able to fit a random slope of `headphones | cluster`. if not, then it doesn't make sense to do so. \n\n\n_If you get an error about model convergence, consider changing the optimiser (see the \"model estimation\" box)_\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nefrep <- efrep %>%\n  mutate(\n    audio = fct_relevel(factor(audio), \"no_audio\"),\n    headphones = fct_relevel(factor(headphones), \"speakers\")\n  )\n\n\nsdmt_mod <- lmer(SDMT ~ audio * headphones + \n              (1 + audio | PID), data = efrep,\n              REML = TRUE, control = lmerControl(optimizer=\"bobyqa\"))\nsummary(sdmt_mod)\n```\n\n`r solend()`\n\n`r qbegin(\"10\")`\nWe now have a model, but we don't have any p-values, confidence intervals, or inferential criteria on which to draw conclusions.  \n\nPick a method of your choosing and perform a test of/provide an interval for the relevant effect of interest.  \nProvide a brief write-up of the results along with a visualisation.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Options\n\nAs with normal regression, we have two main ways in which we can conduct inference. We can focus on our coefficients, or we can compare models.^[What is the difference between testing coefficients and comparing models? This is most easily seen when the model includes a categorical variable as a predictor. The `summary()` function would return, for each level of the predictor (excluding the reference level), a coefficient, its standard error, the t-statistic, and the p-value for a test of whether the coefficient is significantly different from 0.\nA model comparison between that model and a null model without the categorical predictor would collapse all levels into a single test across all levels of the predictor.]  \n\nThere are a whole load of different methods available for drawing inferences from multilevel models, which means it can be a bit of a never-ending rabbit hole. For the purposes of this course, we'll limit ourselves to these two:  \n\n\n|                  | df approximations                                                  | likelihood-based                                                    | \n| ---------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------- | \n| tests or CIs for model parameters | `library(parameters)`<br>`model_parameters(model, ci_method=\"kr\")` | `confint(model, type=\"profile\")`                                    | \n| model comparison<br><small>(different fixed effects, same random effects)</small> | `library(pbkrtest)`<br>`KRmodcomp(model1,model0)`                  | `anova(model0,model)`                                               |\n|                  | fit models with `REML=TRUE`.<br>good option for small samples      | fit models with `REML=FALSE`.<br>needs large N at both levels (40+) | \n\n\n:::\n\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nIn this case we have $n=30$ participants (our level 2 sample size), and each participant has at most 15 observations (and some have fewer). These numbers are possibly a bit small for standard likelihood based methods. We would be better off using models fitted with REML because they will provide more accurate estimates of the variance components (the `(1 + audio | PID)` bit), and so better estimates of the standard errors for our fixed effects. \n\nThe easiest option here is to use the __parameters__ package:  \n```{r}\nlibrary(parameters)\nmodel_parameters(sdmt_mod, ci_method=\"kr\")\n```\n\nAnd if we want to go down the model comparison route, we just need to isolate the relevant part(s) of the model that we are interested in. \nFor instance, to test the interaction:  \n  \n```{r}\nsdmt_res <- lmer(SDMT ~ audio + headphones + \n                   (1 + audio | PID), data = efrep,\n                 REML = TRUE, control = lmerControl(optimizer=\"bobyqa\"))\nlibrary(pbkrtest)\nKRmodcomp(largeModel = sdmt_mod, smallModel = sdmt_res)\n```\n\n\n```{r}\n#| echo: false\nres = as.data.frame(model_parameters(sdmt_mod, ci_method=\"kr\"))\nres[,c(2,3,5,6,7,8)] <- apply(res[,c(2,3,5,6,7,8)], 2, function(x) round(x, 2))\nres[,9] <- format.pval(res[,9],eps=.001,digits=2)\nres[,9][!grepl(\"<\",res[,9])] <- paste0(\"=\",res[,9][!grepl(\"<\",res[,9])])\n```\n\n:::int\nSDMT scores were modelled using linear mixed effects regression, with fixed effects of audio-type (no audio/white noise/music, treatment coded with no audio as the reference level), audio delivery (speakers/ANC-headphones, treatment coded with speakers as the reference level) and their interaction. Participant-level random intercepts and random slopes of audio-type were also included. The model was fitted using the **lme4** package in R, and estimated with restricted estimation maximum likelihood (REML) using the bobyqa optimizer. Denominator degrees of freedom for all tests were approximated using the Kenward-Rogers method.  \n\nInclusion of the interaction between headphones and audio-type was found to improve model fit ($F(2, 26.8) = 10.64, p < .001$), suggesting that the interference of different types of audio on executive functioning is dependent upon whether the audio is presented through ANC-headphones or through speakers.  \nParticipants not wearing headphones and presented with no audio scored on average `r res[1,2]` on the SDMT. Listening to music via speakers was associated with lower scores ($\\beta = `r res[3,2]`, SE = `r res[3,3]`, t(`r res[3,8]`)=`r res[3,7]`, p `r res[3,9]`$) compared to no audio. White noise played via speakers was not associated with a difference in performance on the SDMT compared to no audio.  \n\nWithout any audio playing, wearing ANC-headphones was associated with higher SDMT scores compared to no headphones ($\\beta = `r res[4,2]`, SE = `r res[4,3]`, t(`r res[4,8]`)=`r res[4,7]`, p `r res[4,9]`$). This difference between headphones and speakers was also evident when listening to white-noise ($\\beta = `r res[5,2]`, SE = `r res[5,3]`, t(`r res[5,8]`)=`r res[5,7]`, p `r res[5,9]`$). The apparent detrimental influence of music was not found to differ depending on whether headphones were worn ($\\beta = `r res[6,2]`, p `r res[6,9]`$).  \n\nThese results suggest that while music appears to interfere with executive functioning (resulting in lower SDMT scores) regardless of whether it is heard through headphones or speakers, listening to white noise may actually improve executive functioning, but only when presented via headphones. Furthermore, there appears to be benefits for executive functioning from wearing ANC-headphones even when not-listening to audio, perhaps due to the noise cancellation. The pattern of findings are displayed in @fig-efplot.  \n\n\n```{r}\n#| label: fig-efplot\n#| fig-cap: \"Interaction between the type (no audio/white noise/music) and the delivery (speakers/ANC headphones) on executive functioning task (SDMT)\"\nlibrary(sjPlot)\nplot_model(sdmt_mod, type=\"int\")\n```\n:::\n`r solend()`\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n  \n  \n# Exercises: Longitudinal  \n\n```{r}\n#| eval: false\n#| echo: false\nload(url(\"https://uoepsy.github.io/data/WeightMaintain3.rda\"))\nset.seed(993)\nWeightMaintain3 %>% mutate(\n  wellbeing = scale(WeightChange)[,1]*4,\n  Condition = fct_recode(factor(Condition),\n                         \"4day_week\"=\"None\",\n                         \"unlimited_leave\"=\"MR\",\n                         \"none\"=\"ED\"),\n  TimePoint = Assessment+1\n) %>% \n  group_by(ID) %>% mutate(\n    int = rnorm(n(),40,3.5),\n    Wellbeing = round(wellbeing + int)\n  ) %>% ungroup %>% select(ID, TimePoint, Condition, Wellbeing) -> wellbeingwork3\n\n#save(wellbeingwork3,file=\"../../data/wellbeingwork3.rda\")\n```\n\n\nAnother very crucial advantage of these methods is that we can use them to study how people change over time.  \n\n:::frame\n__Data: Wellbeing in Work__  \n\nThe \"Wellbeing in Work\" dataset contains information on employees who were randomly assigned to one of three employment conditions:\n\n* control: No change to employment. Employees continue at 5 days a week, with standard allocated annual leave quota.    \n* unlimited_leave : Employees were given no limit to their annual leave, but were still expected to meet required targets as specified in their job description. \n* fourday_week: Employees worked a 4 day week for no decrease in pay, and were still expected to meet required targets as specified in their job description.\n\nWellbeing was was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post.  \n\nThe researchers had two main questions: \n\n- Q1): Overall, did the participants' wellbeing stay the same or did it change?\n- Q2): Did the employment condition groups differ in the how wellbeing changed over the assessment period?   \n\nThe data is available, in **.rda** format, at [https://uoepsy.github.io/data/wellbeingwork3.rda](https://uoepsy.github.io/data/wellbeingwork3.rda). You can read it directly into your R environment using: \n```{}\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\n```\nAfter running the code above you will find the data in an object called `wellbeingwork3` in your environment.  \n\n:::\n\n`r qbegin(\"11\")`  \n\n> Q1): Overall, did the participants' wellbeing stay the same or did it change?  \n\n\nEach of our participants have measurements at 4 assessments. \nWe need to think about what this means for the **random effects** that we will include in our model (our **random effect structure**). Would we like our models to accommodate individuals to vary in their overall wellbeing, to vary in how they change in wellbeing over the course of the assessment period, or both?\n\nTo investigate whether wellbeing changed over the course of the assessments, or whether it stayed the same, we can fit and compare 2 models:  \n\n1. A null model. \n2. A model with wellbeing predicted by time point.  \n\nAnd we can then compare them in terms of model fit (as mentioned above, there are lots of different ways we might do this).  \n\nOur sample size here (180 participants, each with 4 observations) is reasonably large given the relative simplicity of our model. We might consider running a straightforward Likelihood Ratio Test using `anova(restricted_model, full_model)` to compare our two models (in which case we should fit them with `REML=FALSE`)\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- Remember, we shouldn't use likelihood ratio tests to compare models with different random effect structures.  \n- (For now, don't worry too much about \"singular fits\". We'll talk more about how we might deal with them next week!)\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\nhead(wellbeingwork3)\n```\n\n\nThis is our null model:\n```{r}\nm.null <- lmer(Wellbeing ~ 1 + (1 | ID), data=wellbeingwork3, REML=FALSE)\nsummary(m.null)\n```\nWe can see the `4.76 / (4.76 + 22.48)`, or `r (4.76 / (4.76 + 22.48)) %>% round(.,2)` of the total variance in wellbeing is attributable to participant-level variation. \n\nNow lets suppose we want to compare this null model with a model with an effect of `TimePoint` (to assess whether there is overall change over time).\nWhich model should we compare `m.null` to?  \n```{r}\n#| eval: false\nmodA <- lmer(Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\nmodB <- lmer(Wellbeing ~ 1 + TimePoint + (1 | ID), data=wellbeingwork3, REML=FALSE)\n```\nA comparison between `m.null` and `modA` will not be assessing the influence of _only_ the fixed effect of TimePoint (remember, we shouldn't compare models that differ in both fixed and random effects)  \nHowever, `modB` doesn't include our by-participant random effects of timepoint, so comparing this to `m.null` is potentially going to mis-attribute random deviations in participants' change to being an overall effect of timepoint.  \n\nIf we want to conduct a model comparison to isolate the average change over time (a fixed effect of `TimePoint`), we _might_ want to compare these two models:\n```{r}\nm.base0 <- lmer(Wellbeing ~ 1 + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\nm.base <- lmer(Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\n```\nThe first of these models is a bit weird to think about - how can we have by-participant random deviations of `TimePoint` if we don't have a fixed effect of `TimePoint`? That makes very little sense. What it is actually fitting is a model where the average participant has no effect of TimePoint. So the fixed effect is 0. \n\n```{r}\n# Straightforward LRT\nanova(m.base0, m.base)\n```\n\n`r solend()`\n\n`r qbegin(\"12\")`\n\n> Q: Did the employment condition groups differ in the how wellbeing changed over the assessment period?   \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nIt helps to break it down. There are two questions here:  \n\n  1. do groups differ overall?  \n  2. do groups differ over time?  \n\nWe can begin to see that we're asking two questions about the `Condition` variable here: \"is there an effect of Condition?\" and \"Is there an interaction between TimePoint and Condition?\".  \n\nTry fitting two more models which incrementally build these levels of complexity, and compare them (perhaps to one another, perhaps to models from the previous question - think about what each comparison is testing!)  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nm.int <- lmer(Wellbeing ~ 1 + TimePoint + Condition + (1 + TimePoint | ID), \n              data=wellbeingwork3, REML=FALSE)\nm.full <- lmer(Wellbeing ~ 1+ TimePoint*Condition + (1 + TimePoint | ID), \n               data=wellbeingwork3, REML=FALSE)\n```\n\nWe're going to compare each model to the previous one to examine the improvement in fit due to inclusion of each parameter. \nWe could do this quickly with\n```{r}\nanova(m.base0, m.base, m.int, m.full)\n```\n:::int \nConditions differed overall in wellbeing change $\\chi^2(2)=11.39, p = .003$  \nConditions differed in change over assessment period $\\chi^2(2)=23.71, p < .001$\n:::\n\n`r solend()`\n\n\n`r qbegin(\"13\")`\n\nVisualise the model estimated change in wellbeing over time for each Condition.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThere are lots of ways you can visualise the model, try a couple: \n\n1. Using the **effects** package, this might help: `as.data.frame(effect(\"TimePoint*Condition\", model))`  \n2. We can also use **sjPlot**, as we have seen in DAPR2  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nUsing the `effect()` function (and then adding the means and SEs from the original data):  \n```{r}\nef <- as.data.frame(effect(\"TimePoint*Condition\", m.full))\n\nggplot(ef, aes(TimePoint, fit, color=Condition)) + \n  geom_line() +\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=Condition), alpha=.2)+\n  stat_summary(data=wellbeingwork3, aes(y=Wellbeing), \n               fun.data=mean_se, geom=\"pointrange\", size=1) +\n  theme_bw()\n```\n\nAdditionally, __sjPlot__ can give us the model fitted values, but it's trickier to add on the observed means. We can add the raw data using `show.data=TRUE`, but that will make it a bit messier\n\n```{r}\nlibrary(sjPlot)\nplot_model(m.full, type=\"int\")\n```\n\n`r solend()`\n\n\n`r qbegin(\"14\")`\n\nExamine the parameter estimates and interpret them (i.e., what does each parameter represent?  \nCan you match them with parts of the plot obtained from `plot_model(m.full, type=\"int\")`?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWe can get the fixed effects using `fixef(model)`, and we can also use `tidy(model)` from the **broom.mixed** package, and similar to `lm` models in DAPR2, we can pull out the bit of the `summary()` using `summary(model)$coefficients`.   \n\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| echo: false\nround(coef(summary(m.full)), 3)\n```\n\n* `(Intercept)` ==> Wellbeing at baseline in 'control' group.  \n* `TimePoint`  ==> Slope of wellbeing change in 'control' group.  \n* `Conditionunlimited_leave` ==> baseline wellbeing difference from 'unlimited_leave' group relative to 'control' group.  \n* `Conditionfourday_week` ==> baseline wellbeing difference from 'fourday_week' group relative to 'control' group.  \n* `TimePoint:Conditionunlimited_leave`  ==> slope of wellbeing change in 'unlimited_leave' group relative to 'control' group.  \n* `TimePoint:Conditionfourday_week`  ==> slope of wellbeing change in 'fourday_week' group relative to 'control' group.  \n\n```{r}\n#| label: fig-wblplot\n#| fig-cap: \"Well-being over time, for employees working in different conditions\"  \nplot_model(m.full, type=\"int\")\n```\n\n* `(Intercept)` ==> the height of the red line at timepoint 0.  \n* `TimePoint`  ==> slope of the red line (basically flat).  \n* `Conditionunlimited_leave` ==> the difference at timepoint 0 in height of the blue line from height of the red line (basically 0, but blue line starts ever so slightly below the red).   \n* `Conditionfourday_week` ==> the difference at timepoint 0 in height of the green line from height of the red line (also very small, but it is a bit further below the red than the blue is).   \n* `TimePoint:Conditionunlimited_leave` ==> the difference from slope of red line to slope of blue line. i.e. while the red line goes -0.023 for every 1 across, the blue line goes $-0.023+1.357=1.334$ up for every 1 across.  \n* `TimePoint:Conditionfourday_week`  ==> difference from slope of red line to slope of green line. i.e. while the red line goes -0.023 for every 1 across, the green line goes $-0.023+2.282=2.259$ up for every 1 across. \n\n\n:::int\nCompared to the control group, wellbeing increased by 1.36 points/year more for employees with unlimited leave, and by 2.28 points/year for employees on the 4 day week.  \n:::\n\n\n`r solend()`\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n```\n\n\n:::{.callout-caution collapse=\"true\"}\n## A Note on terminology\n\nThe methods we're going to learn about in the first five weeks of this course are known by lots of different names: \"multilevel models\"; \"hierarchical linear models\"; \"mixed-effect models\"; \"mixed models\"; \"nested data models\"; \"random coefficient models\"; \"random-effects models\"; \"random parameter models\"... and so on).   \n\nWhat the idea boils down to is that **model parameters vary at more than one level.** This week, we're going to explore what that means.  \n\nThroughout this course, we will tend to use the terms \"mixed effect model\", \"linear mixed model (LMM)\" and \"multilevel model (MLM)\" interchangeably. \n\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Multilevel Model Notation  \n\nMultilevel Models (MLMs) (or \"Linear Mixed Models\" (LMMs)) take the approach of allowing the groups/clusters to vary around our $\\beta$ estimates. \n\nIn the lectures, we saw this as:\n\n$$\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n$$\n\nWe are now assuming $\\color{orange}{\\zeta_0}$, $\\color{orange}{\\zeta_1}$, and $\\varepsilon$ to be normally distributed with a mean of 0, and we denote their variances as $\\sigma_{\\color{orange}{\\zeta_0}}^2$, $\\sigma_{\\color{orange}{\\zeta_1}}^2$, $\\sigma_\\varepsilon^2$ respectively. \n\nThe $\\color{orange}{\\zeta}$ components also get termed the \"random effects\" part of the model, Hence names like \"random effects model\", etc. \n\n:::\n\n::: {.callout-note collapse=\"true\"}\n#### Alternative (\"mixed effect\") notation\n\nMany people use the symbol $u$ in place of $\\zeta$, and in various resources, you are likely to see $\\alpha$ used to denote the intercept instead of $\\beta_0$.  \n\nSometimes, you will see the levels collapsed into one equation, as it might make for more intuitive reading.  \n\nThis often fits with the name \"mixed effects\" for these models, as the \"effect\" of a predictor is a mix of both a fixed and a random part:  \n\n$$\\color{red}{y_{ij}} = \\underbrace{(\\gamma_{00} + \\color{orange}{\\zeta_{0i}})}_{\\color{blue}{\\beta_{0i}}} \\cdot 1 + \\underbrace{(\\gamma_{10} + \\color{orange}{\\zeta_{1i}})}_{\\color{blue}{\\beta_{1i}}} \\cdot x_{ij}  +  \\varepsilon_{ij} \\\\$$\n\nIn words, this equation is denoting: \n$$\n\\begin{align}\n\\color{red}{\\text{outcome}}\\color{black} = &(\\color{blue}{\\text{overall intercept}}\\color{black} + \\color{orange}{\\text{random adjustment to intercept by group}}\\color{black}) \\cdot 1 \\, + \\\\\n&( \\color{blue}{\\text{overall slope}}\\color{black} + \\color{orange}{\\text{random adjustment to slope by group}} \\color{black}) \\cdot \\text{predictor}\\, + \\\\\n& \\text{residual} \\\\\n\\end{align}\n$$\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Fitting Multilevel Models: Model Formula\n\nTo fit multilevel models, we're going to use the `lme4` package, and specifically the functions `lmer()` and `glmer()`.^[\"(g)lmer\" here stands for \"(generalised) linear mixed effects regression\".] \n\nYou will have seen some use of these functions in the lectures. The broad syntax is:  \n<br>\n<div style=\"margin-left:50px;\">\nlmer(**_formula_**,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = *logical*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(*options*) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\n\nWe write the first bit of our **formula** just the same as our old friend the normal linear model `y ~ 1 + x + x2 + ...`, where `y` is the name of our outcome variable, `1` is the intercept (which we don't have to explicitly state as it will be included anyway) and `x`, `x2` etc are the names of our explanatory variables.  \n\nWith **lme4**, we now have the addition of __random effect terms__, specified in parenthesis with the `|` operator (the vertical line | is often found to the left of the z key on QWERTY keyboards). We use the `|` operator to separate the parameters (intercept, slope etc.) on the LHS, from the grouping variable(s) on the RHS, by which we would like to model these parameters as varying.  \n\n:::statbox\n__Random Intercepts__  \nLet us suppose that we wish to model our intercept not as a fixed constant, but as varying randomly according to some grouping around a fixed center. \nWe can such a model by allowing the intercept to vary by our grouping variable (`g` below): \n\n<center>`lmer(y ~ 1 + x + (1|g), data = df)`</center>\n$$\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{Y_{ij}} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot X_{ij}} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\end{align}\n$$\n:::\n\n:::statbox\n__Random Intercepts and Slopes__  \nBy extension we can also allow the effect `y~x` to vary between groups, by including the `x` on the left hand side of `|` in the random effects part of the call to `lmer()`.\n\n<center>`lmer(y ~ 1 + x + (1 + x |g), data = df)`</center>\n$$\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\end{align}\n$$\n:::\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Fitting Multilevel Models: Model Estimation\n\nWe can choose whether to estimate our model parameters with ML (maximum likelihood) or REML (restricted maximum likelihood) with the `REML` argument of `lmer()`:  \n\n<br>\n<div style=\"margin-left:50px;\">\nlmer(*formula*,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = **_logical_**, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(*options*) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\n`lmer()` models are by default fitted with REML, which tends to be better for small samples.    \n\n### Maximum Likelihood (ML)  \n\nRemember back to DAPR2 when we introduced logistic regression, and we briefly discussed **Maximum likelihood estimation** in an explanation of how models are fitted.  \n\nThe key idea of maximum likelihood estimation (MLE) is that we (well, the computer) iteratively finds the set of estimates for our model which it considers to best reproduce our observed data. Recall our simple linear regression model of how time spent outdoors (hrs per week) is associated with mental wellbeing: \n$$\n\\color{red}{Wellbeing_i} = \\color{blue}{\\beta_0 \\cdot{} 1 + \\beta_1 \\cdot{} OutdoorTime_{i}} + \\varepsilon_i\n$$\nThere are values of $\\beta_0$ and $\\beta_1$ and $\\sigma_\\varepsilon$ which maximise the probability of observing the data that we have. For linear regression, these we obtained these same values a different way, via minimising the sums of squares. This approach is not possible for more complex models (e.g., logistic)  which is why we turn to MLE.  \n\n:::statbox\nTo read about the subtle difference between \"likelihood\" and \"probability\", you can find a short explanation [here](./lvp.html){target=\"_blank\"}\n:::\n\nIf we are estimating just one single parameter (e.g. a mean), then we can imagine the process of maximum likelihood estimation in a one-dimensional world - simply finding the top of the curve: \n```{r}\n#| label: fig-mle\n#| echo: false\n#| out.width: \"350px\"\n#| fig-cap: \"MLE\"\nknitr::include_graphics(\"images/intro/mle.png\")\n```\nHowever, our typical models estimate a whole bunch of parameters. The simple regression model above is already having to estimate $\\beta_0$, $\\beta_1$ and $\\sigma_\\varepsilon$, and our multi-level models have far more! With lots of parameters being estimated and all interacting to influence the likelihood, our nice curved line becomes a complex surface (see Left panel of @fig-multisurf). So what we (our computers) need to do is find the maximum, but avoid local maxima and singularities (see @fig-maxima). \n```{r}\n#| label: fig-multisurf\n#| echo: false\n#| out.width: \"49%\" \n#| fig-cap: \"MLE for a more complex model\"\n#| fig-align: 'center'\nknitr::include_graphics(\"images/multisurftb.png\")\n```\n\n### Restricted Maximum Likelihood (REML)\n\nWhen it comes to estimating multilevel models, maximum likelihood will consider the fixed effects as fixed, known values when it estimates the variance components (the random effect variances). This leads to biased estimates of the variance components, specifically biasing them toward being too small, especially if $n_\\textrm{clusters} - n_\\textrm{level 2 predictors} - 1 < 50$. This leads to the standard errors of the fixed effects being too small, thereby inflating our type 1 error rate (i.e. greater chance of incorrectly rejecting our null hypothesis).  \n\nRestricted Maximum Likelihood (REML) is a method that separates the estimation of fixed and random parts of the model, leading to unbiased estimates of the variance components.   \n\n\n:::\n<!-- :::sticky -->\n<!-- __Model Comparisons in MLM__ -->\n\n<!-- When we compare models that differ in their fixed effects via comparing model deviance (e.g. the likelihood ratio), REML should __not__ be used as only the variance components are included in the likelihood. Functions like `anova()` will automatically refit your models with `ML` for you, but it is worth checking.    -->\n\n<!-- We __cannot__ compare (either with ML or REML) models that differ in both the fixed and random parts.  -->\n\n<!-- ::: -->\n\n:::{.callout-note collapse=\"true\"}\n## Fitting Multilevel Models: Model Convergence\n\nAlongside the ML/REML choice for model estimation, we have some control over the underlying algorithm that is used to move around/search the likelihood surface for our estimates. We'll learn more about this next week.  \n\n<br>\n<div style=\"margin-left:50px;\">\nlmer(*formula*,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = *logical*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(**_options_**) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\n\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a *convergence warning*.  There are lots of different ways to [deal with these](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html) (to try to rule out hypotheses about what is causing them).  \n\nFor the time being, if `lmer()` gives you convergence errors, you could try changing the optimizer. Bobyqa is a good one: add `control = lmerControl(optimizer = \"bobyqa\")` when you run your model.  \n\n```{r eval=F}\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))\n```\n\n\n:::statbox\n__What *is* a convergence warning??__  \n\nThere are different techniques for maximum likelihood estimation, which we apply by using different 'optimisers'. Technical problems to do with **model convergence** and **'singular fit'** come into play when the optimiser we are using either can't find a suitable maximum, or gets stuck in a singularity (think of it like a black hole of likelihood, which signifies that there is not enough variation in our data to construct such a complex model).  \n\n```{r}\n#| label: fig-maxima\n#| echo: false\n#| out.width: \"49%\"\n#| fig-cap: \"local/global maxima and singularities\"\n#| fig-align: \"center\"\nknitr::include_graphics(\"images/intro/mle2.png\")\n```\n\n:::\n:::\n<br>  \n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Exercises: Cross-Sectional\n\n:::frame\n__Data: Wellbeing Across Scotland__  \n\nRecall our dataset from last week, in which we used linear regression to determine how outdoor time (hours per week) is associated with wellbeing in different local authority areas (LAAs) of Scotland. We have data from various LAAs, from Glasgow City, to the Highlands.  \n\n```{r message=FALSE,warning=FALSE}\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n```\n```{r echo=FALSE, message=FALSE,warning=FALSE}\nlibrary(gt)\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\ntibble(variable=names(scotmw),\n       description=c(\"Participant ID\",\"Participant Name\",\"Local Authority Area\",\"Self report estimated number of hours per week spent outdoors\",\"Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\",\"LAA Population Density (people per square km)\")\n) %>% gt()\n```\n:::\n\n`r qbegin(\"1\")`\nUsing `lmer()` from the **lme4** package, fit a model predict `wellbeing` from `outdoor_time`, with by-LAA random intercepts.  \nPass the model to `summary()` to see the output. \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n#| include: false\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n```\n\n```{r}\nlibrary(lme4)\nri_model <- lmer(wellbeing ~ outdoor_time + (1 | laa), data = scotmw)\nsummary(ri_model)\n```\n`r solend()`\n\n`r qbegin(\"2\")`\nSometimes the easiest way to start understanding your model is to visualise it. \n \nLoad the package **broom.mixed**. Along with some handy functions `tidy()` and `glance()` which give us the information we see in `summary()`, there is a handy function called `augment()` which returns us the data in the model plus the fitted values, residuals, hat values, Cook's D etc..  \n\n:::{.callout-note collapse=true}\n#### broom.mixed  \n\n```{r}\n#| include: false\nmodel <- ri_model\n```\n\nthe broom.mixed package has some useful functions (just like those in the __broom__ package for `lm()`):  \n\n```{r}\nlibrary(broom.mixed)\nglance(model) # for overall model stats\ntidy(model) # for parameter stats\naugment(model) # observation level stuff (data + model)\n```\n:::\n\n\nAdd to the code below to plot the model fitted values, and color them according to LAA. \n(you will need to edit `ri_model` to be whatever name you assigned to your model).\n\n```{r eval=FALSE}\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = ...... \n```\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line()\n```\n`r solend()`\n\n\n`r qbegin(\"3\")`\nWe have just fitted the model:\n$$\n\\begin{align}\n& \\text{For person } j \\text{ in LAA } i \\\\\n& \\color{red}{\\textrm{Wellbeing}_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot \\textrm{Outdoor Time}_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\end{align}\n$$\n\nFor our estimates of $\\gamma_{00}$ (the fixed value around which LAA intercepts vary) and $\\beta_1$ (the fixed estimate of the relationship between wellbeing and outdoor time), we can use `fixef()`.  \n```{r}\nfixef(ri_model)\n```\nCan you add to the plot in the previous question, a thick black line with the intercept and slope given by `fixef()`?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n**Hint:** `geom_abline()`\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_abline(intercept = fixef(ri_model)[1], slope = fixef(ri_model)[2], lwd = 2)\n```\n`r solend()`\n\n`r qbegin(\"4\")`\nBy now, you should have a plot which looks more or less like the left-hand figure below (we have added on the raw data - the points).  \n<div style=\"display:inline-block; width: 55%;vertical-align: top;\">\n```{r}\n#| label: fig-modfit\n#| echo: false\n#| fig.asp: 1\n#| fig-cap: \"Model fitted values\"\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_abline(intercept = fixef(ri_model)[1], slope = fixef(ri_model)[2], lwd = 2)+\n  geom_point(aes(y=wellbeing), alpha=.4)\n```\n</div>\n<div style=\"display:inline-block; width: 40%;vertical-align: top;\">\n```{r}\n#| label: fig-lmersummap\n#| echo: false\n#| out.width: \"400px\"\n#| fig-cap: \"Summary model output<br>lmer(wellbeing~1 + outdoor_time + (1|laa),<br>data = scotmw)\"\nknitr::include_graphics(\"images/intro/summarylmer2.png\")\n```\n</div>\n<br>\n<br>\nMatch the coloured sections Yellow (W), Red (X), Blue (Y), and Orange (Z) in @fig-lmersummap to the descriptions below of @fig-modfit A through D. \n\nA) where the black line cuts the y axis\nB) the standard deviation of the distances from all the individual LAA lines to the black line\nC) the slope of the black line\nD) the standard deviation of the distances from all the individual observations to the line for the LAA to which it belongs.\n\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n+ A = Blue (Y)\n+ B = Yellow (W)\n+ C = Orange (Z)\n+ D = Red (X)\n\n`r solend()`\n\n\n`r qbegin(\"5\")`\nMatch the colours sections and descriptions from the previous question, to the mathematical terms in the model equation:  \n\n$$\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{Wellbeing_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot OutdoorTime_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\quad \\\\\n& \\text{where} \\\\\n& \\color{orange}{\\zeta_0}\\color{black} \\sim N(0, \\sigma_{\\color{orange}{\\zeta_{0}}}\\color{black})  \\text{ independently} \\\\\n& \\varepsilon \\sim N(0, \\sigma_{\\varepsilon}) \\text{ independently} \\\\\n\\end{align}\n$$\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n+ A = Blue (Y) = $\\gamma_{00}$  \n+ B = Yellow (W) = $\\sigma_{\\color{orange}{\\zeta_{0}}}$\n+ C = Orange (Z) = $\\beta_{1}$     \n+ D = Red (X) = $\\sigma_{\\varepsilon}$    \n\n\n\n`r solend()`\n\n\n`r qbegin(\"6\")`\nFit a model which allows *also* (along with the intercept) the effect of `outdoor_time` to vary by-LAA.   \n\nThen, using `augment()` again, plot the model fitted values. What do you think you will see?  \nDoes it look like this model better represents the individual LAAs? Take a look at, for instance, Glasgow City.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nrs_model <- lmer(wellbeing ~ 1 + outdoor_time + (1 + outdoor_time | laa), data = scotmw)\n\naugment(rs_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_point(aes(y=wellbeing), alpha=.4)\n```\n\n`r solend()`\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Exercises: Repeated Measures\n\nWhile the wellbeing example considers the groupings or 'clusters' of different LAAs, a more common grouping in psychological research is that of several observations belonging to the same individual. One obvious benefit of this is that we can collect many more observations with fewer participants but control for the resulting dependency of observations. \n\n:::frame\n__Data: REPLICATION Audio/SDMT study__  \n\n\nRecall the data from the previous week, from an experiment in which executive functioning was measured (via the SDMT) for people when listening to different types of audio, either via normal speakers or via noise-cancelling headphones.  \n\nThis week, we have data from a __replication__ of that study, in which the researchers managed to recruit 30 participants. Unfortunately, some participants did not complete all the trials, so we have an unbalanced design. The data is available at [https://uoepsy.github.io/data/ef_replication.csv](https://uoepsy.github.io/data/ef_replication.csv).  \n```{r}\n#| include: false\nset.seed(5)\nn_groups = 30\nN = n_groups*3*5\ng = rep(1:n_groups, e = N/n_groups)\n\nw = rep(rep(letters[1:3],5),n_groups)\nw1 = model.matrix(lm(rnorm(N)~w))[,2]\nw2 = model.matrix(lm(rnorm(N)~w))[,3]\n\nb = rep(0:1, e = N/2)\n\nre0 = rnorm(n_groups, sd = 2)[g]\nre_w1  = rnorm(n_groups, sd = 1)[g]\nre_w2  = rnorm(n_groups, sd = 1)[g]\n\nlp = (0 + re0) + \n  (3)*b + \n  (0 + re_w1)*w1 +\n  (-2 + re_w2)*w2 + \n  (2)*b*w1 +\n  (-1)*b*w2\n  \ny = rnorm(N, mean = lp, sd = 1.5) # create a continuous target variable\n\ndf <- data.frame(w, g=factor(g),b, y)\nhead(df)\nwith(df,boxplot(y~interaction(w,b)))\n\nlibrary(tidyverse)\ndf %>% transmute(\n  PID = paste0(\"PPT_\",formatC(g,width=2,flag=0)),\n  audio = fct_recode(factor(w),\n                     no_audio = \"a\",\n                     white_noise = \"b\",\n                     music = \"c\"),\n  headphones = fct_recode(factor(b),\n                          speakers = \"0\",\n                          anc_headphones = \"1\"),\n  SDMT = pmax(0,round(35 + scale(y)[,1]*12))\n) %>% arrange(PID,audio,headphones) -> ef_music\n\nef_music <- ef_music %>% group_by(PID) %>%\n  mutate(trial_n = paste0(\"Trial_\",formatC(sample(1:15),width=2,flag=0))) %>%\n  arrange(PID,trial_n) %>% ungroup()\n\nefrep <- slice_sample(ef_music, prop = .8) %>% select(PID,trial_n,audio,headphones,SDMT)\n\n# write_csv(efrep, file=\"../../data/ef_replication.csv\")\n```\n\n```{r}\n#| echo: false\ntibble(variable=names(efrep),\n       description = c(\n         \"Participant ID\",\n         \"Trial Number (1-15)\",\n         \"Audio heard during the test ('no_audio', 'white_noise','music')\",\n         \"Whether the participant listened via speakers in the room or via noise cancelling headphones\",\n         \"Symbol Digit Modalities Test (SDMT) score\")\n) %>% gt()\n```\n\n:::\n\n`r qbegin(\"7\")`\nHow many participants are there in the data?   \nHow many have complete data (15 trials)?  \nWhat is the average number of trials that participants completed? What is the minimum?   \nDoes every participant have _some_ data for each type of audio?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThe `count()` function will likely be useful here. \n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| eval: false\nefrep <- read_csv(\"https://uoepsy.github.io/data/ef_replication.csv\")\n```\n\nHere are the counts of trials for each participant. We can see that no participant completed all 15 trials. Everyone completed at least 10, and the median was 12. \n```{r}\nefrep %>% \n  count(PID) %>%\n  summary()\n```\n\nWe can add in `audio` to our counting to see that everyone has data from $\\geq 2$ trials for a given audio type.\n```{r}\nefrep %>% \n  count(PID,audio) %>%\n  summary()\n```\n\n`r solend()`\n\n\n`r qbegin(\"8\")`\nThe model below is sometimes referred to as the \"null model\" (or \"intercept only model\"). The grouping structure of the data is specified in the model, but nothing more.  \n\n```{r}\nnullmod <- lmer(SDMT ~ 1 + (1 | PID), data = efrep)\n```\n\nFit the model and examine the summary.  \nHow much of the variation in SDMT scores is down to participant grouping?  \n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nnullmod <- lmer(SDMT ~ 1 + (1 | PID), data = efrep)\nsummary(nullmod)\n```\n```{r}\n#| echo: false\nrr = as.data.frame(VarCorr(nullmod))[,4]\n```\n\n$\\frac{`r round(rr[1],2)`}{`r round(rr[1],2)`+`r round(rr[2],2)`} = `r round(rr[1]/sum(rr),2)`$, or `r round(rr[1]/sum(rr),2)*100`% of the variance in SDMT scores is explained by participant differences.  \n\nWe can check this matches (closely enough) with the `ICCbare()` function from last week:  \n```{r}\nlibrary(ICC)\nICCbare(x = PID, y = SDMT, data = efrep)\n```\n\n\n`r solend()`\n\n`r qbegin(\"9\")`\nSet the reference levels of the `audio` and `headphones` variables to \"no audio\" and \"speakers\" respectively.    \nFit a multilevel model to address the research question below.  \n\n> How do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used? \n \n\n::: {.callout-tip collapse=\"true\"}\n#### things to think about:  \n\n- what is our outcome variable of interest?\n- what are our predictor variables (and interactions?) that we are interested in?\n    - these should be in the fixed effects part.    \n- what is the clustering?\n    - this should be the random effects `(1 | cluster)` part\n- does audio type (`audio`) vary within clusters, or between?\n    - if so, we might be able to fit a random slope of `audio | cluster`. if not, then it doesn't make sense to do so.  \n- does delivery mode (`headphones`) vary within clusters, or between?\n      - if so, we might be able to fit a random slope of `headphones | cluster`. if not, then it doesn't make sense to do so. \n\n\n_If you get an error about model convergence, consider changing the optimiser (see the \"model estimation\" box)_\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nefrep <- efrep %>%\n  mutate(\n    audio = fct_relevel(factor(audio), \"no_audio\"),\n    headphones = fct_relevel(factor(headphones), \"speakers\")\n  )\n\n\nsdmt_mod <- lmer(SDMT ~ audio * headphones + \n              (1 + audio | PID), data = efrep,\n              REML = TRUE, control = lmerControl(optimizer=\"bobyqa\"))\nsummary(sdmt_mod)\n```\n\n`r solend()`\n\n`r qbegin(\"10\")`\nWe now have a model, but we don't have any p-values, confidence intervals, or inferential criteria on which to draw conclusions.  \n\nPick a method of your choosing and perform a test of/provide an interval for the relevant effect of interest.  \nProvide a brief write-up of the results along with a visualisation.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Options\n\nAs with normal regression, we have two main ways in which we can conduct inference. We can focus on our coefficients, or we can compare models.^[What is the difference between testing coefficients and comparing models? This is most easily seen when the model includes a categorical variable as a predictor. The `summary()` function would return, for each level of the predictor (excluding the reference level), a coefficient, its standard error, the t-statistic, and the p-value for a test of whether the coefficient is significantly different from 0.\nA model comparison between that model and a null model without the categorical predictor would collapse all levels into a single test across all levels of the predictor.]  \n\nThere are a whole load of different methods available for drawing inferences from multilevel models, which means it can be a bit of a never-ending rabbit hole. For the purposes of this course, we'll limit ourselves to these two:  \n\n\n|                  | df approximations                                                  | likelihood-based                                                    | \n| ---------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------- | \n| tests or CIs for model parameters | `library(parameters)`<br>`model_parameters(model, ci_method=\"kr\")` | `confint(model, type=\"profile\")`                                    | \n| model comparison<br><small>(different fixed effects, same random effects)</small> | `library(pbkrtest)`<br>`KRmodcomp(model1,model0)`                  | `anova(model0,model)`                                               |\n|                  | fit models with `REML=TRUE`.<br>good option for small samples      | fit models with `REML=FALSE`.<br>needs large N at both levels (40+) | \n\n\n:::\n\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nIn this case we have $n=30$ participants (our level 2 sample size), and each participant has at most 15 observations (and some have fewer). These numbers are possibly a bit small for standard likelihood based methods. We would be better off using models fitted with REML because they will provide more accurate estimates of the variance components (the `(1 + audio | PID)` bit), and so better estimates of the standard errors for our fixed effects. \n\nThe easiest option here is to use the __parameters__ package:  \n```{r}\nlibrary(parameters)\nmodel_parameters(sdmt_mod, ci_method=\"kr\")\n```\n\nAnd if we want to go down the model comparison route, we just need to isolate the relevant part(s) of the model that we are interested in. \nFor instance, to test the interaction:  \n  \n```{r}\nsdmt_res <- lmer(SDMT ~ audio + headphones + \n                   (1 + audio | PID), data = efrep,\n                 REML = TRUE, control = lmerControl(optimizer=\"bobyqa\"))\nlibrary(pbkrtest)\nKRmodcomp(largeModel = sdmt_mod, smallModel = sdmt_res)\n```\n\n\n```{r}\n#| echo: false\nres = as.data.frame(model_parameters(sdmt_mod, ci_method=\"kr\"))\nres[,c(2,3,5,6,7,8)] <- apply(res[,c(2,3,5,6,7,8)], 2, function(x) round(x, 2))\nres[,9] <- format.pval(res[,9],eps=.001,digits=2)\nres[,9][!grepl(\"<\",res[,9])] <- paste0(\"=\",res[,9][!grepl(\"<\",res[,9])])\n```\n\n:::int\nSDMT scores were modelled using linear mixed effects regression, with fixed effects of audio-type (no audio/white noise/music, treatment coded with no audio as the reference level), audio delivery (speakers/ANC-headphones, treatment coded with speakers as the reference level) and their interaction. Participant-level random intercepts and random slopes of audio-type were also included. The model was fitted using the **lme4** package in R, and estimated with restricted estimation maximum likelihood (REML) using the bobyqa optimizer. Denominator degrees of freedom for all tests were approximated using the Kenward-Rogers method.  \n\nInclusion of the interaction between headphones and audio-type was found to improve model fit ($F(2, 26.8) = 10.64, p < .001$), suggesting that the interference of different types of audio on executive functioning is dependent upon whether the audio is presented through ANC-headphones or through speakers.  \nParticipants not wearing headphones and presented with no audio scored on average `r res[1,2]` on the SDMT. Listening to music via speakers was associated with lower scores ($\\beta = `r res[3,2]`, SE = `r res[3,3]`, t(`r res[3,8]`)=`r res[3,7]`, p `r res[3,9]`$) compared to no audio. White noise played via speakers was not associated with a difference in performance on the SDMT compared to no audio.  \n\nWithout any audio playing, wearing ANC-headphones was associated with higher SDMT scores compared to no headphones ($\\beta = `r res[4,2]`, SE = `r res[4,3]`, t(`r res[4,8]`)=`r res[4,7]`, p `r res[4,9]`$). This difference between headphones and speakers was also evident when listening to white-noise ($\\beta = `r res[5,2]`, SE = `r res[5,3]`, t(`r res[5,8]`)=`r res[5,7]`, p `r res[5,9]`$). The apparent detrimental influence of music was not found to differ depending on whether headphones were worn ($\\beta = `r res[6,2]`, p `r res[6,9]`$).  \n\nThese results suggest that while music appears to interfere with executive functioning (resulting in lower SDMT scores) regardless of whether it is heard through headphones or speakers, listening to white noise may actually improve executive functioning, but only when presented via headphones. Furthermore, there appears to be benefits for executive functioning from wearing ANC-headphones even when not-listening to audio, perhaps due to the noise cancellation. The pattern of findings are displayed in @fig-efplot.  \n\n\n```{r}\n#| label: fig-efplot\n#| fig-cap: \"Interaction between the type (no audio/white noise/music) and the delivery (speakers/ANC headphones) on executive functioning task (SDMT)\"\nlibrary(sjPlot)\nplot_model(sdmt_mod, type=\"int\")\n```\n:::\n`r solend()`\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n  \n  \n# Exercises: Longitudinal  \n\n```{r}\n#| eval: false\n#| echo: false\nload(url(\"https://uoepsy.github.io/data/WeightMaintain3.rda\"))\nset.seed(993)\nWeightMaintain3 %>% mutate(\n  wellbeing = scale(WeightChange)[,1]*4,\n  Condition = fct_recode(factor(Condition),\n                         \"4day_week\"=\"None\",\n                         \"unlimited_leave\"=\"MR\",\n                         \"none\"=\"ED\"),\n  TimePoint = Assessment+1\n) %>% \n  group_by(ID) %>% mutate(\n    int = rnorm(n(),40,3.5),\n    Wellbeing = round(wellbeing + int)\n  ) %>% ungroup %>% select(ID, TimePoint, Condition, Wellbeing) -> wellbeingwork3\n\n#save(wellbeingwork3,file=\"../../data/wellbeingwork3.rda\")\n```\n\n\nAnother very crucial advantage of these methods is that we can use them to study how people change over time.  \n\n:::frame\n__Data: Wellbeing in Work__  \n\nThe \"Wellbeing in Work\" dataset contains information on employees who were randomly assigned to one of three employment conditions:\n\n* control: No change to employment. Employees continue at 5 days a week, with standard allocated annual leave quota.    \n* unlimited_leave : Employees were given no limit to their annual leave, but were still expected to meet required targets as specified in their job description. \n* fourday_week: Employees worked a 4 day week for no decrease in pay, and were still expected to meet required targets as specified in their job description.\n\nWellbeing was was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post.  \n\nThe researchers had two main questions: \n\n- Q1): Overall, did the participants' wellbeing stay the same or did it change?\n- Q2): Did the employment condition groups differ in the how wellbeing changed over the assessment period?   \n\nThe data is available, in **.rda** format, at [https://uoepsy.github.io/data/wellbeingwork3.rda](https://uoepsy.github.io/data/wellbeingwork3.rda). You can read it directly into your R environment using: \n```{}\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\n```\nAfter running the code above you will find the data in an object called `wellbeingwork3` in your environment.  \n\n:::\n\n`r qbegin(\"11\")`  \n\n> Q1): Overall, did the participants' wellbeing stay the same or did it change?  \n\n\nEach of our participants have measurements at 4 assessments. \nWe need to think about what this means for the **random effects** that we will include in our model (our **random effect structure**). Would we like our models to accommodate individuals to vary in their overall wellbeing, to vary in how they change in wellbeing over the course of the assessment period, or both?\n\nTo investigate whether wellbeing changed over the course of the assessments, or whether it stayed the same, we can fit and compare 2 models:  \n\n1. A null model. \n2. A model with wellbeing predicted by time point.  \n\nAnd we can then compare them in terms of model fit (as mentioned above, there are lots of different ways we might do this).  \n\nOur sample size here (180 participants, each with 4 observations) is reasonably large given the relative simplicity of our model. We might consider running a straightforward Likelihood Ratio Test using `anova(restricted_model, full_model)` to compare our two models (in which case we should fit them with `REML=FALSE`)\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- Remember, we shouldn't use likelihood ratio tests to compare models with different random effect structures.  \n- (For now, don't worry too much about \"singular fits\". We'll talk more about how we might deal with them next week!)\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\nhead(wellbeingwork3)\n```\n\n\nThis is our null model:\n```{r}\nm.null <- lmer(Wellbeing ~ 1 + (1 | ID), data=wellbeingwork3, REML=FALSE)\nsummary(m.null)\n```\nWe can see the `4.76 / (4.76 + 22.48)`, or `r (4.76 / (4.76 + 22.48)) %>% round(.,2)` of the total variance in wellbeing is attributable to participant-level variation. \n\nNow lets suppose we want to compare this null model with a model with an effect of `TimePoint` (to assess whether there is overall change over time).\nWhich model should we compare `m.null` to?  \n```{r}\n#| eval: false\nmodA <- lmer(Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\nmodB <- lmer(Wellbeing ~ 1 + TimePoint + (1 | ID), data=wellbeingwork3, REML=FALSE)\n```\nA comparison between `m.null` and `modA` will not be assessing the influence of _only_ the fixed effect of TimePoint (remember, we shouldn't compare models that differ in both fixed and random effects)  \nHowever, `modB` doesn't include our by-participant random effects of timepoint, so comparing this to `m.null` is potentially going to mis-attribute random deviations in participants' change to being an overall effect of timepoint.  \n\nIf we want to conduct a model comparison to isolate the average change over time (a fixed effect of `TimePoint`), we _might_ want to compare these two models:\n```{r}\nm.base0 <- lmer(Wellbeing ~ 1 + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\nm.base <- lmer(Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\n```\nThe first of these models is a bit weird to think about - how can we have by-participant random deviations of `TimePoint` if we don't have a fixed effect of `TimePoint`? That makes very little sense. What it is actually fitting is a model where the average participant has no effect of TimePoint. So the fixed effect is 0. \n\n```{r}\n# Straightforward LRT\nanova(m.base0, m.base)\n```\n\n`r solend()`\n\n`r qbegin(\"12\")`\n\n> Q: Did the employment condition groups differ in the how wellbeing changed over the assessment period?   \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nIt helps to break it down. There are two questions here:  \n\n  1. do groups differ overall?  \n  2. do groups differ over time?  \n\nWe can begin to see that we're asking two questions about the `Condition` variable here: \"is there an effect of Condition?\" and \"Is there an interaction between TimePoint and Condition?\".  \n\nTry fitting two more models which incrementally build these levels of complexity, and compare them (perhaps to one another, perhaps to models from the previous question - think about what each comparison is testing!)  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nm.int <- lmer(Wellbeing ~ 1 + TimePoint + Condition + (1 + TimePoint | ID), \n              data=wellbeingwork3, REML=FALSE)\nm.full <- lmer(Wellbeing ~ 1+ TimePoint*Condition + (1 + TimePoint | ID), \n               data=wellbeingwork3, REML=FALSE)\n```\n\nWe're going to compare each model to the previous one to examine the improvement in fit due to inclusion of each parameter. \nWe could do this quickly with\n```{r}\nanova(m.base0, m.base, m.int, m.full)\n```\n:::int \nConditions differed overall in wellbeing change $\\chi^2(2)=11.39, p = .003$  \nConditions differed in change over assessment period $\\chi^2(2)=23.71, p < .001$\n:::\n\n`r solend()`\n\n\n`r qbegin(\"13\")`\n\nVisualise the model estimated change in wellbeing over time for each Condition.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThere are lots of ways you can visualise the model, try a couple: \n\n1. Using the **effects** package, this might help: `as.data.frame(effect(\"TimePoint*Condition\", model))`  \n2. We can also use **sjPlot**, as we have seen in DAPR2  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nUsing the `effect()` function (and then adding the means and SEs from the original data):  \n```{r}\nef <- as.data.frame(effect(\"TimePoint*Condition\", m.full))\n\nggplot(ef, aes(TimePoint, fit, color=Condition)) + \n  geom_line() +\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=Condition), alpha=.2)+\n  stat_summary(data=wellbeingwork3, aes(y=Wellbeing), \n               fun.data=mean_se, geom=\"pointrange\", size=1) +\n  theme_bw()\n```\n\nAdditionally, __sjPlot__ can give us the model fitted values, but it's trickier to add on the observed means. We can add the raw data using `show.data=TRUE`, but that will make it a bit messier\n\n```{r}\nlibrary(sjPlot)\nplot_model(m.full, type=\"int\")\n```\n\n`r solend()`\n\n\n`r qbegin(\"14\")`\n\nExamine the parameter estimates and interpret them (i.e., what does each parameter represent?  \nCan you match them with parts of the plot obtained from `plot_model(m.full, type=\"int\")`?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWe can get the fixed effects using `fixef(model)`, and we can also use `tidy(model)` from the **broom.mixed** package, and similar to `lm` models in DAPR2, we can pull out the bit of the `summary()` using `summary(model)$coefficients`.   \n\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| echo: false\nround(coef(summary(m.full)), 3)\n```\n\n* `(Intercept)` ==> Wellbeing at baseline in 'control' group.  \n* `TimePoint`  ==> Slope of wellbeing change in 'control' group.  \n* `Conditionunlimited_leave` ==> baseline wellbeing difference from 'unlimited_leave' group relative to 'control' group.  \n* `Conditionfourday_week` ==> baseline wellbeing difference from 'fourday_week' group relative to 'control' group.  \n* `TimePoint:Conditionunlimited_leave`  ==> slope of wellbeing change in 'unlimited_leave' group relative to 'control' group.  \n* `TimePoint:Conditionfourday_week`  ==> slope of wellbeing change in 'fourday_week' group relative to 'control' group.  \n\n```{r}\n#| label: fig-wblplot\n#| fig-cap: \"Well-being over time, for employees working in different conditions\"  \nplot_model(m.full, type=\"int\")\n```\n\n* `(Intercept)` ==> the height of the red line at timepoint 0.  \n* `TimePoint`  ==> slope of the red line (basically flat).  \n* `Conditionunlimited_leave` ==> the difference at timepoint 0 in height of the blue line from height of the red line (basically 0, but blue line starts ever so slightly below the red).   \n* `Conditionfourday_week` ==> the difference at timepoint 0 in height of the green line from height of the red line (also very small, but it is a bit further below the red than the blue is).   \n* `TimePoint:Conditionunlimited_leave` ==> the difference from slope of red line to slope of blue line. i.e. while the red line goes -0.023 for every 1 across, the blue line goes $-0.023+1.357=1.334$ up for every 1 across.  \n* `TimePoint:Conditionfourday_week`  ==> difference from slope of red line to slope of green line. i.e. while the red line goes -0.023 for every 1 across, the green line goes $-0.023+2.282=2.259$ up for every 1 across. \n\n\n:::int\nCompared to the control group, wellbeing increased by 1.36 points/year more for employees with unlimited leave, and by 2.28 points/year for employees on the 4 day week.  \n:::\n\n\n`r solend()`\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"02_intromlm.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"theme":["united","assets/style-labs.scss"],"link-citations":true,"code-copy":false,"title":"2. Intro to Multilevel Models","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}