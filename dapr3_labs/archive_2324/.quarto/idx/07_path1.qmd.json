{"title":"7. Path Analysis","markdown":{"yaml":{"title":"7. Path Analysis","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"knitr::opts_chunk$set(cache = TRUE)","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nlibrary(lavaan)\nlibrary(semPlot)\noptions(digits=3, scipen = 3)\n```\n\n\n:::lo\n**Relevant packages**  \n\n+ lavaan  \n+ semPlot or tidySEM\n\n::: \n\n\nBy now, we are getting more comfortable with the regression world, and we can see how it is extended to lots of different types of outcome and data structures. So far in DAPR3 it's been all about the multiple levels. This has brought so many more potential study designs that we can now consider modelling - pretty much any study where we are interested in explaining some outcome variable, and where we have sampled clusters of observations (or clusters of clusters of clusters of ... etc.).  \n\nBut we are still restricted to thinking, similar to how we thought in DAPR2, about one single outcome variable. In fact, if we think about the structure of the fixed effects part of a model (i.e., the bit we're _specifically interested in_), then we're still limited to thinking of the world in terms of \"this is my outcome variable, everything else predicts it\". \n\n::: {.callout-note collapse=\"true\"}\n## Regression as a path diagram\n\n1. Imagine writing the names of all your variables on a whiteboard\n2. Specify which one is your dependent (or \"outcome\" or \"response\") variable. \n3. Sit back and relax, you're done!\n\nIn terms of a _theoretical_ model of the world, there's not really much to it. We have few choices in the model we construct beyond specifying which is our outcome variable.  \nWe can visualise our multiple regression model like this: \n```{r}\n#| label: fig-mregfig\n#| out-width: \"250px\"\n#| echo: false\n#| fig.cap: \"In multiple regression, we decide which variable is our outcome variable, and then everything else is done for us\"\nknitr::include_graphics(\"images/path/mregpath.png\")\n```\n\nOf course, there are a few other things that are included (an intercept term, the residual error, and the fact that our predictors can be correlated with one another), but the idea remains pretty much the same:\n\n```{r}\n#| label: fig-mregfig2\n#| fig.cap: \"Multiple regression with intercept, error, predictor covariances\"\n#| out.width: \"250px\"\n#| echo: false\nknitr::include_graphics(\"images/path/mregpath2.png\")\n```\n:::\n\n::: {.callout-note collapse=\"true\"}\n## A model reflects a theory\n\nWhat if I my theoretical model of the world doesn't fit the structure of \"one outcome, multiple precictors\"?  \n\nLet's suppose I have 5 variables: Age, Parental Income, Income, Autonomy, and Job Satisfaction. I draw them up on my whiteboard:\n```{r}\n#| label: fig-path1nopath\n#| fig.cap: \"My variables\"\n#| out.width: \"350px\"\n#| echo: false\nknitr::include_graphics(\"images/path/paths1nopaths.png\")\n```\n\nMy theoretical understanding of how these things fit together leads me to link my variables to end up with something like that in @fig-path1path. \n```{r}\n#| label: fig-path1path\n#| fig.cap: \"My theory about my system of variables\"\n#| out.width: \"350px\"\n#| echo: false\nknitr::include_graphics(\"images/path/paths1paths.png\")\n```\nIn this diagram, a persons income is influenced by their age, their parental income, and their level of autonomy, and in turn their income predicts their job satisfaction. Job satisfaction is also predicted by a persons age directly, and by their level of autonomy, which is also predicted by age. It's complicated to look at, but in isolation each bit of this makes theoretical sense. \n\nTake each arrow in turn and think about what it represents:\n```{r}\n#| label: fig-path1pathdesc\n#| out.width: \"550px\"\n#| echo: false\nknitr::include_graphics(\"images/path/paths1desc.png\")\n```\n\nIf we think about trying to fit this \"model\" with the tools that we have, then we might end up wanting to fit three separate regression models, which between them specify all the different arrows in the diagram:\n\n$$\n\\begin{align}\n\\textrm{Job Satisfaction} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_3(\\textrm{Income}) + \\varepsilon \\\\\n\\textrm{Income} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_2(\\textrm{Parental Income}) + \\varepsilon \\\\\n\\textrm{Autonomy} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\varepsilon \\\\\n\\end{align}\n$$\n\nThis is all well and good, but what if I want to talk about how well my entire model (@fig-path1path) fits the data we observed?  \n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Introducing Path Analysis\n\nThe starting point for Path Analysis is to think about our theories in terms of the connections between variables drawn on a whiteboard. By representing a theory as paths to and from different variables, we open up a whole new way of 'modelling' the world around us.  \n\nThere are a few conventions to help us understand this sort of diagrammatical way of thinking. By using combinations of rectangles, ovals, single- and double-headed arrows, we can draw all sorts of model structures. In Path Diagrams, we use specific shapes and arrows to represent different things in our model:\n\n__Shapes and Arrows in Path Diagrams__  \n\n- **Observed variables** are represented by squares or rectangles. These are the named variables of interest which exist in our dataset - i.e. the ones which we have measured directly. \n- **Variances/Covariances** are represented by double-headed arrows. In many diagrams these are curved. \n- **Regressions** are shown by single headed arrows (e.g., an arrow from $x$ to $y$ for the path $y~x$).  \n<li style=\"opacity:.3\">**Latent variables** are represented by ovals, and we will return to these in a few weeks time!</li>\n\n```{r}\n#| echo: false\n#| out-width: \"650px\"\nknitr::include_graphics(\"images/path/semplots.png\")\n```\n\n**Terminology refresher**  \n\n- **Exogenous variables** are a bit like what we have been describing with words like \"independent variable\" or \"predictor\". In a path diagram, they have no paths coming from other variables in the system, but have paths *going to* other variables.  \n- **Endogenous variables** are more like the \"outcome\"/\"dependent\"/\"response\" variables we are used to. They have some path coming from another variable in the system (and may also - but not necessarily - have paths going out from them).  \n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## How does it work (in brief)? \n\nThe logic behind path analysis is to estimate a system of equations that can reproduce the covariance structure that we see in the data. \n\n1. We specify our theoretical model of the world as a system of paths between variables\n2. We collect data on the relevant variables and we observe a correlation matrix (i.e. how each variable correlates with all others)\n3. We fit our model to the data, and evaluate how well our theoretical model (a system of paths) can reproduce the correlation matrix we observed. \n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## OPTIONAL How does it work (less brief)? \n\n__Path Diagram Tracing__\n\nFor Path Diagrams that meet a certain set of pre-requisites, we can use a cool technique called Path Tracing to estimate the different paths (i.e., the coefficients) from just the covariance matrix of the dataset. \nFor us to be able to do this, a Path Diagram must meet these criteria:\n\n- All our exogenous variables are correlated (unless we specifically assume that their correlation is zero)\n- All models are 'recursive' (no two-way causal relations, no feedback loops)\n- Residuals are uncorrelated with exogenous variables\n- Endogenous variables are not connected by correlations (we would use correlations between residuals here, because the residuals are not endogenous)\n- All 'causal' relations are linear and additive\n- 'causes' are unitary (if A -> B and A -> C, then it is presumed that this is the same aspect of A resulting in a change in both B and C, and not two distinct aspects of A, which would be better represented by two correlated variables A1 and A2). \n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Causal? \n\nIt is a slippery slope to start using the word 'cause', and personally I am not that comfortable using it here. However, you will likely hear it a lot in resources about path analysis, so it is best to be warned.  \n\nPlease keep in mind that we are using a very broad definition of 'causal', simply to reflect the one way nature of the relationship we are modeling. In @fig-causes, a change in the variable **X1** is associated with a change in **Y**, but not vice versa. \n\n```{r}\n#| label: fig-causes\n#| echo: false\n#| fig.cap: \"Paths are still just regressions.\"\nknitr::include_graphics(\"images/path/causes.png\")\n```\n\n:::\n\n\n\n__Tracing Rules__  \n\nThanks to [Sewal Wright](https://www.jstor.org/stable/pdf/2527551.pdf?casa_token=3QF0ad2ZoBcAAAAA:MbEkDNNdoLZr1SXE4LrnK--qrhhsTXLgsRtcWre1UvWxiQiGNUl5vWytGp34XIxhAYMZJe-MbIcBnEwXSfX6MAONevz04-sMXpEDI3IaYKk6mMX46QvX), we can express the correlation between any two variables in the system as the sum of all *compound paths* between the two variables. \n\n*compound paths* are any paths you can trace between A and B for which there are: \n\n- no loops\n- no going forward then backward\n- maximum of one curved arrow per path\n\n\n__EXAMPLE__  \n\nLet's consider the example below, for which the paths are all labelled with lower case letters $a, b, c, \\text{and } d$. \n\n```{r}\n#| label: fig-patheq1\n#| fig.cap: \"A multiple regression model as a path diagram\"\n#| echo: false\nknitr::include_graphics(\"images/path/patheq1.png\")\n```\n\nAccording to Wright's tracing rules above, write out the equations corresponding to the 3 correlations between our observed variables (remember that $r_{a,b} = r_{b,a}$, so it doesn't matter at which variable we start the paths). \n\n- $r_{x1,x2} =  c$  \n- $r_{x1,y} = a + bc$  \n- $r_{x2,y} =  b + ac$  \n\nNow let's suppose we observed the following correlation matrix:\n```{r}\negdat <- read_csv(\"https://uoepsy.github.io/data/patheg.csv\")\negdat <- read_csv(\"../../data/patheg.csv\")\nround(cor(egdat),2)\n```\nWe can plug these into our system of equations:\n\n- $r_{x1,x2} = c = 0.36$  \n- $r_{x1,y} = a + bc = 0.75$   \n- $r_{x2,y} = b + ac = 0.60$\n\nAnd with some substituting and rearranging, we can work out the values of $a$, $b$ and $c$. \n\n::: {.callout-note collapse=\"true\"}\n## Click for answers\n\nThis is what I get:\n\na = 0.61  \nb = 0.38  \nc = 0.36  \n\n:::\n\nWe can even work out what the path labeled $d$ (the residual variance) is.  \nFirst we sum up all the equations for the paths from Y to Y itself.  \nThese are:\n\n- $a^2$ (from Y to X1 and back)  \n- $b^2$ (from Y to X2 and back)  \n- $acb$ (from Y to X1 to X2 to Y)\n- $bca$ (from Y to X2 to X1 to Y)\n  \nSumming them all up and solving gives us:  \n$$\n\\begin{align}\n r_{y \\cdot x1, x2} & = a^2 + b^2 + acb + bca\\\\\n & = 0.61^2 + 0.38^2 + 2 \\times(0.61 \\times 0.38 \\times 0.36)\\\\\n & = 0.68 \\\\\n\\end{align}\n$$\nWe can think of this as the portion of the correlation of Y with itself that occurs *via the predictors*. Put another way, this is the amount of variance in Y explained jointly by X1 and X2, which sounds an awful lot like an $R^2$!  \nThe path labelled $d$ is simply all that is left in Y after taking out the variance explained by X1 and X2, meaning that the path $d = \\sqrt{1-R^2}$ (i.e., the residual variance!).  \n\nHooray! We've just worked out regression coefficients when all we had was the correlation matrix of the variables! It's important to note that we have been using the correlation matrix, so, somewhat unsurprisingly, our estimates are *standardised* coefficients. \n\nBecause we have the data itself, let's quickly find them with `lm()`\n```{r}\n# model:\nmodel1 <- lm( scale(y) ~ scale(x1) + scale(x2), egdat)\n# extract the coefs\ncoef(model1) %>% round(2)\n# extract the r^2\nsummary(model1)$r.squared\n```\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Introducing `lavaan`  \n\nFor the remaining weeks of the course, we're going to rely heavily on the **lavaan** (**La**tent **Va**riable **An**alysis) package. This is the main package in R for fitting path diagrams (as well as more cool models like factor analysis sructures and structural equation mdoels). There is a huge scope of what this package can do.  \nThe first thing to get to grips with is the various new operators which it allows us to use.   \n\nOur old multiple regression formula in R was specified as `y ~ x1 + x2 + x3 + ... `.  \nIn __lavaan__, we continue to fit regressions using the `~` symbol, but we can also specify the construction of latent variables using `=~` and residual variances & covariances using `~~`.  \n\n|  formula type|  operator|  mnemonic|\n|--:|--:|--:|\n|  latent variable definition|  `=~`|  \"is measured by\"|\n|  regression|  `~`|  \"is regressed on\"|\n|  (residual) (co)variance |  `~~`|  \"is correlated with\"|\n|  intercept |  `~1`|  \"intercept\"|\n|  new parameter |  `:=`|  \"is defined as\"|\n\n(from https://lavaan.ugent.be/tutorial/syntax1.html) \n\nIn practice, fitting models in __lavaan__ tends to be a little different from things like `lm()` and `(g)lmer()`.  \nInstead of including the model formula *inside* the fit function (e.g., `lm(y ~ x1 + x2, data = df)`), we tend to do it in a step-by-step process. \nThis is because as our models become more complex, our formulas can pretty long!   \n\nWe write the model as a character string (e.g. `model <- \"y ~ x1 + x2\"`) and then we pass that formula along with the data to the relevant __lavaan__ function, which for our purposes will be the `sem()` function, `sem(model, data = mydata)`.   \n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n#### Fitting a multiple regression model with lavaan\n\nYou can see a multiple regression fitted with __lavaan__ below.  \n\n```{r}\n#| message: false\n#| warning: false\nlibrary(lavaan)\nscsdat <- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\n\n# the lm() way\nmreg_lm <- lm(dass ~ zo + zc + ze + za + zn + scs, scsdat)\n\n# setting up the model\nmreg_model <- \"\n    #regression\n    dass ~ zo + zc + ze + za + zn + scs\n\"\nmreg_sem <- sem(mreg_model, data=scsdat)\n```\n\nThese are the coefficients from our `lm()` model:\n```{r}\ncoefficients(mreg_lm)\n```\n\nAnd you can see the estimated parameters are the same for our `sem()` model!\n```{r}\nsummary(mreg_sem)\n```\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Doing Path Analysis 1: Model Specification\n\n```{r}\n#| include: false\nset.seed(123456)\nm = \"\njobsat ~ (-0.8)*age + .58*autonomy + .47*income + (-0.39)*x1\nincome ~ .22*autonomy + .57*age + 0.4*parentincome + .1*x1\nautonomy ~ .28*age + 0.8*x1\n\"\ndf <- simulateData(m, sample.nobs=50, standardized = T)\ndf<-df[,c(1:4,6)]\ndf %>% mutate(\n  jobsat = round(50+(jobsat*17)),\n  income = round(35+(income*7)),\n  autonomy = round(45+(autonomy*18)),\n  age = round(50 +(age*7)),\n  parentincome = round(45+(parentincome*4)),\n) -> df\n#write.csv(df, \"../../data/jobsatpath.csv\",row.names=F)\n\n```\n\nThe first part of estimating a path model involves specifying the model. This means basically writing down the paths that are included in your theoretical model. \n\nLet's start by looking at the example about job satisfaction, income, autonomy and age.  \nRecall we had this theoretical model:\n```{r}\n#| label: path1patha\n#| out-width: \"350px\"\n#| echo: false\nknitr::include_graphics(\"images/path/paths1paths.png\")\n```\nAnd now let's suppose that we collected data on these variables: \n```{r}\n#| echo: true\n#| eval: false\njobsatpath <- read_csv(\"https://uoepsy.github.io/data/jobsatpath.csv\")\nhead(jobsatpath)\n```\n```{r}\n#| echo: false\njobsatpath <- read_csv(\"../../data/jobsatpath.csv\")\nknitr::kable(rbind(head(jobsatpath),\"...\"))\n```\n\nRemember we said that we could specify all these paths using three regression models? Well, to specify our path model, we simply write these out like we would do in `lm()`, but this time we do so all in one character string. We still have to make sure that we use the correct variable names, as when we make R estimate the model, it will look in the data for things like \"jobsat\". \n```{r}\nmymodel <- \"\njobsat ~ age + autonomy + income\nincome ~ autonomy + age + parentincome\nautonomy ~ age\n\"\n```\n\nThere are some other things which we will automatically be estimated here: all our exogenous variables (the ones with arrows only going _from_ them) will be free to correlate with one another. We can write this explicitly in the model if we like, using the two tildes `~~` between our two exogenous variables `age` and `parentincome`. We will also get the variances of all our variables.  \n\nWe can see all the paths here:\n```{r}\n#| eval: false\nlavaanify(mymodel)\n```\n```{r}\n#| echo: false\nlavaanify(mymodel)[,2:4] %>% head(13L)\n```\nand even make a nice diagram:\n```{r}\nlibrary(semPlot)\nsemPaths(lavaanify(mymodel))\n```\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Doing Path Analysis 2: Model Identification\n\nYou'll have heard the term \"model fit\" many times since the start of DAPR2, when we began model-based thinking. However, there is a crucial difference in what it means when it is used in for path analysis.  \n\nIn things like multiple regression, we have been using \"model fit\" to be the measure of \"how much variance can we explain in y with our set of predictors?\". For a path model, examining \"model fit\" is more like asking \"how well does our model reproduce the characteristics of the data that we observed?\".  \n  \nWe can represent the \"characteristics of our data\" in a covariance matrix, so one way of thinking of \"model fit\" is as \"how well can our model reproduce our observed covariance matrix?\".  \n\n```{r}\ncov(jobsatpath)\n```\n\nBecause we are working with a covariance matrix here, we are really working with fewer \"bits\" of information than the 50 __people__ in our dataset. We actually are concerned with the 15 unique variances and covariances in our covariance matrix between our 5 variables above.  \n\n__Degrees of freedom__  \n\nWhen we think of \"degrees of freedom\" for a multiple regression model, in DAPR2 we learned that $df = n-k-1$ ($n$ is the number of observations, $k$ is the number of predictors). These degrees of freedom related to the corresponding $F$ and $t$-distributions with which we performed our hypothesis tests (e.g. $t$-tests for a null hypothesis that a coefficient is zero, or $F$-tests for a null that the reduction in residual sums of squares is zero).  \n\nBut in relation to our model's ability to represent a $k \\times k$ covariance matrix (i.e. the covariance matrix of our $k$ variables), we are instead interested in the number of covariances (and not the number of observations). Our \"degrees of freedom\" in the this framework corresponds to the number of *knowns* (observed covariances/variances) minus the number of *unknowns* (parameters to be estimated by the model). A model is only able to be estimated if it has at least 0 degrees of freedom (if there are at least as many knowns as unknowns). A model with 0 degrees of freedom is termed **just-identified**. **Under-** and **Over-** identified models correspond to those with $<0$ and $>0$ degrees of freedom respectively. \n\n:::statbox\n**How many knowns are there?**   \n\nThe number of known covariances in a set of $k$ observed variables is equal to $\\frac{k \\cdot (k+1)}{2}$. \n:::\n\nWhen learning about path models, the visualisations can play a key part. It often helps to draw all our variables (both observed and latent) on the whiteboard, and connect them up according to your theoretical model. You can then count the number of paths (arrows) and determine whether the **number of knowns** > **number of unknowns**. We can reduce the number of unknowns by fixing parameters to be specific values. \n\n:::statbox\n\nBy constraining some estimated parameter to be some specific value, we free-up a degree of freedom! For instance \"the correlation between x1 and x2 is equal to 0.7 ($r_{x_1x_2} = .07$)\". This would turn a previously estimated parameter into a fixed parameter, and this gains us the prize of a lovely degree of freedom!  \n\n**By removing a path altogether, we are constraining it to be zero.**  \n\n:::\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## OPTIONAL:  multiple regression model is just-identified  \n\nThe multiple regression model is an example of a *just-identified* model! In multiple regression, everything is allowed to covary with everything else, which means that there is a unique solution for all of the model's parameters because there are *as many paths as there are observed covariances*. This means that in this path analysis world, a multiple regression model is \"just-identified\". \n\n\n```{r}\n#| echo: false\nscsdat <- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\nscsdat %>% transmute(y=dass,x1=zo,x2=zn) -> somedata\n```\n\nIf I have two predictors and one outcome variable, then there are 6 variances and covariances available. For instance: \n```{r}\ncov(somedata)\n```\nThe multiple regression model will estimate the two variances of the exogenous variables (the predictors), their covariance, the two paths from each exogenous to the endogenous (each predictor to the outcome), and the error variance. This makes up 6 estimated parameters - which is exactly how many known covariances there are.  \nCount the number of arrows (both single and double headed) in the diagram:  \n```{r}\n#| echo: false\n#| out-width: \"350px\"\nmreg_sem <- sem(\"y~x1+x2\", data = somedata)\nsemPaths(mreg_sem, edge.width=2,sizeMan=7)\n```\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Doing Path Analysis 3: Model Estimation\n\nEstimating the model is relatively straightforward. We pass the formula we have written to the `sem()` function, along with the data set in which we want it to look for the variables. It will be estimated using maximum likelihood estimation.    \n```{r}\nmymodel.fit <- sem(mymodel, data = jobsatpath)\n```\n\nWe can then examine the parameter estimates: \n```{r}\nsummary(mymodel.fit)\n```\n\nWe can now, to \"visualise\" our model, add the estimates to the diagram:\n```{r}\n#| echo: false\n#| fig.cap: \"Model estimates\"\nknitr::include_graphics(\"images/path/jobsatmod.png\")\n```\n\n:::\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n## Doing Path Analysis 4: Model Fit\n\nOnce we have estimated a model, we can evaluate how well it fits our sample data. As mentioned above in relation to model identification, when we talk about model fit here, we are asking \"how well does our model reproduce the characteristics of the data that we observed?\", or more specifically \"how well can our model reproduce our observed covariance matrix?\".  \n\n```{r}\ncov(jobsatpath)\n```\n\nThere are too many different indices of model fit for these types of models, and there's lots of controversy over the various merits and disadvantages and proposed cutoffs of each method. We will return to this more in coming weeks, and the lecture this week contains information on some of them. The important thing to remember: \"model fit\" and \"degrees of freedom\" have quite different meanings to those you are likely used to.  \n\nThe simplest metric of fit is a chi-square value that we can compute that reflects reflects the discrepancy between the _model-implied covariance matrix_ and the _observed covariance matrix_. We can then calculate a p-value for this chi-square statistic by using the chi-square distribution with the degrees of freedom equivalent to that of the model.  \n\nIf we denote the sample covariance matrix as $S$ and the model-implied covariance matrix as $\\Sigma(\\theta)$, then we can think of the null hypothesis here as $H_0: S - \\Sigma(\\hat\\theta) = 0$. \n__In this way our null hypothesis is sort of like saying that our theoretical model is correct__ (and can therefore perfectly reproduce the covariance matrix).  \n\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Exercises: Burnout\n\n:::frame\n__Data: Passion & Burnout__  \n\nResearchers are interested in the role of obsessive and harmonious passion in psychological wellbeing. The researchers collect data from 100 participants. Participants respond on sliding scales (0-100) for five measures:\n\n```{r}\n#| echo: false\n#todo\nburnout <- read_csv(\"../../data/passionpath.csv\")\ntibble(\n  variable = names(burnout),\n  description = c(\"Work Satisfaction: On a scale of 0-100, how satisfied are you with your work? \",\"Harmonious Passion: On a scale of 0-100, how much do you feel that you freely choose to engage in work outside of working hours?\", \"Obsessive Passion: On a scale of 0-100, how much do you have uncontrollable urges to work outside of working hours?\",\"Work Conflict: On a scale of 0-100, how much conflict do you experience in your work? \",\"Work Burnout: On a scale of 0-100, how close to burnout at work are you?\")\n) %>% knitr::kable(.)\n```\n\nThe data is available at https://uoepsy.github.io/data/passionpath.csv  \n\n:::\n\n`r qbegin(1)`\nLoad in the libraries we will use in these exercises:\n\n- tidyverse  \n- lavaan  \n- semPlot  \n\nRead in the data.\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nburnout <- read_csv(\"https://uoepsy.github.io/data/passionpath.csv\")\n```\n`r solend()`\n\n`r qbegin(2)`\nThe researchers have this theoretical model:\n```{r}\n#| label: fig-burn\n#| echo: false\n#| fig-cap: \"Burnout Theory\"\nknitr::include_graphics(\"images/path/burnoutpath.png\")\n```\n\nSpecify this model and store the formula as an object in R\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nburnoutmod <- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\n\"\n  \n```\n`r solend()`\n\n`r qbegin(3)`\nFit the model to the data using the `sem()` function.\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nburnoutfit <- sem(burnoutmod, data=burnout)\n```\n`r solend()`\n\n`r qbegin(4)`\nExamine the parameter estimates\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nsummary(burnoutfit)\n```\n`r solend()`\n\n`r qbegin(5)`\nProduce a diagram with the estimates on the paths. \nCan you also produce one which has the _standardised_ estimates?  \n\nTake a look at the help function for `semPaths()`. \nWhat do the arguments `what` and `whatLabels` do? \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n`what` will weight and colour the paths according something like the estimates. `whatLabels` will provide labels for the paths:\n```{r}\nsemPaths(burnoutfit, whatLabels = \"est\")\n```\n\nThis will change them to the standardised estimates:\n```{r}\nsemPaths(burnoutfit, what = \"std\", whatLabels = \"std\")\n```\n`r solend()`\n\n`r qbegin(6)`\n\n- How many variables do you have in your model?\n- How many *knowns* are there in the covariance matrix?  \n- How many *unknowns* are there in your model? \n- How many degrees of freedom do you therefore have?\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n- How many variables do you have in your model?  \n  __5__\n  \n- How many *knowns* are there in the covariance matrix? \n  $\\frac{5 \\times (5 + 1)}{2} = 15$\n  \n- How many *unknowns* are there in your model?  \n  There are 6 paths in @fig-burn, but we also need to consider the variances of the 5 variables, so we have 11 things being estimated\n\n- How many degrees of freedom do you therefore have?\n  __15 - 11 = 4__ \n\n`r solend()`\n\n`r qbegin(7)`\nTake a look at the summary of the model you fitted. Specifically, examine the bit near the top where it mentions the $\\chi^2$ statistic.  \nIs it significant? \nWhat do we conclude?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThe $\\chi^2$ statistic is not significant:\n```{r}\npchisq(5.458, df=4, lower.tail=F)\n```\nWe therefore have no evidence to support rejecting our null hypothesis that our model provides a reasonable fit to the data. \n`r solend()`\n\n`r qbegin(8)`\nTry examing what the other fit measures (RMSEA, SRMR, CLI, TLI:\nHow do they compare with [the cutoffs provided in the lecture?](https://uoepsy.github.io/dapr3/2223/lectures/week1_pathintro.html#37)    \n  \nhint: `summary(modelfit, fit.measures = TRUE)`  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThe fit statistics for our model:\n\nComparative Fit Index (CFI) = 0.968\nTucker-Lewis Index (TLI) = 0.921\nRMSEA = 0.060\nSRMR = 0.063\n\nOnly CFI meets the criteria given in the lecture slides for \"considered good\". \n\n\n```{r}\nsummary(burnoutfit, fit.measures = TRUE)\n```\n\n`r solend()`\n\n\n\n\n`r qbegin(\"Extra: modification indices\")`\nExamine the modification indices of the model (use the `modindices()` function).   \nPay close attention to the `mi` column (this is the \"modification index\", which is the change in the $\\chi^2$ statistic).\nThe other interesting column is going to be the `sepc.all` column, which is the estimated parameter value of the proposed path, in a model where all the variables are standardised. This means we can evaluate whether the estimated parameter is relatively small/moderate/large, because these are all standardised correlations between -1 and 1!  \n\nAre there any paths which the modification indices suggest might improve the model? Do they make theoretical sense to include them?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nmodindices(burnoutfit)\n```\n\nThere seems to be a suggested reasonably large correlation between `burnout` and `conflict`. \nIf we were to fit this model, our fit indices may well improve and meet our cut-offs. But this may well just be overfitting.\n\n```{r}\n#| eval: false\nburnoutmod2 <- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\nburnout ~~ conflict\n\"\nsem(burnoutmod2, data=burnout)\n```\nWe will not start adjusting models based on modification indices today (or indeed in this course at all). __As a general rule for dapR3 course, we want you to specify and test a specific model, and not seek to use exploratory modifications.__  \n`r solend()`\n\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nlibrary(lavaan)\nlibrary(semPlot)\n# knitr::opts_chunk$set(cache = TRUE)\noptions(digits=3, scipen = 3)\n```\n\n\n:::lo\n**Relevant packages**  \n\n+ lavaan  \n+ semPlot or tidySEM\n\n::: \n\n\nBy now, we are getting more comfortable with the regression world, and we can see how it is extended to lots of different types of outcome and data structures. So far in DAPR3 it's been all about the multiple levels. This has brought so many more potential study designs that we can now consider modelling - pretty much any study where we are interested in explaining some outcome variable, and where we have sampled clusters of observations (or clusters of clusters of clusters of ... etc.).  \n\nBut we are still restricted to thinking, similar to how we thought in DAPR2, about one single outcome variable. In fact, if we think about the structure of the fixed effects part of a model (i.e., the bit we're _specifically interested in_), then we're still limited to thinking of the world in terms of \"this is my outcome variable, everything else predicts it\". \n\n::: {.callout-note collapse=\"true\"}\n## Regression as a path diagram\n\n1. Imagine writing the names of all your variables on a whiteboard\n2. Specify which one is your dependent (or \"outcome\" or \"response\") variable. \n3. Sit back and relax, you're done!\n\nIn terms of a _theoretical_ model of the world, there's not really much to it. We have few choices in the model we construct beyond specifying which is our outcome variable.  \nWe can visualise our multiple regression model like this: \n```{r}\n#| label: fig-mregfig\n#| out-width: \"250px\"\n#| echo: false\n#| fig.cap: \"In multiple regression, we decide which variable is our outcome variable, and then everything else is done for us\"\nknitr::include_graphics(\"images/path/mregpath.png\")\n```\n\nOf course, there are a few other things that are included (an intercept term, the residual error, and the fact that our predictors can be correlated with one another), but the idea remains pretty much the same:\n\n```{r}\n#| label: fig-mregfig2\n#| fig.cap: \"Multiple regression with intercept, error, predictor covariances\"\n#| out.width: \"250px\"\n#| echo: false\nknitr::include_graphics(\"images/path/mregpath2.png\")\n```\n:::\n\n::: {.callout-note collapse=\"true\"}\n## A model reflects a theory\n\nWhat if I my theoretical model of the world doesn't fit the structure of \"one outcome, multiple precictors\"?  \n\nLet's suppose I have 5 variables: Age, Parental Income, Income, Autonomy, and Job Satisfaction. I draw them up on my whiteboard:\n```{r}\n#| label: fig-path1nopath\n#| fig.cap: \"My variables\"\n#| out.width: \"350px\"\n#| echo: false\nknitr::include_graphics(\"images/path/paths1nopaths.png\")\n```\n\nMy theoretical understanding of how these things fit together leads me to link my variables to end up with something like that in @fig-path1path. \n```{r}\n#| label: fig-path1path\n#| fig.cap: \"My theory about my system of variables\"\n#| out.width: \"350px\"\n#| echo: false\nknitr::include_graphics(\"images/path/paths1paths.png\")\n```\nIn this diagram, a persons income is influenced by their age, their parental income, and their level of autonomy, and in turn their income predicts their job satisfaction. Job satisfaction is also predicted by a persons age directly, and by their level of autonomy, which is also predicted by age. It's complicated to look at, but in isolation each bit of this makes theoretical sense. \n\nTake each arrow in turn and think about what it represents:\n```{r}\n#| label: fig-path1pathdesc\n#| out.width: \"550px\"\n#| echo: false\nknitr::include_graphics(\"images/path/paths1desc.png\")\n```\n\nIf we think about trying to fit this \"model\" with the tools that we have, then we might end up wanting to fit three separate regression models, which between them specify all the different arrows in the diagram:\n\n$$\n\\begin{align}\n\\textrm{Job Satisfaction} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_3(\\textrm{Income}) + \\varepsilon \\\\\n\\textrm{Income} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_2(\\textrm{Parental Income}) + \\varepsilon \\\\\n\\textrm{Autonomy} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\varepsilon \\\\\n\\end{align}\n$$\n\nThis is all well and good, but what if I want to talk about how well my entire model (@fig-path1path) fits the data we observed?  \n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Introducing Path Analysis\n\nThe starting point for Path Analysis is to think about our theories in terms of the connections between variables drawn on a whiteboard. By representing a theory as paths to and from different variables, we open up a whole new way of 'modelling' the world around us.  \n\nThere are a few conventions to help us understand this sort of diagrammatical way of thinking. By using combinations of rectangles, ovals, single- and double-headed arrows, we can draw all sorts of model structures. In Path Diagrams, we use specific shapes and arrows to represent different things in our model:\n\n__Shapes and Arrows in Path Diagrams__  \n\n- **Observed variables** are represented by squares or rectangles. These are the named variables of interest which exist in our dataset - i.e. the ones which we have measured directly. \n- **Variances/Covariances** are represented by double-headed arrows. In many diagrams these are curved. \n- **Regressions** are shown by single headed arrows (e.g., an arrow from $x$ to $y$ for the path $y~x$).  \n<li style=\"opacity:.3\">**Latent variables** are represented by ovals, and we will return to these in a few weeks time!</li>\n\n```{r}\n#| echo: false\n#| out-width: \"650px\"\nknitr::include_graphics(\"images/path/semplots.png\")\n```\n\n**Terminology refresher**  \n\n- **Exogenous variables** are a bit like what we have been describing with words like \"independent variable\" or \"predictor\". In a path diagram, they have no paths coming from other variables in the system, but have paths *going to* other variables.  \n- **Endogenous variables** are more like the \"outcome\"/\"dependent\"/\"response\" variables we are used to. They have some path coming from another variable in the system (and may also - but not necessarily - have paths going out from them).  \n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## How does it work (in brief)? \n\nThe logic behind path analysis is to estimate a system of equations that can reproduce the covariance structure that we see in the data. \n\n1. We specify our theoretical model of the world as a system of paths between variables\n2. We collect data on the relevant variables and we observe a correlation matrix (i.e. how each variable correlates with all others)\n3. We fit our model to the data, and evaluate how well our theoretical model (a system of paths) can reproduce the correlation matrix we observed. \n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## OPTIONAL How does it work (less brief)? \n\n__Path Diagram Tracing__\n\nFor Path Diagrams that meet a certain set of pre-requisites, we can use a cool technique called Path Tracing to estimate the different paths (i.e., the coefficients) from just the covariance matrix of the dataset. \nFor us to be able to do this, a Path Diagram must meet these criteria:\n\n- All our exogenous variables are correlated (unless we specifically assume that their correlation is zero)\n- All models are 'recursive' (no two-way causal relations, no feedback loops)\n- Residuals are uncorrelated with exogenous variables\n- Endogenous variables are not connected by correlations (we would use correlations between residuals here, because the residuals are not endogenous)\n- All 'causal' relations are linear and additive\n- 'causes' are unitary (if A -> B and A -> C, then it is presumed that this is the same aspect of A resulting in a change in both B and C, and not two distinct aspects of A, which would be better represented by two correlated variables A1 and A2). \n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Causal? \n\nIt is a slippery slope to start using the word 'cause', and personally I am not that comfortable using it here. However, you will likely hear it a lot in resources about path analysis, so it is best to be warned.  \n\nPlease keep in mind that we are using a very broad definition of 'causal', simply to reflect the one way nature of the relationship we are modeling. In @fig-causes, a change in the variable **X1** is associated with a change in **Y**, but not vice versa. \n\n```{r}\n#| label: fig-causes\n#| echo: false\n#| fig.cap: \"Paths are still just regressions.\"\nknitr::include_graphics(\"images/path/causes.png\")\n```\n\n:::\n\n\n\n__Tracing Rules__  \n\nThanks to [Sewal Wright](https://www.jstor.org/stable/pdf/2527551.pdf?casa_token=3QF0ad2ZoBcAAAAA:MbEkDNNdoLZr1SXE4LrnK--qrhhsTXLgsRtcWre1UvWxiQiGNUl5vWytGp34XIxhAYMZJe-MbIcBnEwXSfX6MAONevz04-sMXpEDI3IaYKk6mMX46QvX), we can express the correlation between any two variables in the system as the sum of all *compound paths* between the two variables. \n\n*compound paths* are any paths you can trace between A and B for which there are: \n\n- no loops\n- no going forward then backward\n- maximum of one curved arrow per path\n\n\n__EXAMPLE__  \n\nLet's consider the example below, for which the paths are all labelled with lower case letters $a, b, c, \\text{and } d$. \n\n```{r}\n#| label: fig-patheq1\n#| fig.cap: \"A multiple regression model as a path diagram\"\n#| echo: false\nknitr::include_graphics(\"images/path/patheq1.png\")\n```\n\nAccording to Wright's tracing rules above, write out the equations corresponding to the 3 correlations between our observed variables (remember that $r_{a,b} = r_{b,a}$, so it doesn't matter at which variable we start the paths). \n\n- $r_{x1,x2} =  c$  \n- $r_{x1,y} = a + bc$  \n- $r_{x2,y} =  b + ac$  \n\nNow let's suppose we observed the following correlation matrix:\n```{r}\negdat <- read_csv(\"https://uoepsy.github.io/data/patheg.csv\")\negdat <- read_csv(\"../../data/patheg.csv\")\nround(cor(egdat),2)\n```\nWe can plug these into our system of equations:\n\n- $r_{x1,x2} = c = 0.36$  \n- $r_{x1,y} = a + bc = 0.75$   \n- $r_{x2,y} = b + ac = 0.60$\n\nAnd with some substituting and rearranging, we can work out the values of $a$, $b$ and $c$. \n\n::: {.callout-note collapse=\"true\"}\n## Click for answers\n\nThis is what I get:\n\na = 0.61  \nb = 0.38  \nc = 0.36  \n\n:::\n\nWe can even work out what the path labeled $d$ (the residual variance) is.  \nFirst we sum up all the equations for the paths from Y to Y itself.  \nThese are:\n\n- $a^2$ (from Y to X1 and back)  \n- $b^2$ (from Y to X2 and back)  \n- $acb$ (from Y to X1 to X2 to Y)\n- $bca$ (from Y to X2 to X1 to Y)\n  \nSumming them all up and solving gives us:  \n$$\n\\begin{align}\n r_{y \\cdot x1, x2} & = a^2 + b^2 + acb + bca\\\\\n & = 0.61^2 + 0.38^2 + 2 \\times(0.61 \\times 0.38 \\times 0.36)\\\\\n & = 0.68 \\\\\n\\end{align}\n$$\nWe can think of this as the portion of the correlation of Y with itself that occurs *via the predictors*. Put another way, this is the amount of variance in Y explained jointly by X1 and X2, which sounds an awful lot like an $R^2$!  \nThe path labelled $d$ is simply all that is left in Y after taking out the variance explained by X1 and X2, meaning that the path $d = \\sqrt{1-R^2}$ (i.e., the residual variance!).  \n\nHooray! We've just worked out regression coefficients when all we had was the correlation matrix of the variables! It's important to note that we have been using the correlation matrix, so, somewhat unsurprisingly, our estimates are *standardised* coefficients. \n\nBecause we have the data itself, let's quickly find them with `lm()`\n```{r}\n# model:\nmodel1 <- lm( scale(y) ~ scale(x1) + scale(x2), egdat)\n# extract the coefs\ncoef(model1) %>% round(2)\n# extract the r^2\nsummary(model1)$r.squared\n```\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Introducing `lavaan`  \n\nFor the remaining weeks of the course, we're going to rely heavily on the **lavaan** (**La**tent **Va**riable **An**alysis) package. This is the main package in R for fitting path diagrams (as well as more cool models like factor analysis sructures and structural equation mdoels). There is a huge scope of what this package can do.  \nThe first thing to get to grips with is the various new operators which it allows us to use.   \n\nOur old multiple regression formula in R was specified as `y ~ x1 + x2 + x3 + ... `.  \nIn __lavaan__, we continue to fit regressions using the `~` symbol, but we can also specify the construction of latent variables using `=~` and residual variances & covariances using `~~`.  \n\n|  formula type|  operator|  mnemonic|\n|--:|--:|--:|\n|  latent variable definition|  `=~`|  \"is measured by\"|\n|  regression|  `~`|  \"is regressed on\"|\n|  (residual) (co)variance |  `~~`|  \"is correlated with\"|\n|  intercept |  `~1`|  \"intercept\"|\n|  new parameter |  `:=`|  \"is defined as\"|\n\n(from https://lavaan.ugent.be/tutorial/syntax1.html) \n\nIn practice, fitting models in __lavaan__ tends to be a little different from things like `lm()` and `(g)lmer()`.  \nInstead of including the model formula *inside* the fit function (e.g., `lm(y ~ x1 + x2, data = df)`), we tend to do it in a step-by-step process. \nThis is because as our models become more complex, our formulas can pretty long!   \n\nWe write the model as a character string (e.g. `model <- \"y ~ x1 + x2\"`) and then we pass that formula along with the data to the relevant __lavaan__ function, which for our purposes will be the `sem()` function, `sem(model, data = mydata)`.   \n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n#### Fitting a multiple regression model with lavaan\n\nYou can see a multiple regression fitted with __lavaan__ below.  \n\n```{r}\n#| message: false\n#| warning: false\nlibrary(lavaan)\nscsdat <- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\n\n# the lm() way\nmreg_lm <- lm(dass ~ zo + zc + ze + za + zn + scs, scsdat)\n\n# setting up the model\nmreg_model <- \"\n    #regression\n    dass ~ zo + zc + ze + za + zn + scs\n\"\nmreg_sem <- sem(mreg_model, data=scsdat)\n```\n\nThese are the coefficients from our `lm()` model:\n```{r}\ncoefficients(mreg_lm)\n```\n\nAnd you can see the estimated parameters are the same for our `sem()` model!\n```{r}\nsummary(mreg_sem)\n```\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Doing Path Analysis 1: Model Specification\n\n```{r}\n#| include: false\nset.seed(123456)\nm = \"\njobsat ~ (-0.8)*age + .58*autonomy + .47*income + (-0.39)*x1\nincome ~ .22*autonomy + .57*age + 0.4*parentincome + .1*x1\nautonomy ~ .28*age + 0.8*x1\n\"\ndf <- simulateData(m, sample.nobs=50, standardized = T)\ndf<-df[,c(1:4,6)]\ndf %>% mutate(\n  jobsat = round(50+(jobsat*17)),\n  income = round(35+(income*7)),\n  autonomy = round(45+(autonomy*18)),\n  age = round(50 +(age*7)),\n  parentincome = round(45+(parentincome*4)),\n) -> df\n#write.csv(df, \"../../data/jobsatpath.csv\",row.names=F)\n\n```\n\nThe first part of estimating a path model involves specifying the model. This means basically writing down the paths that are included in your theoretical model. \n\nLet's start by looking at the example about job satisfaction, income, autonomy and age.  \nRecall we had this theoretical model:\n```{r}\n#| label: path1patha\n#| out-width: \"350px\"\n#| echo: false\nknitr::include_graphics(\"images/path/paths1paths.png\")\n```\nAnd now let's suppose that we collected data on these variables: \n```{r}\n#| echo: true\n#| eval: false\njobsatpath <- read_csv(\"https://uoepsy.github.io/data/jobsatpath.csv\")\nhead(jobsatpath)\n```\n```{r}\n#| echo: false\njobsatpath <- read_csv(\"../../data/jobsatpath.csv\")\nknitr::kable(rbind(head(jobsatpath),\"...\"))\n```\n\nRemember we said that we could specify all these paths using three regression models? Well, to specify our path model, we simply write these out like we would do in `lm()`, but this time we do so all in one character string. We still have to make sure that we use the correct variable names, as when we make R estimate the model, it will look in the data for things like \"jobsat\". \n```{r}\nmymodel <- \"\njobsat ~ age + autonomy + income\nincome ~ autonomy + age + parentincome\nautonomy ~ age\n\"\n```\n\nThere are some other things which we will automatically be estimated here: all our exogenous variables (the ones with arrows only going _from_ them) will be free to correlate with one another. We can write this explicitly in the model if we like, using the two tildes `~~` between our two exogenous variables `age` and `parentincome`. We will also get the variances of all our variables.  \n\nWe can see all the paths here:\n```{r}\n#| eval: false\nlavaanify(mymodel)\n```\n```{r}\n#| echo: false\nlavaanify(mymodel)[,2:4] %>% head(13L)\n```\nand even make a nice diagram:\n```{r}\nlibrary(semPlot)\nsemPaths(lavaanify(mymodel))\n```\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Doing Path Analysis 2: Model Identification\n\nYou'll have heard the term \"model fit\" many times since the start of DAPR2, when we began model-based thinking. However, there is a crucial difference in what it means when it is used in for path analysis.  \n\nIn things like multiple regression, we have been using \"model fit\" to be the measure of \"how much variance can we explain in y with our set of predictors?\". For a path model, examining \"model fit\" is more like asking \"how well does our model reproduce the characteristics of the data that we observed?\".  \n  \nWe can represent the \"characteristics of our data\" in a covariance matrix, so one way of thinking of \"model fit\" is as \"how well can our model reproduce our observed covariance matrix?\".  \n\n```{r}\ncov(jobsatpath)\n```\n\nBecause we are working with a covariance matrix here, we are really working with fewer \"bits\" of information than the 50 __people__ in our dataset. We actually are concerned with the 15 unique variances and covariances in our covariance matrix between our 5 variables above.  \n\n__Degrees of freedom__  \n\nWhen we think of \"degrees of freedom\" for a multiple regression model, in DAPR2 we learned that $df = n-k-1$ ($n$ is the number of observations, $k$ is the number of predictors). These degrees of freedom related to the corresponding $F$ and $t$-distributions with which we performed our hypothesis tests (e.g. $t$-tests for a null hypothesis that a coefficient is zero, or $F$-tests for a null that the reduction in residual sums of squares is zero).  \n\nBut in relation to our model's ability to represent a $k \\times k$ covariance matrix (i.e. the covariance matrix of our $k$ variables), we are instead interested in the number of covariances (and not the number of observations). Our \"degrees of freedom\" in the this framework corresponds to the number of *knowns* (observed covariances/variances) minus the number of *unknowns* (parameters to be estimated by the model). A model is only able to be estimated if it has at least 0 degrees of freedom (if there are at least as many knowns as unknowns). A model with 0 degrees of freedom is termed **just-identified**. **Under-** and **Over-** identified models correspond to those with $<0$ and $>0$ degrees of freedom respectively. \n\n:::statbox\n**How many knowns are there?**   \n\nThe number of known covariances in a set of $k$ observed variables is equal to $\\frac{k \\cdot (k+1)}{2}$. \n:::\n\nWhen learning about path models, the visualisations can play a key part. It often helps to draw all our variables (both observed and latent) on the whiteboard, and connect them up according to your theoretical model. You can then count the number of paths (arrows) and determine whether the **number of knowns** > **number of unknowns**. We can reduce the number of unknowns by fixing parameters to be specific values. \n\n:::statbox\n\nBy constraining some estimated parameter to be some specific value, we free-up a degree of freedom! For instance \"the correlation between x1 and x2 is equal to 0.7 ($r_{x_1x_2} = .07$)\". This would turn a previously estimated parameter into a fixed parameter, and this gains us the prize of a lovely degree of freedom!  \n\n**By removing a path altogether, we are constraining it to be zero.**  \n\n:::\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## OPTIONAL:  multiple regression model is just-identified  \n\nThe multiple regression model is an example of a *just-identified* model! In multiple regression, everything is allowed to covary with everything else, which means that there is a unique solution for all of the model's parameters because there are *as many paths as there are observed covariances*. This means that in this path analysis world, a multiple regression model is \"just-identified\". \n\n\n```{r}\n#| echo: false\nscsdat <- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\nscsdat %>% transmute(y=dass,x1=zo,x2=zn) -> somedata\n```\n\nIf I have two predictors and one outcome variable, then there are 6 variances and covariances available. For instance: \n```{r}\ncov(somedata)\n```\nThe multiple regression model will estimate the two variances of the exogenous variables (the predictors), their covariance, the two paths from each exogenous to the endogenous (each predictor to the outcome), and the error variance. This makes up 6 estimated parameters - which is exactly how many known covariances there are.  \nCount the number of arrows (both single and double headed) in the diagram:  \n```{r}\n#| echo: false\n#| out-width: \"350px\"\nmreg_sem <- sem(\"y~x1+x2\", data = somedata)\nsemPaths(mreg_sem, edge.width=2,sizeMan=7)\n```\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Doing Path Analysis 3: Model Estimation\n\nEstimating the model is relatively straightforward. We pass the formula we have written to the `sem()` function, along with the data set in which we want it to look for the variables. It will be estimated using maximum likelihood estimation.    \n```{r}\nmymodel.fit <- sem(mymodel, data = jobsatpath)\n```\n\nWe can then examine the parameter estimates: \n```{r}\nsummary(mymodel.fit)\n```\n\nWe can now, to \"visualise\" our model, add the estimates to the diagram:\n```{r}\n#| echo: false\n#| fig.cap: \"Model estimates\"\nknitr::include_graphics(\"images/path/jobsatmod.png\")\n```\n\n:::\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n## Doing Path Analysis 4: Model Fit\n\nOnce we have estimated a model, we can evaluate how well it fits our sample data. As mentioned above in relation to model identification, when we talk about model fit here, we are asking \"how well does our model reproduce the characteristics of the data that we observed?\", or more specifically \"how well can our model reproduce our observed covariance matrix?\".  \n\n```{r}\ncov(jobsatpath)\n```\n\nThere are too many different indices of model fit for these types of models, and there's lots of controversy over the various merits and disadvantages and proposed cutoffs of each method. We will return to this more in coming weeks, and the lecture this week contains information on some of them. The important thing to remember: \"model fit\" and \"degrees of freedom\" have quite different meanings to those you are likely used to.  \n\nThe simplest metric of fit is a chi-square value that we can compute that reflects reflects the discrepancy between the _model-implied covariance matrix_ and the _observed covariance matrix_. We can then calculate a p-value for this chi-square statistic by using the chi-square distribution with the degrees of freedom equivalent to that of the model.  \n\nIf we denote the sample covariance matrix as $S$ and the model-implied covariance matrix as $\\Sigma(\\theta)$, then we can think of the null hypothesis here as $H_0: S - \\Sigma(\\hat\\theta) = 0$. \n__In this way our null hypothesis is sort of like saying that our theoretical model is correct__ (and can therefore perfectly reproduce the covariance matrix).  \n\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Exercises: Burnout\n\n:::frame\n__Data: Passion & Burnout__  \n\nResearchers are interested in the role of obsessive and harmonious passion in psychological wellbeing. The researchers collect data from 100 participants. Participants respond on sliding scales (0-100) for five measures:\n\n```{r}\n#| echo: false\n#todo\nburnout <- read_csv(\"../../data/passionpath.csv\")\ntibble(\n  variable = names(burnout),\n  description = c(\"Work Satisfaction: On a scale of 0-100, how satisfied are you with your work? \",\"Harmonious Passion: On a scale of 0-100, how much do you feel that you freely choose to engage in work outside of working hours?\", \"Obsessive Passion: On a scale of 0-100, how much do you have uncontrollable urges to work outside of working hours?\",\"Work Conflict: On a scale of 0-100, how much conflict do you experience in your work? \",\"Work Burnout: On a scale of 0-100, how close to burnout at work are you?\")\n) %>% knitr::kable(.)\n```\n\nThe data is available at https://uoepsy.github.io/data/passionpath.csv  \n\n:::\n\n`r qbegin(1)`\nLoad in the libraries we will use in these exercises:\n\n- tidyverse  \n- lavaan  \n- semPlot  \n\nRead in the data.\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nburnout <- read_csv(\"https://uoepsy.github.io/data/passionpath.csv\")\n```\n`r solend()`\n\n`r qbegin(2)`\nThe researchers have this theoretical model:\n```{r}\n#| label: fig-burn\n#| echo: false\n#| fig-cap: \"Burnout Theory\"\nknitr::include_graphics(\"images/path/burnoutpath.png\")\n```\n\nSpecify this model and store the formula as an object in R\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nburnoutmod <- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\n\"\n  \n```\n`r solend()`\n\n`r qbegin(3)`\nFit the model to the data using the `sem()` function.\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nburnoutfit <- sem(burnoutmod, data=burnout)\n```\n`r solend()`\n\n`r qbegin(4)`\nExamine the parameter estimates\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nsummary(burnoutfit)\n```\n`r solend()`\n\n`r qbegin(5)`\nProduce a diagram with the estimates on the paths. \nCan you also produce one which has the _standardised_ estimates?  \n\nTake a look at the help function for `semPaths()`. \nWhat do the arguments `what` and `whatLabels` do? \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n`what` will weight and colour the paths according something like the estimates. `whatLabels` will provide labels for the paths:\n```{r}\nsemPaths(burnoutfit, whatLabels = \"est\")\n```\n\nThis will change them to the standardised estimates:\n```{r}\nsemPaths(burnoutfit, what = \"std\", whatLabels = \"std\")\n```\n`r solend()`\n\n`r qbegin(6)`\n\n- How many variables do you have in your model?\n- How many *knowns* are there in the covariance matrix?  \n- How many *unknowns* are there in your model? \n- How many degrees of freedom do you therefore have?\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n- How many variables do you have in your model?  \n  __5__\n  \n- How many *knowns* are there in the covariance matrix? \n  $\\frac{5 \\times (5 + 1)}{2} = 15$\n  \n- How many *unknowns* are there in your model?  \n  There are 6 paths in @fig-burn, but we also need to consider the variances of the 5 variables, so we have 11 things being estimated\n\n- How many degrees of freedom do you therefore have?\n  __15 - 11 = 4__ \n\n`r solend()`\n\n`r qbegin(7)`\nTake a look at the summary of the model you fitted. Specifically, examine the bit near the top where it mentions the $\\chi^2$ statistic.  \nIs it significant? \nWhat do we conclude?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThe $\\chi^2$ statistic is not significant:\n```{r}\npchisq(5.458, df=4, lower.tail=F)\n```\nWe therefore have no evidence to support rejecting our null hypothesis that our model provides a reasonable fit to the data. \n`r solend()`\n\n`r qbegin(8)`\nTry examing what the other fit measures (RMSEA, SRMR, CLI, TLI:\nHow do they compare with [the cutoffs provided in the lecture?](https://uoepsy.github.io/dapr3/2223/lectures/week1_pathintro.html#37)    \n  \nhint: `summary(modelfit, fit.measures = TRUE)`  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThe fit statistics for our model:\n\nComparative Fit Index (CFI) = 0.968\nTucker-Lewis Index (TLI) = 0.921\nRMSEA = 0.060\nSRMR = 0.063\n\nOnly CFI meets the criteria given in the lecture slides for \"considered good\". \n\n\n```{r}\nsummary(burnoutfit, fit.measures = TRUE)\n```\n\n`r solend()`\n\n\n\n\n`r qbegin(\"Extra: modification indices\")`\nExamine the modification indices of the model (use the `modindices()` function).   \nPay close attention to the `mi` column (this is the \"modification index\", which is the change in the $\\chi^2$ statistic).\nThe other interesting column is going to be the `sepc.all` column, which is the estimated parameter value of the proposed path, in a model where all the variables are standardised. This means we can evaluate whether the estimated parameter is relatively small/moderate/large, because these are all standardised correlations between -1 and 1!  \n\nAre there any paths which the modification indices suggest might improve the model? Do they make theoretical sense to include them?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nmodindices(burnoutfit)\n```\n\nThere seems to be a suggested reasonably large correlation between `burnout` and `conflict`. \nIf we were to fit this model, our fit indices may well improve and meet our cut-offs. But this may well just be overfitting.\n\n```{r}\n#| eval: false\nburnoutmod2 <- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\nburnout ~~ conflict\n\"\nsem(burnoutmod2, data=burnout)\n```\nWe will not start adjusting models based on modification indices today (or indeed in this course at all). __As a general rule for dapR3 course, we want you to specify and test a specific model, and not seek to use exploratory modifications.__  \n`r solend()`\n\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"07_path1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"theme":["united","assets/style-labs.scss"],"link-citations":true,"code-copy":false,"title":"7. Path Analysis","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}