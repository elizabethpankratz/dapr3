{"title":"9. PCA","markdown":{"yaml":{"title":"9. PCA","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"knitr::opts_chunk$set(cache = TRUE)","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nlibrary(lavaan)\nlibrary(semPlot)\noptions(digits=3, scipen = 3)\n```\n\n\n:::lo\n**Relevant packages**\n\n+ psych\n\n::: \n\n\n\n::: {.callout-note collapse=\"true\"}\n#### The goal of PCA  \n\n:::statbox\nThe goal of principal component analysis (PCA) is to find a _smaller_ number of uncorrelated variables which are linear combinations of the original ( _many_ ) variables and explain most of the variation in the data.\n:::\n\nTake a moment to think about the various constructs that you are often interested in as a researcher. This might be anything from personality traits, to language proficiency, social identity, anxiety etc. \nHow we measure such constructs is a very important consideration for research. The things we're interested in are very rarely the things we are *directly* measuring. \n\nConsider how we might assess levels of anxiety or depression. Can we ever directly measure anxiety? ^[Even if we cut open someone's brain, it's unclear what we would be looking for in order to 'measure' it. It is unclear whether anxiety even exists as a physical thing, or rather if it is simply the overarching concept we apply to a set of behaviours and feelings]. More often than not, we measure these things using questionnaire based methods, to capture the multiple dimensions of the thing we are trying to assess. Twenty questions all measuring different aspects of anxiety are (we hope) going to correlate with one another if they are capturing some commonality (the construct of \"anxiety\"). But they introduce a problem for us, which is how to deal with 20 variables that represent (in broad terms) the same thing. How can we assess \"effects on anxiety\", rather than \"effects on anxiety q1 + effects on anxiety q2 + ...\", etc.  \n\nThis leads us to the idea of *reducing the dimensionality of our data*. Can we capture a reasonable amount of the information from our 20 questions in a smaller number of variables? \n\n:::\n\n# Exercises: Police Performance\n\n```{r}\n#| include: false\n#| eval: false\njob <- read_csv('../../data/job_performance.csv')\nlibrary(psych)\njob_pca <- principal(job, nfactors = ncol(job), covar = TRUE, rotate = 'none')\nset.seed(993)\njob$arrest_rate <- round(job_pca$scores[,1:2] %*% c(1, .1) + rnorm(nrow(job),0,6))\njob$arrest_rate <- job$arrest_rate[,1]+16\njob$arrest_rate <- job$arrest_rate / (max(job$arrest_rate)+2)\n# m <- lm(nr_arrests ~ ., job)\n# car::vif(m)\n# m <- lm(job$nr_arrests ~ job_pca$scores[,1:2])\n# write_csv(job, \"../../data/police_performance.csv\")\n```\n\n\n:::frame\n__Data: police_performance.csv__  \n\nThe file [police_performance.csv](https://uoepsy.github.io/data/police_performance.csv) (available at https://uoepsy.github.io/data/police_performance.csv) contains data on fifty police officers who were rated in six different categories as part of an HR procedure. The rated skills were:\n\n- communication skills: `commun`\n- problem solving: `probl_solv`\n- logical ability: `logical`\n- learning ability: `learn`\n- physical ability: `physical`\n- appearance: `appearance`\n\nThe data also contains information on each police officer's arrest rate (proportion of arrests that lead to criminal charges). \n\nWe are interested in if the skills ratings by HR are a good set of predictors of police officer success (as indicated by their arrest rate).  \n\n:::\n\n## 1. Explore  \n\nFirst things first, we should always plot and describe our data. This is always a sensible thing to do - while many of the datasets we give you are nice and clean and organised, the data you get out of questionnaire tools, experiment software etc, are almost always quite a bit messier. It is also very useful to just eyeball the patterns of relationships between variables.  \n\n\n`r qbegin(1)`\nLoad the job performance data into R and call it `job`. \nCheck whether or not the data were read correctly into R - do the dimensions correspond to the description of the data above?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nLet's load the data:\n\n```{r}\n#| message: false\nlibrary(tidyverse)\n\njob <- read_csv('https://uoepsy.github.io/data/police_performance.csv')\ndim(job)\n```\nThere are 50 observations on 6 variables.\n\nThe top 6 rows in the data are:\n```{r}\nhead(job)\n```\n`r solend()`\n\n`r qbegin(2)`\nProvide descriptive statistics for each variable in the dataset.\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe now inspect some descriptive statistics for each variable in the dataset:\n```{r}\n# Quick summary\nsummary(job)\n```\n\n__OPTIONAL__\n\nIf you wish to create a nice looking table for a report, you could try the following code. \nHowever, I should warn you: this code is quite difficult to understand so, if you are interested, attend a lab!\n\n```{r}\nlibrary(gt)\n\n# Mean and SD of each variable\njob %>% \n  summarise(across(everything(), list(M = mean, SD = sd))) %>%\n  pivot_longer(everything()) %>% \n  mutate(\n    value = round(value, 2),\n    name = str_replace(name, '_M', '.M'),\n    name = str_replace(name, '_SD', '.SD')\n  ) %>%\n  separate(name, into = c('variable', 'summary'), sep = '\\\\.') %>%\n  pivot_wider(names_from = summary, values_from = value) %>% \n  gt()\n```\n`r solend()`\n\n\n## 2. Is PCA needed?\n\n\n::: {.callout-note collapse=\"true\"}\n#### When might we want to reduce dimensionality?  \n\nThere are many reasons we might want to reduce the dimensionality of data:  \n\n- Theory testing\n  - What are the number and nature of dimensions that best describe a theoretical construct?\n- Test construction\n  - How should I group my items into sub-scales?\n  - Which items are the best measures of my  constructs?\n- Pragmatic\n  - I have multicollinearity issues/too many variables, how can I defensibly combine my variables? \n  \nPCA is most often used for the latter - we are less interested in the theory behind our items, we just want a useful way of simplifying lots of variables down to a smaller number. \n\nRecall that we are wanting to see how well the skills ratings predict arrest rate.  \nWe might fit this model: \n```{r}\nmod <- lm(arrest_rate ~ commun + probl_solv + logical + learn + physical + appearance, data = job)\n```\nHowever, we might have reason to think that many of these predictors might be quite highly correlated with one another, and so we may be unable to draw accurate inferences. This is borne out in our variance inflation factor (VIF):  \n```{r}\nlibrary(car)\nvif(mod)\n```\n\nAs the original variables are highly correlated, it is possible to reduce the dimensionality of the problem under investigation without losing too much information.\n\nOn the other side, if the correlation between the variables under study are weak, a larger number of components is needed in order to explain sufficient variability.\n\n:::\n\n`r qbegin(3)`\nWorking with _only_ the skills ratings (not the arrest rate - we'll come back to that right at the end), investigate whether or not the variables are highly correlated and explain whether or not you PCA might be useful in this case.  \n\n**Hint:** We only have 6 variables here, but if we had many, how might you visualise `cor(job)`?\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nLet's start by looking at the correlation matrix of the data:\n```{r}\n#| label: fig-jobcor\n#| fig-align: 'center'\n#| fig-height: 4\n#| fig-width: 5\n#| fig.cap: \"Correlation between the variables in the ``Job'' dataset\"\nlibrary(pheatmap)\n\njob_skills <- job %>% select(-arrest_rate)\n\nR <- cor(job_skills)\n\npheatmap(R, breaks = seq(-1, 1, length.out = 100))\n```\n\nThe correlation between the variables seems to be quite large (it doesn't matter about direction here, only magnitude; if negative correlations were present, we would think in absolute value).\n\nThere appears to be a group of highly correlated variables comprising physical ability, appearance, communication skills, and learning ability which are correlated among themselves but uncorrelated with another group of variables.\nThe second group comprises problem solving and logical ability.\n\nThis suggests that PCA might be useful in this problem to reduce the dimensionality without a significant loss of information.\n`r solend()`\n\n## 3. Cov vs Cor?\n\n\n::: {.callout-note collapse=\"true\"}\n#### Should we perform PCA on the covariance or the correlation matrix?\n\nThis depends on the variances of the variables in the dataset. If the variables have large differences in their variances, then the variables with the largest variances will tend to dominate the first few principal components.  \nA solution to this is to standardise the variables prior to computing the covariance matrix - i.e., compute the correlation matrix!  \n\n```{r}\n# show that the correlation matrix and the covariance matrix of the standardized variables are identical\nall.equal(cor(job_skills), cov(scale(job_skills)))\n```\n\n:::\n\n`r qbegin(4)`\nLook at the variance of the skills ratings in the data set. Do you think that PCA should be carried on the covariance matrix or the correlation matrix?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nLet's have a look at the standard deviation of each variable:\n```{r}\njob_skills %>% \n  summarise(across(everything(), sd))\n```\n\nAs the standard deviations appear to be fairly similar (and so will the variances) we can perform PCA using the covariance matrix.\n`r solend()`\n\n## 4. Perform PCA\n\n`r qbegin(5)`\nUsing the `principal()` function from the `psych` package, we can perform a PCA of the job performance data, Call the output `job_pca`.\n```\njob_pca <- principal(job_skills, nfactors = ncol(job_skills), covar = ..., rotate = 'none')\n```\nDepending on your answer to the previous question, either set `covar = TRUE` or `covar = FALSE` within the `principal()` function.\n\n**Warning:** the output of the function will be in terms of standardized variables nevertheless. So you will see output with standard deviation of 1.  \n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n```{r}\nlibrary(psych)\n\njob_pca <- principal(job_skills, nfactors = ncol(job_skills), covar = TRUE, rotate = 'none')\n```\n`r solend()`\n\n::: {.callout-note collapse=\"true\"}\n#### PCA OUTPUT  \n\nWe can print the output by just typing the name of our PCA: \n\n```{r}\njob_pca\n```\n\nThe output is made up of two parts.\n\nFirst, it shows the *loading matrix*. In each column of the loading matrix we find how much each of the measured variables contributes to the computed new axis/direction (that is, the principal component). Notice that there are as many principal components as variables. \n\nThe second part of the output displays the contribution of each component to the total variance: \n\n- SS loadings: The sum of the squared loadings. The eigenvalues (see [Lecture](https://uoepsy.github.io/dapr3/2324/lectures/dapr3_08_pca#24){target=\"_blank\"}).  \n- Proportion Var: how much of the overall variance the component accounts for out of all the variables. \n- Cumulative Var: cumulative sum of Proportion Var.\n- Proportion Explained: relative amount of variance explained ($\\frac{\\text{Proportion Var}}{\\text{sum(Proportion Var)}}$.\n- Cumulative Proportion: cumulative sum of Proportion Explained.\n\nLet's focus on the row of that output called \"Cumulative Var\". This displays the cumulative sum of the variances of each principal component. Taken all together, the six principal components taken explain all of the total variance in the original data. In other words, the total variance of the principal components (the sum of their variances) is equal to the total variance in the original data (the sum of the variances of the variables).\n\nHowever, our goal is to reduce the dimensionality of our data, so it comes natural to wonder which of the six principal components explain most of the variability, and which components instead do not contribute substantially to the total variance.\n\nTo that end, the second row \"Proportion Var\" displays the proportion of the total variance explained by each component, i.e. the variance of the principal component divided by the total variance.\n\nThe last row, as we saw, is the cumulative proportion of explained variance: `0.67`, `0.67 + 0.21`, `0.67 + 0.21 + 0.11`, and so on.\n\nWe also notice that the first PC alone explains 67.3% of the total variability, while the first two components together explain almost 90% of the total variability.\nFrom the third component onwards, we do not see such a sharp increase in the proportion of explained variance, and the cumulative proportion slowly reaches the total ratio of 1 (or 100%).  \n\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: (some of) the math behind it  \n\nDoing data reduction can feel a bit like magic, and in part that's just because it's quite complicated. \n\n**The intuition**  \n\nConsider one way we might construct a correlation matrix - as the product of vector $\\mathbf{f}$ with $\\mathbf{f'}$ (f transposed): \n$$\n\\begin{equation*}\n\\mathbf{f} = \n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix} \n\\qquad \n\\mathbf{f} \\mathbf{f'} = \n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n0.9, 0.8, 0.7, 0.6, 0.5, 0.4 \\\\\n\\end{bmatrix} \n\\qquad = \\qquad\n\\begin{bmatrix}\n0.81, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 0.64, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 0.49, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 0.36, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 0.25, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 0.16 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nBut we constrain this such that the diagonal has values of 1 (the correlation of a variable with itself is 1), and lets call it **R**.\n$$\n\\begin{equation*}\n\\mathbf{R} = \n\\begin{bmatrix}\n1.00, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 1.00, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 1.00, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 1.00, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 1.00, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 1.00 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nPCA is about trying to determine a vector **f** which generates the correlation matrix **R**. a bit like unscrambling eggs!  \n\nin PCA, we express $\\mathbf{R = CC'}$, where $\\mathbf{C}$ are our principal components.  \nIf $n$ is number of variables in $R$, then $i^{th}$ component $C_i$ is the linear sum of each variable multiplied by some weighting:  \n$$\nC_i = \\sum_{j=1}^{n}w_{ij}x_{j}\n$$\n\n**How do we find $C$?**\n\nThis is where \"eigen decomposition\" comes in.  \nFor the $n \\times n$ correlation matrix $\\mathbf{R}$, there is an **eigenvector** $x_i$ that solves the equation \n$$\n\\mathbf{x_i R} = \\lambda_i \\mathbf{x_i}\n$$\nWhere the vector multiplied by the correlation matrix is equal to some **eigenvalue** $\\lambda_i$ multiplied by that vector.  \nWe can write this without subscript $i$ as: \n$$\n\\begin{align}\n& \\mathbf{R X} = \\mathbf{X \\lambda} \\\\\n& \\text{where:} \\\\\n& \\mathbf{R} = \\text{correlation matrix} \\\\\n& \\mathbf{X} = \\text{matrix of eigenvectors} \\\\\n& \\mathbf{\\lambda} = \\text{vector of eigenvalues}\n\\end{align}\n$$\nthe vectors which make up $\\mathbf{X}$ must be orthogonal [($\\mathbf{XX' = I}$)](https://miro.medium.com/max/700/1*kyg5XbrY1AOB946IE5nWWg.png), which means that $\\mathbf{R = X \\lambda X'}$\n \nWe can actually do this in R manually. \nCreating a correlation matrix\n```{r}\n# lets create a correlation matrix, as the produce of ff'\nf <- seq(.9,.4,-.1)\nR <- f %*% t(f)\n#give rownames and colnames\nrownames(R)<-colnames(R)<-paste0(\"V\",seq(1:6))\n#constrain diagonals to equal 1\ndiag(R)<-1\nR\n```\n\nEigen Decomposition\n```{r}\n# do eigen decomposition\ne <- eigen(R)\nprint(e, digits=2)\n```\n\nThe eigenvectors are orthogonal ($\\mathbf{XX' = I}$):\n```{r}\nround(e$vectors %*% t(e$vectors),2)\n```\n\nThe Principal Components $\\mathbf{C}$ are the eigenvectors scaled by the square root of the eigenvalues:\n```{r}\n#eigenvectors\ne$vectors\n#scaled by sqrt of eigenvalues\ndiag(sqrt(e$values))\n\nC <- e$vectors %*% diag(sqrt(e$values))\nC\n```\n\nAnd we can reproduce our correlation matrix, because $\\mathbf{R = CC'}$. \n```{r}\nC %*% t(C)\n```\nNow lets imagine we only consider 1 principal component.  \nWe can do this with the `principal()` function: \n```{r}\nlibrary(psych)\npc1<-principal(R, nfactors = 1, covar = FALSE, rotate = 'none')\npc1\n```\n\nLook familiar? It looks like the first component we computed manually. The first column of $\\mathbf{C}$:\n```{r}\ncbind(pc1$loadings, C=C[,1])\n```\nWe can now ask \"how well does this component (on its own) recreate our correlation matrix?\" \n```{r}\nC[,1] %*% t(C[,1])\n```\nIt looks close, but not quite. How much not quite? Measurably so!\n```{r}\nR - (C[,1] %*% t(C[,1]))\n```\n\nNotice the values on the diagonals of $\\mathbf{c_1}\\mathbf{c_1}'$.\n```{r}\ndiag(C[,1] %*% t(C[,1]))\n```\nThese aren't 1, like they are in $R$. But they are proportional: this is the amount of variance in each observed variable that is explained by this first component. Sound familiar? \n```{r}\npc1$communality\n```\nAnd likewise the 1 minus these is the unexplained variance:  \n```{r}\n1 - diag(C[,1] %*% t(C[,1]))\npc1$uniquenesses\n```\n\n:::\n\n## 5. How many components to keep?\n\nThere is no single best method to select the optimal number of components to keep, while discarding the remaining ones (which are then considered as noise components).\n\nThe following three heuristic rules are commonly used in the literature:\n\n- The cumulative proportion of explained variance criterion\n- Kaiser's rule\n- The scree plot\n- Velicer's Minimum Average Partial method\n- Parallel analysis\n\nIn the next sections we will look at each of them in turn.\n\n::: {.callout-note collapse=\"true\"}\n#### The cumulative proportion of explained variance criterion\n\nThe rule suggests to *keep as many principal components as needed in order to explain approximately 80-90% of the total variance.*\n\n:::\n\n`r qbegin(6)`\nLooking again at the PCA output, how many principal components would you keep if you were following the cumulative proportion of explained variance criterion?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nLet's look again at the PCA summary:\n\n```{r}\njob_pca$loadings\n```\n\nThe following part of the output tells us that the first two components explain 88.3% of the total variance.\n```\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n```\n\nAccording to this criterion, we should keep 2 principal components.\n`r solend()`\n\n::: {.callout-note collapse=\"true\"}\n#### Kaiser's rule\n  \nAccording to Kaiser's rule, we should **keep the principal components having variance larger than 1**. Standardized variables have a variance equal 1. Because we have 6 variables in the data set, and the total variance is 6, the value 1 represents the average variance in the data:\n$$\n\\frac{1 + 1 + 1 + 1 + 1 + 1}{6} = 1\n$$\n\n__Hint:__\n\nThe variances of each PC are shown in the row of the output named `SS loadings` and also in\n`job_pca$values`. The average variance is:\n\n```{r}\nmean(job_pca$values)\n```\n:::\n\n`r qbegin(7)`\nLooking again at the PCA output, how many principal components would you keep if you were following Kaiser's criterion?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\njob_pca$loadings\n```\n\nThe variances are shown in the row\n```\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\n```\n\nFrom the result we see that only the first two principal components have variance greater than 1, so this rule suggests to keep 2 PCs only.\n`r solend()`\n\n::: {.callout-note collapse=\"true\"}\n#### The scree plot\n\nThe scree plot is a graphical criterion which involves plotting the variance for each principal component.\nThis can be easily done by calling `plot` on the variances, which are stored in `job_pca$values`\n\n```{r}\nplot(x = 1:length(job_pca$values), y = job_pca$values, \n     type = 'b', xlab = '', ylab = 'Variance', \n     main = 'Police officers: scree plot', frame.plot = FALSE)\n```\n\nwhere the argument `type = 'b'` tells R that the plot should have _both_ points and lines.\n\nA typical scree plot features higher variances for the initial components and quickly drops to small variances where the curve is almost flat. The flat part of the curve represents the noise components, which are not able to capture the main sources of variability in the system. \n\nAccording to the scree plot criterion, we should **keep as many principal components as where the \"elbow\" in the plot occurs.** By elbow we mean the variance before the curve looks almost flat.\n\nAlternatively, some people prefer to use the function `scree()` from the `psych` package:\n\n```{r}\nscree(job_skills, factors = FALSE)\n```\n\nThis also draws a horizontal line at y = 1. So, if you are making a decision about how many PCs to keep by looking at where the plot falls below the y = 1 line, you are basically following Kaiser's rule. In fact, Kaiser's criterion tells you to keep as many PCs as are those with a variance (= eigenvalue) greater than 1.\n  \n  \n__NOTE: Scree plots are subjective and may have multiple or no obvious kinks/elbows, making them hard to interpret__  \n\n\n:::\n\n`r qbegin(8)`\nAccording to the scree plot, how many principal components would you retain?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThis criterion then suggests to keep three principal components.\n`r solend()`\n\n::: {.callout-note collapse=\"true\"}\n#### Velicer’s Minimum Average Partial (MAP) method\n\nThe Minimum Average Partial (MAP) test computes the partial correlation matrix (removing and adjusting for a component from the correlation matrix), sequentially partialling out each component. At each step, the partial correlations are squared and their average is computed.  \nAt first, the components which are removed will be those that are most representative of the shared variance between 2+ variables, meaning that the \"average squared partial correlation\" will decrease. At some point in the process, the components being removed will begin represent variance that is specific to individual variables, meaning that the average squared partial correlation will increase.  \nThe MAP method is to keep the number of components for which the average squared partial correlation is at the minimum. \n\nWe can conduct MAP in R using:\n```{r}\n#| eval: false\nVSS(data, plot = FALSE, method=\"pc\", n = ncol(data))\n```\n(be aware there is a lot of other information in this output too! For now just focus on the map column)\n\n__NOTE: The MAP method will sometimes tend to under-extract (suggest too few components)__  \n\n:::\n\n`r qbegin(9)`\nHow many components should we keep according to the MAP method?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nVSS(job_skills, plot=FALSE, method=\"pc\", n = ncol(job_skills))\n```\n\n```{r}\n#| include: false\njob_map <- VSS(job_skills, plot=FALSE, method=\"pc\", n = ncol(job_skills))$map\npaste(\"MAP is lowest for\", which.min(job_map), \"components\")\n```\n\nAccording to the MAP criterion we should keep 2 principal components.\n`r solend()`\n\n\n::: {.callout-note collapse=\"true\"}\n#### Parallel analysis\n\nParallel analysis involves simulating lots of datasets of the same dimension but in which the variables are uncorrelated. For each of these simulations, a PCA is conducted on its correlation matrix, and the eigenvalues are extracted. We can then compare our eigenvalues from the PCA on our *actual* data to the average eigenvalues across these simulations. \nIn theory, for uncorrelated variables, no components should explain more variance than any others, and eigenvalues should be equal to 1. In reality, variables are rarely truly uncorrelated, and so there will be slight variation in the magnitude of eigenvalues simply due to chance. \nThe parallel analysis method suggests keeping those components for which the eigenvalues are greater than those from the simulations. \n\nIt can be conducted in R using:\n```{r}\n#| eval: false\nfa.parallel(job_skills, fa=\"pc\", n.iter = 500)\n```\n\n__NOTE: Parallel analysis will sometimes tend to over-extract (suggest too many components)__  \n\n:::\n\n\n`r qbegin(10)`\nHow many components should we keep according to parallel analysis?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nfa.parallel(job_skills, fa=\"pc\", n.iter = 500)\n```\n\nParallel analysis suggests to keep 1 principal component only as there is only one PC with an eigenvalue higher than the simulated random ones in red.\n\n`r solend()`\n\n## 6. Retaining N Components  \n\n`r qbegin(11)`\nBased on the set of criteria above, make a decision on how many components you will keep.  \n\nSometimes, there may also be pragmatic reasons for keeping a certain number (e.g. if you want specifically 1 dimension, you may be willing to accept a lower proportion of explained variance).  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nIt's a common thing to see disagreement between the methods to determine how many components we keep, and ultimately this is a decision that we as researchers have to make and explain.\n\n\n| method                    | recommendation                  | \n| ------------------------- | ------------------------------- |\n| explaining >80\\% variance | keep 2 components               |\n| kaiser's rule             | keep 2 components               |\n| scree plot                | keep 3 components? (subjective) |\n| MAP                       | keep 2 components               |\n| parallel analysis         | keep 1 component                |\n\n\nBecause three out of the five selection criteria above suggest to keep 2 principal components, here we will keep 2 components. This solution explains a reasonable proportion of the variance (88%), and it would be perfectly defensible to instead go for 3, explaining 98%\n\n`r solend()`\n\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Examining loadings  \n\n\nLet's have a look at the selected principal components:\n```{r}\njob_pca$loadings[, 1:2]\n```\n\nand at their corresponding proportion of total variance explained:\n```{r}\njob_pca$values / sum(job_pca$values)\n```\n\nWe see that the first PC accounts for 67.3% of the total variability. All loadings seem to have the same magnitude apart from `probl_solv` and `logical` which are closer to zero.\nThe first component looks like a sort of average of the officers performance scores excluding problem solving and logical ability.\n\nThe second principal component, which explains only 21% of the total variance, has two loadings clearly distant from zero: the ones associated to problem solving and logical ability.\nIt distinguishes police officers with strong logical and problem solving skills and low scores on other skills (note the negative magnitudes).  \n\nWe have just seen how to interpret the first components by looking at the magnitude and sign of the coefficients for each measured variable.\n\n:::rtip\nFor interpretation purposes, it might help hiding very small loadings. This can be done by specifying the cutoff value in the `print()` function. However, this only works when you pass the loadings for **all** the PCs:\n```{r}\nprint(job_pca$loadings, cutoff = 0.3)\n```\n\n:::\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: How well are the units represented in the reduced space?  \n\nWe now focus our attention on the following question: Are all the statistical units (police officers) well represented in the 2D plot?\n\nThe 2D representation of the original data, which comprise 6 measured variables, is an approximation and henceforth it may happen that not all units are well represented in this new space.\n\nTypically, it is good to assess the approximation for each statistical unit by inspecting the scores on the discarded principal components.\nIf a unit has a high score on those components, then this is a sign that the unit might be highly misplaced in the new space and misrepresented.\n\nConsider the 3D example below. There are three cases (= units or individuals). In the original space they are all very different from each other. For example, cases 1 and 2 are very different in their x and y values, but very similar in their z value. Cases 2 and 3 are very similar in their x and y values but very different in their z value. Cases 1 and 3 have very different values for all three variables x, y, and z.\n\nHowever, when represented in the 2D space given by the two principal components, units 2 and 3 seems like they are very similar when, in fact, they were very different in the original space which also accounted for the z variable.\n\n```{r}\n#| echo: false\n#| out-width: \"\\\\textwidth\"\n#| include: true\n#| fig-align: 'center'\nknitr::include_graphics('images/pcaefa/pca_bad_representation.png')\n```\n\nWe typically measure how badly a unit is represented in the new coordinate system by considering the **sum of squared scores on the discarded principal components:**\n\n```{r}\nscores_discarded <- job_pca$scores[, -(1:2)]\nsum_sq <- rowSums(scores_discarded^2)\nsum_sq\n```\n\nUnits with a high score should be considered for further inspection as it may happen that they are represented as close to another unit when, in fact, they might be very different.\n\n```{r}\nboxplot(sum_sq)\n```\n\nThere seem to be only five outliers, and they are not too high compared to the rest of the scores. For this reason, we will consider the 2D representation of the data to be satisfactory.\n\n:::\n\n\n## 7. Using PCA scores\n\n::: {.callout-note collapse=\"true\"}\n#### PCA scores\n\nNow that we have decided to reduce our six variables down to two principal components, we can, for each of our observations, get their _scores_ on each of our components.  \n\n```{r}\njob_pca2 <- principal(job_skills, nfactors = 2, covar = TRUE, rotate = 'none')\nhead(job_pca2$scores)\n```\n\nPCA scores are essentially weighted combinations of an individuals responses to the items.  \n$$\n\\text{score}_{\\text{component j}} = w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 +\\, ... \\, + w_{pj}x_p\n$$\nWhere $w$ are the weights, $x$ the variable scores. \n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: How are weights calculated?  \n\nThe weights are calculated from the eigenvectors as \n$$\nw_{ij} = \\frac{a_{ij}}{\\sqrt(\\lambda_j)}\n$$\nwhere $w_{ij}$ is the weight for a given variable $i$ on component $j$ , $a_{ij}$ is the value from the eigenvector for item $i$ on component $j$ and $\\lambda_{j}$ is the eigenvalue for that component.\n\n:::\n\nIn the literature, some authors also suggest to look at the correlation between the principal component scores and the measured variables:\n\n```{r}\n# First PC\ncor(job_pca2$scores[,1], job_skills)\n```\n\nThe first PC is strongly correlated with all the measured variables except `probl_solv` and `logical`. \nAs mentioned when looking at loadings, all these variables seem to contributed to the first PC.\n\n```{r}\n# Second PC\ncor(job_pca2$scores[,2], job_skills)\n```\n\nThe second PC is strongly correlated with `probl_solv` and `logical`, and slightly negatively correlated with the remaining variables. This separates police officers with clear logical and problem solving skills and a low rating on other skills.\n\nWe can also visualise our observations (the police officers) in the reduced space given by the retained principal component scores.\n\n```{r}\n#| fig-width: 5\n#| fig-align: 'center'\ntibble(pc1 = job_pca2$scores[, 1],\n       pc2 = job_pca2$scores[, 2]) %>%\n  ggplot(.,aes(x=pc1,y=pc2))+\n  geom_point()\n```\n\n:::\n\n\n`r qbegin(12)`\nWe have reduced our six variables down to two principal components, and we are now able to use the scores on each component in a subsequent analysis! \n\nJoin the principal component scores for your retained components to the original dataset which has the arrest rates in.  \n\nThen fit a linear model to look at how the arrest rate of police officers is predicted by the two components representing different composites of the skills ratings by HR.   \n\nCheck for multicollinearity between your predictors.  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n# add the PCA scores to the dataset\njob <- \n  job %>% mutate(\n    pc1 = job_pca2$scores[,1],\n    pc2 = job_pca2$scores[,2]\n  )\n# use the scores in an analysis\nmod <- lm(arrest_rate ~ pc1 + pc2, data = job)\n\n# multicollinearity isn't a problem, because the components are orthogonal!! \nlibrary(car)\nvif(mod)\n\nsummary(mod)\n```\n`r solend()`\n\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nlibrary(lavaan)\nlibrary(semPlot)\n# knitr::opts_chunk$set(cache = TRUE)\noptions(digits=3, scipen = 3)\n```\n\n\n:::lo\n**Relevant packages**\n\n+ psych\n\n::: \n\n\n\n::: {.callout-note collapse=\"true\"}\n#### The goal of PCA  \n\n:::statbox\nThe goal of principal component analysis (PCA) is to find a _smaller_ number of uncorrelated variables which are linear combinations of the original ( _many_ ) variables and explain most of the variation in the data.\n:::\n\nTake a moment to think about the various constructs that you are often interested in as a researcher. This might be anything from personality traits, to language proficiency, social identity, anxiety etc. \nHow we measure such constructs is a very important consideration for research. The things we're interested in are very rarely the things we are *directly* measuring. \n\nConsider how we might assess levels of anxiety or depression. Can we ever directly measure anxiety? ^[Even if we cut open someone's brain, it's unclear what we would be looking for in order to 'measure' it. It is unclear whether anxiety even exists as a physical thing, or rather if it is simply the overarching concept we apply to a set of behaviours and feelings]. More often than not, we measure these things using questionnaire based methods, to capture the multiple dimensions of the thing we are trying to assess. Twenty questions all measuring different aspects of anxiety are (we hope) going to correlate with one another if they are capturing some commonality (the construct of \"anxiety\"). But they introduce a problem for us, which is how to deal with 20 variables that represent (in broad terms) the same thing. How can we assess \"effects on anxiety\", rather than \"effects on anxiety q1 + effects on anxiety q2 + ...\", etc.  \n\nThis leads us to the idea of *reducing the dimensionality of our data*. Can we capture a reasonable amount of the information from our 20 questions in a smaller number of variables? \n\n:::\n\n# Exercises: Police Performance\n\n```{r}\n#| include: false\n#| eval: false\njob <- read_csv('../../data/job_performance.csv')\nlibrary(psych)\njob_pca <- principal(job, nfactors = ncol(job), covar = TRUE, rotate = 'none')\nset.seed(993)\njob$arrest_rate <- round(job_pca$scores[,1:2] %*% c(1, .1) + rnorm(nrow(job),0,6))\njob$arrest_rate <- job$arrest_rate[,1]+16\njob$arrest_rate <- job$arrest_rate / (max(job$arrest_rate)+2)\n# m <- lm(nr_arrests ~ ., job)\n# car::vif(m)\n# m <- lm(job$nr_arrests ~ job_pca$scores[,1:2])\n# write_csv(job, \"../../data/police_performance.csv\")\n```\n\n\n:::frame\n__Data: police_performance.csv__  \n\nThe file [police_performance.csv](https://uoepsy.github.io/data/police_performance.csv) (available at https://uoepsy.github.io/data/police_performance.csv) contains data on fifty police officers who were rated in six different categories as part of an HR procedure. The rated skills were:\n\n- communication skills: `commun`\n- problem solving: `probl_solv`\n- logical ability: `logical`\n- learning ability: `learn`\n- physical ability: `physical`\n- appearance: `appearance`\n\nThe data also contains information on each police officer's arrest rate (proportion of arrests that lead to criminal charges). \n\nWe are interested in if the skills ratings by HR are a good set of predictors of police officer success (as indicated by their arrest rate).  \n\n:::\n\n## 1. Explore  \n\nFirst things first, we should always plot and describe our data. This is always a sensible thing to do - while many of the datasets we give you are nice and clean and organised, the data you get out of questionnaire tools, experiment software etc, are almost always quite a bit messier. It is also very useful to just eyeball the patterns of relationships between variables.  \n\n\n`r qbegin(1)`\nLoad the job performance data into R and call it `job`. \nCheck whether or not the data were read correctly into R - do the dimensions correspond to the description of the data above?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nLet's load the data:\n\n```{r}\n#| message: false\nlibrary(tidyverse)\n\njob <- read_csv('https://uoepsy.github.io/data/police_performance.csv')\ndim(job)\n```\nThere are 50 observations on 6 variables.\n\nThe top 6 rows in the data are:\n```{r}\nhead(job)\n```\n`r solend()`\n\n`r qbegin(2)`\nProvide descriptive statistics for each variable in the dataset.\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe now inspect some descriptive statistics for each variable in the dataset:\n```{r}\n# Quick summary\nsummary(job)\n```\n\n__OPTIONAL__\n\nIf you wish to create a nice looking table for a report, you could try the following code. \nHowever, I should warn you: this code is quite difficult to understand so, if you are interested, attend a lab!\n\n```{r}\nlibrary(gt)\n\n# Mean and SD of each variable\njob %>% \n  summarise(across(everything(), list(M = mean, SD = sd))) %>%\n  pivot_longer(everything()) %>% \n  mutate(\n    value = round(value, 2),\n    name = str_replace(name, '_M', '.M'),\n    name = str_replace(name, '_SD', '.SD')\n  ) %>%\n  separate(name, into = c('variable', 'summary'), sep = '\\\\.') %>%\n  pivot_wider(names_from = summary, values_from = value) %>% \n  gt()\n```\n`r solend()`\n\n\n## 2. Is PCA needed?\n\n\n::: {.callout-note collapse=\"true\"}\n#### When might we want to reduce dimensionality?  \n\nThere are many reasons we might want to reduce the dimensionality of data:  \n\n- Theory testing\n  - What are the number and nature of dimensions that best describe a theoretical construct?\n- Test construction\n  - How should I group my items into sub-scales?\n  - Which items are the best measures of my  constructs?\n- Pragmatic\n  - I have multicollinearity issues/too many variables, how can I defensibly combine my variables? \n  \nPCA is most often used for the latter - we are less interested in the theory behind our items, we just want a useful way of simplifying lots of variables down to a smaller number. \n\nRecall that we are wanting to see how well the skills ratings predict arrest rate.  \nWe might fit this model: \n```{r}\nmod <- lm(arrest_rate ~ commun + probl_solv + logical + learn + physical + appearance, data = job)\n```\nHowever, we might have reason to think that many of these predictors might be quite highly correlated with one another, and so we may be unable to draw accurate inferences. This is borne out in our variance inflation factor (VIF):  \n```{r}\nlibrary(car)\nvif(mod)\n```\n\nAs the original variables are highly correlated, it is possible to reduce the dimensionality of the problem under investigation without losing too much information.\n\nOn the other side, if the correlation between the variables under study are weak, a larger number of components is needed in order to explain sufficient variability.\n\n:::\n\n`r qbegin(3)`\nWorking with _only_ the skills ratings (not the arrest rate - we'll come back to that right at the end), investigate whether or not the variables are highly correlated and explain whether or not you PCA might be useful in this case.  \n\n**Hint:** We only have 6 variables here, but if we had many, how might you visualise `cor(job)`?\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nLet's start by looking at the correlation matrix of the data:\n```{r}\n#| label: fig-jobcor\n#| fig-align: 'center'\n#| fig-height: 4\n#| fig-width: 5\n#| fig.cap: \"Correlation between the variables in the ``Job'' dataset\"\nlibrary(pheatmap)\n\njob_skills <- job %>% select(-arrest_rate)\n\nR <- cor(job_skills)\n\npheatmap(R, breaks = seq(-1, 1, length.out = 100))\n```\n\nThe correlation between the variables seems to be quite large (it doesn't matter about direction here, only magnitude; if negative correlations were present, we would think in absolute value).\n\nThere appears to be a group of highly correlated variables comprising physical ability, appearance, communication skills, and learning ability which are correlated among themselves but uncorrelated with another group of variables.\nThe second group comprises problem solving and logical ability.\n\nThis suggests that PCA might be useful in this problem to reduce the dimensionality without a significant loss of information.\n`r solend()`\n\n## 3. Cov vs Cor?\n\n\n::: {.callout-note collapse=\"true\"}\n#### Should we perform PCA on the covariance or the correlation matrix?\n\nThis depends on the variances of the variables in the dataset. If the variables have large differences in their variances, then the variables with the largest variances will tend to dominate the first few principal components.  \nA solution to this is to standardise the variables prior to computing the covariance matrix - i.e., compute the correlation matrix!  \n\n```{r}\n# show that the correlation matrix and the covariance matrix of the standardized variables are identical\nall.equal(cor(job_skills), cov(scale(job_skills)))\n```\n\n:::\n\n`r qbegin(4)`\nLook at the variance of the skills ratings in the data set. Do you think that PCA should be carried on the covariance matrix or the correlation matrix?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nLet's have a look at the standard deviation of each variable:\n```{r}\njob_skills %>% \n  summarise(across(everything(), sd))\n```\n\nAs the standard deviations appear to be fairly similar (and so will the variances) we can perform PCA using the covariance matrix.\n`r solend()`\n\n## 4. Perform PCA\n\n`r qbegin(5)`\nUsing the `principal()` function from the `psych` package, we can perform a PCA of the job performance data, Call the output `job_pca`.\n```\njob_pca <- principal(job_skills, nfactors = ncol(job_skills), covar = ..., rotate = 'none')\n```\nDepending on your answer to the previous question, either set `covar = TRUE` or `covar = FALSE` within the `principal()` function.\n\n**Warning:** the output of the function will be in terms of standardized variables nevertheless. So you will see output with standard deviation of 1.  \n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n```{r}\nlibrary(psych)\n\njob_pca <- principal(job_skills, nfactors = ncol(job_skills), covar = TRUE, rotate = 'none')\n```\n`r solend()`\n\n::: {.callout-note collapse=\"true\"}\n#### PCA OUTPUT  \n\nWe can print the output by just typing the name of our PCA: \n\n```{r}\njob_pca\n```\n\nThe output is made up of two parts.\n\nFirst, it shows the *loading matrix*. In each column of the loading matrix we find how much each of the measured variables contributes to the computed new axis/direction (that is, the principal component). Notice that there are as many principal components as variables. \n\nThe second part of the output displays the contribution of each component to the total variance: \n\n- SS loadings: The sum of the squared loadings. The eigenvalues (see [Lecture](https://uoepsy.github.io/dapr3/2324/lectures/dapr3_08_pca#24){target=\"_blank\"}).  \n- Proportion Var: how much of the overall variance the component accounts for out of all the variables. \n- Cumulative Var: cumulative sum of Proportion Var.\n- Proportion Explained: relative amount of variance explained ($\\frac{\\text{Proportion Var}}{\\text{sum(Proportion Var)}}$.\n- Cumulative Proportion: cumulative sum of Proportion Explained.\n\nLet's focus on the row of that output called \"Cumulative Var\". This displays the cumulative sum of the variances of each principal component. Taken all together, the six principal components taken explain all of the total variance in the original data. In other words, the total variance of the principal components (the sum of their variances) is equal to the total variance in the original data (the sum of the variances of the variables).\n\nHowever, our goal is to reduce the dimensionality of our data, so it comes natural to wonder which of the six principal components explain most of the variability, and which components instead do not contribute substantially to the total variance.\n\nTo that end, the second row \"Proportion Var\" displays the proportion of the total variance explained by each component, i.e. the variance of the principal component divided by the total variance.\n\nThe last row, as we saw, is the cumulative proportion of explained variance: `0.67`, `0.67 + 0.21`, `0.67 + 0.21 + 0.11`, and so on.\n\nWe also notice that the first PC alone explains 67.3% of the total variability, while the first two components together explain almost 90% of the total variability.\nFrom the third component onwards, we do not see such a sharp increase in the proportion of explained variance, and the cumulative proportion slowly reaches the total ratio of 1 (or 100%).  \n\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: (some of) the math behind it  \n\nDoing data reduction can feel a bit like magic, and in part that's just because it's quite complicated. \n\n**The intuition**  \n\nConsider one way we might construct a correlation matrix - as the product of vector $\\mathbf{f}$ with $\\mathbf{f'}$ (f transposed): \n$$\n\\begin{equation*}\n\\mathbf{f} = \n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix} \n\\qquad \n\\mathbf{f} \\mathbf{f'} = \n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n0.9, 0.8, 0.7, 0.6, 0.5, 0.4 \\\\\n\\end{bmatrix} \n\\qquad = \\qquad\n\\begin{bmatrix}\n0.81, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 0.64, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 0.49, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 0.36, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 0.25, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 0.16 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nBut we constrain this such that the diagonal has values of 1 (the correlation of a variable with itself is 1), and lets call it **R**.\n$$\n\\begin{equation*}\n\\mathbf{R} = \n\\begin{bmatrix}\n1.00, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 1.00, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 1.00, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 1.00, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 1.00, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 1.00 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nPCA is about trying to determine a vector **f** which generates the correlation matrix **R**. a bit like unscrambling eggs!  \n\nin PCA, we express $\\mathbf{R = CC'}$, where $\\mathbf{C}$ are our principal components.  \nIf $n$ is number of variables in $R$, then $i^{th}$ component $C_i$ is the linear sum of each variable multiplied by some weighting:  \n$$\nC_i = \\sum_{j=1}^{n}w_{ij}x_{j}\n$$\n\n**How do we find $C$?**\n\nThis is where \"eigen decomposition\" comes in.  \nFor the $n \\times n$ correlation matrix $\\mathbf{R}$, there is an **eigenvector** $x_i$ that solves the equation \n$$\n\\mathbf{x_i R} = \\lambda_i \\mathbf{x_i}\n$$\nWhere the vector multiplied by the correlation matrix is equal to some **eigenvalue** $\\lambda_i$ multiplied by that vector.  \nWe can write this without subscript $i$ as: \n$$\n\\begin{align}\n& \\mathbf{R X} = \\mathbf{X \\lambda} \\\\\n& \\text{where:} \\\\\n& \\mathbf{R} = \\text{correlation matrix} \\\\\n& \\mathbf{X} = \\text{matrix of eigenvectors} \\\\\n& \\mathbf{\\lambda} = \\text{vector of eigenvalues}\n\\end{align}\n$$\nthe vectors which make up $\\mathbf{X}$ must be orthogonal [($\\mathbf{XX' = I}$)](https://miro.medium.com/max/700/1*kyg5XbrY1AOB946IE5nWWg.png), which means that $\\mathbf{R = X \\lambda X'}$\n \nWe can actually do this in R manually. \nCreating a correlation matrix\n```{r}\n# lets create a correlation matrix, as the produce of ff'\nf <- seq(.9,.4,-.1)\nR <- f %*% t(f)\n#give rownames and colnames\nrownames(R)<-colnames(R)<-paste0(\"V\",seq(1:6))\n#constrain diagonals to equal 1\ndiag(R)<-1\nR\n```\n\nEigen Decomposition\n```{r}\n# do eigen decomposition\ne <- eigen(R)\nprint(e, digits=2)\n```\n\nThe eigenvectors are orthogonal ($\\mathbf{XX' = I}$):\n```{r}\nround(e$vectors %*% t(e$vectors),2)\n```\n\nThe Principal Components $\\mathbf{C}$ are the eigenvectors scaled by the square root of the eigenvalues:\n```{r}\n#eigenvectors\ne$vectors\n#scaled by sqrt of eigenvalues\ndiag(sqrt(e$values))\n\nC <- e$vectors %*% diag(sqrt(e$values))\nC\n```\n\nAnd we can reproduce our correlation matrix, because $\\mathbf{R = CC'}$. \n```{r}\nC %*% t(C)\n```\nNow lets imagine we only consider 1 principal component.  \nWe can do this with the `principal()` function: \n```{r}\nlibrary(psych)\npc1<-principal(R, nfactors = 1, covar = FALSE, rotate = 'none')\npc1\n```\n\nLook familiar? It looks like the first component we computed manually. The first column of $\\mathbf{C}$:\n```{r}\ncbind(pc1$loadings, C=C[,1])\n```\nWe can now ask \"how well does this component (on its own) recreate our correlation matrix?\" \n```{r}\nC[,1] %*% t(C[,1])\n```\nIt looks close, but not quite. How much not quite? Measurably so!\n```{r}\nR - (C[,1] %*% t(C[,1]))\n```\n\nNotice the values on the diagonals of $\\mathbf{c_1}\\mathbf{c_1}'$.\n```{r}\ndiag(C[,1] %*% t(C[,1]))\n```\nThese aren't 1, like they are in $R$. But they are proportional: this is the amount of variance in each observed variable that is explained by this first component. Sound familiar? \n```{r}\npc1$communality\n```\nAnd likewise the 1 minus these is the unexplained variance:  \n```{r}\n1 - diag(C[,1] %*% t(C[,1]))\npc1$uniquenesses\n```\n\n:::\n\n## 5. How many components to keep?\n\nThere is no single best method to select the optimal number of components to keep, while discarding the remaining ones (which are then considered as noise components).\n\nThe following three heuristic rules are commonly used in the literature:\n\n- The cumulative proportion of explained variance criterion\n- Kaiser's rule\n- The scree plot\n- Velicer's Minimum Average Partial method\n- Parallel analysis\n\nIn the next sections we will look at each of them in turn.\n\n::: {.callout-note collapse=\"true\"}\n#### The cumulative proportion of explained variance criterion\n\nThe rule suggests to *keep as many principal components as needed in order to explain approximately 80-90% of the total variance.*\n\n:::\n\n`r qbegin(6)`\nLooking again at the PCA output, how many principal components would you keep if you were following the cumulative proportion of explained variance criterion?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nLet's look again at the PCA summary:\n\n```{r}\njob_pca$loadings\n```\n\nThe following part of the output tells us that the first two components explain 88.3% of the total variance.\n```\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n```\n\nAccording to this criterion, we should keep 2 principal components.\n`r solend()`\n\n::: {.callout-note collapse=\"true\"}\n#### Kaiser's rule\n  \nAccording to Kaiser's rule, we should **keep the principal components having variance larger than 1**. Standardized variables have a variance equal 1. Because we have 6 variables in the data set, and the total variance is 6, the value 1 represents the average variance in the data:\n$$\n\\frac{1 + 1 + 1 + 1 + 1 + 1}{6} = 1\n$$\n\n__Hint:__\n\nThe variances of each PC are shown in the row of the output named `SS loadings` and also in\n`job_pca$values`. The average variance is:\n\n```{r}\nmean(job_pca$values)\n```\n:::\n\n`r qbegin(7)`\nLooking again at the PCA output, how many principal components would you keep if you were following Kaiser's criterion?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\njob_pca$loadings\n```\n\nThe variances are shown in the row\n```\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\n```\n\nFrom the result we see that only the first two principal components have variance greater than 1, so this rule suggests to keep 2 PCs only.\n`r solend()`\n\n::: {.callout-note collapse=\"true\"}\n#### The scree plot\n\nThe scree plot is a graphical criterion which involves plotting the variance for each principal component.\nThis can be easily done by calling `plot` on the variances, which are stored in `job_pca$values`\n\n```{r}\nplot(x = 1:length(job_pca$values), y = job_pca$values, \n     type = 'b', xlab = '', ylab = 'Variance', \n     main = 'Police officers: scree plot', frame.plot = FALSE)\n```\n\nwhere the argument `type = 'b'` tells R that the plot should have _both_ points and lines.\n\nA typical scree plot features higher variances for the initial components and quickly drops to small variances where the curve is almost flat. The flat part of the curve represents the noise components, which are not able to capture the main sources of variability in the system. \n\nAccording to the scree plot criterion, we should **keep as many principal components as where the \"elbow\" in the plot occurs.** By elbow we mean the variance before the curve looks almost flat.\n\nAlternatively, some people prefer to use the function `scree()` from the `psych` package:\n\n```{r}\nscree(job_skills, factors = FALSE)\n```\n\nThis also draws a horizontal line at y = 1. So, if you are making a decision about how many PCs to keep by looking at where the plot falls below the y = 1 line, you are basically following Kaiser's rule. In fact, Kaiser's criterion tells you to keep as many PCs as are those with a variance (= eigenvalue) greater than 1.\n  \n  \n__NOTE: Scree plots are subjective and may have multiple or no obvious kinks/elbows, making them hard to interpret__  \n\n\n:::\n\n`r qbegin(8)`\nAccording to the scree plot, how many principal components would you retain?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThis criterion then suggests to keep three principal components.\n`r solend()`\n\n::: {.callout-note collapse=\"true\"}\n#### Velicer’s Minimum Average Partial (MAP) method\n\nThe Minimum Average Partial (MAP) test computes the partial correlation matrix (removing and adjusting for a component from the correlation matrix), sequentially partialling out each component. At each step, the partial correlations are squared and their average is computed.  \nAt first, the components which are removed will be those that are most representative of the shared variance between 2+ variables, meaning that the \"average squared partial correlation\" will decrease. At some point in the process, the components being removed will begin represent variance that is specific to individual variables, meaning that the average squared partial correlation will increase.  \nThe MAP method is to keep the number of components for which the average squared partial correlation is at the minimum. \n\nWe can conduct MAP in R using:\n```{r}\n#| eval: false\nVSS(data, plot = FALSE, method=\"pc\", n = ncol(data))\n```\n(be aware there is a lot of other information in this output too! For now just focus on the map column)\n\n__NOTE: The MAP method will sometimes tend to under-extract (suggest too few components)__  \n\n:::\n\n`r qbegin(9)`\nHow many components should we keep according to the MAP method?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nVSS(job_skills, plot=FALSE, method=\"pc\", n = ncol(job_skills))\n```\n\n```{r}\n#| include: false\njob_map <- VSS(job_skills, plot=FALSE, method=\"pc\", n = ncol(job_skills))$map\npaste(\"MAP is lowest for\", which.min(job_map), \"components\")\n```\n\nAccording to the MAP criterion we should keep 2 principal components.\n`r solend()`\n\n\n::: {.callout-note collapse=\"true\"}\n#### Parallel analysis\n\nParallel analysis involves simulating lots of datasets of the same dimension but in which the variables are uncorrelated. For each of these simulations, a PCA is conducted on its correlation matrix, and the eigenvalues are extracted. We can then compare our eigenvalues from the PCA on our *actual* data to the average eigenvalues across these simulations. \nIn theory, for uncorrelated variables, no components should explain more variance than any others, and eigenvalues should be equal to 1. In reality, variables are rarely truly uncorrelated, and so there will be slight variation in the magnitude of eigenvalues simply due to chance. \nThe parallel analysis method suggests keeping those components for which the eigenvalues are greater than those from the simulations. \n\nIt can be conducted in R using:\n```{r}\n#| eval: false\nfa.parallel(job_skills, fa=\"pc\", n.iter = 500)\n```\n\n__NOTE: Parallel analysis will sometimes tend to over-extract (suggest too many components)__  \n\n:::\n\n\n`r qbegin(10)`\nHow many components should we keep according to parallel analysis?\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nfa.parallel(job_skills, fa=\"pc\", n.iter = 500)\n```\n\nParallel analysis suggests to keep 1 principal component only as there is only one PC with an eigenvalue higher than the simulated random ones in red.\n\n`r solend()`\n\n## 6. Retaining N Components  \n\n`r qbegin(11)`\nBased on the set of criteria above, make a decision on how many components you will keep.  \n\nSometimes, there may also be pragmatic reasons for keeping a certain number (e.g. if you want specifically 1 dimension, you may be willing to accept a lower proportion of explained variance).  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nIt's a common thing to see disagreement between the methods to determine how many components we keep, and ultimately this is a decision that we as researchers have to make and explain.\n\n\n| method                    | recommendation                  | \n| ------------------------- | ------------------------------- |\n| explaining >80\\% variance | keep 2 components               |\n| kaiser's rule             | keep 2 components               |\n| scree plot                | keep 3 components? (subjective) |\n| MAP                       | keep 2 components               |\n| parallel analysis         | keep 1 component                |\n\n\nBecause three out of the five selection criteria above suggest to keep 2 principal components, here we will keep 2 components. This solution explains a reasonable proportion of the variance (88%), and it would be perfectly defensible to instead go for 3, explaining 98%\n\n`r solend()`\n\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Examining loadings  \n\n\nLet's have a look at the selected principal components:\n```{r}\njob_pca$loadings[, 1:2]\n```\n\nand at their corresponding proportion of total variance explained:\n```{r}\njob_pca$values / sum(job_pca$values)\n```\n\nWe see that the first PC accounts for 67.3% of the total variability. All loadings seem to have the same magnitude apart from `probl_solv` and `logical` which are closer to zero.\nThe first component looks like a sort of average of the officers performance scores excluding problem solving and logical ability.\n\nThe second principal component, which explains only 21% of the total variance, has two loadings clearly distant from zero: the ones associated to problem solving and logical ability.\nIt distinguishes police officers with strong logical and problem solving skills and low scores on other skills (note the negative magnitudes).  \n\nWe have just seen how to interpret the first components by looking at the magnitude and sign of the coefficients for each measured variable.\n\n:::rtip\nFor interpretation purposes, it might help hiding very small loadings. This can be done by specifying the cutoff value in the `print()` function. However, this only works when you pass the loadings for **all** the PCs:\n```{r}\nprint(job_pca$loadings, cutoff = 0.3)\n```\n\n:::\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: How well are the units represented in the reduced space?  \n\nWe now focus our attention on the following question: Are all the statistical units (police officers) well represented in the 2D plot?\n\nThe 2D representation of the original data, which comprise 6 measured variables, is an approximation and henceforth it may happen that not all units are well represented in this new space.\n\nTypically, it is good to assess the approximation for each statistical unit by inspecting the scores on the discarded principal components.\nIf a unit has a high score on those components, then this is a sign that the unit might be highly misplaced in the new space and misrepresented.\n\nConsider the 3D example below. There are three cases (= units or individuals). In the original space they are all very different from each other. For example, cases 1 and 2 are very different in their x and y values, but very similar in their z value. Cases 2 and 3 are very similar in their x and y values but very different in their z value. Cases 1 and 3 have very different values for all three variables x, y, and z.\n\nHowever, when represented in the 2D space given by the two principal components, units 2 and 3 seems like they are very similar when, in fact, they were very different in the original space which also accounted for the z variable.\n\n```{r}\n#| echo: false\n#| out-width: \"\\\\textwidth\"\n#| include: true\n#| fig-align: 'center'\nknitr::include_graphics('images/pcaefa/pca_bad_representation.png')\n```\n\nWe typically measure how badly a unit is represented in the new coordinate system by considering the **sum of squared scores on the discarded principal components:**\n\n```{r}\nscores_discarded <- job_pca$scores[, -(1:2)]\nsum_sq <- rowSums(scores_discarded^2)\nsum_sq\n```\n\nUnits with a high score should be considered for further inspection as it may happen that they are represented as close to another unit when, in fact, they might be very different.\n\n```{r}\nboxplot(sum_sq)\n```\n\nThere seem to be only five outliers, and they are not too high compared to the rest of the scores. For this reason, we will consider the 2D representation of the data to be satisfactory.\n\n:::\n\n\n## 7. Using PCA scores\n\n::: {.callout-note collapse=\"true\"}\n#### PCA scores\n\nNow that we have decided to reduce our six variables down to two principal components, we can, for each of our observations, get their _scores_ on each of our components.  \n\n```{r}\njob_pca2 <- principal(job_skills, nfactors = 2, covar = TRUE, rotate = 'none')\nhead(job_pca2$scores)\n```\n\nPCA scores are essentially weighted combinations of an individuals responses to the items.  \n$$\n\\text{score}_{\\text{component j}} = w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 +\\, ... \\, + w_{pj}x_p\n$$\nWhere $w$ are the weights, $x$ the variable scores. \n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: How are weights calculated?  \n\nThe weights are calculated from the eigenvectors as \n$$\nw_{ij} = \\frac{a_{ij}}{\\sqrt(\\lambda_j)}\n$$\nwhere $w_{ij}$ is the weight for a given variable $i$ on component $j$ , $a_{ij}$ is the value from the eigenvector for item $i$ on component $j$ and $\\lambda_{j}$ is the eigenvalue for that component.\n\n:::\n\nIn the literature, some authors also suggest to look at the correlation between the principal component scores and the measured variables:\n\n```{r}\n# First PC\ncor(job_pca2$scores[,1], job_skills)\n```\n\nThe first PC is strongly correlated with all the measured variables except `probl_solv` and `logical`. \nAs mentioned when looking at loadings, all these variables seem to contributed to the first PC.\n\n```{r}\n# Second PC\ncor(job_pca2$scores[,2], job_skills)\n```\n\nThe second PC is strongly correlated with `probl_solv` and `logical`, and slightly negatively correlated with the remaining variables. This separates police officers with clear logical and problem solving skills and a low rating on other skills.\n\nWe can also visualise our observations (the police officers) in the reduced space given by the retained principal component scores.\n\n```{r}\n#| fig-width: 5\n#| fig-align: 'center'\ntibble(pc1 = job_pca2$scores[, 1],\n       pc2 = job_pca2$scores[, 2]) %>%\n  ggplot(.,aes(x=pc1,y=pc2))+\n  geom_point()\n```\n\n:::\n\n\n`r qbegin(12)`\nWe have reduced our six variables down to two principal components, and we are now able to use the scores on each component in a subsequent analysis! \n\nJoin the principal component scores for your retained components to the original dataset which has the arrest rates in.  \n\nThen fit a linear model to look at how the arrest rate of police officers is predicted by the two components representing different composites of the skills ratings by HR.   \n\nCheck for multicollinearity between your predictors.  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n# add the PCA scores to the dataset\njob <- \n  job %>% mutate(\n    pc1 = job_pca2$scores[,1],\n    pc2 = job_pca2$scores[,2]\n  )\n# use the scores in an analysis\nmod <- lm(arrest_rate ~ pc1 + pc2, data = job)\n\n# multicollinearity isn't a problem, because the components are orthogonal!! \nlibrary(car)\nvif(mod)\n\nsummary(mod)\n```\n`r solend()`\n\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"09_pca.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"theme":["united","assets/style-labs.scss"],"link-citations":true,"code-copy":false,"title":"9. PCA","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}