{"title":"11. EFA 2","markdown":{"yaml":{"title":"11. EFA 2","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"knitr::opts_chunk$set(cache = TRUE)","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nlibrary(lavaan)\nlibrary(semPlot)\noptions(digits=3, scipen = 3)\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\n```\n\n\n:::lo\n**Relevant packages**\n\n+ tidyverse\n+ psych\n+ GPArotation\n\n::: \n\n```{r}\n#| eval: false\n#| echo: false\nlibrary(lavaan)\nset.seed(11)\n\n# paste(paste0(abs(round(rnorm(9,0,.3),1)),\"*\",paste0(\"PHQ_\",1:9), collapse=\" + \"),\" + \",\n# paste0(abs(round(rnorm(7,0,1),1)),\"*\",paste0(\"GAD_\",1:7), collapse=\" + \"))\n\nm = \"\ndepression =~ 0.5*PHQ_1 + 0.6*PHQ_2 + 0.8*PHQ_3 + 1.2*PHQ_4 + 0.5*PHQ_5 + 1.4*PHQ_6 + 1*PHQ_7 + 0.7*PHQ_8 + 1.8*PHQ_9  +  0.1*GAD_1 + 0.1*GAD_3 + 0.1*GAD_5 + 0.1*GAD_6\nanxiety =~ 0.8*PHQ_3 + 0.1*PHQ_4 + 0.1*PHQ_7 + 0.9*GAD_1 + 1.2*GAD_2 + 0.8*GAD_3 + 0.9*GAD_4 + 0.5*GAD_5 + 0.6*GAD_6 + 0.8*GAD_7\nanxiety ~~ 0.3*depression\npetowner ~ 0.3*anxiety\npetowner ~~ 0.3*petowner\n\"\ndf <- simulateData(m, sample.nobs = 620)\npetowner = 1-(rbinom(620, size=1,prob=plogis(scale(df$petowner)[,1])))\ndf <- df[,-17]\ndf$PHQ_7<-rnorm(620)\nmakelik <- function(x){\n    round(pmax(1,pmin(7,(4+(x*1)))))\n}\ndf <- apply(df, MARGIN=2, FUN=makelik)  \nprint(fa(cor(df),nfactors = 2)$loadings)\n\nphqgad_items = c(\n\"Little interest or pleasure in doing things?\",\n\"Feeling down, depressed, or hopeless?\",\n\"Trouble falling or staying asleep, or sleeping too much?\",\n\"Feeling tired or having little energy?\",\n\"Poor appetite or overeating?\",\n\"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?\",\n\"Trouble concentrating on things, such as reading the newspaper or watching television?\",\n\"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?\",\n\"A lack of motivation to do anything at all?\",\n\"Feeling nervous, anxious or on edge?\",\n\"Not being able to stop or control worrying?\",\n\"Worrying too much about different things?\",\n\"Trouble relaxing?\",\n\"Being so restless that it is hard to sit still?\",\n\"Becoming easily annoyed or irritable?\",\n\"Feeling afraid as if something awful might happen?\")\n\ndf <- as_tibble(df)\nnames(df) <- phqgad_items\ndf <- janitor::clean_names(df)\ndf$do_you_own_a_pet <- petowner\ndf$ppt_id <- paste0(\"ppt\",1:620)\n#write.csv(df, \"../../data/pgpets.csv\",row.names=F)\n\n```\n\n\n# Practical Issues with EFA\n\n:::frame\n__Data: pgpets.csv__  \n\nA pet food company has conducted a questionnaire on the internet ($n = 620$) to examine whether owning a pet influences low mood. They asked 16 questions on a Likert scale (1-7, detailed below) followed by a simple Yes/No question concerning whether the respondent owned a pet.   \nThere are lots of questions, and the researchers don't really know much about the theory of mood disorders, but they think that they are likely picking up on multiple different types of \"low mood\". They want to conduct a factor analysis to examine this, and then plan on investigating the group differences (pet owners vs not pet owners) on the factor scores.  \n\nThe data is available at https://uoepsy.github.io/data/pgpets.csv  \n\n```{r}\n#| echo: false\ntibble(QuestionNumber = paste0(\"item\",1:16),`Over the last 2 weeks, how much have you had/have you been...` = \n         c(\"Little interest or pleasure in doing things?\",\n\"Feeling down, depressed, or hopeless?\",\n\"Trouble falling or staying asleep, or sleeping too much?\",\n\"Feeling tired or having little energy?\",\n\"Poor appetite or overeating?\",\n\"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?\",\n\"Reading the newspaper or watching television?\",\n\"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?\",\n\"A lack of motivation to do anything at all?\",\n\"Feeling nervous, anxious or on edge?\",\n\"Not being able to stop or control worrying?\",\n\"Worrying too much about different things?\",\n\"Trouble relaxing?\",\n\"Being so restless that it is hard to sit still?\",\n\"Becoming easily annoyed or irritable?\",\n\"Feeling afraid as if something awful might happen?\")) %>% knitr::kable() %>% kableExtra::kable_styling(full_width = T)\n\n```\n\n:::\n\n`r qbegin(qcounter())`\nRead the data into R.  \nCreate a new object in R that contains a subset the data. It should include all variables except for the participant ID and the variable corresponding to whether or not they have a pet (we're going to come back to these later on).  \nIn this new object, change the names of the columns to match the question number, rather than the question itself (see the data description above). This will be easier to work with.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nCheck the output of the code `paste0(\"item\", 1:10)`. Consider this code in combination with another function: `names(data)`. How could you combine the two codes to assign the new names to the current variable names?  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(tidyverse)\nlibrary(psych)\n```\n\n```{r}\npgpets <- read_csv(\"https://uoepsy.github.io/data/pgpets.csv\")\n```\n\n```{r}\nnames(pgpets)\n```\n\n```{r}\ndf <- pgpets %>% \n    select(-ppt_id, -do_you_own_a_pet)\n```\n\n```{r}\nnames(df) <- paste0(\"item\", 1:ncol(df))\nhead(df)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nVisualise the items (this might be the histograms of all marginal distributions, or a scatterplot matrix, or both).  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThe function `multi.hist()` from the __psych__ package can be pretty useful if we're just wanting the distributions. Things like `pairs.panels()` can get pretty chaotic when we have lots of variables, so you might need to use that on subsets.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(psych)\nmulti.hist(df)\n```\n\nThe data have 6 variables, so we will create two plots each focusing on 8 at a time:\n```{r}\npairs.panels(df[, 1:8])\npairs.panels(df[, 9:16])\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nCompute the correlation matrix for the items, and assess the suitability for factor analysis, using the Bartlett test and the Kaiser-Meyer-Olkin factor adequacy.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nLook back in last week's lab to see how Bartlett & KMO are conducted in R\n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\ncordf = cor(df)\n```\n\n\n```{r}\ncortest.bartlett(cordf, n = nrow(df))\n```\n:::int\nBartlett's test indicates that the correlation matrix is proportionally different from the identity matrix ($\\chi^2 (120) = 3008, p<.001$), suggesting our correlations are significantly different from zero. \n:::\n\n```{r}\nKMO(cordf)\n```\n\n:::int\nThe KMO for individual items are mainly \"meritorious\" or \"marvelous\". The only item that may need further investigation is item 7, having an \"unacceptable\" KMO. \nThe overall measure of sampling adequacy is 0.90, which suggests we have suitable data to perform a factor analysis\n:::\n\n`r solend()`\n\n`r qbegin(qcounter())`\nDetermine how many factors you will extract.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nAs always, there are lots of methods to help you decide, but the ultimate decision is yours.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nUsing parallel analysis:\n\n```{r}\nfa.parallel(df, fa = 'fa')\n```\n\nUsing Velicer's Minimum Average Partial:\n\n```{r}\nVSS(df)\n```\n:::int\nBoth parallel analysis and Velicerâ€™s Minimum Average Partial suggested retaining 2 factors.\n:::\n`r solend()`\n\n`r qbegin(qcounter())`\nChoosing an appropriate estimation method and rotation, perform a factor analysis to extract the desired number of factors (based on your answer to the previous question).  \n\nIf you get an error, you may need to install the \"GPArotation\" package: `install.packages(\"GPArotation\")`.\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe probably shouldn't use MLE as an estimation method here, because we're going to be treating what looks like some sort of Likert data (ordinal, responses of 1-7) as if it is continuous.  \nLet's use `minres` here.  \n\nFinally, do we have any reason to think that the factors we are going to extract are orthogonal? Given what the questions look to be evaluating, it's hard to see distinctly unrelated constructs within the 16 items. \n\n```{r}\npgmod <- fa(df, nfactors = 2, rotate = \"oblimin\", fm = \"minres\")\npgmod\n```\n\n`r solend()`\n\n<br>\n\n::: {.callout-note collapse=\"true\"}\n#### EFA output\n\n\\newcommand{\\item}[1]{ \\text{Item}_{#1} }\n\\newcommand{\\fact}[1]{ \\text{Factor}_{#1} }\n\\newcommand{\\e}[1]{ \\epsilon_{#1} }\n\\newcommand{\\LL}[2]{ {\\lambda_{#1,#2}} }\n\nIf we think about a factor analysis being a set of regressions (convention in factor analysis is to use $\\LL{}{}$ instead of $\\beta$), then we can think of a given item being the manifestation of some latent factors, plus a bit of randomness (or 'stray causes'):\n\n\\begin{aligned}\n\\item{1}  &= \\LL{1}{1} \\cdot \\fact{1} + \\LL{2}{1} \\cdot \\fact{2} + u_{1} \\\\\n\\item{2}  &= \\LL{1}{2} \\cdot \\fact{1} + \\LL{2}{2} \\cdot \\fact{2} + u_{2} \\\\\n&\\vdots \\\\\n\\item{16} &= \\LL{1}{16} \\cdot \\fact{1} + \\LL{2}{16} \\cdot \\fact{2} + u_{16}\n\\end{aligned}\n\nAs you can see from the above, the 16 different items all stem from the same two factors ($\\fact{1}, \\fact{2}$), plus some item-specific errors ($u_{1}, \\dots, u_{16}$). The $\\LL{}{}$ terms are called factor loadings, or loadings in short\n\n__Communality__ is sum of the squared factor loadings for each item.  \n\nIntuitively, for each row, the two $\\LL{}{}$s tell us how much each item depends on the two factors shared by the 16 items. The sum of the squared loadings tells us how much of one item's information is due to the shared factors.\n\nThe communality is a bit like the $R^2$ (the proportion of variance of an item that is explained by the factor structure). And the standardised loadings are the proportion of variance in an item explained by each factor after accounting for other other factors.  \n\n\nThe __Uniqueness__ of each item is simply $1 - \\text{communality}$.  \nThis is the leftover bit; the variance in each item that is left unexplained by the latent factors (this could be specific variance, or it could be error variance. the one thing we know is, it's not common variance).  \n\n_Side note: this is what sets Factor Analysis apart from PCA, which is the linear combination of total variance (including error) in all our items. FA allows some of the variance to be shared by the underlying factors, and considers the remainder to be unique to the individual items (or, in another, error in how each item measures the construct)._\n\nThe __Complexity__ of an item corresponds to how well an item reflects a _single_ underlying construct. Specifically, it is ${(\\sum \\lambda_i^2)^2}/{\\sum \\lambda_i^4}$, where $\\lambda_i$ is the loading on to the $i^{th}$ factor. It will be equal to 1 for an item which loads _only_ on one factor, and 2 if it loads evenly on to two factors, and so on. \n\n:::rtip\nIn R, we will often see these estimats under specific columns:  \n\n+ __h2__ = item communality  \n+ __u__  = item uniqueness  \n+ __com__ = item complexity\n\n:::\n\n:::\n\n`r qbegin(qcounter())`\nUsing `fa.sort()`, examine the loadings of each item onto the factors, along with communalities, uniqueness and complexity scores.  \n\n\n- Do all factors load on 3+ items at a salient level?\n- Do all items have at least one loading above a salient cut off?\n- Are there any Heywood cases (communalities or standardised loadings $\\geq1$)? \n- Should we perhaps remove some complex items? \n- Does the solution account for an acceptable level of variance?  \n- Is the factor structure (items that load on to each factor) coherent, and does it make theoretical sense?  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nfa.sort(pgmod)\n```\n\n- Both factors load on 3+ items at a salient level. \n- All except item7 has loadings $>0.3$. \n- There are no Heywood cases.\n- Item 3 loads quite highly on both factors (high complexity) \n- The solution explains 38% of the variance.  \n- The coherence of the factor structure will require us to look back at the questions themselves. We'll not do this right now, but do that later on.  \n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nIf you think any items should be removed, do so now, and perform the factor analysis once more.  \n\nReally, this starts the whole process again, so we'll have to go through our steps to get to our new factor model on the reduced set of items.  \n`r qend()`\n`r solbegin(\"remove items\", slabel=FALSE,show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe're going to remove that problematic item 7 (which seems to be about \"reading the newspaper and watching tv\"), as well as item 3 (which seems to be about sleepiness, and relates to both factors quite highly).  \n```{r}\ndf2 <- df %>% \n    select(-item7,-item3)\n```\n`r solend()`\n`r solbegin(\"assess factorability\", slabel=FALSE,show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nAs before we removed items 3 and 7, these all look fine:  \n```{r}\ncortest.bartlett(cor(df2), n = nrow(df2))\nKMO(cor(df2))\n```\n\n`r solend()`\n`r solbegin(\"how many factors?\", slabel=FALSE,show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nBoth parallel analysis and MAP suggest 2:\n\n```{r}\nfa.parallel(df2, fa = 'fa')\nVSS(df2)\n```\n\n`r solend()`\n`r solbegin(\"conduct and inspect EFA\", slabel=FALSE,show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\npgmod2 <- fa(df2, nfactors = 2, rotate = \"oblimin\", fm = \"minres\")\npgmod2$loadings\nfa.sort(pgmod2)\n```\n\n- Both factors load on 3+ items at a salient level. \n- All items have a loading $>0.3$. \n- There are no Heywood cases.\n- No items have high loadings on multiple factors (complexity is low for all items)\n- The solution explains 40% of the variance.  \n\nThis looks much better!  \n\n`r solend()`\n\n<br>\n\n::: {.callout-note collapse=\"true\"}\n#### Reliability \n\n<!-- parallel tests -->\n\n<!-- test-retest reliability   -->\n<!-- split-half reliability   -->\n<!-- cronbach's alpha   -->\n<!-- mcdonald's omega   -->\n<!-- inter-rater reliability   -->\n<!-- intra-class correlations!    -->\n<!-- - link back to MLM   -->\n\n<!-- attenuation for measurement error -->\n\n__Measurement Error & Reliability__  \n\nYou will often find research that foregoes the measurement model by taking a scale score (i.e., the sum or mean of a set of likert-type questions). For instance, think back to our exercises on path mediation, where \"Health Locus of Control\" was measured as the \"average score on a set of items relating to perceived control over ones own health\". In doing so, we make the assumption that these variables provide measurements of the underlying latent construct **without error**.  \nIn fact, if we think about it a little, in our simple regression framework $y = \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k + \\varepsilon$ all our predictor variables $x_1$ to $x_k$ are assumed to be measured without error - it is only our outcome $y$ that our model considers to have some randomness included (the $\\varepsilon$). This seems less problematic if our variables are representing something that is quite easily and reliably measured (time, weight, height, age, etc.), but it seems inappropriate when we are concerned with something more abstract. For instance, two people both scoring 11 on the Generalised Anxiety Disorder 7 (GAD-7) scale does not necessarily mean that they have _identical_ levels of anxiety.  \n  \nThe inconsistency with which an observed variable reflects the underlying construct that we consider it to be measuring is termed the *reliability*.  \n\n:::frame\n**Reliability: A silly example** \n\nSuppose I'm trying to weigh [my dog](https://photos.app.goo.gl/f26FrRDyJxTvXvGS9). I have a set of scales, and I put him on the scales. He weighs in at 13.53kg. \nI immediately do it again, and the scales this time say 13.41kg. I do it again. 13.51kg, and again, 13.60kg. \nWhat is happening? Is Dougal's weight (Dougal is the dog, by the way) randomly fluctuating by 100g? Or are my scales just a bit inconsistent, and my observations contain measurement error?  \n\n```{r}\n#| echo: false\ntibble(\n    measurement =1:4,\n    dougal = c(13.53,13.41,13.51,13.6)\n) %>% \n    ggplot(.,aes(x=measurement,y=dougal))+\n    geom_point()+\n    geom_hline(yintercept=13.483,lty=\"dashed\")+\n    geom_text(x=3,y=13.484,label=\"Dougal's true weight (unknown to us)\",\n              vjust=-.5)+\n    labs(y=\"Dougal's Weight\")\n```\n\nI take him to the vets, where they have a much better set of weighing scales, and I do the same thing (measure him 4 times). The weights are 13.47, 13.49, 13.48, 13.48.  \nThe scales at the vets are clearly *more __reliable__*. We still don't know Dougal's *true* weight, but we are better informed to estimate it if we go on the measurements from the scales at the vet.^[Of course this all assuming that the scales aren't completely miscalibrated] \n\n```{r}\n#| echo: false\ntibble(\n    scales = rep(c(\"mine\",\"vets\"),each=4),\n    measurement = c(1:4,1:4),\n    dougal = c(13.53,13.41,13.51,13.6, 13.47, 13.49, 13.48, 13.48)\n) %>% \n    ggplot(.,aes(x=measurement,y=dougal, col=scales))+\n    geom_point()+\n    geom_path()+\n    geom_hline(yintercept=13.483,lty=\"dashed\")+\n    geom_text(inherit.aes=F, x=3,y=13.484,label=\"Dougal's true weight (unknown to us)\",vjust=-.5)+\n    labs(y=\"Dougal's Weight\")\n```\n:::\n\nAnother way to think about reliability is to consider the idea that more error means less reliability: \n$$\\text{observations = truth + error}$$  \n\n\nThere are different types of reliability:  \n\n- **test re-test reliability:** correlation between values over repeated measurements. \n- **alternate-form reliability:** correlation between scores on different forms/versions of a test (we might want different versions to avoid practice effects). \n- **Inter-rater reliability:** correlation between values obtained from different raters. \n- **split-half reliability:** correlation between scores of two equally sized subsets of items. \n\nThe form of reliability we are going to be most concerned with here is known as **Internal Consistency**. This is the extent to which items within a scale are correlated with one another. There are two main measures of this:\n\n  \n\n__Alpha & Omega__  \n\n**Cronbach's $\\alpha$** ranges from 0 to 1 (higher is better). You can get this using the `alpha()` function from the **psych** package. The formula is:\n\n$$\n\\begin{aligned}\n\\text{Cronbach's } \\alpha &= \\frac{n \\cdot \\overline{cov(ij)}}{\\overline{\\sigma^2_i} + (n-1) \\cdot \\overline{cov(ij)}}  \\\\\n\\\\\n\\text{where}:  & \\qquad\n& n = \\text{number of items} \\\\\n& \\overline{cov(ij)} = \\text{average covariance between item-pairs}  \\\\\n& \\overline{\\sigma^2_i} = \\text{average item variance}  \\\\\n\\end{aligned}\n$$\n\n**McDonald's Omega ($\\omega$)** is substantially more complicated, but avoids the limitation that Cronbach's alpha which assumes that all items are equally related to the construct. You can get it using the `omega()` function from the **psych** package. If you want more info about it then the help docs (`?omega()`) are a good place to start. \n\n:::\n\n`r qbegin(qcounter())`\nUsing the relevant function, obtain alpha for the set of items in each factor from your final factor analysis model of low mood. \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThe first factor has high loadings for items 1 to 9 (we have excluded 3 and 7). \n```{r}\nnames(df2)\nalpha(select(df2, item1:item9))\nalpha(select(df2, item10:item16))\n```\n:::int\nCronbach's $\\alpha$ of 0.82 and 0.81 for the two factors and suggests high internal consistency. \n:::\n\n`r solend()`\n\n\n<!-- :::frame -->\n<!-- You can't test the structural model if the measurement model is bad -->\n\n<!-- if you test the relationships between a set of latent factors, and they are not reliably measured by the observed items, then this error propagates up to influence the fit of the model.   -->\n<!-- To test the measurement model, it is typical to *saturate* the structural model (i.e., allow all the latent variables to correlate with one another). This way any misfit is due to the measurement model.   -->\n\n<!-- ::: -->\n\n<!-- what can we __do__ with this knowledge?   -->\n<!-- attenuation due to measurement error -->\n<br>\n\n# Replicability \n\n`r qbegin(qcounter())`\nSplit the dataset in half, and assess the replicability of your factor structure of low mood by examining the factor congruence in scores on each subset of the data. \n\n__Hint:__ see the lectures! \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(tidyverse)\nset1 <- df2 %>%\n  sample_frac(.5) # randomly select one half\nset2 <- anti_join(df2, set1) # select the non-matching cases\n\nres1 <- fa(set1, nfactors = 2, rotate = \"oblimin\")\nres2 <- fa(set2, nfactors = 2, rotate = \"oblimin\")\nfa.congruence(res1, res2) \n```\n\nWe can see there is a high level of factor congruence across the two subsets of data.  \n`r solend()`\n\n<br>\n\n# Factor Scores\n\n`r qbegin(qcounter())`\nExtract the factor scores for each factor of low-mood from your model, and attach them to original dataset (the one which has information on pet ownership).  \nThen, conduct a $t$-test to examine whether the pet-owners differ from non-pet-owners in their levels of each factor of low mood.  \n\nAs a bonus, can you come up with some description of the two factors? you will have to look back to what the items represent (i.e. the questions that each item is asking. \n\n::: {.callout-note collapse=\"true\"}\n#### reminder of questions\n\n```{r}\n#| echo: false\ntibble(QuestionNumber = paste0(\"item\",1:16),`Over the last 2 weeks, how much have you had/have you been...` = \n         c(\"Little interest or pleasure in doing things?\",\n\"Feeling down, depressed, or hopeless?\",\n\"~~Trouble falling or staying asleep, or sleeping too much?~~\",\n\"Feeling tired or having little energy?\",\n\"Poor appetite or overeating?\",\n\"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?\",\n\"~~Reading the newspaper or watching television?~~\",\n\"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?\",\n\"A lack of motivation to do anything at all?\",\n\"Feeling nervous, anxious or on edge?\",\n\"Not being able to stop or control worrying?\",\n\"Worrying too much about different things?\",\n\"Trouble relaxing?\",\n\"Being so restless that it is hard to sit still?\",\n\"Becoming easily annoyed or irritable?\",\n\"Feeling afraid as if something awful might happen?\")) %>%\n  gt::gt()\n```\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nIt looks like all the items 1-9 (excluding items 3 and 7, which aren't in our factor model in the end) are all about lower mood or lack of energy - i.e. 'depression'.  \nThe latter items that load onto the second factor (items 10-16) all seem related to 'anxiety'.  \n\nSo we can use the estimated factor scores as a representation of peoples' relative standings on those underlying latent variables of 'depression' and 'anxiety', and we can then look at whether people who own a pet significantly differ on those two constructs from those who don't own a pet!  \n\n```{r}\npgpets <- \n  pgpets %>% \n    mutate(\n      depression = pgmod2$scores[,1],\n      anxiety = pgmod2$scores[,2],\n      pet = factor(do_you_own_a_pet)\n    )\n\nt.test(pgpets$depression ~ pgpets$pet)\nggplot(pgpets, aes(x=pet, y=depression))+\n  geom_boxplot()\n\nt.test(pgpets$anxiety~pgpets$pet)\nggplot(pgpets, aes(x=pet, y=anxiety))+\n  geom_boxplot()\n```\n\n`r solend()`\n\n\n\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(xaringanExtra)\nxaringanExtra::use_panelset()\nlibrary(lavaan)\nlibrary(semPlot)\n# knitr::opts_chunk$set(cache = TRUE)\noptions(digits=3, scipen = 3)\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\n```\n\n\n:::lo\n**Relevant packages**\n\n+ tidyverse\n+ psych\n+ GPArotation\n\n::: \n\n```{r}\n#| eval: false\n#| echo: false\nlibrary(lavaan)\nset.seed(11)\n\n# paste(paste0(abs(round(rnorm(9,0,.3),1)),\"*\",paste0(\"PHQ_\",1:9), collapse=\" + \"),\" + \",\n# paste0(abs(round(rnorm(7,0,1),1)),\"*\",paste0(\"GAD_\",1:7), collapse=\" + \"))\n\nm = \"\ndepression =~ 0.5*PHQ_1 + 0.6*PHQ_2 + 0.8*PHQ_3 + 1.2*PHQ_4 + 0.5*PHQ_5 + 1.4*PHQ_6 + 1*PHQ_7 + 0.7*PHQ_8 + 1.8*PHQ_9  +  0.1*GAD_1 + 0.1*GAD_3 + 0.1*GAD_5 + 0.1*GAD_6\nanxiety =~ 0.8*PHQ_3 + 0.1*PHQ_4 + 0.1*PHQ_7 + 0.9*GAD_1 + 1.2*GAD_2 + 0.8*GAD_3 + 0.9*GAD_4 + 0.5*GAD_5 + 0.6*GAD_6 + 0.8*GAD_7\nanxiety ~~ 0.3*depression\npetowner ~ 0.3*anxiety\npetowner ~~ 0.3*petowner\n\"\ndf <- simulateData(m, sample.nobs = 620)\npetowner = 1-(rbinom(620, size=1,prob=plogis(scale(df$petowner)[,1])))\ndf <- df[,-17]\ndf$PHQ_7<-rnorm(620)\nmakelik <- function(x){\n    round(pmax(1,pmin(7,(4+(x*1)))))\n}\ndf <- apply(df, MARGIN=2, FUN=makelik)  \nprint(fa(cor(df),nfactors = 2)$loadings)\n\nphqgad_items = c(\n\"Little interest or pleasure in doing things?\",\n\"Feeling down, depressed, or hopeless?\",\n\"Trouble falling or staying asleep, or sleeping too much?\",\n\"Feeling tired or having little energy?\",\n\"Poor appetite or overeating?\",\n\"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?\",\n\"Trouble concentrating on things, such as reading the newspaper or watching television?\",\n\"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?\",\n\"A lack of motivation to do anything at all?\",\n\"Feeling nervous, anxious or on edge?\",\n\"Not being able to stop or control worrying?\",\n\"Worrying too much about different things?\",\n\"Trouble relaxing?\",\n\"Being so restless that it is hard to sit still?\",\n\"Becoming easily annoyed or irritable?\",\n\"Feeling afraid as if something awful might happen?\")\n\ndf <- as_tibble(df)\nnames(df) <- phqgad_items\ndf <- janitor::clean_names(df)\ndf$do_you_own_a_pet <- petowner\ndf$ppt_id <- paste0(\"ppt\",1:620)\n#write.csv(df, \"../../data/pgpets.csv\",row.names=F)\n\n```\n\n\n# Practical Issues with EFA\n\n:::frame\n__Data: pgpets.csv__  \n\nA pet food company has conducted a questionnaire on the internet ($n = 620$) to examine whether owning a pet influences low mood. They asked 16 questions on a Likert scale (1-7, detailed below) followed by a simple Yes/No question concerning whether the respondent owned a pet.   \nThere are lots of questions, and the researchers don't really know much about the theory of mood disorders, but they think that they are likely picking up on multiple different types of \"low mood\". They want to conduct a factor analysis to examine this, and then plan on investigating the group differences (pet owners vs not pet owners) on the factor scores.  \n\nThe data is available at https://uoepsy.github.io/data/pgpets.csv  \n\n```{r}\n#| echo: false\ntibble(QuestionNumber = paste0(\"item\",1:16),`Over the last 2 weeks, how much have you had/have you been...` = \n         c(\"Little interest or pleasure in doing things?\",\n\"Feeling down, depressed, or hopeless?\",\n\"Trouble falling or staying asleep, or sleeping too much?\",\n\"Feeling tired or having little energy?\",\n\"Poor appetite or overeating?\",\n\"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?\",\n\"Reading the newspaper or watching television?\",\n\"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?\",\n\"A lack of motivation to do anything at all?\",\n\"Feeling nervous, anxious or on edge?\",\n\"Not being able to stop or control worrying?\",\n\"Worrying too much about different things?\",\n\"Trouble relaxing?\",\n\"Being so restless that it is hard to sit still?\",\n\"Becoming easily annoyed or irritable?\",\n\"Feeling afraid as if something awful might happen?\")) %>% knitr::kable() %>% kableExtra::kable_styling(full_width = T)\n\n```\n\n:::\n\n`r qbegin(qcounter())`\nRead the data into R.  \nCreate a new object in R that contains a subset the data. It should include all variables except for the participant ID and the variable corresponding to whether or not they have a pet (we're going to come back to these later on).  \nIn this new object, change the names of the columns to match the question number, rather than the question itself (see the data description above). This will be easier to work with.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nCheck the output of the code `paste0(\"item\", 1:10)`. Consider this code in combination with another function: `names(data)`. How could you combine the two codes to assign the new names to the current variable names?  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(tidyverse)\nlibrary(psych)\n```\n\n```{r}\npgpets <- read_csv(\"https://uoepsy.github.io/data/pgpets.csv\")\n```\n\n```{r}\nnames(pgpets)\n```\n\n```{r}\ndf <- pgpets %>% \n    select(-ppt_id, -do_you_own_a_pet)\n```\n\n```{r}\nnames(df) <- paste0(\"item\", 1:ncol(df))\nhead(df)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nVisualise the items (this might be the histograms of all marginal distributions, or a scatterplot matrix, or both).  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThe function `multi.hist()` from the __psych__ package can be pretty useful if we're just wanting the distributions. Things like `pairs.panels()` can get pretty chaotic when we have lots of variables, so you might need to use that on subsets.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(psych)\nmulti.hist(df)\n```\n\nThe data have 6 variables, so we will create two plots each focusing on 8 at a time:\n```{r}\npairs.panels(df[, 1:8])\npairs.panels(df[, 9:16])\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nCompute the correlation matrix for the items, and assess the suitability for factor analysis, using the Bartlett test and the Kaiser-Meyer-Olkin factor adequacy.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nLook back in last week's lab to see how Bartlett & KMO are conducted in R\n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\ncordf = cor(df)\n```\n\n\n```{r}\ncortest.bartlett(cordf, n = nrow(df))\n```\n:::int\nBartlett's test indicates that the correlation matrix is proportionally different from the identity matrix ($\\chi^2 (120) = 3008, p<.001$), suggesting our correlations are significantly different from zero. \n:::\n\n```{r}\nKMO(cordf)\n```\n\n:::int\nThe KMO for individual items are mainly \"meritorious\" or \"marvelous\". The only item that may need further investigation is item 7, having an \"unacceptable\" KMO. \nThe overall measure of sampling adequacy is 0.90, which suggests we have suitable data to perform a factor analysis\n:::\n\n`r solend()`\n\n`r qbegin(qcounter())`\nDetermine how many factors you will extract.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nAs always, there are lots of methods to help you decide, but the ultimate decision is yours.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nUsing parallel analysis:\n\n```{r}\nfa.parallel(df, fa = 'fa')\n```\n\nUsing Velicer's Minimum Average Partial:\n\n```{r}\nVSS(df)\n```\n:::int\nBoth parallel analysis and Velicerâ€™s Minimum Average Partial suggested retaining 2 factors.\n:::\n`r solend()`\n\n`r qbegin(qcounter())`\nChoosing an appropriate estimation method and rotation, perform a factor analysis to extract the desired number of factors (based on your answer to the previous question).  \n\nIf you get an error, you may need to install the \"GPArotation\" package: `install.packages(\"GPArotation\")`.\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe probably shouldn't use MLE as an estimation method here, because we're going to be treating what looks like some sort of Likert data (ordinal, responses of 1-7) as if it is continuous.  \nLet's use `minres` here.  \n\nFinally, do we have any reason to think that the factors we are going to extract are orthogonal? Given what the questions look to be evaluating, it's hard to see distinctly unrelated constructs within the 16 items. \n\n```{r}\npgmod <- fa(df, nfactors = 2, rotate = \"oblimin\", fm = \"minres\")\npgmod\n```\n\n`r solend()`\n\n<br>\n\n::: {.callout-note collapse=\"true\"}\n#### EFA output\n\n\\newcommand{\\item}[1]{ \\text{Item}_{#1} }\n\\newcommand{\\fact}[1]{ \\text{Factor}_{#1} }\n\\newcommand{\\e}[1]{ \\epsilon_{#1} }\n\\newcommand{\\LL}[2]{ {\\lambda_{#1,#2}} }\n\nIf we think about a factor analysis being a set of regressions (convention in factor analysis is to use $\\LL{}{}$ instead of $\\beta$), then we can think of a given item being the manifestation of some latent factors, plus a bit of randomness (or 'stray causes'):\n\n\\begin{aligned}\n\\item{1}  &= \\LL{1}{1} \\cdot \\fact{1} + \\LL{2}{1} \\cdot \\fact{2} + u_{1} \\\\\n\\item{2}  &= \\LL{1}{2} \\cdot \\fact{1} + \\LL{2}{2} \\cdot \\fact{2} + u_{2} \\\\\n&\\vdots \\\\\n\\item{16} &= \\LL{1}{16} \\cdot \\fact{1} + \\LL{2}{16} \\cdot \\fact{2} + u_{16}\n\\end{aligned}\n\nAs you can see from the above, the 16 different items all stem from the same two factors ($\\fact{1}, \\fact{2}$), plus some item-specific errors ($u_{1}, \\dots, u_{16}$). The $\\LL{}{}$ terms are called factor loadings, or loadings in short\n\n__Communality__ is sum of the squared factor loadings for each item.  \n\nIntuitively, for each row, the two $\\LL{}{}$s tell us how much each item depends on the two factors shared by the 16 items. The sum of the squared loadings tells us how much of one item's information is due to the shared factors.\n\nThe communality is a bit like the $R^2$ (the proportion of variance of an item that is explained by the factor structure). And the standardised loadings are the proportion of variance in an item explained by each factor after accounting for other other factors.  \n\n\nThe __Uniqueness__ of each item is simply $1 - \\text{communality}$.  \nThis is the leftover bit; the variance in each item that is left unexplained by the latent factors (this could be specific variance, or it could be error variance. the one thing we know is, it's not common variance).  \n\n_Side note: this is what sets Factor Analysis apart from PCA, which is the linear combination of total variance (including error) in all our items. FA allows some of the variance to be shared by the underlying factors, and considers the remainder to be unique to the individual items (or, in another, error in how each item measures the construct)._\n\nThe __Complexity__ of an item corresponds to how well an item reflects a _single_ underlying construct. Specifically, it is ${(\\sum \\lambda_i^2)^2}/{\\sum \\lambda_i^4}$, where $\\lambda_i$ is the loading on to the $i^{th}$ factor. It will be equal to 1 for an item which loads _only_ on one factor, and 2 if it loads evenly on to two factors, and so on. \n\n:::rtip\nIn R, we will often see these estimats under specific columns:  \n\n+ __h2__ = item communality  \n+ __u__  = item uniqueness  \n+ __com__ = item complexity\n\n:::\n\n:::\n\n`r qbegin(qcounter())`\nUsing `fa.sort()`, examine the loadings of each item onto the factors, along with communalities, uniqueness and complexity scores.  \n\n\n- Do all factors load on 3+ items at a salient level?\n- Do all items have at least one loading above a salient cut off?\n- Are there any Heywood cases (communalities or standardised loadings $\\geq1$)? \n- Should we perhaps remove some complex items? \n- Does the solution account for an acceptable level of variance?  \n- Is the factor structure (items that load on to each factor) coherent, and does it make theoretical sense?  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nfa.sort(pgmod)\n```\n\n- Both factors load on 3+ items at a salient level. \n- All except item7 has loadings $>0.3$. \n- There are no Heywood cases.\n- Item 3 loads quite highly on both factors (high complexity) \n- The solution explains 38% of the variance.  \n- The coherence of the factor structure will require us to look back at the questions themselves. We'll not do this right now, but do that later on.  \n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nIf you think any items should be removed, do so now, and perform the factor analysis once more.  \n\nReally, this starts the whole process again, so we'll have to go through our steps to get to our new factor model on the reduced set of items.  \n`r qend()`\n`r solbegin(\"remove items\", slabel=FALSE,show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe're going to remove that problematic item 7 (which seems to be about \"reading the newspaper and watching tv\"), as well as item 3 (which seems to be about sleepiness, and relates to both factors quite highly).  \n```{r}\ndf2 <- df %>% \n    select(-item7,-item3)\n```\n`r solend()`\n`r solbegin(\"assess factorability\", slabel=FALSE,show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nAs before we removed items 3 and 7, these all look fine:  \n```{r}\ncortest.bartlett(cor(df2), n = nrow(df2))\nKMO(cor(df2))\n```\n\n`r solend()`\n`r solbegin(\"how many factors?\", slabel=FALSE,show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nBoth parallel analysis and MAP suggest 2:\n\n```{r}\nfa.parallel(df2, fa = 'fa')\nVSS(df2)\n```\n\n`r solend()`\n`r solbegin(\"conduct and inspect EFA\", slabel=FALSE,show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\npgmod2 <- fa(df2, nfactors = 2, rotate = \"oblimin\", fm = \"minres\")\npgmod2$loadings\nfa.sort(pgmod2)\n```\n\n- Both factors load on 3+ items at a salient level. \n- All items have a loading $>0.3$. \n- There are no Heywood cases.\n- No items have high loadings on multiple factors (complexity is low for all items)\n- The solution explains 40% of the variance.  \n\nThis looks much better!  \n\n`r solend()`\n\n<br>\n\n::: {.callout-note collapse=\"true\"}\n#### Reliability \n\n<!-- parallel tests -->\n\n<!-- test-retest reliability   -->\n<!-- split-half reliability   -->\n<!-- cronbach's alpha   -->\n<!-- mcdonald's omega   -->\n<!-- inter-rater reliability   -->\n<!-- intra-class correlations!    -->\n<!-- - link back to MLM   -->\n\n<!-- attenuation for measurement error -->\n\n__Measurement Error & Reliability__  \n\nYou will often find research that foregoes the measurement model by taking a scale score (i.e., the sum or mean of a set of likert-type questions). For instance, think back to our exercises on path mediation, where \"Health Locus of Control\" was measured as the \"average score on a set of items relating to perceived control over ones own health\". In doing so, we make the assumption that these variables provide measurements of the underlying latent construct **without error**.  \nIn fact, if we think about it a little, in our simple regression framework $y = \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k + \\varepsilon$ all our predictor variables $x_1$ to $x_k$ are assumed to be measured without error - it is only our outcome $y$ that our model considers to have some randomness included (the $\\varepsilon$). This seems less problematic if our variables are representing something that is quite easily and reliably measured (time, weight, height, age, etc.), but it seems inappropriate when we are concerned with something more abstract. For instance, two people both scoring 11 on the Generalised Anxiety Disorder 7 (GAD-7) scale does not necessarily mean that they have _identical_ levels of anxiety.  \n  \nThe inconsistency with which an observed variable reflects the underlying construct that we consider it to be measuring is termed the *reliability*.  \n\n:::frame\n**Reliability: A silly example** \n\nSuppose I'm trying to weigh [my dog](https://photos.app.goo.gl/f26FrRDyJxTvXvGS9). I have a set of scales, and I put him on the scales. He weighs in at 13.53kg. \nI immediately do it again, and the scales this time say 13.41kg. I do it again. 13.51kg, and again, 13.60kg. \nWhat is happening? Is Dougal's weight (Dougal is the dog, by the way) randomly fluctuating by 100g? Or are my scales just a bit inconsistent, and my observations contain measurement error?  \n\n```{r}\n#| echo: false\ntibble(\n    measurement =1:4,\n    dougal = c(13.53,13.41,13.51,13.6)\n) %>% \n    ggplot(.,aes(x=measurement,y=dougal))+\n    geom_point()+\n    geom_hline(yintercept=13.483,lty=\"dashed\")+\n    geom_text(x=3,y=13.484,label=\"Dougal's true weight (unknown to us)\",\n              vjust=-.5)+\n    labs(y=\"Dougal's Weight\")\n```\n\nI take him to the vets, where they have a much better set of weighing scales, and I do the same thing (measure him 4 times). The weights are 13.47, 13.49, 13.48, 13.48.  \nThe scales at the vets are clearly *more __reliable__*. We still don't know Dougal's *true* weight, but we are better informed to estimate it if we go on the measurements from the scales at the vet.^[Of course this all assuming that the scales aren't completely miscalibrated] \n\n```{r}\n#| echo: false\ntibble(\n    scales = rep(c(\"mine\",\"vets\"),each=4),\n    measurement = c(1:4,1:4),\n    dougal = c(13.53,13.41,13.51,13.6, 13.47, 13.49, 13.48, 13.48)\n) %>% \n    ggplot(.,aes(x=measurement,y=dougal, col=scales))+\n    geom_point()+\n    geom_path()+\n    geom_hline(yintercept=13.483,lty=\"dashed\")+\n    geom_text(inherit.aes=F, x=3,y=13.484,label=\"Dougal's true weight (unknown to us)\",vjust=-.5)+\n    labs(y=\"Dougal's Weight\")\n```\n:::\n\nAnother way to think about reliability is to consider the idea that more error means less reliability: \n$$\\text{observations = truth + error}$$  \n\n\nThere are different types of reliability:  \n\n- **test re-test reliability:** correlation between values over repeated measurements. \n- **alternate-form reliability:** correlation between scores on different forms/versions of a test (we might want different versions to avoid practice effects). \n- **Inter-rater reliability:** correlation between values obtained from different raters. \n- **split-half reliability:** correlation between scores of two equally sized subsets of items. \n\nThe form of reliability we are going to be most concerned with here is known as **Internal Consistency**. This is the extent to which items within a scale are correlated with one another. There are two main measures of this:\n\n  \n\n__Alpha & Omega__  \n\n**Cronbach's $\\alpha$** ranges from 0 to 1 (higher is better). You can get this using the `alpha()` function from the **psych** package. The formula is:\n\n$$\n\\begin{aligned}\n\\text{Cronbach's } \\alpha &= \\frac{n \\cdot \\overline{cov(ij)}}{\\overline{\\sigma^2_i} + (n-1) \\cdot \\overline{cov(ij)}}  \\\\\n\\\\\n\\text{where}:  & \\qquad\n& n = \\text{number of items} \\\\\n& \\overline{cov(ij)} = \\text{average covariance between item-pairs}  \\\\\n& \\overline{\\sigma^2_i} = \\text{average item variance}  \\\\\n\\end{aligned}\n$$\n\n**McDonald's Omega ($\\omega$)** is substantially more complicated, but avoids the limitation that Cronbach's alpha which assumes that all items are equally related to the construct. You can get it using the `omega()` function from the **psych** package. If you want more info about it then the help docs (`?omega()`) are a good place to start. \n\n:::\n\n`r qbegin(qcounter())`\nUsing the relevant function, obtain alpha for the set of items in each factor from your final factor analysis model of low mood. \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThe first factor has high loadings for items 1 to 9 (we have excluded 3 and 7). \n```{r}\nnames(df2)\nalpha(select(df2, item1:item9))\nalpha(select(df2, item10:item16))\n```\n:::int\nCronbach's $\\alpha$ of 0.82 and 0.81 for the two factors and suggests high internal consistency. \n:::\n\n`r solend()`\n\n\n<!-- :::frame -->\n<!-- You can't test the structural model if the measurement model is bad -->\n\n<!-- if you test the relationships between a set of latent factors, and they are not reliably measured by the observed items, then this error propagates up to influence the fit of the model.   -->\n<!-- To test the measurement model, it is typical to *saturate* the structural model (i.e., allow all the latent variables to correlate with one another). This way any misfit is due to the measurement model.   -->\n\n<!-- ::: -->\n\n<!-- what can we __do__ with this knowledge?   -->\n<!-- attenuation due to measurement error -->\n<br>\n\n# Replicability \n\n`r qbegin(qcounter())`\nSplit the dataset in half, and assess the replicability of your factor structure of low mood by examining the factor congruence in scores on each subset of the data. \n\n__Hint:__ see the lectures! \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(tidyverse)\nset1 <- df2 %>%\n  sample_frac(.5) # randomly select one half\nset2 <- anti_join(df2, set1) # select the non-matching cases\n\nres1 <- fa(set1, nfactors = 2, rotate = \"oblimin\")\nres2 <- fa(set2, nfactors = 2, rotate = \"oblimin\")\nfa.congruence(res1, res2) \n```\n\nWe can see there is a high level of factor congruence across the two subsets of data.  \n`r solend()`\n\n<br>\n\n# Factor Scores\n\n`r qbegin(qcounter())`\nExtract the factor scores for each factor of low-mood from your model, and attach them to original dataset (the one which has information on pet ownership).  \nThen, conduct a $t$-test to examine whether the pet-owners differ from non-pet-owners in their levels of each factor of low mood.  \n\nAs a bonus, can you come up with some description of the two factors? you will have to look back to what the items represent (i.e. the questions that each item is asking. \n\n::: {.callout-note collapse=\"true\"}\n#### reminder of questions\n\n```{r}\n#| echo: false\ntibble(QuestionNumber = paste0(\"item\",1:16),`Over the last 2 weeks, how much have you had/have you been...` = \n         c(\"Little interest or pleasure in doing things?\",\n\"Feeling down, depressed, or hopeless?\",\n\"~~Trouble falling or staying asleep, or sleeping too much?~~\",\n\"Feeling tired or having little energy?\",\n\"Poor appetite or overeating?\",\n\"Feeling bad about yourself - or that you are a failure or have let yourself or your family down?\",\n\"~~Reading the newspaper or watching television?~~\",\n\"Moving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?\",\n\"A lack of motivation to do anything at all?\",\n\"Feeling nervous, anxious or on edge?\",\n\"Not being able to stop or control worrying?\",\n\"Worrying too much about different things?\",\n\"Trouble relaxing?\",\n\"Being so restless that it is hard to sit still?\",\n\"Becoming easily annoyed or irritable?\",\n\"Feeling afraid as if something awful might happen?\")) %>%\n  gt::gt()\n```\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nIt looks like all the items 1-9 (excluding items 3 and 7, which aren't in our factor model in the end) are all about lower mood or lack of energy - i.e. 'depression'.  \nThe latter items that load onto the second factor (items 10-16) all seem related to 'anxiety'.  \n\nSo we can use the estimated factor scores as a representation of peoples' relative standings on those underlying latent variables of 'depression' and 'anxiety', and we can then look at whether people who own a pet significantly differ on those two constructs from those who don't own a pet!  \n\n```{r}\npgpets <- \n  pgpets %>% \n    mutate(\n      depression = pgmod2$scores[,1],\n      anxiety = pgmod2$scores[,2],\n      pet = factor(do_you_own_a_pet)\n    )\n\nt.test(pgpets$depression ~ pgpets$pet)\nggplot(pgpets, aes(x=pet, y=depression))+\n  geom_boxplot()\n\nt.test(pgpets$anxiety~pgpets$pet)\nggplot(pgpets, aes(x=pet, y=anxiety))+\n  geom_boxplot()\n```\n\n`r solend()`\n\n\n\n\n\n\n<div class=\"tocify-extend-page\" data-unique=\"tocify-extend-page\" style=\"height: 0;\"></div>"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"11_efa2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"theme":["united","assets/style-labs.scss"],"link-citations":true,"code-copy":false,"title":"11. EFA 2","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}