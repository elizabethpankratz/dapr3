{"title":"Likelihood vs Probability","markdown":{"yaml":{"title":"Likelihood vs Probability","params":{"SHOW_SOLS":true,"TOGGLE":true}},"headingText":"Setup","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\n```\n\n\nUpon hearing the terms \"probability\" and \"likelihood\", people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood).  \n\n:::statbox\nProbability refers to the chance of observing possible results if some certain state of the world were true^[This is the typical frequentist stats view. In Bayesian statistics, probability relates to the reasonable expectation (or \"plausibility\") of a belief]\n\nLikelihood refers to the probability of some hypotheses, given the data we have.  \n\n:::\n\n\n\nLet's consider a coin toss. For a fair coin, the chance of getting a heads/tails for any given toss is 0.5.  \nWe can simulate the number of \"heads\" in a single fair coin toss with the following code (because it is a single toss, it's just going to return 0 or 1):\n```{r}\nrbinom(n = 1, size = 1, prob = 0.5)\n```\nWe can simulate the number of \"heads\" in 8 fair coin tosses with the following code: \n```{r}\nrbinom(n = 1, size = 8, prob = 0.5)\n```\nAs the coin is fair, what number of heads would we expect to see out of 8 coin tosses? Answer: 4! \nDoing another 8 tosses:\n```{r}\nrbinom(n = 1, size = 8, prob = 0.5)\n```\nand another 8:\n```{r}\nrbinom(n = 1, size = 8, prob = 0.5)\n```\nWe see that they tend to be around our intuition expected number of 4 heads. We can change `n = 1` to ask `rbinom()` to not just do 1 set of 8 coin tosses, but to do 1000 sets of 8 tosses:\n```{r}\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n```\n\n## Probability\n\nWe can get to the __probability__ of observing $k$ heads in 8 tosses of a fair coin using `dbinom()`.  \nLet's calculate the probability of observing 2 heads in 8 tosses. \n\nAs coin tosses are independent, we can calculate probability using the product rule (\"$P(AB) = P(A)\\cdot P(B)$ where $A$ and $B$ are independent). So the probability of observing 2 heads in 2 tosses is $0.5 \\cdot 0.5 = 0.25$:\n```{r}\ndbinom(2, size=2, prob=0.5)\n```\nIn 8 tosses, those two heads could occur in various ways:\n```{r echo=FALSE}\nways = replicate(10, paste0(sample(c(rep(\"T\",6),rep(\"H\",2)), 8),collapse=\"\"))\nlibrary(tidyverse)\ntibble(\n  `Ways to get 2 heads in 8 tosses` = c(ways[!duplicated(ways)], \"...\")\n)\n```\nIn fact there are 28 different ways this could happen:\n```{r}\ndim(combn(8, 2))\n```\nThe probability of getting 2 heads in 8 tosses of a fair coin is, therefore:\n```{r}\n28 * (0.5^8)\n```\nOr, using `dbinom()`\n```{r}\ndbinom(2, size = 8, prob = 0.5)\n```\n\n:::imp\nThe important thing here is that when we are computing the probability, two things are fixed: \n\n - the number of coin tosses (8)\n - the value(s) that govern the coin's behaviour (0.5 chance of landing on heads for _any given toss_)\n \nWe can then can compute the probabilities for observing various numbers of heads:\n```{r}\ndbinom(0:8, 8, prob = 0.5)\n```\n\n```{r echo=FALSE, message=FALSE,warning=FALSE}\nlibrary(tidyverse)\ntibble(\n  n_heads = 0:8,\n  prob = dbinom(0:8, 8, prob = 0.5)\n) %>%\n  ggplot(., aes(x=n_heads, y=prob))+\n  geom_point(size = 2)+\n  geom_segment(aes(x=n_heads,xend=n_heads,y=0,yend=prob), alpha=.3)\n```\nNote that the probability of observing 10 heads in 8 coin tosses is 0, as we would hope! \n```{r}\ndbinom(10, 8, prob = 0.5)\n```\n:::\n\n## Likelihood \n\nFor likelihood, we are interested in hypotheses about our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?).  \n\nTo consider these hypotheses, we need to observe some data, and so we need to have a given number of tosses, and a given number of heads. Whereas above we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given toss, for the likelihood we fix the number of heads observed, and can make statements about different possible parameters that might govern the coins behaviour. \n\nFor example, if we __did__ observe 2 heads in 8 tosses, what is the likelihood of this data given various parameters?  \nOur parameter can take any real number between from 0 to 1, but let's do it for a selection:\n```{r}\nposs_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, poss_parameters)\n```\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin tosses, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). You can visualise this as below: \n\n```{r echo=FALSE}\ncurve(\n  dbinom(2,8,x), xlim = c(0,1),\n  ylab=\"Likelihood\",\n  xlab=expression(paste(\"Binomial \", rho)),\n)\n```\n\n\nFormalizing the intuition\nWe have a stochastic process that takes discrete values (i.e. outcomes of tossing a coin 10 times). We calculated the probability of observing a particular set of outcomes (8 correct predictions) by making assumptions about the underlying stochastic process, that is, the probability that our test subject can correctly predict the outcome of the coin toss is \\(p\\) (e.g. 0.8). We also assumed implicitly that the coin tosses are independent.\n\n## A slightly more formal approach\n\nLet $d$ be our data (our _observed_ outcome), and let $\\theta$ be the parameters that govern the data generating process. \n\nWhen talking about \"probability\" we are talking about $P(d | \\theta)$ for a given value of $\\theta$. \n\nIn reality, we don't actually know what $\\theta$ is, but we do observe some data $d$. \nGiven that we know that _if we have a specific value for $\\theta$_, then $P(d | \\theta)$ gives us the probability of observing $d$, it follows that we would like to figure out what values of $\\theta$ maximise $\\mathcal{L}(\\theta \\vert d) = P(d \\vert \\theta)$, where $\\mathcal{L}(\\theta \\vert d)$ is the \"likelihood function\" of our unknown parameters $\\theta$, conditioned upon our observed data $d$.\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nsource('assets/setup.R')\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(effects)\n```\n\n\nUpon hearing the terms \"probability\" and \"likelihood\", people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood).  \n\n:::statbox\nProbability refers to the chance of observing possible results if some certain state of the world were true^[This is the typical frequentist stats view. In Bayesian statistics, probability relates to the reasonable expectation (or \"plausibility\") of a belief]\n\nLikelihood refers to the probability of some hypotheses, given the data we have.  \n\n:::\n\n\n## Setup\n\nLet's consider a coin toss. For a fair coin, the chance of getting a heads/tails for any given toss is 0.5.  \nWe can simulate the number of \"heads\" in a single fair coin toss with the following code (because it is a single toss, it's just going to return 0 or 1):\n```{r}\nrbinom(n = 1, size = 1, prob = 0.5)\n```\nWe can simulate the number of \"heads\" in 8 fair coin tosses with the following code: \n```{r}\nrbinom(n = 1, size = 8, prob = 0.5)\n```\nAs the coin is fair, what number of heads would we expect to see out of 8 coin tosses? Answer: 4! \nDoing another 8 tosses:\n```{r}\nrbinom(n = 1, size = 8, prob = 0.5)\n```\nand another 8:\n```{r}\nrbinom(n = 1, size = 8, prob = 0.5)\n```\nWe see that they tend to be around our intuition expected number of 4 heads. We can change `n = 1` to ask `rbinom()` to not just do 1 set of 8 coin tosses, but to do 1000 sets of 8 tosses:\n```{r}\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n```\n\n## Probability\n\nWe can get to the __probability__ of observing $k$ heads in 8 tosses of a fair coin using `dbinom()`.  \nLet's calculate the probability of observing 2 heads in 8 tosses. \n\nAs coin tosses are independent, we can calculate probability using the product rule (\"$P(AB) = P(A)\\cdot P(B)$ where $A$ and $B$ are independent). So the probability of observing 2 heads in 2 tosses is $0.5 \\cdot 0.5 = 0.25$:\n```{r}\ndbinom(2, size=2, prob=0.5)\n```\nIn 8 tosses, those two heads could occur in various ways:\n```{r echo=FALSE}\nways = replicate(10, paste0(sample(c(rep(\"T\",6),rep(\"H\",2)), 8),collapse=\"\"))\nlibrary(tidyverse)\ntibble(\n  `Ways to get 2 heads in 8 tosses` = c(ways[!duplicated(ways)], \"...\")\n)\n```\nIn fact there are 28 different ways this could happen:\n```{r}\ndim(combn(8, 2))\n```\nThe probability of getting 2 heads in 8 tosses of a fair coin is, therefore:\n```{r}\n28 * (0.5^8)\n```\nOr, using `dbinom()`\n```{r}\ndbinom(2, size = 8, prob = 0.5)\n```\n\n:::imp\nThe important thing here is that when we are computing the probability, two things are fixed: \n\n - the number of coin tosses (8)\n - the value(s) that govern the coin's behaviour (0.5 chance of landing on heads for _any given toss_)\n \nWe can then can compute the probabilities for observing various numbers of heads:\n```{r}\ndbinom(0:8, 8, prob = 0.5)\n```\n\n```{r echo=FALSE, message=FALSE,warning=FALSE}\nlibrary(tidyverse)\ntibble(\n  n_heads = 0:8,\n  prob = dbinom(0:8, 8, prob = 0.5)\n) %>%\n  ggplot(., aes(x=n_heads, y=prob))+\n  geom_point(size = 2)+\n  geom_segment(aes(x=n_heads,xend=n_heads,y=0,yend=prob), alpha=.3)\n```\nNote that the probability of observing 10 heads in 8 coin tosses is 0, as we would hope! \n```{r}\ndbinom(10, 8, prob = 0.5)\n```\n:::\n\n## Likelihood \n\nFor likelihood, we are interested in hypotheses about our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?).  \n\nTo consider these hypotheses, we need to observe some data, and so we need to have a given number of tosses, and a given number of heads. Whereas above we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given toss, for the likelihood we fix the number of heads observed, and can make statements about different possible parameters that might govern the coins behaviour. \n\nFor example, if we __did__ observe 2 heads in 8 tosses, what is the likelihood of this data given various parameters?  \nOur parameter can take any real number between from 0 to 1, but let's do it for a selection:\n```{r}\nposs_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, poss_parameters)\n```\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin tosses, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). You can visualise this as below: \n\n```{r echo=FALSE}\ncurve(\n  dbinom(2,8,x), xlim = c(0,1),\n  ylab=\"Likelihood\",\n  xlab=expression(paste(\"Binomial \", rho)),\n)\n```\n\n\nFormalizing the intuition\nWe have a stochastic process that takes discrete values (i.e. outcomes of tossing a coin 10 times). We calculated the probability of observing a particular set of outcomes (8 correct predictions) by making assumptions about the underlying stochastic process, that is, the probability that our test subject can correctly predict the outcome of the coin toss is \\(p\\) (e.g. 0.8). We also assumed implicitly that the coin tosses are independent.\n\n## A slightly more formal approach\n\nLet $d$ be our data (our _observed_ outcome), and let $\\theta$ be the parameters that govern the data generating process. \n\nWhen talking about \"probability\" we are talking about $P(d | \\theta)$ for a given value of $\\theta$. \n\nIn reality, we don't actually know what $\\theta$ is, but we do observe some data $d$. \nGiven that we know that _if we have a specific value for $\\theta$_, then $P(d | \\theta)$ gives us the probability of observing $d$, it follows that we would like to figure out what values of $\\theta$ maximise $\\mathcal{L}(\\theta \\vert d) = P(d \\vert \\theta)$, where $\\mathcal{L}(\\theta \\vert d)$ is the \"likelihood function\" of our unknown parameters $\\theta$, conditioned upon our observed data $d$.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"lvp.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"theme":["united","assets/style-labs.scss"],"link-citations":true,"code-copy":false,"title":"Likelihood vs Probability","params":{"SHOW_SOLS":true,"TOGGLE":true}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}