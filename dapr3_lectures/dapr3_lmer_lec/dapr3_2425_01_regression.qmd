---
title: "Regression Refresh"
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(patchwork)
source('_theme/theme_quarto.R')
```

# Model Structures

## Models

::::{.columns}
:::{.column width="50%"}
__deterministic__  

given the same input, deterministic functions return *exactly* the same output

- $y = mx + c$  

- area of sphere = $4\pi r^2$  

- height of fall = $1/2 g t^2$

    <!-- - $g =$ gravitational constant, $9.8m/s^2$ -->
    <!-- - $t =$ time (in seconds) of fall -->

:::

:::{.column width="50%" .fragment}
__statistical__  

$$ 
\color{red}{\textrm{outcome}} \color{black}{=} \color{blue}{(\textrm{model})} + \color{black}{\textrm{error}}
$$


- handspan = height + randomness  

- cognitive test score = age + premorbid IQ + ... + randomness
:::
::::

## The Linear Model

$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error} \\
\qquad \\
\qquad \\
\color{red}{y_i} & = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \varepsilon_i \\
\qquad \\
& \text{where } \varepsilon_i \sim N(0, \sigma) \text{ independently} \\
\end{align}
$$


## The Linear Model

::::{.columns}

:::{.column width="50%"}
Our proposed model of the world:

$\color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \varepsilon_i$  

:::{.fragment}
```{r bb}
x <- tibble(x=c(-1,4))
f <- function(x) {5+2*x}
p1 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=f,size=1,colour="blue") +
  geom_segment(aes(x=0,xend=0,y=0,yend=f(0)),colour="blue", lty="dotted") +
  geom_segment(aes(x=0,xend=1,y=f(0),yend=f(0)),colour="blue",linetype="dotted") +
  geom_segment(aes(x=1,y=f(0),xend=1,yend=f(1)),colour="blue",linetype="dotted") +
  geom_point(x=0,y=f(0),col="blue",size=3)+
  annotate("text",x=0.5,y=3,hjust=0,label=expression(paste(b[0], " (intercept)")),
           size=8,parse=TRUE,colour="blue") +
  geom_segment(x=.5,xend=0.01,y=3,yend=4.9,arrow = arrow(length=unit(0.30,"cm")),col="blue")+
  geom_segment(x=1.02,xend=1.02,y=5,yend=7,col="blue")+
  geom_segment(x=c(1,1),xend=c(1.02,1.02),y=c(5,7),yend=c(5,7),col="blue")+
  annotate("text",x=1,y=6,hjust=-.1,label=expression(paste(b[1], " (slope)")),
           size=8,parse=TRUE,colour="blue") +
    ggtitle(expression(paste(b[0]," = 5, ",b[1]," = 2")))+
  scale_y_continuous(breaks=0:13)+
  scale_x_continuous(limits = c(-0.3, 4), breaks=0:4)
p1 +
  ggtitle("")+
  geom_segment(x=2.02,xend=2.02,y=7,yend=9,col="blue",alpha=.4)+
  geom_segment(x=c(2,2),xend=c(2.02,2.02),y=c(7,9),yend=c(7,9),col="blue",alpha=.4)+
  annotate("text",x=2,y=8,hjust=-.1,label=expression(paste(b[1], " (slope)")),
           size=8,parse=TRUE,colour="blue",alpha=.4) +
  
  geom_segment(x=3.02,xend=3.02,y=9,yend=11,col="blue",alpha=.1)+
  geom_segment(x=c(3,3),xend=c(3.02,3.02),y=c(9,11),yend=c(9,11),col="blue",alpha=.1)+
  annotate("text",x=3,y=10,hjust=-.1,label=expression(paste(b[1], " (slope)")),
           size=8,parse=TRUE,colour="blue",alpha=.1) +
  
  scale_y_continuous("y",labels=NULL)+
  scale_x_continuous(limits=c(-0.3,4), breaks=c(0,1), labels=c("0","1"))
  
```
:::
:::

:::{.column width="50%" .fragment}

Our model $\widehat{\textrm{fitted}}$ to some data:  

$\hat{y}_i = \color{blue}{\hat b_0 \cdot{} 1 + \hat b_1 \cdot{} x_i}$  



:::{.fragment}
For the $i^{th}$ observation:  

  - $\color{red}{y_i}$ is the value we observe for $x_i$   
  - $\hat{y}_i$ is the value the model _predicts_ for $x_i$   
  - $\color{red}{y_i} = \hat{y}_i + \hat\varepsilon_i$  
  
:::
  
  
:::
::::


## An example

::::{.columns}

:::{.column width="50%"}
$\color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \hat\varepsilon_i$  
$\color{red}{y_i} = \color{blue}{5 \cdot{} 1 + 2 \cdot{} x_i} + \hat\varepsilon_i$  
  
__e.g.__   
for the observation $x_i = 1.2, \; y_i = 9.9$:  

$$
\begin{align}
\color{red}{9.9} & = \color{blue}{5 \cdot{}} 1 + \color{blue}{2 \cdot{}} 1.2 + \hat\varepsilon_i \\
& = 7.4 + \hat\varepsilon_i \\
& = 7.4 + 2.5 \\
\end{align}
$$
:::
:::{.column width="50%"}
```{r}
#| label: errplot
xX <-1.2
yY <- 9.9
p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  geom_point(aes(x=xX,y=yY),size=3,colour="red") +
  geom_segment(aes(x=xX,xend=xX,y=f(xX),yend=yY),linetype="dotted",colour="black") +
  annotate("text",1.1,8.6,hjust=1,label=expression(paste(epsilon[i]," (residual)")),colour="black",size=8)
```

:::
::::

## Categorical Predictors

::::{.columns}
:::{.column width="50%"}

```{r}
#| label: catpred
set.seed(993)
tibble(
  x = sample(c("Category0","Category1"), size = 30, replace = T),
  y = 5 + 2*(x == "Category1") + rnorm(30,0,1) %>% round(2)
) %>% select(y,x) -> df
df %>% sample_n(6) %>% rbind(., c("...","...")) %>% kableExtra::kbl()
```

:::
:::{.column width="50%"}

```{r}
#| label: catpredplot
ggplot(df,aes(x=as.numeric(x=="Category1"),y=y))+
  geom_point(size=3,alpha=.5)+
  stat_summary(geom="point",shape=4,size=6)+
  stat_summary(geom="path", aes(group=1))+
  scale_x_continuous(name="isCategory1",breaks=c(0,1),
                     labels=c("0\n(FALSE)","1\n(TRUE)"))+
  geom_segment(x=0,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category0"]),
               lty="dashed",col="blue")+
  geom_segment(x=1,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category1"]),
               lty="dashed",col="blue")+
  annotate("text",x=1.05,y=6,
           label=expression(paste(b[1], " (slope)")),
           angle=90,
           size=8,parse=TRUE,colour="blue")+
  labs(title="y ~ x    (x is categorical)")
```
:::
::::

## Multiple Regression

More than one predictor?   

$\color{red}{y} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_1 + \, ... \, + b_k \cdot x_k} + \varepsilon$

:::aside
I've stopped showing the $i$'s for now because we start to have more subscripts, but for any individual $i$, our model is $\color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_{1i} + \, ... \, + b_k \cdot x_{ki}} + \varepsilon$
:::

## Multiple Regression 

::::{.columns}
:::{.column width="50%"}
```{r echo=FALSE, fig.asp=.8}
mwdata <- read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,main="y~x1+x2")
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

```

:::
:::{.column width="50%" .fragment}

```{r echo=FALSE, fig.asp=.8}
fit<-lm(wellbeing~outdoor_time+social_int+routine, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int, routine = "Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,70), main="y~x1+x2+x3\n(x3 is categorical)")
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int-2, routine = "No Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)
y <- wellbeing
par(new=TRUE)
persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,80), axes=F)

```
:::
::::

## tangent: {.smaller}

![](img_sandbox/ssvenn/Slide1.PNG)


## tangent:  {.smaller}

::::{.columns}
:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide2.PNG)


X and Y are 'orthogonal' (perfectly unrelated)  

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide3.PNG)

X and Y are correlated.  
**a** = portion of Y's variance that is shared with X
**e** = portion of Y's variance unrelated to X

:::
::::

:::{.notes}
think signal, noise 

with me? 

:::


## tangent: enter X2... {.smaller}


::::{.columns}
:::{.column width="50%"}

- X2 is also related to Y (**c**)
- X2 is orthogonal to X1 (no overlap)

<br>

- relation between X1 and Y is unaffected (**a**)
- unexplained variance in Y (**e**) is reduced, so **a**:**e** ratio greater.

Design is so important! If possible, we could *ensure* that X1 and X2 are orthogonal.  
:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide4.PNG)
:::
::::

:::{.notes}
so accounting for x2 differences can make it easier to detect differences due to x

experiments - if we randomly allocate people in to condition A vs B, then we ensure that, in the long run, condition is unrelated to e.g. age. 

:::



## tangent: enter X2 ... {.smaller}


::::{.columns}
:::{.column width="50%"}

- X2 is also related to Y (**c**)  
- X2 *is* related to X1 (**b + d**)

Association between X1 and Y is changed if we consider X2 (**a** is smaller than previous slide), because there is a bit (**b**) that could be attributed to X2 instead.  

- regression coefficients for X1 and X2 are like areas **a** and **c** (scaled to be in terms of 'per unit change in the predictor')
- total variance explained by _both_ X1 and X2 is **a+b+c**


:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide5.PNG)
:::
::::


## Interactions

what if the effect of x1 depends on the level of x2?  

. . .

adding in a product term (x1 $\times$ x2) to our model, we can model this.. 

$\color{red}{y} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_1 + b_2 \cdot x_2 + b_3 \cdot x_1 \cdot x_2} + \varepsilon$


:::aside
to get at why the product works this way, we could rearrange the above model equation to:  
$\color{red}{y} = \color{blue}{b_0 \cdot{} 1 + (b_1 + b_3 \cdot x_2) \cdot{} x_1 + b_2 \cdot x_2} + \varepsilon$  

you can see the "the effect of x1 on y" is now $(b_1 + b_3 \cdot x_2)$ - i.e. "some number $b_1$, plus some number $b_3$ that changes depending on $x_2$".  

:::

## Interactions

::::{.columns}
:::{.column width="50%"}
```{r}
mwdata2<-read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv") %>% mutate(
  isRural = factor(ifelse(location=="rural","rural","notrural"))
)
par(mfrow=c(1,2))
fit<-lm(wellbeing~outdoor_time+isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2], col="red")

fit<-lm(wellbeing~outdoor_time*isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2+x1:x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2]+coef(fit)[4], col="red")
par(mfrow=c(1,1))
```

:::
:::{.column width="50%" .fragment}

```{r}
set.seed(913)
df <- tibble(
  x1 = round(pmax(0,rnorm(50,4,2)),1),
  x2 = round(pmax(0,rnorm(50,4,2)),1),
  y = 8 + .6*x1 + .5*x2 + .8*x1*x2 + rnorm(50,0,4),
  yb = rbinom(50,1,plogis(scale(y)))
)
modl2 <- lm(y~x1*x2,df)

x1_pred <- df |> with(seq(min(x1),max(x1),length.out=100))
x2_pred <- df |> with(seq(min(x2),max(x2),length.out=100))

ac <- expand.grid(x1=x1_pred,x2=x2_pred) 

ypred <- matrix(predict(modl2,type="response",
                             newdata=ac),nrow=100)

library(plot3D)

persp3D(x=x1_pred,y=x2_pred,z=ypred,theta=65,phi=15,
        type="surface",xlab="x1",ylab="x2",zlab="y",
        xlim=c(0,9), ylim=c(0,8),
        zlim=c(min(df$y),max(df$y)+2),colkey=FALSE)
points3D(x=df$x1,y=df$x2,z=df$y,col="black",add=TRUE)
```

:::
::::


## Interactions

::::{.columns}
:::{.column width="50%"}
```{r}
mwdata2<-read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv") %>% mutate(
  isRural = factor(ifelse(location=="rural","rural","notrural"))
)
par(mfrow=c(1,2))
fit<-lm(wellbeing~outdoor_time+isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2], col="red")

fit<-lm(wellbeing~outdoor_time*isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2+x1:x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2]+coef(fit)[4], col="red")
par(mfrow=c(1,1))
```

:::
:::{.column width="50%"}
```{r}

x1_pred <- df |> with(seq(min(x1),max(x1),length.out=100))
x2_pred <- df |> with(seq(min(x2),max(x2),by=1))

library(plot3D)
lines3D(x=x1_pred,y=rep(1,100),theta=65,phi=15,
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=1)),
        xlab="x1",ylab="x2",zlab="y",
        xlim=c(0,9), ylim=c(0,8),
        zlim=c(min(df$y),max(df$y)+2),colkey=FALSE)
lines3D(x=x1_pred,y=rep(2,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=2)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(3,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=3)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(4,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=4)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(5,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=5)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(6,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=6)),
        colkey=FALSE,add=TRUE)
lines3D(x=x1_pred,y=rep(7,100),
        z=predict(modl2, newdata=tibble(x1=x1_pred,x2=7)),
        colkey=FALSE,add=TRUE)
```
:::
::::

## Interactions

::::{.columns}
:::{.column width="50%"}
```{r}
mwdata2<-read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv") %>% mutate(
  isRural = factor(ifelse(location=="rural","rural","notrural"))
)
par(mfrow=c(1,2))
fit<-lm(wellbeing~outdoor_time+isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2], col="red")

fit<-lm(wellbeing~outdoor_time*isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2+x1:x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2]+coef(fit)[4], col="red")
par(mfrow=c(1,1))
```

:::
:::{.column width="50%"}
```{r}
modl2 <- lm(y~x1*x2,df)

x1_pred <- df |> with(seq(min(x1),max(x1),by=1))
x2_pred <- df |> with(seq(min(x2),max(x2),length.out=100))

library(plot3D)
lines3D(x=rep(1,100),y=x2_pred,theta=65,phi=15,
        z=predict(modl2, newdata=tibble(x1=1,x2=x2_pred)),
        xlab="x1",ylab="x2",zlab="y",
        xlim=c(0,9), ylim=c(0,8),
        zlim=c(min(df$y),max(df$y)+2),colkey=FALSE)
lines3D(x=rep(2,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=2,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(3,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=3,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(4,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=4,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(5,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=5,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(6,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=6,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
lines3D(x=rep(7,100),y=x2_pred,
        z=predict(modl2, newdata=tibble(x1=7,x2=x2_pred)),
        colkey=FALSE,add=TRUE)
```
:::
::::

## Notation

$\begin{align} \color{red}{y} \;\;\;\; & = \;\;\;\;\; \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_1 + ... + b_k \cdot x_k} & + & \;\;\;\varepsilon \\ \qquad \\ \color{red}{\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ \vdots \\ y_n \end{bmatrix}} & = \color{blue}{\begin{bmatrix} 1 & x_{11} & x_{21} & \dots & x_{k1} \\ 1 & x_{12} & x_{22} &  & x_{k2} \\ 1 & x_{13} & x_{23} &  & x_{k3} \\ 1 & x_{14} & x_{24} &  & x_{k4} \\ 1 & x_{15} & x_{25} &  & x_{k5} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{1n} & x_{2n} & \dots & x_{kn} \end{bmatrix} \begin{bmatrix} b_0 \\ b_1 \\ b_2 \\ \vdots \\ b_k \end{bmatrix}} & + & \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \varepsilon_4 \\ \varepsilon_5 \\ \vdots \\ \varepsilon_n \end{bmatrix} \\ \qquad \\ \\\color{red}{\boldsymbol y} \;\;\;\;\; & = \qquad \qquad \;\;\; \mathbf{\color{blue}{X \qquad \qquad \qquad \;\;\;\: \boldsymbol \beta}} & + & \;\;\; \boldsymbol \varepsilon \\ \end{align}$


## Link functions

$\begin{align} \color{red}{y} = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & (-\infty, \infty) \\ \qquad \\ \qquad \\ \color{red}{ln \left( \frac{p}{1-p} \right) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & [0,1] \\ \qquad \\ \qquad \\ \color{red}{ln (y) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & (0, \infty) \\ \end{align}$  


## Generalised Linear Models in R

- Linear regression
```{r}
#| eval: false
#| echo: true
linear_model <- lm(continuous_y ~ 1 + x1 + x2 ... , data = df)
```

- Logistic regression
```{r}
#| eval: false
#| echo: true
logistic_model <- glm(binary_y ~ 1 + x1 + x2 + ..., data = df, family=binomial(link="logit"))
```

- Poisson regression
```{r}
#| eval: false
#| echo: true
poisson_model <- glm(count_y ~ 1 + x1 + x2 + ..., data = df, family=poisson(link="log"))
```

:::aside
We can (out of laziness) omit the 1 in `y~1+x1+...` and just write `y~x1+...`.  
The `1` just tells R to estimate the intercept, and it will do this by default anyway!  
:::


# Inference

## What is inference?

![](img_sandbox/statistical_inference.png){width=60%}

:::aside
image source: Umberto, DAPR1
:::

## Null Hypothesis Testing

```{r}
set.seed(2394)
samplemeans <- replicate(2000, mean(rnorm(n=100, mean=0, sd=5)))
g <- ggplot(data=tibble(samplemeans),aes(x=samplemeans))+
  #geom_histogram(alpha=.3)+
  stat_function(geom="line",fun=~dnorm(.x, mean=0,sd=sd(samplemeans))*270,lwd=1)

ld <- layer_data(g) %>% filter(x <= sd(samplemeans) & x >= (-sd(samplemeans)))
ld2 <- layer_data(g) %>% filter(x <= 2*sd(samplemeans) & x >= (-2*sd(samplemeans)))
ld3 <- layer_data(g) %>% filter(x >= 1.2)

g + geom_area(data=ld,aes(x=x,y=y),fill="grey30",alpha=.3) + 
  geom_area(data=ld2,aes(x=x,y=y),fill="grey30",alpha=.1) +
  geom_area(data=ld3,aes(x=x,y=y),fill="tomato1",alpha=.3) +
  geom_segment(aes(x=0,xend=0,y=0,yend=dnorm(0,0,sd=sd(samplemeans))*280), lty="dashed")+
  geom_segment(aes(x=1.2,xend=1.2,y=0,yend=180), lty="dashed", col="tomato1")+ 
  # geom_vline(aes(xintercept=1.2),lty="dashed",col="tomato1")+
  labs(x = "statistic")+
  scale_y_continuous(NULL, breaks=NULL)+
  scale_x_continuous(NULL, breaks=NULL, limits=c(-1.5,2))+
  annotate("text",x=-.5, y=250, label="population parameter\nunder null hypothesis", col="grey30")+
  annotate("text",x=1, y=210, label="statistics we would expect from\nsamples of size n if the\nnull hypothesis is true", col="grey30")+
  annotate("text",x=1.65, y=100, label="statistic we observed\nin our sample", col="tomato1")+
  annotate("text",x=1.5, y=40, label="p-value", col="tomato1")+
  geom_curve(aes(x=-.5, xend=0, y=240, yend=220), col="grey30", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=1, xend=0.5, y=190, yend=150), col="grey30", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=1.6, xend=1.2, y=90, yend=40), col="tomato1", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc"))) +
  geom_curve(aes(x=1.5, xend=1.3, y=35, yend=2.5), col="tomato1", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, "npc")))
```

## test of individual parameters

![](img_sandbox/sum1.png)

<!-- :::aside -->
<!-- the model "parameters" are the things being estimated - the intercept and all the slopes (and the residual standard deviation, but we're less interested in that).  -->
<!-- ::: -->

## test of individual parameters (2)

![](img_sandbox/sum2.png)

## test of individual parameters (3)

![](img_sandbox/sum3.png)

## test of individual parameters (4)

![](img_sandbox/sum4.png)


## Sums of Squares {.smaller}

::::{.columns}
:::{.column width="50%"}

Rather than focussing on slope coefficients, we can also think of our model in terms of sums of squares (SS).  

- $SS_{total} = \sum^{n}_{i=1}(y_i - \bar y)^2$.  

- $SS_{model} = \sum^{n}_{i=1}(\hat y_i - \bar y)^2$  
  
- $SS_{residual} = \sum^{n}_{i=1}(y_i - \hat y_i)^2$  

:::

:::{.column width="50%" .fragment}
![](img_sandbox/ssvenn/Slide5.PNG)

*SStotal = a+b+c+e, SSmodel = a+b+c, SSresidual = e*
:::
::::

## Sums of Squares {.smaller}

::::{.columns}
:::{.column width="50%"}

Rather than phrasing as slope coefficients, we can also think of our model in terms of sums of squares (SS).  

- $SS_{total} = \sum^{n}_{i=1}(y_i - \bar y)^2$  

- $SS_{model} = \sum^{n}_{i=1}(\hat y_i - \bar y)^2$  
  
- $SS_{residual} = \sum^{n}_{i=1}(y_i - \hat y_i)^2$  

:::

:::{.column width="50%"}
```{r}
set.seed(993)
df <- 
  tibble(
    x1 = rnorm(100)+3,
    y = x1*2 + rnorm(100)
  )

# SST, SSM and SSR
plt_sst = 
  ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=mean(df$y)), lty="dashed",col="red")+
  geom_text(x=1,y=8,label="bar(y)",parse=T, size=6)+
  geom_curve(x=1.1,xend=2,y=8,yend=mean(df$y),curvature=-.2)+
  labs(title="SS Total")

plt_ssr = ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=fitted(lm(y~x1,df))), lty="dashed",col="red")+
  labs(title="SS Residual")

plt_ssm = ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,yend=mean(df$y),y=fitted(lm(y~x1,df))), lty="dashed",col="blue")+
  labs(title="SS Model")

plt_sst / (plt_ssm + plt_ssr) & scale_x_continuous("x",breaks=NULL) & scale_y_continuous("y",breaks=NULL)

```
:::
::::

## $R^2$


::::{.columns}
:::{.column width="50%"}

![](img_sandbox/ssvenn/Slide5.PNG)
:::

:::{.column width="50%"}
$R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}$
```{r echo=FALSE}
x1 = rnorm(100)
x2 = rnorm(100)
y = .2*x1 + .3*x2 + rnorm(100)
```
```{r}
#| echo: true
#| eval: false
mdl <- lm(y ~ 1 + x1 + x2)
summary(mdl)
```
```
...

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)   ...       ...      ...    ...
x1            ...       ...      ...    ...
x2            ...       ...      ...    ...
...

...
Multiple R-squared:  0.134,	Adjusted R-squared:  0.116
...
```
:::
::::


## tests of multiple parameters

```{r echo=F}
set.seed(2345)
tibble(
  x1=rnorm(100),
  x2=rnorm(100),
  x3=rnorm(100),
  species=rep(c("cat","dog","parrot","horse"),e=25),
  y=10+x1-2*x2+2*x3+(species=="cat")*4 + rnorm(100)
) -> df
```

We can test reduction in residual sums of squares:


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
m1 <- lm(y ~ 1 + x1, data = df)
```
![](img_sandbox/ssvenn/Slide8.PNG)
:::

:::{.column width="50%"}
```{r}
#| echo: true
m2 <- lm(y ~ 1 + x1 + x2 + x3, data = df)
```
![](img_sandbox/ssvenn/Slide7.PNG)
:::
::::

## tests of multiple parameters

isolating the improvement in model fit due to inclusion of additional parameters



::::{.columns}
:::{.column width="50%"}

```{r}
#| echo: true
m1 <- lm(y ~ 1 + x1, data = df)
m2 <- lm(y ~ 1 + x1 + x2 + x3, data = df)
anova(m1,m2)
```

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide7a.PNG)
:::
::::

## tests of multiple parameters

or testing everything in the model all at once (by comparing it to a 'null model' with no predictors)


::::{.columns}
:::{.column width="50%"}

```{r}
#| echo: true
m1 <- lm(y ~ 1, data = df)
m2 <- lm(y ~ 1 + x1 + x2 + x3, data = df)
anova(m1,m2)
```

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide7b.PNG)
:::
::::


## tests of multiple parameters {.smaller}

- Recall that a categorical predictor with $k$ levels involves fitting $k-1$ coefficients
- We can test "are there differences in group means?" by testing the reduction in residual sums of squares resulting from the inclusion of all $k-1$ coefficients at once  

```{r}
#| echo: true
m1 = lm(y ~ 1, data = df)
m2 = lm(y ~ 1 + species, data = df)
coef(m2)
anova(m1, m2)
```


## tangent: traditional ANOVA/ANCOVA {.smaller}

This is kind of where traditional "analysis of (co)variance" sits.  

::::{.columns}
:::{.column width="50%"}

- Type 1 ("sequential"): tests the addition of each variable entered in to the model, **in order**

```{r}
#| echo: true
m1 = lm(y ~ x1 + x2 + x3, data = df)
anova(m2)
```

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide7c.PNG)
:::
::::

:::aside
this is not really relevant for DAPR3, but put a mental pin here for if your dissertation supervisor wants you to do an ANOVA/ANCOVA. You've already got all the tools to do these in the regression framework with `lm()`.  
:::

## tangent: traditional ANOVA/ANCOVA {.smaller}

This is kind of where traditional "analysis of (co)variance" sits.  

::::{.columns}
:::{.column width="50%"}
- Type 3: tests the addition of each variable *as if it were the last one entered in to the model*:  

```{r}
#| echo: true
m1 = lm(y ~ x1 + x2 + x3, data = df)
car::Anova(m1, type="III")
```

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide7d.PNG)
:::
::::

:::aside
this is not really relevant for DAPR3, but put a mental pin here for if your dissertation supervisor wants you to do an ANOVA/ANCOVA. You've already got all the tools to do these in the regression framework with `lm()`.  
:::


# Assumptions

## models have assumptions

Our model:  

$\color{red}{y} = \color{blue}{\mathbf{X \boldsymbol \beta}} + \varepsilon \qquad \text{where } \boldsymbol \varepsilon \sim N(0, \sigma) \text{ independently}$
<br>

Our ability to generalise from the model we fit on sample data to the wider population requires making some _assumptions._

. . .

- assumptions about the nature of the **model**
(linear)

. . .

- assumptions about the nature of the **errors** (normal)


:::aside
You can also phrase the linear model as: $\color{red}{\boldsymbol  y} \sim Normal(\color{blue}{\mathbf{X \boldsymbol \beta}}, \sigma)$
:::


## The broader idea {.smaller}

All our work here is in
aim of making **models of the world**.  


::::{.columns}
:::{.column width="80%" .incremental}

- Models are models. They are simplifications. They are therefore wrong.  

- Our residuals reflect everything that we **don't** account for in our model. $y - \hat{y}$

- In an ideal world, our model accounts for _all_ the systematic relationships. The leftovers (our residuals) are just random noise.  


  - If our model is mis-specified, or we don't measure some systematic relationship, then our residuals will reflect this.


- We check by examining how much "like randomness" the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d ("independent and identically distributed")
    - _these ideas tend to get referred to as our "assumptions"_


- We will **never** know whether our residuals contain only randomness - we can never observe everything! 


:::

:::{.column width="20%"}
![](img_sandbox/joeymap.jpg)
:::
::::


## assumptions 

::::{.columns}
:::{.column width="50%"}


What does randomness look like? "zero mean and constant variance"

- mean of the residuals = zero across the predicted values of the model.  

- spread of residuals is normally distributed and constant across the predicted values of the model.  

:::

:::{.column width="50%"}

```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
df<-tibble(x=runif(1000,1,10),xr = round(x), y=1*x+rnorm(1000))
df$y2 <- resid(lm(y~x,df))
#df$y[df$x==6]<-6+rnorm(100,0,3)
df %>% group_by(xr) %>% summarise(m=mean(y2), s=sd(y2)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y")
p2 <- ggplot(df, aes(x=xr,y=y2))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  #geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("residuals")

p1 / p2
```
:::
::::

## assumptions 

::::{.columns}
:::{.column width="50%"}


What does randomness look like? "zero mean and constant variance"

- **mean of the residuals = zero across the predicted values of the model.**  

- spread of residuals is normally distributed and constant across the predicted values of the model.  

:::

:::{.column width="50%"}

```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
tibble(x = runif(1000,1,10),
       xr = round(x),
       s = abs(5-x)*2,
       e = map_dbl(s,~rnorm(1,0,1)),
       y = x + s + e) -> df
df$y2 <- resid(lm(y~x,df))
#df$y[df$x==6]<-6+rnorm(100,0,3)
df %>% group_by(xr) %>% summarise(m=mean(y2), s=sd(y2)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y")
p2 <- ggplot(df, aes(x=xr,y=y2))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("residuals")

p1 / p2
```
:::
::::

## assumptions 

::::{.columns}
:::{.column width="50%"}


What does randomness look like? "zero mean and constant variance"

- mean of the residuals = zero across the predicted values of the model.  

- **spread of residuals is normally distributed and constant across the predicted values of the model.**  

:::

:::{.column width="50%"}

```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
tibble(x = runif(1000,1,10),
       xr = round(x),
       s = abs(x)/2,
       e = map_dbl(s,~rnorm(1,0,.)),
       y = x + e) -> df
df$y2 <- resid(lm(y~x,df))
#df$y[df$x==6]<-6+rnorm(100,0,3)
df %>% group_by(xr) %>% summarise(m=mean(y2), s=sd(y2)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y")
p2 <- ggplot(df, aes(x=xr,y=y2))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("residuals")

p1 / p2
```
:::
::::

## assumptions 

::::{.columns}
:::{.column width="50%"}


What does randomness look like? "zero mean and constant variance"

- mean of the residuals = zero across the predicted values of the model.  

- spread of residuals is normally distributed and constant across the predicted values of the model.  

:::

:::{.column width="50%"}

__`plot(model)`__

```{r echo=FALSE}
df<-tibble(x=runif(1000,1,10),xr = round(x),x2=rnorm(1000),y=1*x+.4*x2+rnorm(1000))
```

```{r echo=c(2,3)}
par(mfrow=c(2,2))
my_model <- lm(y ~ x + x2, data = df)
plot(my_model)
par(mfrow=c(1,1))
```

:::
::::

## Assumptions: Recipe Book
  
  
**L**inearity  
**I**ndependence  
**N**ormality  
**E**qual variances  

:::aside
"A Line without N is a Lie!" (Umberto)
:::

## assumption plots looking weird? {.smaller}

- is our model mis-specified?  
  - is the relationship non-linear? higher order terms? (e.g. $y \sim x + x^2$)
  - is there an omitted variable or interaction term? 

...
  
## assumption plots looking weird? {.smaller}

...

- transform the outcome variable?
  - makes things look more "normal"
  - but can make things more tricky to interpret:  
    `lm(y ~ x)` and `lm(log(y) ~ x)` are quite different models

...

## assumption plots looking weird? {.smaller}

...

- bootstrap\*
  - do many times: resample (with replacement) your data, and refit your model.
  - obtain a distribution of parameter estimate of interest. 
  - summarise the distribution to compute a confidence interval for the estimate
  - celebrate?  

...

:::aside
\* not great with small samples.
:::

## What about independence?  

- you can't tell violations of independence from plots - we need to know about how the data were generated.  

- transformations/bootstraps/ etc don't help us if we have violated our assumption of independence...

# Summary

## Summary

- we can fit a linear regression model which takes the form $\color{red}{y} = \color{blue}{\mathbf{X} \boldsymbol{\beta}} + \boldsymbol{\varepsilon}$  

- in R, we fit this with `lm(y ~ x1 + .... xk, data = mydata)`.  

- we can extend this to different link functions to model outcome variables which follow different distributions.  

- when drawing inferences from a fitted model to the broader population, we rely on certain assumptions.  

  - one of these is that the errors are independent.




