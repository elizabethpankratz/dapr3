<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multilevel Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josiah King" />
    <script src="jk_libs/libs/header-attrs/header-attrs.js"></script>
    <script src="jk_libs/libs/clipboard/clipboard.min.js"></script>
    <link href="jk_libs/libs/shareon/shareon.min.css" rel="stylesheet" />
    <script src="jk_libs/libs/shareon/shareon.min.js"></script>
    <link href="jk_libs/libs/xaringanExtra-shareagain/shareagain.css" rel="stylesheet" />
    <script src="jk_libs/libs/xaringanExtra-shareagain/shareagain.js"></script>
    <link href="jk_libs/libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="jk_libs/libs/tile-view/tile-view.js"></script>
    <link href="jk_libs/libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="jk_libs/libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="jk_libs/libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="jk_libs/tweaks.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# <b>Multilevel Models</b>
]
.subtitle[
## Data Analysis for Psychology in R 3
]
.author[
### Josiah King
]
.institute[
### Department of Psychology<br/>The University of Edinburgh
]

---






---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

&lt;h2 style="text-align: left;opacity:0.3;"&gt;Part 1: LM to MLM&lt;/h2&gt;
&lt;h2&gt;Part 2: Inference in MLM&lt;/h2&gt;


???
okay, let's talk about inference in multilevel models. 
this is a thorny issue, and there are a lot papers on the topic.
what we're aiming for is to have a broad overview of why it's a tricky issue, cover some of the options we have, and make a recommendation on which approach might be the most reliable to use.


---
# &lt;p&gt;&lt;/p&gt;

.pull-left[
you might have noticed...


```r
summary(cogtime_model)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: cog ~ visit_n + (1 + visit_n | participant)
##    Data: cogtime
## 
## REML criterion at convergence: 1357
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -2.274 -0.663 -0.091  0.577  3.227 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev. Corr
##  participant (Intercept) 16.09    4.01         
##              visit_n      1.22    1.11     0.82
##  Residual                37.93    6.16         
## Number of obs: 200, groups:  participant, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
*## (Intercept)    67.34       1.21    55.7
*## visit_n        -1.22       0.29    -4.2
## 
## Correlation of Fixed Effects:
##         (Intr)
## visit_n 0.222
```
]
.pull-right[
![](jk_img_sandbox/wotnop.png)
{{content}}

]

???
you will have probably noticed when we were discussing the output of an lmer() object, that we don't get any p-values. 
in lm, we're used to seeing an extra column here, with those little stars that people love to see. 

--

&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;center&gt;&lt;b&gt;Extensive debate about how best to test parameters from MLMs.&lt;/b&gt;&lt;/center&gt;  

???
there is a lot of debate on how we should be conducting inferential tests from multilevel models. 


---
# p-values in `lm()`

In simple LM, we test the reduction in residual SS (sums of squares), which follows an `\(F\)` distribution with a known `\(df\)`.
$$
`\begin{align}
F \qquad = \qquad \frac{MS_{model}}{MS_{residual}} \qquad = \qquad \frac{SS_{model}/df_{model}}{SS_{residual}/df_{residual}} \\
\quad \\
df_{model} = k \\
df_{residual} = n-k-1 \\
\end{align}`
$$
???
when we learned about standard linear model in DAPR2, tests were conducted on the reduction in sums of squared residuals, which follow an F distribution with k and n-k-1 degrees of freedom

--

The `\(t\)`-statistic for a coefficient in a simple regression model is the square root of `\(F\)` ratio between models with and without that parameter. 

- Such `\(F\)` will have 1 numerator degree of freedom (and `\(n-k-1\)` denominator degrees of freedom).
- The analogous `\(t\)`-distribution has `\(n-k-1\)` degrees of freedom

???
the t-statistics of the coefficients were tested against t distributions with n-k-1 degrees of freedom accordingly. 

---
# p-values in `lmer()`?  

- In certain very specific conditions, we can work out what the df1, df2. 

- Parameter estimates from these models are not based on minimising sums of squared residuals  

  - They are the ML/REML estimates

- This is good - it means they can handle unbalanced designs and complex random effect structures

- But: 
  - unclear how to calculate denominator `\(df\)`
  - unclear whether the test statistics even follow an `\(F\)` distribution
  
???
In all but the most controlled experimental designs, it is unclear how to calculate the denominator degrees of freedom in order to perform the relevant test. 
  
In MLM, the distribution of a test statistic when the null hypothesis is true is **unknown.**

???
in multilevel models, the distribution of our test statistics is often unknown. remember that we cannot solve our equations here and find the parameter estimates algebraically to minimise residual sums of squares, but instead we find the best estimates via max likelihood. 

---
# Options for inference

&lt;ol&gt;
&lt;li&gt;approximating ddf&lt;/li&gt;
  &lt;ul&gt;
  &lt;li&gt;satterthwaite&lt;/li&gt;
  &lt;li&gt;kenward-rogers&lt;/li&gt;
  &lt;li&gt;m-l-1&lt;/li&gt;
  &lt;li&gt;...&lt;/li&gt;
  &lt;/ul&gt;
&lt;li&gt;likelihood based methods&lt;/li&gt;
  &lt;ul&gt;
  &lt;li&gt;profile likelihood confidence interval&lt;/li&gt;
  &lt;li&gt;likelihood ratio tests&lt;/li&gt;
  &lt;/ul&gt;
&lt;li&gt;bootstrap&lt;/li&gt;
  &lt;ul&gt;
  &lt;li&gt;parametric&lt;/li&gt;
    &lt;ul&gt;
    &lt;li&gt;confidence interval&lt;/li&gt;
    &lt;li&gt;likelihood ratio test&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;case-based&lt;/li&gt;
    &lt;ul&gt;
    &lt;li&gt;confidence interval&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/ul&gt;
&lt;/ol&gt;
  
&lt;p&gt;(there are others...)&lt;/p&gt;  

---
# Options for inference

&lt;ol&gt;
&lt;li&gt;approximating ddf&lt;/li&gt;
  &lt;ul&gt;
  &lt;li style="opacity:.4"&gt;satterthwaite&lt;/li&gt;
  &lt;li&gt;kenward-rogers&lt;/li&gt;
  &lt;li style="opacity:.4"&gt;m-l-1&lt;/li&gt;
  &lt;li style="opacity:.4"&gt;...&lt;/li&gt;
  &lt;/ul&gt;
&lt;li&gt;likelihood based methods&lt;/li&gt;
  &lt;ul&gt;
  &lt;li&gt;profile likelihood confidence interval&lt;/li&gt;
  &lt;li&gt;likelihood ratio tests&lt;/li&gt;
  &lt;/ul&gt;
&lt;li&gt;bootstrap&lt;/li&gt;
  &lt;ul&gt;
  &lt;li style="opacity:.4"&gt;parametric&lt;/li&gt;
    &lt;ul&gt;
    &lt;li style="opacity:.4"&gt;confidence interval: &lt;code&gt;confint(model, method = "boot")&lt;/code&gt;&lt;/li&gt;
    &lt;li style="opacity:.4"&gt;likelihood ratio test: &lt;code&gt;pbkrtest::PBmodcomp(full_model, reduced_model)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li style="opacity:.4"&gt;case-based&lt;/li&gt;
    &lt;ul&gt;
    &lt;li style="opacity:.4"&gt;confidence interval&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/ul&gt;
&lt;/ol&gt;
  
&lt;p style="opacity:.4"&gt;(there are others...)&lt;/p&gt;  
???
There are three main approaches to drawing inferences from multilevel models. 


---
count:false
# df approximations

.pull-left[

Loading the package **lmerTest** will fit your models and print the summary with p-values approximated by the Satterthwaite method.

```r
library(lmerTest)
full_model &lt;- lmer(cog ~  1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime, REML = TRUE)
summary(full_model)
```

```
Linear mixed model fit by REML. t-tests use Satterthwaite

 ...
 ...

Random effects:
Groups      Name        Variance Std.Dev. Corr
participant (Intercept) 10.06    3.17         
            visit_n      1.22    1.11     0.69
Residual                37.93    6.16         
Number of obs: 200, groups:  participant, 20

Fixed effects:
           Estimate Std. Error    df t value Pr(&gt;|t|)    
(Intercept)    68.56       1.18 19.00    58.2  &lt; 2e-16 ***
visit_n        -1.22       0.29 19.00    -4.2  0.00048 ***

 ...
```

]

.pull-right[
The **pbkrtest** package implements the slightly more reliable Kenward-Rogers method for model comparison.  
Good for small samples

```r
library(pbkrtest)
restricted_model &lt;- lmer(cog ~ 1 + 
                           (1 + visit_n | participant), 
                         data = cogtime, REML = TRUE)
full_model &lt;- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime, REML = TRUE)
KRmodcomp(full_model, restricted_model)
```

```
## large : cog ~ 1 + visit_n + (1 + visit_n | participant)
## small : cog ~ 1 + (1 + visit_n | participant)
##       stat  ndf  ddf F.scaling p.value    
## Ftest 17.7  1.0 19.0         1 0.00048 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]

---
# Likelihood ratio tests

- Compares the log-likelihood of two competing models.  

- remember "likelihood" = a function that associates to a parameter the probability (or probability density) of observing the given sample data. 

- ratio of two likelihoods is **asymptotically** `\(\chi^2\)`-square distributed.

    - *this means for small samples (at any "levels") it may be unreliable*


```r
restricted_model &lt;- lmer(cog ~ 1 + 
                           (1 + visit_n | participant), 
                         data = cogtime, REML = FALSE)
full_model &lt;- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime, REML = FALSE)
anova(restricted_model, full_model)
```

```
## Data: cogtime
## Models:
## restricted_model: cog ~ 1 + (1 + visit_n | participant)
## full_model: cog ~ 1 + visit_n + (1 + visit_n | participant)
##                  npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## restricted_model    5 1382 1398   -686     1372                        
## full_model          6 1370 1390   -679     1358  13.2  1    0.00029 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---
# Bootstrap

- Parametric Bootstrap  
  assumes that explanatory variables are fixed and that model specification and the distributions such as `\(\zeta_i \sim N(0,\sigma_{\zeta})\)` and `\(\varepsilon_i \sim N(0,\sigma_{\varepsilon})\)` are correct.
  
- Case-based Bootstrap  
  minimal assumptions - we just need to ensure that we correctly specify the hierarchical dependency of data.  
  requires decision of at which levels to resample.  
  (discussed more next week)


???

What we are going to focus on here is bootstrapping multilevel models.  
There are various different ways we can bootstrap, which vary in the assumptions they make about our model and about the process that generates our data.  
Today we'll focus on parametric bootstrapping. This assumes that our model is correct, for instance, in that our residuals and our random effect terms are normally distributed. 

---
# Parametric Bootstrap

The basic premise is that we:  

1. fit model(s) to data

2. Do many times:

  - simulate data based on the parameters of the fitted model(s)
  - compute some statistic/estimates from the simulated data

3. Construct a distribution of possible values  

---
# Parametric Bootstrap

.pull-left[
## Confidence Intervals

```r
full_model &lt;- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime)
confint(full_model, method = "boot")
```
```
              2.5 %  97.5 %
.sig01       0.6561  5.4831
.sig02      -0.1509  1.0000
.sig03       0.6630  1.5906
.sigma       5.5275  6.8391
(Intercept) 66.3313 70.8488
visit_n     -1.7774 -0.6318
```
]
.pull-right[
## LRT
(a bootstrapped version of the likelihood ratio test):  

```r
library(pbkrtest)
restricted_model &lt;- lmer(cog ~ 1 + 
                           (1 + visit_n | participant), 
                         data = cogtime)
full_model &lt;- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime)
PBmodcomp(full_model, restricted_model)
```
```
Bootstrap test; time: 79.81 sec; samples: 1000; extremes: 1;
Requested samples: 1000 Used samples: 979 Extremes: 1
large : cog ~ 1 + visit_n + (1 + visit_n | participant)
cog ~ 1 + (1 + visit_n | participant)
       stat df p.value    
LRT    13.1  1 0.00029 ***
PBtest 13.1    0.00204 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
]


---
# Summary

- Lots of debate around how best to conduct statistical inferences based on multi-level models. 

- Lots of options:

  - approximations for `\(df\)`: _easy to implement. generally reliable. debate as to whether null is actually F. distribution_  
      - fit `lmer()` with package **lmerTest**   
      - `KRmodcomp(mod2, mod1)` from package **pbkrtest**  
      
  - likelihood ratio tests: _very quick, but best avoided unless big `\(n\)` at all levels_  
      - `anova(mod1, mod2)`   
      
  - parametric bootstrap: _time consuming, but probably most reliable option (although can be problematic with unstable models)_  
      - `PBmodcomp(mod2, mod1)` from package **pbkrtest**    
      - `confint(mod, method="boot")`   
      


???
In summary then, making statistical inferences from multilevel models is a difficult, and slightly contentious issue. 
There are various approaches that we can use, many of which have certain drawbacks, such as requiring perfectly balanced designs, or being less reliable with small sample sizes. 
ultimately, the decision is yours to make - you may find conducting standard likelihood ratio tests, using the anova function, are more convenient as they do not take much time to compute, but we would recommend a bootstrapping approach as being worth the extra little bit of time. 


---
class: inverse, center, middle, animated, rotateInDownLeft

# End

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="jk_libs/macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
