---
title: "<b>Multilevel Models</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---



```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)

options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")

xaringanExtra::use_share_again()
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)

knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
theme_set(
    theme_minimal() + 
    theme(text = element_text(size=20))
)
source("jk_source/jk_presfuncs.R")

library(xaringanthemer)
style_mono_accent(
  base_color = "#88B04B", # DAPR3 
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  code_font_size = "0.7rem",
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)
```

class: inverse, center, middle

<h2>Part 1: LM to MLM</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>

---
# Terminology

```{r echo=FALSE, eval=FALSE}
tribble(
  ~word, ~freq,
  "multi-level model", 26200+109000,
  "hierarchical linear model", 21300,
  "mixed-effect model", 40900+19400+130000,
  "mixed model", 1200000,
  "linear mixed model", 169000,
  "nested data model", 295,
  "random coefficient model", 10400,
  "random-effect model", 69100+371000,
  "random parameter model", 1750,
  "random-intercept model", 14300,
  "split-plot designs", 7020,
  "variance components model", 5580,
  "partial pooling", 4220,
  "mixed error-component model", 44,
  "random slope model", 3180,
  "panel data model", 34900,
  "latent curve model", 1230,
  "growth curve model", 16300
) -> mlmname


mlmname$freq[mlmname$freq > 100000] <- c(75000,85000, 110000,80000,95000)*1.5

#wordcloud2(mlmname, shape="diamond", size=.4)
library(wordcloud)
wordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,
          min.freq=1,
          scale=c(4,.5),rot.per=0,
          fixed.asp=T,
          #ordered.colors=T,
          colors="#88B04B")
```

```{r echo=FALSE, fig.cap="(size weighted by hits on google scholar)", fig.align="center", fig.asp=.9}
knitr::include_graphics("jk_img_sandbox/mlmname.png")
```

???
- there are lots of different names for these sorts of models, some of which you may have come across before. typically in psychology they get referred to as mlm or lmm. 

---
# Notation 
<!-- $$ -->
<!-- \begin{align} -->
<!-- & \text{for observation }i \\ -->
<!-- \quad \\ -->
<!-- & \color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 \; + \; \beta_1 \cdot{} x_{i} } + \varepsilon_i \\ -->
<!-- \end{align} -->
<!-- $$ -->
**Simple regression**  
.pull-left[
$\begin{align} & \text{for observation }i \\ \quad \\ \quad \\ & \color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 \; + \; \beta_1 \cdot{} x_{i} } + \varepsilon_i \\ \end{align}$
]

???
we spent pretty much the entire of dapr2 studying models that take this form.  
red, blue, black.  
the indexing here is just at one level, the units of observation i. 
for instance, if we were modelling people's scores on a test, we would have the score of person i, is equal to the some intercept, plus some amount increase for every hours revision they did, plus some some random error specific to that person

---
# Notation 

<!-- $$ -->
<!-- \begin{align} -->
<!-- & \text{for observation }j\text{ in group }i \\ -->
<!-- \quad \\ -->
<!-- & \text{Level 1:} \\ -->
<!-- & \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ -->
<!-- & \text{Level 2:} \\ -->
<!-- & \color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}} \\ -->
<!-- & \color{blue}{\beta_{1i}} = \gamma_{10} + \color{orange}{\zeta_{1i}} \\ -->
<!-- \quad \\ -->
<!-- & \text{Where:} \\ -->
<!-- & \gamma_{00}\text{ is the population intercept, and }\color{orange}{\zeta_{0i}}\text{ is the deviation of group }i\text{ from }\gamma_{00} \\ -->
<!-- & \gamma_{10}\text{ is the population slope, and }\color{orange}{\zeta_{1i}}\text{ is the deviation of group }i\text{ from }\gamma_{10} \\ -->
<!-- $$ -->
**Multi-level**  
.pull-left[
$\begin{align} & \text{for observation }j\text{ in group }i \\ \quad \\ & \text{Level 1:} \\ & \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ & \text{Level 2:} \\ & \color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}} \\ & \color{blue}{\beta_{1i}} = \gamma_{10} + \color{orange}{\zeta_{1i}} \\ \quad \\ \end{align}$
]

???
multi-level models are models that fit a structure such that there are different levels of observation.   
so we may have, for instance, person j from class i.  
our level 1 equation is at the lower level units of observation, 
BUT,
note that we can now allow our betas to contain an index i,  
so the intercept beta 0, and the effect of x, beta_1, are now specific to group i.  



--

.pull-right[
$\begin{align} & \text{Where:} \\ & \gamma_{00}\text{ is the population intercept}\\ & \text{and  }\color{orange}{\zeta_{0i}}\text{ is the deviation of group }i\text{ from }\gamma_{00} \\ \qquad \\ & \gamma_{10}\text{ is the population slope,}\\ & \text{and }\color{orange}{\zeta_{1i}}\text{ is the deviation of group }i\text{ from }\gamma_{10} \\ \end{align}$
]

???
And these are specified in our level 2 equations below, where each beta is some value gamma, plus the adjustment zeta for that group i. 

--

We are now assuming $\color{orange}{\zeta_0}$, $\color{orange}{\zeta_1}$, and $\varepsilon$ to be normally distributed with a mean of 0, and we denote their variances as $\sigma_{\color{orange}{\zeta_0}}^2$, $\sigma_{\color{orange}{\zeta_1}}^2$, $\sigma_\varepsilon^2$ respectively.   

The $\color{orange}{\zeta}$ components also get termed the "random effects" part of the model, Hence names like "random effects model", etc.

???
note that these equations take a similar form in that the epsilon, and the zeta, are modelled as random deviations around some estimate.  
The zetas are the random deviations for each group i around the values gamma,  
and the epsilon is the random deviations for each observation j from the group i values that to which it belongs.  
and so these zetas also get referred to as the "random effects" because they are assumed to be randomly drawn from a population. 
- NOTE we don't have to allow everything to vary by-group. we could also fix beta1, the effect of x, to be the same for each group. so it's just one value like in our simple regression, while still modelling the intercept as randomly varying by groups.  so we would remove this line, and remove the index i here, and we would have a random intercept only model. 


---
# Notation 

**Mixed-effects == Multi Level**

Sometimes, you will see the levels collapsed into one equation, as it might make for more intuitive reading:

$\color{red}{y_{ij}} = \underbrace{(\gamma_{00} + \color{orange}{\zeta_{0i}})}_{\color{blue}{\beta_{0i}}} \cdot 1 + \underbrace{(\gamma_{10} + \color{orange}{\zeta_{1i}})}_{\color{blue}{\beta_{1i}}} \cdot x_{ij}  +  \varepsilon_{ij} \\$

.footnote[
**other notation to be aware of**  

- Many people use the symbol $u$ in place of $\zeta$  

- Sometimes people use $\beta_{00}$ instead of $\gamma_{00}$  

- In various resources, you are likely to see $\alpha$ used to denote the intercept instead of $\beta_0$  

]

???
We can also collapse these multi-level equations into one line, such as we've done here.  
this is often the way of writing that gets termed "mixed effects" because the effects, the betas, are a mix of fixed values and random deviations around them. 


---
count:false
# Notation 

__Matrix form__

And then we also have the condensed matrix form of the model, in which the Z matrix represents the grouping structure of the data, and $\zeta$ contains the estimated random deviations. 


$\begin{align} \color{red}{\begin{bmatrix} y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\ y_{32} \\ \end{bmatrix}} & = \color{blue}{\begin{bmatrix} 1 & x_{11} \\ 1 & x_{12} \\ 1 & x_{21} \\ 1 & x_{22} \\1 & x_{31} \\ 1 & x_{32} \\ \end{bmatrix} \begin{bmatrix} \gamma_{00} \\ \beta_1 \\  \end{bmatrix}} & + & \color{orange}{ \begin{bmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 1 \\ \end{bmatrix} \begin{bmatrix}\zeta_{01} \\ \zeta_{02} \\ \zeta_{03} \end{bmatrix}} & + & \begin{bmatrix} \varepsilon_{11} \\ \varepsilon_{12} \\ \varepsilon_{21} \\ \varepsilon_{22} \\ \varepsilon_{31} \\ \varepsilon_{32} \end{bmatrix} \\ \qquad \\ \\ \color{red}{\boldsymbol y}\;\;\;\;\; & = \qquad \mathbf{\color{blue}{X \qquad \;\;\boldsymbol \beta}} & + & \qquad \; \mathbf{\color{orange}{Z \qquad \;\;\;\;\; \boldsymbol \zeta}} & + & \;\;\;\varepsilon \\ \end{align}$

<!-- $$ -->
<!-- \begin{align}  -->
<!-- \color{red}{ -->
<!-- \begin{bmatrix} -->
<!-- y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\ y_{32} \\ -->
<!-- \end{bmatrix} -->
<!-- } & =  -->
<!-- \color{blue}{ -->
<!-- \begin{bmatrix} -->
<!-- 1 & x_{11} \\ -->
<!-- 1 & x_{12} \\ -->
<!-- 1 & x_{21} \\ -->
<!-- 1 & x_{22} \\ -->
<!-- 1 & x_{31} \\ -->
<!-- 1 & x_{32} \\ -->
<!-- \end{bmatrix}  -->
<!-- \begin{bmatrix}  -->
<!-- \gamma_{00} \\ \beta_1 \\   -->
<!-- \end{bmatrix} -->
<!-- }  -->
<!-- & -->
<!-- + & -->
<!-- \color{orange}{ -->
<!-- \begin{bmatrix}  -->
<!-- 1 & 0 & 0 \\  -->
<!-- 1 & 0 & 0 \\ -->
<!-- 0 & 1 & 0 \\ -->
<!-- 0 & 1 & 0 \\ -->
<!-- 0 & 0 & 1 \\ -->
<!-- 0 & 0 & 1 \\ -->
<!-- \end{bmatrix} -->
<!-- \begin{bmatrix}  -->
<!-- \zeta_{01} \\ \zeta_{02} \\ \zeta_{03}  -->
<!-- \end{bmatrix} -->
<!-- } -->
<!-- & + & -->
<!-- \begin{bmatrix}  -->
<!-- \varepsilon_{11} \\ \varepsilon_{12} \\ \varepsilon_{21} \\ \varepsilon_{22} \\ \varepsilon_{31} \\ \varepsilon_{32}  -->
<!-- \end{bmatrix} \\  -->
<!-- \qquad \\  -->
<!-- \\ -->
<!-- \color{red}{\boldsymbol y}\;\;\;\;\; & = \qquad \mathbf{\color{blue}{X \qquad \;\;\boldsymbol \beta}} & + & \qquad \; \mathbf{\color{orange}{Z \qquad \;\;\;\;\; \boldsymbol \zeta}} & + & \;\;\;\varepsilon \\  -->
<!-- \end{align} -->
<!-- $$ -->

???
What this actually corresponds to in terms of data, is that we have 

- our outcome variable y, which is a vector of length n  
- an n by p matrix, which are the values of our predictors for each of observation in our sample
- a vector of p coefficients
- an n by g matrix representing our grouping structure
- a matrix of random effects zeta, 


---
# "Fixed" vs "Random"

.pull-left[
$\begin{align}& \text{Level 1:} \\ & \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ & \text{Level 2:} \\ & \color{blue}{\beta_{0i}} = \underbrace{\gamma_{00}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{0i}}_{\textrm{random}}} \\ & \color{blue}{\beta_{1i}} = \underbrace{\gamma_{10}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{1i}}_{\textrm{random}}} \\ \quad \\ \end{align}$
]
.pull-right[
$\color{red}{y_{ij}} = (\underbrace{\gamma_{00}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{0i}}_{\textrm{random}}}) \cdot 1 + (\underbrace{\gamma_{10}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{1i}}_{\textrm{random}}}) \cdot x_{ij}  +  \varepsilon_{ij} \\$
]

$\color{orange}{\zeta_i}$ is "random" because considered a random sample from larger population such that $\color{orange}{\zeta_i} \sim N(0, \sigma^2_{\color{orange}{\zeta_i}})$. 

???
so in either formulation, what multilevel, or mixed effects, models allow us to do is model our "effects" (i.e., the estimated influence that variables exert over our outcome) as randomly varying across our observed groups. 
our effects can now be either a fixed estimate for all our groups, or a fixed estimate around which our groups randomly vary.  

---
# Fixed vs Random

What is the difference? 

When specifying a random effects model, think about the data you have and how they fit in the following table:

| Criterion: | Repetition: <br> _If the experiment were repeated:_ | Desired inference: <br> _The conclusions refer to:_ |
|----------------|--------------------------------------------------|----------------------------------------------------|
| Fixed effects  | <center>Same levels would be used</center>     |    <center>The levels used </center>                                   |
| Random effects | <center>Different levels would be used</center>   | <center>A population from which the levels used<br> are just a (random) sample</center> |

???
another way of thinking about this distinction is that the random parts of our model are those which we consider to be a random sample from a larger population.  
so we might have data from children within schools, and the schools themselves are a sample of the larger population of schools. 
you can think of it as "if someone were to replicate this experiment, would they use the same levels of this variable?". 

--

- Sometimes, there isn't much variability in a specific random effect and to allow your model to fit it is common to just model that variable as a fixed effect. 

- Other times, you don't have sufficient data or levels to estimate the random effect variance, and you are forced to model it as a fixed effect. 

???
However, it's important to note that not everything that can be modelled as a random effect SHOULD be modelled as such.  
fitting this sort of model requires considerations of the amount of variability in our data. 


---
# Advantages of MLM

Multi-level models can be used to answer multi-level questions!  
<br><br>
Do phenomena at Level X predict __outcomes__ at Level Y?  

.pull-left[
Does population density in school district predict variation in scores in childrens' first year of school?  
]
.pull-right[
$\textrm{score}_{ij} = \beta_{0i} + \beta_1\textrm{school_year}_j + \varepsilon_{ij}$  
<br>
$\beta_{0i} = \gamma_{00} + \gamma_{01}\textrm{district_pop_dens}_i + \zeta_{0i}$  
]

???
So why would we want to fit these sort of models?  
Well, beyond the benefit that it includes terms that explicitly model the by-clustering variation that might cause our standard regression to lead to inappropriate inferences, multilevel models allow us to ask lots of really interesting multi-level questions.  
for instance, whether some predictor at level one level influences outcomes at another. 

    
---
# Advantages of MLM

Multi-level models can be used to answer multi-level questions!  
<br><br>
Do phenomena at Level X influence __effects__ at Level Y?  

.pull-left[
Does amount of school funding influence childrens' improvement in scores over time?  
]
.pull-right[
$\textrm{score}_{ij} = \beta_{0} + \beta_{1i}\textrm{school_year}_j + \varepsilon_{ij}$  
<br>
$\beta_{1i} = \gamma_{10} + \gamma_{11}\textrm{school_funding}_i + \zeta_{1i}$
]
  
???
or if a variable at some level moderates the effect of a variable at a different level

---
# Advantages of MLM

Multi-level models can be used to answer multi-level questions!  
<br><br>
Do random variances covary?  

.pull-left[
Do children who score higher at the start of school show greater improvements than those who start lower?  
]
.pull-right[
$\textrm{score}_{ij} = \beta_{0i} + \beta_{1i}\textrm{school_year}_j + \varepsilon_{ij}$  
<br>
$\beta_{0i} = \gamma_{00} + \zeta_{0i}$  
$\beta_{1i} = \gamma_{10} + \zeta_{1i}$  
<br>
$$
\begin{bmatrix}
\sigma^2_{\zeta_{0}} & \\ \sigma_{\zeta_{0},\zeta_{1}} & \sigma^2_{\zeta_{1}} \\
\end{bmatrix}
$$
]

???
we can also get more complicated and ask questions concerning the correlations between these random effects - these zeta terms.  
are groups that have have higher (relative to other groups) intercepts, show greater effects of X?  

---
# lme4

- **lme4** package (many others are available, but **lme4** is most popular).  

- `lmer()` function.  

- syntax is similar to `lm()`, in that we specify:   

    __*[outcome variable]*__ ~ __*[explanatory variables]*__, data = __*[name of dataframe]*__
    
- in `lmer()`, we add to this the random effect structure in parentheses:  

    __*[outcome variable]*__ ~ __*[explanatory variables]*__ + (__*[vary this]*__ | __*[by this grouping variable]*__), data = __*[name of dataframe]*__, REML = __*[TRUE/FALSE]*__

???
okay, so let's take a look at how we fit these models in R. 
fitting a multilevel model in R is relatively straightforward, and can be achieved with various packages. The one we are going to use, and which is most common, is the lme4 package.   
we fit the model using the same formula notation, our outcome, the tilde, and our predictor variables. 
we then add the parameters for which we want to add random effects into parentheses at the end of the formula, with the vertical line separating the grouping variable. 


---
class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: LM to MLM </b><b>A Visual Explanation</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>


---
# Data

.pull-left[

> 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ). 

]
.pull-right[
```{r}
crq_data <- read_csv("https://uoepsy.github.io/data/crqdata.csv")
head(crq_data)
```
]

???
First let's introduce some data. We have 200 students from 20 schools, and their scores on an emotion dysregulation scale, and on the childs routines quesstionnire. 
you can see the two variables here, and the school identifier here.

---
count: false
# Data

.pull-left[

> 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ). 

```{r echo=TRUE, fig.show='hide'}
schoolplots <- 
  ggplot(crq_data, aes(x = crq, y = emot_dysreg, 
                  col = schoolid)) +
  geom_point()+
  facet_wrap(~schoolid) + 
  guides(col = "none") +
  labs(x = "Child Routines Questionnaire (CRQ)", 
       y = "Emotion Dysregulation Scale (EDS)")
```
]
.pull-right[

```{r fig.asp=.9}
schoolplots
```

]

???
we can see some panel plots here. each panel a school, each dot a child. 

---
# ICC

.pull-left[
```{r fig.asp=.6}
library(ggridges)
ggplot(crq_data, aes(x = emot_dysreg, y = schoolid, 
                fill = schoolid)) +
  geom_density_ridges(jittered_points = TRUE, 
                      position = "raincloud", alpha = .4,
                      quantile_lines=TRUE,
                      quantile_fun=function(x,...) mean(x)) +
  guides(fill=FALSE)
```
]
.pull-right[
```{r}
library(ICC)
ICCbare(schoolid, emot_dysreg, data = crq_data)
```

__Reminder:__ the Intraclass Correlation Coefficient is ratio of variance between clusters to the total variance (variance within + variance between).

]

???
And we can see the clustering of our outcome variable, the emotion dysregulation scale, by different schools. These are just the distributions of children from each schools' scores, and we have an icc of .244, so we can say that roughly 24% of the variance in our outcome is explained by the clustering of schools. 

---
# R: fitting lm

.pull-left[
```{r highlight.output=c(11,12)}
lm_mod <- lm(emot_dysreg ~ 1 + crq, data = crq_data)
summary(lm_mod)
```

]

???
now lets fit a straightforward simple linear model to our data. 
remember that this 1 is implicit in a linear model, and is just telling R that we want to model an intercept.

--

.pull-right[
```{r fig.asp=.8}
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1)
```

]

???
we can add the fitted values from our model to our plot.  
note that while we have a facet for each school, the line is the same in all these. 

---
# R: Adding a random intercept

.pull-left[
vary the intercept by schools.
```{r highlight.output=c(13,19)}
library(lme4)
ri_mod <- lmer(emot_dysreg ~ 1 + crq + 
                 (1 | schoolid), data = crq_data)
summary(ri_mod)
```

]

???
here we've fitted a random intercept model.  
the "1" which we talked about in the simple linear model, the intercept, we are now specifying as being modelled by a fixed part and some random by-school variation around this. 

--

.pull-right[
```{r fig.asp=.8}
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1) + 
  geom_line(aes(y=fitted(ri_mod)), col = "red", lwd=1)
```
] 

???
and as you might expect, this allows the intercept to vary between our schools. so the fitted values from this random intercept model are the red lines, and we can see that the height is different for each school. some are higher, some are lower, and the average is 4.43, in our fixed effects here.
note that the slope of the line is the same for all schools. 

---
# R: Adding a random slope

.pull-left[
vary the intercept and the effect (slope) of CRQ by schools
```{r highlight.output=c(13,14,20,21)}
rs_mod <- lmer(emot_dysreg ~ crq + 
                 (1 + crq | schoolid), data = crq_data)
summary(rs_mod)
```

]

???
and here we add in the random slope, so we are specifying that the effect of CRQ on emotion dysregulation is different for each school. The effect randomly varies between schools, around some fixed center. 


--

.pull-right[
```{r fig.asp=.8}
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1) + 
  geom_line(aes(y=fitted(ri_mod)), col = "red", lwd=1) + 
  geom_line(aes(y=fitted(rs_mod)), col = "orange", lwd=1)
```
]

???
the orange lines, which show the fitted values from the random slope model, vary in their slope for each school! 

---
# Partial Pooling vs No Pooling

.pull-left[
Why not fit a fixed effect adjustment to the slope of x for each group?  
`lm(y ~ x * group)`?

```{r}
fe_mod <- lm(emot_dysreg ~ crq * schoolid, data = crq_data)
```
]

.pull-right[
```{r fig.asp=.8}
schoolplots + 
  geom_line(aes(y=fitted(fe_mod)), col = "black", lwd=1)
```
]

???
There's an important aspect of multilevel models that we haven't yet touched upon. 
We saw in the previous lecture that the no-pooling approach, which is where we include a fixed effect of cluster, can fit cluster-specific effects. So here, the black lines show the fitted values for this simple linear model with an interaction between our predictor and our cluster. So we have cluster specific intercepts, and cluster specific effects.

---
# Partial Pooling vs No Pooling

.pull-left[
- We talked last week about how this results in a lot of output. With 20 schools, we get: intercept at reference school, adjustment for every other school, the effect of x at reference school, adjustment to effect of x for every other school. 
    ```{r}
    length(coef(fe_mod))
    ```
- information is not combined in anyway (data from school $i$ contributes to differences from reference school to school $i$, but nothing else. No overall estimates)

]

???
these are modelled as different intercept and slope adjustment parameters for each cluster. 

--

.pull-right[
```{r echo=FALSE}
m1<-lm(emot_dysreg~crq*schoolid, data = crq_data)
m2<-lmer(emot_dysreg~crq + (1 + crq | schoolid), data = crq_data)
crq_data %>% 
  mutate(
    lm_fit = fitted(m1),
    rs_fit = fitted(m2)
  ) %>%
  filter(schoolid %in% paste0("school",c(13,17,11,18))) %>%
  ggplot(., aes(x = crq)) + 
    geom_point(aes(y = emot_dysreg)) + 
    facet_wrap(~schoolid) +
    geom_line(aes(y = lm_fit, lty="fixed effects:\ny ~ x * g",col="fixed effects:\ny ~ x * g"), lwd=1) + 
    geom_line(aes(y = rs_fit, lty="random effects:\ny ~ x + (1 + x | g)", col="random effects:\ny ~ x + (1 + x | g)"), lwd=1) +
  scale_linetype_manual("model fitted values",values = c("fixed effects:\ny ~ x * g"=2,"random effects:\ny ~ x + (1 + x | g)"=1)) + 
  scale_color_manual("model fitted values",values = c("fixed effects:\ny ~ x * g"="black","random effects:\ny ~ x + (1 + x | g)"="orange"))+
  theme(legend.position="bottom")
```
]

???
multilevel models provide an alternative, where we have cluster specific effects, but 'borrow strength' from the population-average effects.   
The borrowing of strength is more apparent for the (what would be) more extreme clusters, as well as those that have fewer datapoints. What happens to these cluster estimates is that they are shrunk towards the population average. 


---
class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: LM to MLM </b><b>lme4 Output</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>

???
let's look now towards how we interpret the output from multilevel models in the lme4 package in R. 

---
# Understanding MLM output

.pull-left[
```{r echo=FALSE, highlight.output=c(20,21)}
my_data<-read_csv("lme4output.csv")
m=lmer(y ~ x + (1 + x | group), my_data)
summary(m, correlation=F)
```
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lmer2.png")
```
]

???
In the output of lmer object, as you'd expect we have a bit more information than we do from lm. 
first.. lets look at the fixed effect part.  
as in standard linear regression, we have coefficient estimates and the associated standard errors. We also have the t statistic, which is estimate divided by std. error.  

between them, here the intercept and the effect of x here provide us with the overall intercept and slope.


---
count: false
# Understanding MLM output

.pull-left[
```{r echo=FALSE, highlight.output=c(13,14)}
my_data<-read_csv("lme4output.csv")
m=lmer(y ~ x + (1 + x | group), my_data)
summary(m, correlation=F)
```
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lmer2a.png")
```
]

???
moving to our random part, we have the estimated variances of our random effects. so the cluster level variance of our intercept, and of our effect of x. 
Remember that our random effects are normally distributed with a mean of 0. so the random intercepts have a standard deviation of 1.15.
so this blue cluster has an intercept of 1.79 plus about 2  
and this yellow cluster has an intercept of 1.79 minus about 2
and the same applies to the effects of x


---
count: false
# Understanding MLM output

.pull-left[
```{r echo=FALSE, highlight.output=c(13,14,20,21)}
my_data<-read_csv("lme4output.csv")
m=lmer(y ~ x + (1 + x | group), my_data)
summary(m, correlation=F)
```
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lmer3.png")
```
]

???
The fixed and random parts together provide the cluster specific lines here.  
so we said that the blue one had a higher intercept, and it has a more negative slope, and we can see that here.  


---
count: false
# Understanding MLM output

.pull-left[
```{r echo=FALSE, highlight.output=c(15)}
my_data<-read_csv("lme4output.csv")
m=lmer(y ~ x + (1 + x | group), my_data)
summary(m, correlation=F)
```
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lmer4.png")
```
]

???
finally, we have the estimated standard deviation of our level 1 residuals. again, these are assumed to be normally distributed with a mean of 0.  
and these are the deviations from our cluster specific lines to our individual observations y


---
# Extracting MLM output

.pull-left[
```{r echo=FALSE}
my_data<-read_csv("lme4output.csv")
model=lmer(y ~ x + (1 + x | group), my_data)
summary(model, correlation=F)
```
]
.pull-right[
```{r}
fixef(model)
```
```{r eval=F}
ranef(model)
```
```{r echo=F}
head(ranef(model)$group %>% round(.,4), 5L) %>% rbind(.,"...") -> op
row.names(op)[6] <- "..."
op
```
```{r eval=F}
coef(model)
```
```{r echo=F}
head(coef(model)$group %>% round(.,4), 5L) %>% rbind(.,"...") -> op
row.names(op)[6] <- "..."
op
```

]

???
we can extract all these parts using the functions fixef, ranef and coef.  
fixef gives us our fixed effects, ranef our random effects.  
coef gives us our cluster specific coefficients, which are the fixed + ranef

---
# ICC in lmer

.pull-left[
```{r highlight.output=c(13,14)}
base_mod <- lmer(emot_dysreg ~ 1 + (1 | schoolid), data = crq_data) 
summary(base_mod)
```
]
.pull-right[
```{r}
0.0845 / (0.0845 + 0.2588)
```

Note: ICC is conditional on zero values of random-effects covariates.
In other words, it has computed the ICC based on a value of zero for the random slope variable(s), so any interpretation of the ICC is also based on a value of zero for the slope variable(s).

]

???
recall that ICC is the proportion of variance between groups to the total variance.  
we can get this from our lmer output!  
we're back to our childhood emotional dysregulation data here, and we can see that we have an estimated variance between schools, and the residual variance. between those, we can calculate the ratio of this to the total.  


Once we introduce random slopes/coefficients, things get more complicated. the ICC will be a function of the variable(s) for which random slopes are specified. 


---
# Explained Variance in MLM

.pull-left[
__ $R^2$ __  

- Recall $R^2$ is proportion of variance explained

- In MLM, multiple variance components (not just $\varepsilon$). Do random effects "explain" variance?  
    - "marginal $R^2$" = variance explained due to fixed effects
    - "conditional $R^2$" = variance explained due to fixed + random

```{r warning=F, message=F}
library(MuMIn)
mod1 <- lmer(emot_dysreg ~ 1 + crq + (1 | schoolid), data = crq_data)
r.squaredGLMM(mod1)
```


]

???
you will likely remember r squared from standard linear regression. in multilevel models, our r squared becomes a little more difficult - we have two metrics, the marginal (variance explained by fixed effects only), and the conditional (that explained by fixed and random)

--

.pull-right[
__Proportional Reduction in Variance (PRV)__  

- $PRV = \frac{\text{var}_{m0} - \text{var}_{m1}}{\text{var}_{m0}}$

- where $\text{var}_{m0}$ and $\text{var}_{m1}$ are variance components from models with and without a parameter.  

]

???
Another metric we have is the proportional reduction in variance. We can consider how the inclusion of a variable in a model reduces a variance component, for instance the residual variance. 

---
class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: LM to MLM </b><b>Estimation</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>



---
# Model Estimation

- For standard linear models, we can calculate the parameters using a *closed form solution*.


- Multilevel models are too complicated, we *estimate* all the parameters using an iterative procedure like Maximum Likelihood Estimation (MLE).

???
When it comes to how the parameters of our multilevel model are estimated, things can become more complicated. 
with simple linear regression models, there is one best set of values for our betas, which is the one that minimizes the sums of squared residuals, and these can be calculated algebraicially.  
but for multilevel models, which are considerably more complicated, we estimate the parameters using an iterative procedure like MLE

---
# Model Estimation: MLE

Aim: find the values for the unknown parameters that maximize the probability of obtaining the observed data.
How: This is done via finding values for the parameters that maximize the (log) likelihood function.

```{r echo=FALSE, out.height="450px"}
knitr::include_graphics("jk_img_sandbox/1stderiv.png")
```

???
you might remember this idea from logistic regression models at the end of DAPR2. 
the idea here is that the process step-by-step finds values for the parameters that have the greatest likelihood, given the data we observed - i.e, those values that maxmimise the probability of observing our data. 

Although we can think of max likelihood estimation in terms of a reaching the apex of a curve, this only really applies in the estimation of a single parameter

---
count:false
# Model Estimation: (log)Likelihood

- Data = multiple observations: $1, ..., n$ 

- From our axioms of probability, we can combine these *i.i.d* by multiplication to get our likelihood of our parameters given our entire sample

- Instead of taking the **product** of the individual likelihoods, we can take the **summation** of the log-likelihoods
    - This is considerably easier to do, and can be achieved because multiplication is addition on a log scale.

---
# Model Estimation: MLE

In multilevel models, our parameter space is more complex (e.g. both fixed effects and variance components).

```{r echo=FALSE, out.height="450px"}
knitr::include_graphics("jk_img_sandbox/multisurftb.png")
```

???
in complex models with many parameters, our optimisation algorithms are trying to find the high point of a hugely complex surface. 

---
# Model Estimation: Convergence

TODO 
get used to warnings
sometimes model is too complex to be supported by the data 
balancing act between simplifying our model while preserving attribution of variance to various sources  


---
# Model Estimation: ML vs REML

- Standard ML results in biased estimates of variance components.

???
in simple regression, it is rarely variance estimates that are of interest - rather, we are often focussed on the coefficients.  
but in multilevel models were are more often specifically interested in estimated variance of our random effects. 

--

- Restricted Maximum Likelihood (REML) is the default in `lmer()`.

???
an adaptation called REML can be used in which estimation of the random parts is done assuming the fixed parts to be known. 
sort of like estimating the fixed effects first and then the variance components. 

--

    - REML separates the estimation of fixed and random parts of the model, leading to less biased estimates of the variance components.  

???
this can lead to less biased estimates of our random effect variances
  
--

- **Use ML to compare models that differ in their fixed effects.**    

???
because in REML the variance components are included in the likelihood.


---
class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: LM to MLM </b><b>Example</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>

???
Okay, let's take a look at an example

---
# MLM Example

Researchers are interested in how cognition changes over time. 

.pull-left[
```{r}
cogtime <- read_csv("https://uoepsy.github.io/data/cogtimerpm.csv")
cogtime <- cogtime %>% 
  mutate(across(c(participant, sexFemale, alc), factor))
head(cogtime, 12L)
```

]

???
let's suppose that we have a dataset on scores on a cognitive test over time. 
we have data from some participants - here's our participant identifier, and each one has 10 visits, and a score at each visit.

--

.pull-right[
```{r fig.asp=.7}
ggplot(cogtime, aes(x=visit_n, y = cog, col=participant))+
  geom_line(alpha = 0.5)+
  guides(col=FALSE)+
  scale_x_continuous(breaks=1:10)
```
]

???
we can plot our data as a line for each participant, and see the overall pattern. 


---
# MLM Example

__determining our random effect structure__

- multiple data-points per participant: **1 | participant**

???
let's think about what our random effect structure might look like. we have several observations for each participant, and we might expect scores to vary between participants. so we'll fit a random intercept. 

--

- explanatory variable of interest (`visit_n`) varies *within* participants: **visit_n | participant**

???
our independent variable, visit_n, is also likely to vary between participants. some participants will decline slower, or faster, than others. 


--

- allow by-participant intercepts to correlate with by-participant slopes: **1 + visit_n | participant**  
(more on this in future weeks)

???
we'll also allow our random intercepts and slopes to correlate. we'll touch on this more in future weeks, but the default for lme4 is to allow random effects to covary. essentially, this means that we model a parameter to account for the extent that people's intercepts and slopes are correlated, for instance, people who start higher might decline less steeply,

--

__fitting the model__

```{r}
cogtime_model <- lmer(cog ~ visit_n + (1 + visit_n | participant), data = cogtime)
```

???
so here's our model 

---
# MLM Example

.pull-left[
__model output__

```{r}
summary(cogtime_model)
```
]
.pull-right[
__raw data__  

```{r fig.asp=.7}
ggplot(cogtime, aes(x=visit_n, y = cog, col=participant))+
  geom_path(alpha = 0.5)+
  guides(col=FALSE)+
  scale_x_continuous(breaks=1:10)
```
]

???
and if we examine our model output, we see that at baseline, the estimated average score is 68.6, and participants tend to vary around this, with a standard deviation of 3.17. 
for every visit, there is an estimated decline of 1.22 in our outcome, cognitive test scores.
again, participants vary around this estimate, some have less decline, some have more. 


---
# MLM Example: Plotting the model
#### **sjPlot::plot_model()**

.pull-left[
```{r eval=FALSE}
library(sjPlot)
plot_model(cogtime_model, type="pred")
```
]

???
as before, we can use sjPlot to plot our model predicted values

--
.pull-right[
```{r echo=FALSE, fig.asp=.7}
library(sjPlot)
plot_model(cogtime_model, type="pred")
```
]


---
# MLM Example: Plotting the model
#### **effects::effect()**

.pull-left[
```{r}
library(effects)
as.data.frame(effect("visit_n",cogtime_model))
```

```{r eval=FALSE}
as.data.frame(effect("visit_n",cogtime_model)) %>%
  ggplot(.,aes(x=visit_n, y=fit))+
  geom_line()+
  geom_ribbon(aes(ymin=lower,ymax=upper), alpha=.3)
```
]


.pull-right[
```{r echo=FALSE, fig.asp=.7}
as.data.frame(effect("visit_n",cogtime_model)) %>%
  ggplot(.,aes(x=visit_n, y=fit))+
  geom_line()+
  geom_ribbon(aes(ymin=lower,ymax=upper), alpha=.3)
```
]

???
if we want to extract the data and do this manually, we can use the effects package

---
# MLM Example: Plotting the model
#### **broom.mixed::augment()** for cluster-specific fits

.pull-left[
```{r}
library(broom.mixed)
augment(cogtime_model)
```
```{r eval=FALSE}
ggplot(augment(cogtime_model), 
       aes(x=visit_n, y=.fitted,
           col=participant))+
  geom_line() +
  guides(col=FALSE)
```
]

???
and if we want to extract the cluster-specific fitted values we can use a function like augment() from the broom.mixed package. or fitted() 

--

.pull-right[
```{r echo=F, fig.asp=.8}
library(broom.mixed)
ggplot(augment(cogtime_model), 
       aes(x=visit_n, y=.fitted,
           col=participant))+
  geom_line() +
  guides(col=FALSE)
```
]

???
so here you can see we've plotted the fitted values, specifying via colour the different lines for each participant 


---
# MLM Example: Tables

```{r}
library(sjPlot)
tab_model(cogtime_model)
```

???
finally, we can also easily create tables of our model output. 


---
# Summary

- We can extend our linear model equation to model certain parameters as random cluster-level adjustments around a fixed center.

- $\color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 \; + \; \beta_1 \cdot{} x_{i} } + \varepsilon_i$  
becomes  
$\color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij}$  
$\color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}}$

- We can express this as one equation if we prefer:
$\color{red}{y_{ij}} = \underbrace{(\gamma_{00} + \color{orange}{\zeta_{0i}})}_{\color{blue}{\beta_{0i}}} \cdot 1 +  \color{blue}{\beta_{1i} \cdot x_{ij}}  +  \varepsilon_{ij}$

- This allows us to model cluster-level variation around the intercept ("random intercept") and around slopes ("random slope"). 

- We can fit this using the **lme4** package in R

???
okay, let's take stock before we have a longer break. 
we've covered extending linear model equations to have equations specified at multiple levels, packaged in these parentheses if you prefer. 
what this allowed us to do, using the lme4 package, is model group or cluster level variation in our effects, and we talked briefly about how this has certain advantages such as the idea of more extreme clusters and more sparse clusters, being shrunk towards the average effect. 

we've also seen some of the questions that this allows us to answer. we used the children in schools example to discuss how we might ask whether school level predictors influence things that happen at the child level. 






---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: LM to MLM</h2>
<h2>Part 2: Inference in MLM</h2>


???
okay, let's talk about inference in multilevel models. 
this is a thorny issue, and there are a lot papers on the topic.
what we're aiming for is to have a broad overview of why it's a tricky issue, cover some of the options we have, and make a recommendation on which approach might be the most reliable to use.


---
# <p></p>

.pull-left[
you might have noticed...

```{r highlight.output=c(20,21)}
summary(cogtime_model)
```
]
.pull-right[
![](jk_img_sandbox/wotnop.png)
]

???
you will have probably noticed when we were discussing the output of an lmer() object, that we don't get any p-values. 
in lm, we're used to seeing an extra column here, with those little stars that people love to see. 


---
# Why no p-values?

**Extensive debate about how best to test parameters from MLMs.**  

???
there is a lot of debate on how we should be conducting inferential tests from multilevel models. 

--

In simple LM, we test the reduction in residual SS (sums of squares), which follows an $F$ distribution with a known $df$.
$$
\begin{align}
F \qquad = \qquad \frac{MS_{model}}{MS_{residual}} \qquad = \qquad \frac{SS_{model}/df_{model}}{SS_{residual}/df_{residual}} \\
\quad \\
df_{model} = k \\
df_{residual} = n-k-1 \\
\end{align}
$$
???
when we learned about standard linear model in DAPR2, tests were conducted on the reduction in sums of squared residuals, which follow an F distribution with k and n-k-1 degrees of freedom

--

The $t$-statistic for a coefficient in a simple regression model is the square root of $F$ ratio between models with and without that parameter. 

- Such $F$ will have 1 numerator degree of freedom (and $n-k-1$ denominator degrees of freedom).
- The analogous $t$-distribution has $n-k-1$ degrees of freedom

???
the t-statistics of the coefficients were tested against t distributions with n-k-1 degrees of freedom accordingly. 

---
# Why no p-values?

In MLM, the distribution of a test statistic when the null hypothesis is true is **unknown.**

???
in multilevel models, the distribution of our test statistics is often unknown. remember that we cannot solve our equations here and find the parameter estimates algebraically to minimise residual sums of squares, but instead we find the best estimates via max likelihood. 

--

Under very specific conditions (normally distributed outcome variable, perfectly balanced designs), we can use an $F$ distribution and correctly determine the denominator $df$.  

But for most situations:
  - unclear how to calculate denominator $df$
  - unclear whether the test statistics even follow an $F$ distribution
  
???
In all but the most controlled experimental designs, it is unclear how to calculate the denominator degrees of freedom in order to perform the relevant test. 
  

---
# Options for inference

1. df approximations  

2. Likelihood Ratio Tests  

3. Bootstrap  

???
There are three main approaches to drawing inferences from multilevel models. 


---
count:false
# Satterthwaite df approximation

.pull-left[
- There are some suggested approaches to approximating the denominator $df$. 
<br><br>
- Loading the package **lmerTest** will fit your models and print the summary with p-values approximated by the Satterthwaite method.
```{r eval=F}
library(lmerTest)
full_model <- lmer(cog ~  1 + visit_n + (1 + visit_n | participant), data = cogtime)
summary(full_model)
```
]
.pull-right[
```{r echo=F}
library(lmerTest)
full_model <- lmer(cog ~  1 + visit_n + (1 + visit_n | participant), data = cogtime)
summary(full_model)
```
]
```{r echo=F}
detach("package:lmerTest", unload=TRUE)
```


---
count:false
# Kenward-Rogers df approximations

- The **pbkrtest** package implements the slightly more reliable Kenward-Rogers method. 

```{r}
library(pbkrtest)
restricted_model <- lmer(cog ~ 1 + (1 + visit_n | participant), data = cogtime)
full_model <- lmer(cog ~ 1 + visit_n + (1 + visit_n | participant), data = cogtime)
KRmodcomp(full_model, restricted_model)
```


---
count:false
# Likelihood ratio tests

.pull-left[
We can also conduct a Likelihood Ratio Test (LRT). 

```{r}
anova(restricted_model, full_model, test = "LRT")
```

]

.pull-right[
- Compares the log-likelihood of two competing models.  
<br><br>
- what is the "likelihood"?  
    - a function that associates to a parameter the probability (or probability density) of observing the given sample data. 
<br><br>
- ratio of two likelihoods is asymptotically $\chi^2$-square distributed.
    - **this means for small samples it may be unreliable**
]


---
# Options for inference

1. <p style="opacity:.4"> df approximations - assumes $F$-distributed just with unknown $ddf$.<p> 

2. <p style="opacity:.4">Likelihood Ratio Tests - differences in logLik are only asymptotically $\chi^2$distributed.</p>  

3. Bootstrap  
  - Parametric Bootstrap  
  assumes that explanatory variables are fixed and that model specification and the distributions such as $\zeta_i \sim N(0,\sigma_{\zeta})$ and $\varepsilon_i \sim N(0,\sigma_{\varepsilon})$ are correct.
  
  - Case-based Bootstrap  
  minimal assumptions - we just need to ensure that we correctly specify the hierarchical dependency of data.  
  requires decision of at which levels to resample.  
  (discussed more next week)


???
two of these, we are not going to discuss here, but there are some extra slides in the slide deck for if you are interested in learning about them. 
What we are going to focus on here is bootstrapping multilevel models.  
There are various different ways we can bootstrap, which vary in the assumptions they make about our model and about the process that generates our data.  
Today we'll focus on parametric bootstrapping. This assumes that our model is correct, for instance, in that our residuals and our random effect terms are normally distributed. 

---
# Parametric Bootstrap

.pull-left[
The idea here is that in order to do inference on the effect of a (set of) predictor(s), you 

1. fit the reduced model (without the predictors) to the data. 
{{content}}
]

???
parametric bootstrapping is so called because it draws samples based on the estimated parameters of our model. 
the first thing we do is fit the reduced model, so the one without our parameters of interest. 

--

2. Do many times:  
  - simulate data from the reduced model
  - fit both the reduced and the full model to the simulated (null) data
  - compute some statistic(s), e.g. likelihood ratio.
{{content}}

???
we then simulate some data based on the reduced model, and fit both the reduced and the full model to the simulated data, and compute our test statistic. for instance, the likelihood ratio  
because each simulation is based on the reduced model, which doesn't have our parameters of interest included, this means the process that generates the simulated data doesn't specify an effect of these parameters (i.e., they will be centered at 0). 
so what we are doing is essentially constructing a load of estimates of our statistic based on lots of simulations of data where the parameter is 0, which is like constructing a null sampling distribution! i.e. assuming there to be no effect, what would our statistic of interest look like if we repeatedly sample. 

--

3. Compare the parameter estimates obtained from fitting models to the **data**, to the "null distribution" constructed in step 2. 

???
so we can compare the estimates obtained when we fit the model to the _DATA_, to this null sampling distribution that we have constructed

--

.pull-right[
Easy to do with `PBmodcomp()` in the **pbkrtest** package.
```{r}
library(pbkrtest)
PBmodcomp(full_model, restricted_model)
```
]

???
it all sounds like quite a lot, but there's a very convenient package we can use to do this for us and bootstrap the likelihood ratio test.

---
# Summary

- Lots of debate around how best to conduct statistical inferences based on multi-level models. 
  - denominator degrees of freedom can't be calculated, so traditional $F$ tests cannot be conducted

- Lots of other options (approximations for $df$, likelihood ratio tests, bootstrap)
  - The choice is yours, but we recommend bootstrapping (of which there are also many different approaches!)

???
In summary then, making statistical inferences from multilevel models is a difficult, and slightly contentious issue. 
There are various approaches that we can use, many of which have certain drawbacks, such as requiring perfectly balanced designs, or being less reliable with small sample sizes. 
ultimately, the decision is yours to make - you may find conducting standard likelihood ratio tests, using the anova function, are more convenient as they do not take much time to compute, but we would recommend a bootstrapping approach as being worth the extra little bit of time. 

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: LM to MLM</h2>
<h2><b style="opacity:0.4;">Part 2: Inference in MLM </b><b>Examples</b></h2>

???
let's look at some examples

---
# Data

```{r}
nursedf <- read_csv("https://uoepsy.github.io/data/nurse_stress.csv")
nursedf <- nursedf %>% 
  mutate(across(c(hospital, expcon, gender, wardtype, hospsize), factor))
head(nursedf)
```


_The files nurses.csv contains three-level simulated data from a hypothetical study on stress in hospitals. The data are from nurses working in wards nested within hospitals. It is a cluster-randomized experiment. In each of 25 hospitals, four wards are selected and randomly assigned to an experimental and a control condition. In the experimental condition, a training program is offered to all nurses to cope with job-related stress. After the program is completed, a sample of about 10 nurses from each ward is given a test that measures job-related stress. Additional variables are: nurse age (years), nurse experience (years), nurse gender (0 = male, 1 = female), type of ward (0 = general care, 1 = special care), and hospital size (0 = small, 1 = medium, 2 = large)._  
(From https://multilevel-analysis.sites.uu.nl/datasets/ )

???
our data come from nurses within 25 hospitals and we have information on their age, gender, years of experience, stress levels, and whether they took part in a training program to cope with jobrelated stress. 


---
# test of a single parameter

> After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much?

--

```{r}
mod1 <- lmer(Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital), data = nursedf)
summary(mod1)
```

???
we can see our model summary here.
after fitting many multilevel models, you will start to get into the habit of being able to spot when an effect is likely to be significant.
t stats > about 2 will tend to pattern with p < .05. NOTE this isn't a rule to follow or based decisions on. 

---
count:false
# test of a single parameter

> After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much?

__Likelihood Ratio Test:__
```{r}
mod0 <- lmer(Zstress ~ 1 + experien + age + gender + (1 | hospital), data = nursedf)
mod1 <- lmer(Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital), data = nursedf)
anova(mod0, mod1, test="Chisq")
```


---
count:false
# test of a single parameter

> After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much?

__Parametric Bootstrap__ 
```{r eval=F}
mod0 <- lmer(Zstress ~ 1 + experien + age + gender + (1 | hospital), data = nursedf)
mod1 <- lmer(Zstress ~ 1 + experien + age + gender +  expcon + (1 | hospital), data = nursedf)
PBmodcomp(mod1, mod0)
```
```{r echo=F}
pbs = PBmodcomp(mod1, mod0)
pbs 
```

???
here we fit a reduced model without the IV, and the full model.
the parametric bootstrap indicates an improvement in model fit for the full model, suggesting the attending the program influences nurses' stress levels

---
count:false
# test of a single parameter

> After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, **by how much?**

__Parametric Bootstrap Confidence Intervals__
```{r echo=T, eval=F}
mod1 <- lmer(Zstress ~ 1 + experien + age + gender +  expcon + (1 | hospital), data = nursedf)
confint(mod1, method="boot")
```
```{r echo=F, highlight.output=c(8)}
cis = confint(mod1, method="boot")
cis
```

???
we can also construct parametric bootstrap confidence intervals.  
this way we can get an estimate for the value of the fixed effect


---
count:false
# test of a single parameter

> After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much?

Attendance of training programs on job-related stress was found to predict stress levels of nurses in `r length(unique(nursedf$hospital))` hospitals, beyond individual nurses' years of experience, age and gender (Parametric Bootstrap Likelihood Ratio Test statistic = `r pbs$test[2,"stat"]`, p`r map_chr(pbs$test[2,"p.value"], ~ifelse(.<.001, "<.001", paste0("=",.)))`). Having attended the training program was associated with a decrease in `r fixef(mod1)["expcon1"]` (Bootstrap 95% CI [`r paste(round(cis[7,],2), collapse=", ")`] ) standard deviations on the measure of job-related stress. 

???
allowing us to discuss the effect size - how much it estimates the change in stress. 

---
count:false
# testing that several parameters are simultaneously zero

> Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience? 

__Likelihood Ratio Test__
```{r}
mod0 <- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf)
mod1 <- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf)
anova(mod0, mod1, test="Chisq")
```

---
count:false
# testing that several parameters are simultaneously zero

> Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience? 

__Kenward-Rogers $df$-approximation__
```{r}
mod0 <- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf)
mod1 <- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf)
KRmodcomp(mod1, mod0)
```

---
# testing that several parameters are simultaneously zero

> Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience? 

__Parametric Bootstrap__
```{r}
mod0 <- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf)
mod1 <- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf)
PBmodcomp(mod1, mod0)
```

???
If we are interested in the collective effects of various parameters, such as the influence of workplace-related factors, like wardtype and hospital size, then we can do the same but where the full model has a number of further predictors

---
# testing random effects 

__are you sure you want to?__

- Justify the random effect structure based on study design, theory, and practicalities more than tests of significance.

- If needed, the __RLRsim__ package can test a single random effect (e.g. `lm()` vs `lmer()`).


```{r}
library(RLRsim)
mod0 <- lm(stress ~ expcon + experien + age + gender + wardtype + hospsize, data = nursedf)
mod1 <- lmer(stress ~ expcon + experien + age + gender + wardtype + hospsize + 
               (1 | hospital), data = nursedf)
exactLRT(m = mod1, m0 = mod0)
```

???
we can also, should we wish to, test the benefit of including our random effect. We would rarely want to do this, because our random effects should be based on our theory and/or study design. But if we really need to - then we can use an exact likelihood ratio test. 

---
class: inverse, center, middle, animated, rotateInDownLeft

# End 

