---
title: "<b>Assumptions & Diagnostics<br>More random effects</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
params: 
    show_extra: true
    finalcompile: TRUE
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
#knitr::opts_chunk$set(eval=FALSE)
options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_tile_view()
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)
xaringanExtra::use_share_again()
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)
knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
themedapr3 = function(){
  theme_minimal() + 
    theme(text = element_text(size=20))
}
source("jk_source/jk_presfuncs.R")
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  # base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  code_font_size = "0.7rem",
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)
```

class: inverse, center, middle

<h2>Part 1: Assumptions</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Case Diagnostics in MLM</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Random Effect Structures</h2>

???
the first thing we're going to discuss today is the set of assumptions that underly drawing inferences from our multilevel models

---
# Assumptions in LM

.pull-left[
#### The general idea

- $\varepsilon_i \sim N(0,\sigma^2)$ iid
- "zero mean and constant variance"

```{r echo=FALSE, fig.asp=.7}
set.seed(20)
tibble(
  fitted_values = 1:1000,
  residuals = rnorm(1000,0,20)
) -> plotdat

bb = seq(0,975,10)
map_dbl(bb, ~filter(plotdat, between(fitted_values, ., .+10)) %>% summarise(s=sd(residuals)) %>% pull(s)) %>% 
cbind(fitted_values = bb + 5, sd = .) %>%
  cbind(., m = map_dbl(bb, ~filter(plotdat, between(fitted_values, ., .+10)) %>% summarise(m = mean(residuals)) %>% pull(m))) %>% as_tibble %>% mutate(
    u = m + sd*2,
    l = m - sd*2
  ) -> prib


ggplot(plotdat, aes(x=fitted_values, y=residuals))+
  geom_point(alpha=.3) + 
  geom_smooth(se=F)+
  geom_smooth(data=prib, inherit.aes = F,
              aes(x=fitted_values,y=u), alpha=.3, se=F, lty="dashed", col="darkorange")+
  geom_smooth(data=prib, inherit.aes = F,
              aes(x=fitted_values,y=l), alpha=.3, se=F, lty="dashed", col="darkorange")+
  themedapr3()+
  theme(axis.text = element_blank())
```

]

???
lets begin with simple linear regression. 
recall that the broad idea of a linear model is that the structural model part (our betas and explanatory variables) are our way of specifying systematic relationships. 
the residual, the epsilon bit, is, we hope, just leftover randomness. 
and so what we term our "assumptions" are primarily concerned with checking that our residuals look random (that we haven't missed out something systematic). so we hope that, across the fitted values of the model, the residuals have zero mean and constant variance. 


--

.pull-right[
#### Recipe book

+ **L**inearity
+ **I**ndependence
+ **N**ormality
+ **E**qual Variances

]

???
the more formulaic approach to checking assumptions of linear regression is to evaluate the linearity, indepndence, normality, and equal variance of error.s

---
# What's different in MLM?

- Not much is different!  

--

- General idea is unchanged: error is random  

<!-- Consequently, we want to check for homoscedasiticity of the error term as well as normality of the error term’s distribution -->

--

- We now have residuals at multiple levels!

???
nothing really changes when we move to the multilevel model.  
we're still concerned with residuals - we'll want to check for homoscedasiticity of the error term as well as normality of the error term’s distribution.  
the key difference is that we now have residuals at multiple levels! 



---
# Random effects as level 2 residuals

```{r echo=FALSE, fig.align="left", fig.asp=.5}
knitr::include_graphics("jk_img_sandbox/lmmwodot.png")
```

???
remember that the random effects are normally distributed with a mean of 0. 
the blue line here is the fixed effect, and the lines for each cluster are in green. 
the deviations for each cluster from the overall slope are in red. we have a deviation for the intercept, and a deviation for the slope. 
the model is estimating the variance of all these deviations, and assumes them to be normally distributed. 
just like a residual, but at a higher level. 
this fits with the idea that when specifying random effects, we are treating our level 2 groups as if they are a random sample from a larger population of these level 2 units. because we treat them as a random sample, we expect random deviations for each group. 

---
count:false
# Random effects as level 2 residuals

```{r echo=FALSE, fig.align="left", fig.asp=.5}
knitr::include_graphics("jk_img_sandbox/lmmwdot.png")
```

???
in additional to these level 2 residuals, we have the level 1 residuals. these are the random deviations of the lower level observations around each cluster specific line.  
so if we imagine we have randomly sampled some children from a random sample of schools, then we can think of each line as a school, and each school has a random deviation. some schools are higher on the outcome, some lower. some have steeper slopes, some less steep. 
then within each school, the randomly sampled children will deviate randomly around that schools specific line. some children will be higher than expected for that school, some lower etc. 


---
# Random effects as level 2 residuals

$\begin{align} & \text{for observation }j\text{ in group }i \\ \quad \\ & \text{Level 1:} \\ & \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ & \text{Level 2:} \\ & \color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}} \\ & \color{blue}{\beta_{1i}} = \gamma_{10} + \color{orange}{\zeta_{1i}} \\ \quad \\ \end{align}$

$\varepsilon, \, \color{orange}{\zeta_0}, \, \text{ and } \, \color{orange}{\zeta_1}$ are all assumed to be normally distributed with mean 0. 

???
We can see this also in our model equations. 
in this example, with a random intercept and random slope, we have the level 1 equation, with the random part (the epsilon), 
and then with the level 2 equations, we have the random part, the zeta. 
and because we are assuming these to be random, we are assuming them to be normally distributed with mean of 0

---
count:false
# Random effects as level 2 residuals
<!-- > 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ). Eight of the schools were taking part in an initiative to specifically teach emotion regulation as part of the curriculum. Data were also gathered regarding the average hours each child slept per night. -->
```{r echo=FALSE}
library(sjPlot)
library(lme4)
crq <- read_csv("https://uoepsy.github.io/data/crqdata.csv")
crq %>% mutate(
  y = emot_dysreg,
  x1 = crq,
  x2 = int, 
  x3 = age, 
  cluster = gsub("school","cluster",schoolid)
) %>% filter(!schoolid %in% c("school6","school15", "school17")) -> df
full_model<-lmer(y ~ x1 * x2 + x3 + (1 + x1 | cluster), data = df)
model <- full_model
pp <- plot_model(full_model, type = "diag")
```


.pull-left[

$\varepsilon$  
`resid(model)`  
mean zero, constant variance  
<br><br>
```{r echo=FALSE, fig.asp=.5, out.width="400px"}
pp[[1]]
```

]

???
recall that in a QQplot, we are plotting our observed data, in against the theoretical quantiles of the normal distribution. So if we have 100 residuals, then each one is plotted against the 1st, 2nd, 3rd, and so on percentile of the standard normal distribution.
so we have the level 1 residuals...

--

.pull-right[
$\color{orange}{\zeta}$  
`ranef(model)`  
mean zero, constant variance  

```{r echo=FALSE, fig.asp=.5, out.width="400px"}
pp[[2]]
```

]

???
and we have the level 2 residuals. our random effects.

---
# Assumption Plots: Residuals vs Fitted

```{r fig.asp=.7}
plot(model, type=c("p","smooth"))
```

???
so let's look at some different visualisations that we can use to assess the extent to which we believe our assumptions to hold.   
first off, we have the residuals vs fitted plot. so these are our  residuals (our level 1 residuals, that is) on the y axis, and the fitted values of the model on the x. across the fitted values of the model, we want our residuals to have a mean of zero.  
the smooth here provides an indication of this. 

---
# Assumption Plots: qqplots

```{r fig.asp=.7}
library(lattice)
qqmath(model, id=.05)
```

???
we've already seen qqplots.  
the lattice package has a nice functionality for labelling the extreme ends of the distribution.  


---
# Assumption Plots: Scale-Location

```{r fig.asp=.7}
plot(model, 
     form = sqrt(abs(resid(.))) ~ fitted(.),
     type = c("p","smooth"))
```

???
scale location or, spread location, is a similar plot to the residuals vs fitted. 
however, in this one  the y axis is the sqrt of the absolute value of the residuals. 
if you think a second about what this is doing, it is removing any negative sign, and squarerooting to normalise it. 
what this means is that the mean of the sqrt abs residuals will reflect changes in the spread/variance of the residuals. 


---
count:false
# Assumption Plots: Scale-Location

```{r fig.asp=.7}
plot(model, 
     form = sqrt(abs(resid(.))) ~ fitted(.) | cluster,
     type = c("p","smooth"))
```

???
We can can also group these plots by cluster, which makes most sense if we have lots of data within our clusters, and means we can evaluate the extent to which certain clusters might have non-constant variance

---
# Assumption Plots: Ranefs

.pull-left[
```{r fig.asp=.7}
qqmath(ranef(model))
```
]
.pull-right[
```{r eval=FALSE}
rans <- as.data.frame(ranef(model)$cluster)

ggplot(rans, aes(sample = `(Intercept)`)) + 
  stat_qq() + stat_qq_line() +
  labs(title="random intercept")

ggplot(rans, aes(sample = x1)) + 
  stat_qq() + stat_qq_line()
  labs(title="random slope")
```
```{r echo=FALSE, fig.asp=.4}
rans <- as.data.frame(ranef(model)$cluster)
ggplot(rans, aes(sample = `(Intercept)`)) + 
  stat_qq() + stat_qq_line() +
  labs(title="random intercept") + 
ggplot(rans, aes(sample = x1)) + 
  stat_qq() + stat_qq_line()+
  labs(title="random slope")
```

]

???
for our random effects, our random intercepts and random slopes, we can also assess their normality. 
to get the qqline, we have to actually extract the random effects, and then plot them manually. 

---
# for a quick check

if nothing else... 

```{r eval=FALSE}
sjPlot::plot_model(model, type = "diag")
```
```{r echo=FALSE, fig.asp=.8}
(pp[[1]] + pp[[2]]) / (pp[[3]] + pp[[4]])
```

???
there are various packages which can spit out a selection of figures. 
in general, we recommend constructing plots and assessing things in such a way that you are more in control of what you are plotting. this is because it forces us to think a little more, rather than adopting a point-and-click approach. 
if you DO just want to do a quick once-over, then the familiar plot_model function from the sjPlot package has a nice option to set type = "diag" (for diagnostics) to give you these figures.

---

class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: Assumptions </b><b>Troubleshooting</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Case Diagnostics in MLM</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Random Effect Structures</h2>

???
we're now going to take a look at how we might troubleshoot when our assumptions appear to be violated


---
# Some Data

.pull-left[

> 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ). Eleven of the schools were taking part in an initiative to specifically teach emotion regulation as part of the curriculum.  
  
  
>Adjusting for levels of daily routines, do children from schools partaking in the intervention present with lower levels of emotional dysregulation? 

]
.pull-right[
```{r echo=F, fig.asp=.7}
library(ggExtra)
library(patchwork)
crq <- read_csv("https://uoepsy.github.io/data/crqdata.csv")
p <- ggplot(crq[crq$int=="Control",], aes(x=crq, y=emot_dysreg, col=schoolid)) + 
  geom_point()+geom_smooth(method="lm",se=F)+
  guides(col="none")+themedapr3()+
  labs(title="Control")
p1 <- ggplot(crq[crq$int=="Treatment",], aes(x=crq, y=emot_dysreg, col=schoolid)) + 
  geom_point()+geom_smooth(method="lm",se=F)+
  guides(col="none")+themedapr3()+
  labs(title="Treatment")
gridExtra::grid.arrange(
ggMarginal(p, type="boxplot", margins="y"),
ggMarginal(p1, type="boxplot", margins="y"),
nrow=1
)
```
]

???
first things first, we need some data. 
we're going to build on the example we saw in a previous lecture here. 
we are measuring children's emotion dysregulation, and the level of routine in their lives.  
so on the y we have the emotion dysregulation, on the x we have the child routines questionnaire. and all the children are nested within schools. furthermore, some of these schools take part in a program which specifically teaches emotion regulation. so we each school is either control, or treatment 

---
# When things look wrong

```{r}
mymodel <- lmer(emot_dysreg ~ crq + int + (1 | schoolid), data = crq)
```
```{r echo=FALSE, fig.asp=.7}
pp <- sjPlot::plot_model(mymodel, type = "diag")
pp[[1]] + pp[[4]]
#(pp[[1]] + pp[[2]]) / (pp[[3]] + pp[[4]])
```

???
so we fit this model here, where emotion dysregulation is predicted by amount of routine, and by whether or not the child has received training on emotion regulation (and of course specifying that children are clustered in schools). 
and it doesn't look great. 

---
# When things look wrong
### __Model mis-specification?__

.pull-left[
```{r eval=F}
mymodel <- lmer(emot_dysreg ~ crq + int + (1 | schoolid), data = crq)
```

```{r echo=FALSE, fig.asp=.7}
model<-lmer(emot_dysreg ~ crq + int + (1 | schoolid), data = crq)
pp <- plot_model(model, type = "diag")
pp[[1]]
```
]
.pull-right[
```{r eval=F}
mymodel <- lmer(emot_dysreg ~ crq + age + int + (1 | schoolid), data = crq)
```

```{r echo=FALSE, fig.asp=.7}
model<-mymodel <- lmer(emot_dysreg ~ crq + age + int +   (1 | schoolid), data = crq)
pp <- plot_model(model, type = "diag")
pp[[1]]
```
]

???
now it may well be that we have omitted an important variable or relationship here. It could be we are missing an interaction, or it could even be that our model is missing something systematic which has a large effect on emotion dysregulation, in this case we are missing the child's age. 
so one of the first things i recommend thinking about when faced with a model that doesn't appear to meet assumptions, is "is my model sensible?"
by "sensible" here i am meaning that theory should play a part. don't simply include any and all variables that have a statistically significant relationship with your outcome variable - try to think about what makes theoretical sense as influencing your outcome. that's not to say that you should pointblank ignore a significant interaction because it doesn't make theoretical sense, simply that theory has a large part to play here. 

---
# When things look wrong
### __Transformations?__  

- Transforming your outcome variable may help to satisfy model assumptions

???
another thing you MIGHT want to do in order to build a model that meets assumptions, is to transform your outcome variable.

--

  - log(y)
  - 1/y
  - sqrt(y)
  - forecast::BoxCox(y)

???
there are various transformations but the more common ones you will see are these, which shift the distribution left/right to varying degrees.
  

---
count:false
# When things look wrong
### __Transformations?__  

```{r include=F}
set.seed(1166050148)
d = c()
for(i in 1:10){
  x=rnorm(rdunif(1,6,12), 0, 1)
  g=sample(0:1,1)
  xmat=cbind(1,x,g)
  lp = xmat %*% c(rnorm(1,10,2), rnorm(1,2,2), 1)
  d = rbind(d, cbind(xmat, lp, i))
}
df <- as_tibble(d[,2:5])
names(df) <- c("x1","g","y","cluster")
df$y <- df$y + sn::rsn(nrow(df),0,30,50)
df<-df[-c(37,47, 65, 68, 82 ), ]
```


- Transforming your outcome variable may help to satisfy model assumptions


.pull-left[
```{r eval=F}
lmer(y ~ x1 + g + (1 | cluster), df)
```
```{r echo=FALSE, fig.asp=.5}
model <- lmer(y ~ x1 + g + (1  | cluster), df)
pp <- plot_model(model, type = "diag")
ggplot(df, aes(x=y)) + geom_histogram() +
pp[[1]]
```

]
.pull-right[
```{r eval=F}
lmer(forecast::BoxCox(y,lambda="auto") ~ x1 + g + (1 | cluster), df)
```
```{r echo=FALSE, fig.asp=.5}
model <- lmer(forecast::BoxCox(y,lambda="auto") ~ x1 + g + (1 | cluster), df)
pp <- plot_model(model, type = "diag")
ggplot(df, aes(x=forecast::BoxCox(y,lambda="auto"))) + geom_histogram() +
pp[[1]]
```
]

???
so here we have a model of an untransformed y variable, and on the right we have applied a boxcox transform. 
remember that our assumptions are about our residuals, NOT our outcome variable, so a non-normal outcome variable does not mean your model residuals will be non-normal. 


---
count:false
# When things look wrong
### __Transformations?__  

- Transforming your outcome variable may help to satisfy model assumptions **but it comes at the expense of interpretability.**  

.pull-left[
```{r eval=F}
lmer(y ~ x1 + g + (1 | cluster), df)
```
```{r echo=FALSE}
model <- lmer(y ~ x1 + g + (1 | cluster), df)
fixef(model)
```

]
.pull-right[
```{r eval=F}
lmer(forecast::BoxCox(y,lambda="auto") ~ x1 + g + (1 | cluster), df)
```
```{r echo=FALSE}
model <- lmer(forecast::BoxCox(y,lambda="auto") ~ x1 + g + (1 | cluster), df)
fixef(model)
```
]

???
personally i tend to avoid transformations for the very fact that we are changing what we are modelling. 
so although we may get a better looking model, it is a model of something else. 
in this model i know that the coefficient for g is saying that an increase in 1 on g is associated with an increase in 10 units of y. 
but in the right hand model, we are saying that it is associated with an increase of .05 on y to the power of lambda, whatever lambda is selected as. 
so the main thing with transformations is that because you are modelling y on a different scale, you are changing the interpretation considerably. 

---
count:false
class: extra
exclude: `r params$show_extra`
# When things look wrong
### __robustlmm__

.pull-left[
```{r}
mymodel <- lmer(emot_dysreg ~ crq * int + age + (1 | schoolid), data = crq)
summary(mymodel)$coefficients
```
]
.pull-right[
```{r}
library(robustlmm)
mymodelr <- rlmer(emot_dysreg ~ crq * int + age + (1 | schoolid), data = crq)
summary(mymodelr)$coefficients
```
]


---
# When things look wrong

### __Bootstrap?__

basic idea: 

1. do many many times:  
    &ensp;a. take a sample (e.g. sample with replacement from your data, or simulated from your model parameters)  
    &ensp;b. fit the model to the sample  
2. then:  
    &ensp;a. based on all the models fitted in step 1, obtain a distribution of parameter estimate of interest.  
    &ensp;b. based on the bootstrap distribution from 2a, compute a confidence interval for estimate.  
    &ensp;c. celebrate  

???
another option, which you might remember from DAPR2, is the idea of bootstrapping. 
bootstrapping doesn't make distributional assumptions about our _data_, and so can lead to more accurate inferences for small sample sizes and when the data are not well behaved. 
it does this by means of simulating a *sampling distribution* for our statistic of interest.  
remember the sampling distribution of x = "probability of obtaining different values of x if we were to collecting a new sample and calculate x". bootstrapping actually acts this out, but instead of collecting an entirely new sample each time, it resamples with replacement from our original sample.


---
# Bootstrap: What do we (re)sample?

resample based on the estimated distributions of parameters?  
  - assumes explanatory variables are fixed, model specification and the distributions (e.g. $\zeta \sim N(0,\sigma_{\zeta})$ and $\varepsilon \sim N(0,\sigma_{\varepsilon})$) are correct.  


???
however, thigns get more complicated as there are options for what it actually is that we sample to get a bootstrap distribution. 
what we talked about in last week's lecture, the "parametric bootstrap", involves sampling from our models estimated parameters. so this assumes that our model specification is correct, and that our residuals and random effects are truly normal in the population. 

--

resample residuals
  - $y* = \hat{y} + \hat{\varepsilon}_{\textrm{sampled with replacement}}$
  - assumes explanatory variables are fixed, and model specification is correct. 
  
???
another option, which we're not going to cover, is resampling the residuals themselves. again, this keeps the explanatory variables fixed and constructs a sampling distributon around them.

--

resample cases
  - **minimal** assumptions - that we have correctly specified the hierarchical structure of data
  - **But** do we resample:
      - observations?
      - clusters?
      - both?
      
???
the last option which we're going to go into in a bit more depth is to resample the data itself. Now, this is actually a lot more like what we did for the standard linear model in DAPR2. 
we resample our data, fit a model, resample our data fit our model, and so on. building up a bootstrap distribution.
because our data changes each time, our explanatory variables are not fixed as they are in the parametric and residual bootstrapping. so this approach makes very minimal assumptions, mainly that we've have the correct grouping structure. 
however, with multiple levels of observation, it does bring up questions - do we resample schools but not the children within them? or resample the children but not the schools? or resample both?

---
# Bootstrap: Parametric

```{r}
reducedmodel <- lmer(emot_dysreg ~ crq + age + (1 | schoolid), data = crq)
mymodel <- lmer(emot_dysreg ~ crq + age + int + (1 | schoolid), data = crq)
```

- bootstrap LRT
    ```{r eval=F}
    library(pbkrtest)
    PBmodcomp(mymodel, reducedmodel)
    ```

- bootstrap CIs
    ```{r eval=F}
    confint(mymodel, method="boot")
    ```

???
we have already seen the parametric bootstrap last week, where we used the PBmodcomp function to perform a parametric bootstrap likelihood ratio test. we also saw the use of confint() to construct bootstrapped confidence intervals for our estimates. 


--

- __lmeresampler__ package bootstrap() function
    ```{r eval=F}
    library(lmeresampler)
    mymodelBS <- bootstrap(mymodel, .f = fixef, type = "parametric", B = 2000)
    confint(mymodelBS, type = "norm")
    ```

.footnote[At time of writing, there is a minor bug with the version of **lmeresampler** that you can download from CRAN, so we recommend installing directly from the package maintainer: `devtools::install_github("aloy/lmeresampler")`]

???
there is another package which we are going to introduce now, called lmeresampler which allows us a bit more flexibility in our bootstrapping. we can give it different functions . f, that tell R which bit of the model we would like to get bootstrap estimates for. 
note also the type = "parametric" here. we can also use this function to do case based bootstrapping. 

---
# Bootstrap: Cases

```{r}
mymodel <- lmer(emot_dysreg ~ crq + age + int + (1 | schoolid), data = crq)
```

.pull-left[
```{r eval=params$finalcompile}
# devtools::install_github("aloy/lmeresampler")
library(lmeresampler)
# resample only children, not schools
mymodelBScase <- bootstrap(mymodel, .f = fixef, 
                           type = "case", B = 2000, 
                           resample = c(FALSE, TRUE))
summary(mymodelBScase)
```

]
.pull-right[
```{r eval=params$finalcompile}
confint(mymodelBScase, type = "basic")
```
]

.footnote[<br>For a nice how-to guide on the **lmeresampler** package, see http://aloy.github.io/lmeresampler/articles/lmeresampler-vignette.html.  
For a discussion of different bootstrap methods for multilevel models, see Leeden R.., Meijer E., Busing F.M. (2008) Resampling Multilevel Models. In: Leeuw J.., Meijer E. (eds) Handbook of Multilevel Analysis. Springer, New York, NY. DOI: 10.1007/978-0-387-73186-5_11 ]

???
note that there are a few things here of specific interest. 
firstly, we have type = "case" to denote that we want to resample the cases themselves. B is set to 2000, so we're going to have 2000 resamples
now.. we're here saying the we want to resample the lower level units, but not the level 2 units. so recall our model is of children within schools. this means we have the same schools in each bootstrap resample, but within each school we are resampling children.  

So the question now is when should we resample which levels? This can be quite a tricky issue. let's suppose we have a study where we have 50 participants, each measured at 5 timepoints. in that context, it makes sense to resample our participants, but not the lower level units of the repeated measurements. 
so, individuals are resampled, and once an individual enters the bootstrap sample, all the measurements for that individual are included. 

for another example, if we have individuals grouped into different countries, it might make more sense to always have the same set of countries in each bootstrap sample, with the lower level units (the individuals) being resampled. 

Which is most appropriate depends mainly on two things: the degree of randomness of the sampling at both levels, and the (average) sample size at both levels. 

the lmeresampler package gives us back an object which we can then extract confidence intervals from. 

---
count:false
# Bootstrap: Cases

```{r}
mymodel <- lmer(emot_dysreg ~ crq + age + int + (1 | schoolid), data = crq)
```

.pull-left[
```{r eval=FALSE}
# devtools::install_github("aloy/lmeresampler")
library(lmeresampler)
# resample only children, not schools
mymodelBScase <- bootstrap(mymodel, .f = fixef, 
                           type = "case", B = 2000, 
                           resample = c(FALSE, TRUE))
summary(mymodelBScase)
```
```{r eval=params$finalcompile,echo=FALSE}
summary(mymodelBScase)
```

]
.pull-right[
```{r eval=params$finalcompile,fig.asp=.6}
plot(mymodelBScase,"intTreatment")
```
]

.footnote[<br>For a nice how-to guide on the **lmeresampler** package, see http://aloy.github.io/lmeresampler/articles/lmeresampler-vignette.html.  
For a discussion of different bootstrap methods for multilevel models, see Leeden R.., Meijer E., Busing F.M. (2008) Resampling Multilevel Models. In: Leeuw J.., Meijer E. (eds) Handbook of Multilevel Analysis. Springer, New York, NY. DOI: 10.1007/978-0-387-73186-5_11 ]

???
or that we can visualise. so this is the distributino of 2000 resampled statistics, where each resample has the same set of schools, but the children within them are resampled. 

---
# Summary

- Our assumptions for multi-level models are similar to that of a standard linear model in that we are concerned with the our residuals
  - in the multi-level case, we have residuals are multiple levels. 
  
- When assumptions appear violated, there are various courses of action to consider. 
  - primarily, we should think about whether our model makes theoretical sense
  
- Resampling methods (e.g. Bootstrapping) can be used to obtain confidence intervals and bias-corrected estimates of model parameters. 
  - There are various forms of the bootstrap, with varying assumptions. 

---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Assumptions</h2>
<h2>Part 2: Case Diagnostics in MLM</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Random Effect Structures</h2>

???
okay, we're going to now look at case diagnostics in multilevel models. 
assumptions and case diagnostics often go hand-in-hand as they involve an amount of criticism. 
what we are doing when thinking about model assumptions is considering the extent to which our inferences from our model hold in the broader population.
case diagnostics are concerned with whether certain observations, or groups of observations are exerting undue influence on our estimates


---
# Influence

Just like standard `lm()`, observations can have unduly high influence on our model through a combination of high leverage and outlyingness. 

```{r echo=FALSE, fig.asp=.5, fig.width=12, fig.align="center"}
set.seed(18)
tibble(
  x = rnorm(20),
  y = 2*x + rnorm(20,0,.3)
) -> df
loo = coef(lm(y~x,df))
df[21,]<-cbind(4,8)
ggplot(df,aes(x=x,y=y))+geom_point(alpha=.5)+
  theme_minimal()+
  geom_abline(intercept=loo[1],slope=loo[2], lty="dotted", lwd=1)+
  scale_y_continuous(limits=c(-3,8))+
  scale_x_continuous(limits=c(-2,4))+
  geom_point(x=4,y=8,size=2,col="red")+
  geom_smooth(method="lm",se=F) +
  labs(title="not outlying, high leverage") -> p1

df[21,]<-cbind(0,6)
ggplot(df,aes(x=x,y=y))+geom_point(alpha=.5)+
  theme_minimal()+
  geom_abline(intercept=loo[1],slope=loo[2], lty="dotted", lwd=1)+
  scale_y_continuous(NULL,limits=c(-3,8))+
  scale_x_continuous(limits=c(-2,4))+
  geom_point(x=0,y=6,size=2,col="red")+
  geom_smooth(method="lm",se=F) +
  labs(title="high outlier, low leverage") -> p2

df[21,]<-cbind(4,0)
ggplot(df,aes(x=x,y=y))+geom_point(alpha=.5)+
  theme_minimal()+
  geom_abline(intercept=loo[1],slope=loo[2], lty="dotted", lwd=1)+
  scale_y_continuous(NULL, limits=c(-3,8))+
  scale_x_continuous(limits=c(-2,4))+
  geom_point(x=4,y=0,size=2,col="red")+
  geom_smooth(method="lm",se=F) +
  labs(title="high outlier, high leverage") -> p3

p1 + p2 + p3

```


???
as we may remember from the normal linear model, we can have observations that may have an extreme residual, and that may have high leverage, and combined these can exert influence on our model. 
for instance, consider the addition of the red points in each of these figures. the dotted line is hte model without the red point - this is the same in each plot. the blue line is the model with the red point. 

---
# multiple levels...

- Both observations (level 1 units) __and__ clusters (level 2+ units) can be influential. 

???
when we have multiple levels, both the individual observations and higher level groups may exert influence on our model estimates

--

- several packages, but current recommendation is **HLMdiag:** http://aloy.github.io/HLMdiag/index.html 

???
we're going to use the HLMdiag package here to investigate


---
# Level 1 influential points

.pull-left[
```{r fig.asp=.5}
mymodel <- lmer(emot_dysreg ~ crq + age + 
                  int + (1 | schoolid), 
                data = crq)
qqmath(mymodel, id=0.05)
```
]

???
we can see our level 1 residuals here. 

--

.pull-right[
```{r}
library(HLMdiag)
infl1 <- hlm_influence(mymodel, level = 1)
names(infl1)
infl1
```
]

???
the HLMdiag package can provide various influence measures:


---
count:false
# Level 1 influential points

.pull-left[
```{r fig.asp=.5}
mymodel <- lmer(emot_dysreg ~ crq + age + 
                  int + (1 | schoolid), 
                data = crq)
qqmath(mymodel, id=0.05)
```
]
.pull-right[
```{r fig.asp=.7}
library(HLMdiag)
infl1 <- hlm_influence(mymodel, level = 1)
dotplot_diag(infl1$cooksd, cutoff = "internal")
```
]

???
such as cooks distance. and also provides some nice ways of plotting.
you can provide a cut-off yourself. 
BUT IT IS IMPORTANT TO REMEMBER to not simply blindly follow cut-offs, think carefully about outliers and influential points and whether you want to exclude them

---
# Level 2 influential clusters

```{r eval=F}
infl2 <- hlm_influence(mymodel, level = "schoolid")
dotplot_diag(infl2$cooksd, cutoff = "internal", index=infl2$schoolid)
```
```{r echo=FALSE, fig.asp=.7}
infl2 <- hlm_influence(mymodel, level = "schoolid")
dotplot_diag(infl2$cooksd, cutoff = "internal", index=infl2$schoolid) +
  scale_y_continuous(limits=c(0,.45))
```


???
we can also calculate cooks distance at higher levels. 
the measure of  Cook's distance here measures the change in the fixed effects estimates based on deletion of a subset of observations. e.g. deletion of school6

---
# What to do?

- In this context (children from schools), I would be inclined not to worry too much about the individual children who have high values on cook's distance, __if__ we plan on case-based bootstrap for our inferential tests (and plan on resampling the level 1 units - the children). 

???
in this specific case, we can think about how we plan on conducting our inferential tests. as we may plan on a case-based bootstrap, and resample the children - the lower level units, then we need not worry too much about the level one influence. 

--

- It's worth looking into school 6 a bit further. 

- `mdffits` is a measure of multivariate "difference in fixed effects"
    ```{r}
    infl2 %>% arrange(desc(mdffits))
    ```

???
but we might want to look into school 6 a bit more.


---
count:false
# What to do?

- In this context (children from schools), I would be inclined not to worry too much about the individual children who have high values on cook's distance, __if__ we plan on bootstrapping our inferential tests (and plan on resampling the level 1 units - the children). 

- It's worth looking into school 6 a bit further. 

- examine fixed effects upon deletion of schools 6
    ```{r}
    delete6 <- case_delete(mymodel, level = "schoolid", type = "fixef", delete = "school6")
    cbind(delete6$fixef.original, delete6$fixef.delete)
    ```

???
one easy approach is to simply compare the values of the fixed effects with and without that group. 
it's up to us to decide what difference is "big enough to be concerning"


---
# Sensitivity Analysis?

Would our conclusions change if we excluded these schools?  

```{r eval=params$finalcompile}
mymodelrm6 <- lmer(emot_dysreg ~ crq + age +
                  int + (1 | schoolid), 
                data = crq %>% 
                  filter(!schoolid %in% c("school6")))
mymodelrm6BS <- bootstrap(mymodelrm6, .f = fixef, 
                           type = "case", B = 2000, 
                           resample = c(FALSE, TRUE))
confint(mymodelrm6BS, type = "basic")
```

???
another approach might be to conduct our analysis without this group. That way we assess directly whether our conclusions about an effect would change if we were to exclude those observations. 

---
# Summary

- Influence can be exerted by individual observations and higher lever groups of observations  
  - e.g. by children and by schools, or by individual trials and by participants.   
  
- We can get measures of influence at different levels, and consider how estimates and conclusions might change when certain observations (or groups) are excluded

- Bootstrapping is relevant as whether we are resampling at the level of an influential group/observation is going to affect the extent to which our estimates are biased by that observation/group

---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 3

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Assumptions</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Case Diagnostics in MLM</h2>
<h2>Part 3: Random Effect Structures</h2>

???
Now that we've taken a bit of time to look at assumptions and diagnostic in multilevel models, we're going to move on to thinking about random effect structures in a bit more detail. 

---
# What have we seen so far?

- children within schools

- birds within gardens

- measurements within participants

- nurses within hospitals

- and probably some others...

???
when i talk about random effect structures, i mean the hierarchical structure we specify in our model. 
we've already seen a few instances of 2 levels.
In each of these cases we had levels of "nesting". and this is probably the easiest structure to get our heads around initially. 
so, take the exampel of surveying differnet children from various schools. 
consider a random child in our dataset. 
does she attend multiple schools, or just the one?

---
# Nested

- the level $j$ observations in a level $i$ group belong __only__ to that level $i$ group. 

```{r out.width="450px", echo=FALSE, fig.align="center"}
knitr::include_graphics("https://media.gettyimages.com/photos/albatross-chick-between-parents-feet-falkland-islands-picture-id642348358?s=2048x2048")
```

???
the idea of a nested structure here is that each observation belongs to only one higher up level. 
so each chick is in a nest, and they don't go into other nests.  
each nest is in a tree, and the nests don't move to other trees. 


---
count:false
# Nested

- the level $j$ observations in a level $i$ group belong __only__ to that level $i$ group.  

- __`(1 | school/class)`__ or __`(1 | school) + (1 | class:school)`__

```{r echo=FALSE, fig.align="center"}
knitr::include_graphics("jk_img_sandbox/structure_id.png")
```

???
we can have multiple levels of nesting. for instance, children in classes in schools. 
in this case, we need to specify the nesting in our model. 
so we use a forward slash, or a longer format which is more explicitly fitting a random intercept for each school, and a random intercept for each class within each school.

---
count:false
# Nested

- the level $j$ observations in a level $i$ group belong __only__ to that level $i$ group.  

- If labels are unique, __`(1 | school) + (1 | class)`__ is the same as __`(1 | school/class)`__  

```{r echo=FALSE, fig.align="center"}
knitr::include_graphics("jk_img_sandbox/structure_nested.png")
```

???
one thing to note is that if our grouping labels are unique. for instance, all the classes from school A are called "Class A"-something, then we can also write it like this. 
in this case we don't have to specify the nesting of classes within schools explicitly because the labels of the classes are unique to each school. 
rather than the previous slide where there was a "class 1" label in all 3 schools, but these were actually DIFFERENT classes. 


---
# Crossed

- "crossed" = not nested!

???
crossed random effects are simply anything which is not nested. 

--

- __`(1 | subject) + (1 | task)`__  

```{r echo=FALSE, fig.align="center", out.height="450px"}
knitr::include_graphics("jk_img_sandbox/structure_crossed.png")
```

???
a typical example is where each participant completes the same set of tasks, or sees the same set of stimuli. 
if you want to carry on the schools, children example, you could consider subjects maths, english, history etc are not nested within schools, but history is taught in every school, and each school teaches every subject. 


---
# Fixed or random

.pull-left[

| Criterion: | Repetition: <br> _If the experiment were repeated:_ | Desired inference: <br> _The conclusions refer to:_ |
|----------------|--------------------------------------------------|----------------------------------------------------|
| Fixed effects  | <center>Same levels would be used</center>     |    <center>The levels used </center>                                   |
| Random effects | <center>Different levels would be used</center>   | <center>A population from which the levels used<br> are just a (random) sample</center> |

]

.pull-right[

- If only small number of clusters, estimating variance components may be unstable.  

- Partialling out cluster-differences as fixed effects *may* be preferable. 

]

???
in discussing random effect structures, it's worth reiterating our distinction between fixed and random effects. sometimes it may be preferable to fit grouping as a fixed effect, if, for instance, we have a small number of clusters. 

---
# Maximal Structures

- "maximal" = the most complex random effect structure that you can fit to the data

???
one thing we need to introduce is the notion of a maximal random effect structure. this is typically something we can work out from the study design, and is essentially the most complete specification of the grouping structure. 

--

- requires sufficient variance at all levels (for both intercepts and slopes where relevant). Which is often not the case.  

???
fitting the maximal structure requires sufficient variance though, and may not always be possible. 

--

```{r warning=T}
maxmodel <- lmer(emot_dysreg ~ crq + age + int + (1 + crq + age | schoolid), data = crq)
```

--

another example: 16 items each occur in 4 different combinations: condition A vs B $\times$ type 1 vs 2.  
40 participants see all items in all conditions (64 trials each participant).

```{r echo=FALSE}
kelly<-read.csv("https://uoepsy.github.io/data/kelly2010_replication.csv")
kelly %>% filter(condition!="baseline") %>%
  mutate(
    condition = fct_recode(factor(modality), "A"="gesture","B"="speech"),
    type = fct_recode(factor(strength), "1"="strong","2"="weak"),
    outcome = rtime, 
    ppt = subject_nr
  ) -> kelly
```

```{r warning=T,message=T}
mmod <- lmer(outcome ~ condition * type + (1 + condition * type | ppt) + 
               (1 + condition * type | item), data = kelly)
```


???
in our school example where there is are just two levels, the maximal model involves fitting a random intercept by-schools, and random effects of all level-1 predictors. 
we can't fit a random effect of the intervention here, because each school is either a control or a treatment. there's no "effect of intervention in school X". 

if we consider another example, where we run an experiment in which we have repeated measures for participants, and each of these observations is on one of a set of items (the same set for each participant) seen in different conditions. then we have crossed random effects. 
the maximal model involves specifying random effects of our predictors, and their interaction, for both participants and items. 

what you might notice here, is that we're getting some warnings. and this because both our models are too complex to fit on our available data. 

---
# Model Convergence

- Don't report results from a model that doesn't converge. You will probably not be able to trust the estimates. 

???
recall last week we spoke about model estimation, being achieved by an iterative process like max likelihood, or restricted max likelihood. 
model convergence is when our process finds a suitable answer. if our model doesn't converge, that means we shouldn't really trust our estimates. 

--

- Try a different optimiser, adjust the max iterations, or the stopping tolerances

```{r echo=FALSE, fig.asp=.3}
knitr::include_graphics("jk_img_sandbox/tolerance.png")
```

???
the optimiser is the algorithm that tries to find the maximum likelihood estimates, and does so by wandering around (in a logical way) a multidimensional likelihood surface corresponding to different values of our estimates. optimisers converge when they stop searching because they think they've got a maximum. we can control the stopping tolerances, such as the minimum distance moved x, or likelihood gained y, or how much change in gradient. 
we can also try different optimisers, which may be better, but more time consuming. 


---
count:false
# Model Convergence

- Don't report results from a model that doesn't converge. You will probably not be able to trust the estimates. 

- Try a different optimiser, adjust the max iterations, or the stopping tolerances

<br><br>

- Remove random effects with least variance until model converges (see Barr et al., 2013)

- Use a criterion for model selection (e.g. AIC, BIC) to choose a random effect structure that is supported by the data (see Matsuchek et al., 2017)

???
there are various suggestions as to choosing an appropriate random effect strucutre. 
perhaps the most simple, and most used in psychology, is to start with the maximal and then remove parameters with the least variance until the model converges. 

--

- __No right answer__

???
however, important to note that there's no obvious right approach to this. the maximal model will often have lower power, and others suggest a model selection criteria is preferable. 

---
# correlations between random effects

```{r echo=FALSE}
VarCorr(mmod)
```

???
because we have been discussing random effect structures and maximal models, we need to talk about the correlations between random effects, which are also estimated in our model. 
our model is not just estimating the variances (and so standard deviations) of our random intercepts and random effects, but is actually estimating a variance-covariance matrix, which means we are estimating the correlation between different random effects. 


---
count:false
# correlations between random effects

.pull-left[
__perfect correlations__

```{r echo=FALSE}
data <- read_csv('https://uoepsy.github.io/data/MathsAchievement.csv')
data$clusterid <- as.factor(data$childid)
data$x1<-data$year
data$y <- data$math
df<-data
```
```{r}
m1 <- lmer(y ~ 1 + x1 + 
             (1 + x1 | clusterid), data = df)
VarCorr(m1)
```
```{r echo=FALSE, fig.asp=.5}
dotplot.ranef.mer(ranef(m1))
```
]

???
to understand these, we can take it to the extreme and consider a correlation of 1 between a random intercept and random effect. so these are perfectly correlated. 
what does that look like?

--

.pull-right[
```{r echo=FALSE}
set.seed(33)
tibble(
  g = 1:10,
  i = rnorm(10,1,1),
  x = i+rnorm(10,0,.1)
) %>% mutate(
  pre = list(1:10)
) %>% unnest() %>% mutate(
  y = i+x*pre
) %>%
  ggplot()+
  geom_line(aes(x=pre,y=y,group=g))+
  labs(x="x1",y=".fitted")+
  themedapr3()
```
]

???
well, the higher the intercept, the greater the slope. the model is saying that groups with higher intercepts have more positive effects of x1. 

obtaining a random effect correlation estimate of +1 or -1 means that out optimiser has hit a sort of "boundary".
correlations cannot exceed 1 or -1, and even if you don't get an explicit error or warning, these potentially indicate some problems with convergence. 
Why? because we don't expect true correlations to lie on the boundary. This often means that there are not enough data to estimate all the parameters reliably

---
count:false
# correlations between random effects

.pull-left[
__perfect correlations__

```{r echo=FALSE}
data <- read_csv('https://uoepsy.github.io/data/MathsAchievement.csv')
data$clusterid <- as.factor(data$childid)
data$x1<-data$year
data$y <- data$math
df<-data
```
```{r}
m1 <- lmer(y ~ 1 + x1 + 
             (1 + x1 | clusterid), data = df)
VarCorr(m1)
```
```{r echo=FALSE, fig.asp=.5}
dotplot.ranef.mer(ranef(m1))
```
]

.pull-right[
__zero correlations__

```{r}
zcpmodel <- lmer(y ~ 1 + x1 + 
                   (1 + x1 || clusterid), data = df)
VarCorr(zcpmodel)
```
```{r echo=FALSE, fig.asp=.5}
dotplot.ranef.mer(ranef(zcpmodel))
```
]

???
we can, if we choose, remove the estimation of the correlation in order to simplify our model. 
However, it is important to note that this is like fixing that correlation to be exactly zero. So we are putting a constraint on our model that the correlation between intercepts and slopes is 0. 

---
# correlations between random effects

When should we remove them?

???
so when does it make sense to fix the correlation to 0? well

--
 
__When it makes theoretical sense to do so.__

???
in certain designs, you might consider there to be no theoretical justification for there to be a correlation between random effects.  
however, you have to be careful and this should only be done if your predictor is on a ratio scale - so if it has a meaningful zero. 
this is for the slghtly complicated reason that the zero correlation model is sensitive to shifts in the predictor. so our estimates from a model with a zero correlation parameter will change slightly as we shift our predictor. 

---
# Summary

- random effect structures can get complicated quite quickly
    - we can have multiple levels of nesting
    - we can have crossed random effects 

- the maximal random effect structure is the most complex structure we can fit to the data. 
    - it often leads to problems with model convergence
    
- building MLMs is a balancing act between accounting for different sources of variance and attempting to fit a model that is too complex for our data.  

---
class: inverse, center, middle, animated, rotateInDownLeft

# End

