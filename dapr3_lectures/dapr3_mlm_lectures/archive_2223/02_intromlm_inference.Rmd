---
title: "<b>Multilevel<br>Models</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    self-contained: true
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)

options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")

xaringanExtra::use_share_again()
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)

knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
theme_set(
    theme_minimal() + 
    theme(text = element_text(size=20))
)
source("jk_source/jk_presfuncs.R")

library(xaringanthemer)
style_mono_accent(
  base_color = "#88B04B", # DAPR3 
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  code_font_size = "0.7rem",
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)

cogtime <- read_csv("https://uoepsy.github.io/data/cogtimerpm.csv")
cogtime <- cogtime %>% 
  mutate(across(c(participant, isLhanded), factor))
library(lme4)
cogtime_model <- lmer(cog ~ visit_n + (1 + visit_n | participant), data = cogtime)


```


---
class: inverse, center, middle, animated, rotateInDownLeft

# End of<br> Part 1

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: LM to MLM</h2>
<h2>Part 2: Inference in MLM</h2>


???
okay, let's talk about inference in multilevel models. 
this is a thorny issue, and there are a lot papers on the topic.
what we're aiming for is to have a broad overview of why it's a tricky issue, cover some of the options we have, and make a recommendation on which approach might be the most reliable to use.


---
# <p></p>

.pull-left[
you might have noticed...

```{r highlight.output=c(20,21)}
summary(cogtime_model)
```
]
.pull-right[
![](jk_img_sandbox/wotnop.png)
{{content}}

]

???
you will have probably noticed when we were discussing the output of an lmer() object, that we don't get any p-values. 
in lm, we're used to seeing an extra column here, with those little stars that people love to see. 

--

<br><br><br>
<center><b>Extensive debate about how best to test parameters from MLMs.</b></center>  

???
there is a lot of debate on how we should be conducting inferential tests from multilevel models. 


---
# p-values in `lm()`

In simple LM, we test the reduction in residual SS (sums of squares), which follows an $F$ distribution with a known $df$.
$$
\begin{align}
F \qquad = \qquad \frac{MS_{model}}{MS_{residual}} \qquad = \qquad \frac{SS_{model}/df_{model}}{SS_{residual}/df_{residual}} \\
\quad \\
df_{model} = k \\
df_{residual} = n-k-1 \\
\end{align}
$$
???
when we learned about standard linear model in DAPR2, tests were conducted on the reduction in sums of squared residuals, which follow an F distribution with k and n-k-1 degrees of freedom

--

The $t$-statistic for a coefficient in a simple regression model is the square root of $F$ ratio between models with and without that parameter. 

- Such $F$ will have 1 numerator degree of freedom (and $n-k-1$ denominator degrees of freedom).
- The analogous $t$-distribution has $n-k-1$ degrees of freedom

???
the t-statistics of the coefficients were tested against t distributions with n-k-1 degrees of freedom accordingly. 

---
# p-values in `lmer()`?  

- In certain very specific conditions, we can work out what the df1, df2. 

- Parameter estimates from these models are not based on minimising sums of squared residuals  

  - They are the ML/REML estimates

- This is good - it means they can handle unbalanced designs and complex random effect structures

- But: 
  - unclear how to calculate denominator $df$
  - unclear whether the test statistics even follow an $F$ distribution
  
???
In all but the most controlled experimental designs, it is unclear how to calculate the denominator degrees of freedom in order to perform the relevant test. 
  
In MLM, the distribution of a test statistic when the null hypothesis is true is **unknown.**

???
in multilevel models, the distribution of our test statistics is often unknown. remember that we cannot solve our equations here and find the parameter estimates algebraically to minimise residual sums of squares, but instead we find the best estimates via max likelihood. 

---
# Options for inference

<ol>
<li>approximating ddf</li>
  <ul>
  <li>satterthwaite</li>
  <li>kenward-rogers</li>
  <li>m-l-1</li>
  <li>...</li>
  </ul>
<li>likelihood based methods</li>
  <ul>
  <li>profile likelihood confidence interval</li>
  <li>likelihood ratio tests</li>
  </ul>
<li>bootstrap</li>
  <ul>
  <li>parametric</li>
    <ul>
    <li>confidence interval</li>
    <li>likelihood ratio test</li>
    </ul>
  <li>case-based</li>
    <ul>
    <li>confidence interval</li>
    </ul>
  </ul>
</ol>
  
<p>(there are others...)</p>  

---
# Options for inference

<ol>
<li>approximating ddf</li>
  <ul>
  <li style="opacity:.4">satterthwaite</li>
  <li>kenward-rogers</li>
  <li style="opacity:.4">m-l-1</li>
  <li style="opacity:.4">...</li>
  </ul>
<li>likelihood based methods</li>
  <ul>
  <li>profile likelihood confidence interval</li>
  <li>likelihood ratio tests</li>
  </ul>
<li>bootstrap</li>
  <ul>
  <li style="opacity:.4">parametric</li>
    <ul>
    <li style="opacity:.4">confidence interval: <code>confint(model, method = "boot")</code></li>
    <li style="opacity:.4">likelihood ratio test: <code>pbkrtest::PBmodcomp(full_model, reduced_model)</code></li>
    </ul>
  <li style="opacity:.4">case-based</li>
    <ul>
    <li style="opacity:.4">confidence interval</li>
    </ul>
  </ul>
</ol>
  
<p style="opacity:.4">(there are others...)</p>  
???
There are three main approaches to drawing inferences from multilevel models. 


---
# ml and reml again.


---
# kenward rogers

.pull-left[


]

.pull-right[



]

---
# Likelihood ratio tests

- Compares the log-likelihood of two competing models.  

- remember "likelihood" = a function that associates to a parameter the probability (or probability density) of observing the given sample data. 

- ratio of two likelihoods is **asymptotically** $\chi^2$-square distributed.

    - *this means for small samples (at any "levels") it may be unreliable*

```{r}
restricted_model <- lmer(cog ~ 1 + 
                           (1 + visit_n | participant), 
                         data = cogtime, REML = FALSE)
full_model <- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime, REML = FALSE)
anova(restricted_model, full_model)
```


---
# Bootstrap

- Parametric Bootstrap  
  assumes that explanatory variables are fixed and that model specification and the distributions such as $\zeta_i \sim N(0,\sigma_{\zeta})$ and $\varepsilon_i \sim N(0,\sigma_{\varepsilon})$ are correct.
  
- Case-based Bootstrap  
  minimal assumptions - we just need to ensure that we correctly specify the hierarchical dependency of data.  
  requires decision of at which levels to resample.  
  (discussed more next week)


???

What we are going to focus on here is bootstrapping multilevel models.  
There are various different ways we can bootstrap, which vary in the assumptions they make about our model and about the process that generates our data.  
Today we'll focus on parametric bootstrapping. This assumes that our model is correct, for instance, in that our residuals and our random effect terms are normally distributed. 

---
# Parametric Bootstrap

The basic premise is that we:  

1. fit model(s) to data

2. Do many times:

  - simulate data based on the parameters of the fitted model(s)
  - compute some statistic/estimates from the simulated data

3. Construct a distribution of possible values  

---
# Parametric Bootstrap

.pull-left[
## Confidence Intervals
```{r eval=FALSE}
full_model <- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime)
confint(full_model, method = "boot")
```
```
              2.5 %  97.5 %
.sig01       0.6561  5.4831
.sig02      -0.1509  1.0000
.sig03       0.6630  1.5906
.sigma       5.5275  6.8391
(Intercept) 66.3313 70.8488
visit_n     -1.7774 -0.6318
```
]
.pull-right[
## LRT
(a bootstrapped version of the likelihood ratio test):  
```{r eval=FALSE}
library(pbkrtest)
restricted_model <- lmer(cog ~ 1 + 
                           (1 + visit_n | participant), 
                         data = cogtime)
full_model <- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime)
PBmodcomp(full_model, restricted_model)
```
```
Bootstrap test; time: 79.81 sec; samples: 1000; extremes: 1;
Requested samples: 1000 Used samples: 979 Extremes: 1
large : cog ~ 1 + visit_n + (1 + visit_n | participant)
cog ~ 1 + (1 + visit_n | participant)
       stat df p.value    
LRT    13.1  1 0.00029 ***
PBtest 13.1    0.00204 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
]


---
# Summary

- Lots of debate around how best to conduct statistical inferences based on multi-level models. 

- Lots of options:

  - approximations for $df$: _easy to implement. generally reliable. debate as to whether null is actually F. distribution_  
      - fit `lmer()` with package **lmerTest**   
      - `KRmodcomp(mod2, mod1)` from package **pbkrtest**  
      
  - likelihood ratio tests: _very quick, but best avoided unless big $n$ at all levels_  
      - `anova(mod1, mod2)`   
      
  - parametric bootstrap: _time consuming, but probably most reliable option (although can be problematic with unstable models)_  
      - `PBmodcomp(mod2, mod1)` from package **pbkrtest**    
      - `confint(mod, method="boot")`   
      


???
In summary then, making statistical inferences from multilevel models is a difficult, and slightly contentious issue. 
There are various approaches that we can use, many of which have certain drawbacks, such as requiring perfectly balanced designs, or being less reliable with small sample sizes. 
ultimately, the decision is yours to make - you may find conducting standard likelihood ratio tests, using the anova function, are more convenient as they do not take much time to compute, but we would recommend a bootstrapping approach as being worth the extra little bit of time. 


---
class: inverse, center, middle, animated, rotateInDownLeft

# End

