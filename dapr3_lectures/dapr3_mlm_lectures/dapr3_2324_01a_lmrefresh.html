<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Linear Regression Refresh</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josiah King" />
    <script src="jk_libs/libs/header-attrs/header-attrs.js"></script>
    <script src="jk_libs/libs/clipboard/clipboard.min.js"></script>
    <link href="jk_libs/libs/shareon/shareon.min.css" rel="stylesheet" />
    <script src="jk_libs/libs/shareon/shareon.min.js"></script>
    <link href="jk_libs/libs/xaringanExtra-shareagain/shareagain.css" rel="stylesheet" />
    <script src="jk_libs/libs/xaringanExtra-shareagain/shareagain.js"></script>
    <link href="jk_libs/libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="jk_libs/libs/tile-view/tile-view.js"></script>
    <link href="jk_libs/libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="jk_libs/libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="jk_libs/libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="jk_libs/tweaks.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# <b>Linear Regression Refresh</b>
]
.subtitle[
## Data Analysis for Psychology in R 3
]
.author[
### Josiah King
]
.institute[
### Department of Psychology<br/>The University of Edinburgh
]

---





# QUICK ANNOUNCEMENTS

- This afternoon's lecture is in EICC Sidlaw Theatre (see Learn announcement)

- Please look at the "Hello and Welcome to DAPR3 (README FIRST)" page on Learn.
  - The course intro video covers _A LOT_ of logistics  
  
- Please update/install RStudio on your computers
  - see Learn page for details and instructions
  - Do this sooner rather than later.  


---
class: inverse, center, middle

&lt;h1 style="text-align: left;"&gt;This Lecture:&lt;/h1&gt;
&lt;h3 style="text-align: left;"&gt;1. Linear Models Refresher&lt;/h3&gt;


???

- WHAT IS THIS LECTURE?  
  - dapr2 &gt;&gt; dapr3
  - limited time, so high level overview
  - don't forget. piazza. OH. 
  - don't expect you to know this stuff inside-out worry.
  - what i'm hoping is that while some of it may be presented in a slightly different way, most of it will be vaguely recognisable from previous courses.
  - 


---

# Models

.pull-left[
__deterministic__  

given the same input, deterministic functions return *exactly* the same output

- `\(y = mx + c\)`  

- area of sphere = `\(4\pi r^2\)`  

- height of fall = `\(1/2 g t^2\)`
    - `\(g = \textrm{gravitational constant, }9.8m/s^2\)`
    - `\(t = \textrm{time (in seconds) of fall}\)`

]

???

- (DAPR2) MODELS
- DETERMINISTIC 

--

.pull-right[
__statistical__  

.br2.f4.white.bg-gray[
$$ 
\textrm{outcome} = (\textrm{model}) + \color{black}{\textrm{error}}
$$
]

- handspan = height + randomness  

- cognitive test score = age + premorbid IQ + ... + randomness

]

???
- STATISTICAL!! 
  - pattern + randomness
  - free will is in the error term


---
# The Linear Model

.br3.pa2.f2[
$$
`\begin{align}
\color{red}{\textrm{outcome}} &amp; = \color{blue}{(\textrm{model})} + \textrm{error} \\
\color{red}{y_i} &amp; = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_i} + \varepsilon_i \\
\text{where } \\
\varepsilon_i &amp; \sim N(0, \sigma) \text{ independently} \\
\end{align}`
$$
]

???
- LINEAR MODEL
- OUTCOME = linear combination of set of PREDICTORS X, 
  - plus some error

- DAPR2: EQUATION

---
# The Linear Model

.flex.items-top[
.w-50.pa2[
Our proposed model of the world:

`\(\color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_i} + \varepsilon_i\)`  
  
{{content}}
]
.w-50.pa2[
![](dapr3_2324_01a_lmrefresh_files/figure-html/bb-1.svg)&lt;!-- --&gt;
]]

???
- SIMPLEST FORM = ONE PREDICTOR
- STRAIGHT LINE


--

Our model _fitted_ to some data (note the `\(\widehat{\textrm{hats}}\)`):  

`\(\hat{y}_i = \color{blue}{\hat \beta_0 \cdot{} 1 + \hat \beta_1 \cdot{} x_i}\)`  

{{content}}

???
- MODEL = BLUE LINE
- MODEL PREDICTED: YHAT


--

For the `\(i^{th}\)` observation:
  - `\(\color{red}{y_i}\)` is the value we observe for `\(x_i\)`   
  - `\(\hat{y}_i\)` is the value the model _predicts_ for `\(x_i\)`   
  - `\(\color{red}{y_i} = \hat{y}_i + \hat\varepsilon_i\)`  
  
???
- VARIATION
  - RESIDUALS = deviations from observed to model predicted  

---
# An Example


.flex.items-top[
.w-50.pa2[

`\(\color{red}{y_i} = \color{blue}{5 \cdot{} 1 + 2 \cdot{} x_i} + \hat\varepsilon_i\)`  
  
__e.g.__   
for the observation `\(x_i = 1.2, \; y_i = 9.9\)`:  

$$
`\begin{align}
\color{red}{9.9} &amp; = \color{blue}{5 \cdot{}} 1 + \color{blue}{2 \cdot{}} 1.2 + \hat\varepsilon_i \\
&amp; = 7.4 + \hat\varepsilon_i \\
&amp; = 7.4 + 2.5 \\
\end{align}`
$$
]
.w-50.pa2[
![](dapr3_2324_01a_lmrefresh_files/figure-html/errplot-1.svg)&lt;!-- --&gt;
]]

???
- EXAMPLE
- model defined by two parameters.
- line defined by two numbers
  - INTERCEPT - 5
  - SLOPE - 2

- line hits the y axis at 5
- every 1 it increases in x, it increases by 2 in y

- RESIDUALS = STRAY CAUSES/RANDOMNESS




---
# Categorical Predictors

.pull-left[  
&lt;br&gt;&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; y &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; x &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 7.99 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4.73 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3.66 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3.41 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5.75 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5.66 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ... &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ... &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]
.pull-right[
![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-2-1.svg)&lt;!-- --&gt;
]

???
- LM extend logic
- predictors can be categorical. 
- default = entered as 0s and 1s.
- we can do clever things to change these, but default:
  - 0 = reference group
  - change of 1 = change from reference group to other group


---
# Multiple Predictors

.pull-left[
![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-3-1.svg)&lt;!-- --&gt;
]

???
- We can add more predictors!  
- line defined by 2 params
- surface defined by 3 params. 
- more predictors = more dimensions = not able to visualise  

--

.pull-right[
![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-4-1.svg)&lt;!-- --&gt;
]

???
- i've tried here:  
- another categorical = maybe two surfaces?  
- another continuous predictor? maybe an animation of the surface changing shape over time.  

---
# Multiple Predictors

.pull-left[
&lt;img src="jk_img_sandbox/SSblank1cov.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]
.pull-right[



```r
lm(y ~ x1)
```

```
## 
## Call:
## lm(formula = y ~ x1)
## 
## Coefficients:
## (Intercept)           x1  
##     -0.0509       0.2007
```

]

???
- Another way to think about models
- explaining variance in Y

- simple model, one predictor.
- DIAGRAM
  - BLUE = bit of Y explained by X1

---
# Multiple Predictors

.pull-left[
&lt;img src="jk_img_sandbox/SSblank1_uncor.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]
.pull-right[


```r
lm(y ~ x1 + x2u)
```

```
## 
## Call:
## lm(formula = y ~ x1 + x2u)
## 
## Coefficients:
## (Intercept)           x1          x2u  
##     -0.0446       0.2007       0.1304
```

]

???
- Add another predictor.
- If that predictor is completely uncorrelated with X1
  - it doesn't do anything to our coefficient of X1


---
# Multiple Predictors

.pull-left[
&lt;img src="jk_img_sandbox/SStype3.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]
.pull-right[

```r
lm(y ~ x1 + x2)
```

```
## 
## Call:
## lm(formula = y ~ x1 + x2)
## 
## Coefficients:
## (Intercept)           x1           x2  
##     -0.0644       0.0912       0.3302
```
{{content}}

]


???
- If it IS correlated, our coefficient for X1 changes!  
- WHAT DO THESE ASSOCIATIONS MEAN?  
- bit of y explained by X1 _AFTER_ controlling for X2  
  - that is unique to X1
  - we talk about "holding constant"
- [BACK TWO SLIDES]


--

**(Optional:)**  

```r
resx1.x2 = resid(lm(x1 ~ x2))
lm(y ~ resx1.x2)
```

```
## 
## Call:
## lm(formula = y ~ resx1.x2)
## 
## Coefficients:
## (Intercept)     resx1.x2  
##     -0.0697       0.0912
```

---
# Interactions

.pull-left[
![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-13-1.svg)&lt;!-- --&gt;
]

???
- We can add predictors
- We can also add interactions between predictors  
- effect of x1 _depends on_ x2  
- HERE: continuous * categorical
  - non parallel lines

--

.pull-right[
![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-14-1.svg)&lt;!-- --&gt;

]

???
- Continuous * continuous = twisted surface

- logic is the same. 
  - take close edge + far edge  
  - it's just that x2 can now take any value in a continuum


---
# Interactions


.pull-left[

![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-15-1.svg)&lt;!-- --&gt;
]
.pull-right[

![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-16-1.svg)&lt;!-- --&gt;
]
???
- WRITE EQUATION  
- Interaction term is an adjustment to the slope  


---
# Notation

`\(\begin{align} \color{red}{y} \;\;\;\; &amp; = \;\;\;\;\; \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_1 + ... + \beta_k \cdot x_k} &amp; + &amp; \;\;\;\varepsilon \\ \qquad \\ \color{red}{\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ \vdots \\ y_n \end{bmatrix}} &amp; = \color{blue}{\begin{bmatrix} 1 &amp; x_{11} &amp; x_{21} &amp; \dots &amp; x_{k1} \\ 1 &amp; x_{12} &amp; x_{22} &amp;  &amp; x_{k2} \\ 1 &amp; x_{13} &amp; x_{23} &amp;  &amp; x_{k3} \\ 1 &amp; x_{14} &amp; x_{24} &amp;  &amp; x_{k4} \\ 1 &amp; x_{15} &amp; x_{25} &amp;  &amp; x_{k5} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{1n} &amp; x_{2n} &amp; \dots &amp; x_{kn} \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix}} &amp; + &amp; \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \varepsilon_4 \\ \varepsilon_5 \\ \vdots \\ \varepsilon_n \end{bmatrix} \\ \qquad \\ \\\color{red}{\boldsymbol y} \;\;\;\;\; &amp; = \qquad \qquad \;\;\; \mathbf{\color{blue}{X \qquad \qquad \qquad \;\;\;\: \boldsymbol \beta}} &amp; + &amp; \;\;\; \boldsymbol \varepsilon \\ \end{align}\)`

???
- DON'T WORRY
- most of this is optional
- _hopefully_ one bit is familiar  
- just showing other ways of writing it  
- you don't need to know these  

- bottom is shorthand way of saying:  
  - y is modeled by _some_ variables and their respecitve coefficients
  - and some residuals  

---
# Link functions

`\(\begin{align} \color{red}{y} = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} &amp; \qquad  &amp; (-\infty, \infty) \\ \qquad \\ \qquad \\ \color{red}{ln \left( \frac{p}{1-p} \right) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} &amp; \qquad  &amp; [0,1] \\ \qquad \\ \qquad \\ \color{red}{ln (y) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} &amp; \qquad  &amp; (0, \infty) \\ \end{align}\)`  

???
- end of DAPR2: talked logistic  
- instead of modelling y directly
- we model some function of y
- e.g the "log odds of y" or the "log of y".  

- so we can generalise the linear model to allow us to study different outcomes. 
- fundamentally the model is the same idea - there are just extra bits/difficulties in interpreting the coefficients
- (no longer represent 'change in y')



---
# Linear Models in R

- Linear regression

```r
linear_model &lt;- lm(continuous_y ~ x1 + x2 + x3*x4, data = df)
```

- Logistic regression

```r
logistic_model &lt;- glm(binary_y ~ x1 + x2 + x3*x4, data = df, family=binomial(link="logit"))
```

- Poisson regression

```r
poisson_model &lt;- glm(count_y ~ x1 + x2 + x3*x4, data = df, family=poisson(link="log"))
```

???
- we saw fitting these in R  

---
# Null Hypothesis Testing

![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-20-1.svg)&lt;!-- --&gt;

???
- THROUGHOUT
- we are performing _TESTS_ on parts of our models  
- this is the best i can do for "NHST in a nutshell"  
- [EXPLAIN]

---
# Inference for Coefficients

&lt;img src="jk_img_sandbox/sum1.png" width="2560" height="500px" /&gt;

???
- In linear models, tests of coefficients were all in the summary  
- ESTIMATES


---
# Inference for Coefficients

&lt;img src="jk_img_sandbox/sum2.png" width="2560" height="500px" /&gt;

???
- we have STANDARD ERRORs for each coef
- so we can define values that we _would_ expect from a sample of this size if the true value in the population is zero  


---
# Inference for Coefficients

&lt;img src="jk_img_sandbox/sum3.png" width="2560" height="500px" /&gt;

???
- From this, we calculate a test statistic
- "standardised measure of how far from the null"  

---
# Inference for Coefficients

&lt;img src="jk_img_sandbox/sum4.png" width="2560" height="500px" /&gt;

???
- and we can then get a P VALUE
- how likely is it we see a test statistic as least as big as this.  


- we take a sample, and calculate an estimate
- samples will vary, so the estimate could be higher/lower than the true value in the population
- we're asking _if the true value in the population is zero_, what's the probability of observing a sample estimate as extreme as this one? that's the p-value


---
# Sums of Squares

Rather than specific coefficients, we can also think of our model as a whole.  
We can talk in terms of the sums of squared residuals  
`\(\sum^{n}_{i=1}(y_i - \hat y_i)^2\)`.  

.pull-left[
&lt;img src="jk_img_sandbox/SS3xmodel1.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]


.pull-right[

![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-26-1.svg)&lt;!-- --&gt;

]

???
- ADDITIONALLY. OVERALL MODEL
- VARIANCE EXPLAINED  

- we saw the idea of 
- TOTAL VARIANCE in Y
- VARIANCE in Y explained by MODEL
- VARIANCE in Y left UNexplained  


---
# `\(R^2\)`


.pull-left[
&lt;img src="jk_img_sandbox/SSr2.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]
.pull-right[
`\(R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}\)`


```r
mdl &lt;- lm(y ~ x1 + x2)
summary(mdl)$r.squared
```

```
## [1] 0.1935
```
]

???
- METRICS like R2

- EXPLAINED / TOTAL

- [DIAGRAM]


---
# Inference: Joint tests



We can test reduction in residual sums of squares:

.pull-left[

```r
m1 &lt;- lm(y ~ x1, data = df)
```
&lt;img src="jk_img_sandbox/SS3xmodel1.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]
.pull-right[

```r
m2 &lt;- lm(y ~ x1 + x2 + x3, data = df)
```
&lt;img src="jk_img_sandbox/SS3xfullmodel.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;

]

???
- WE CAN TEST
- MODEL COMPARISON

---
# Inference: Joint tests

i.e. isolating the improvement in model fit due to inclusion of additional parameters

.pull-left[

```r
m1 &lt;- lm(y ~ x1, data = df)
m2 &lt;- lm(y ~ x1 + x2 + x3, data = df)
anova(m1,m2)
```

```
## Analysis of Variance Table
## 
## Model 1: y ~ x1
## Model 2: y ~ x1 + x2 + x3
##   Res.Df  RSS Df Sum of Sq   F Pr(&gt;F)    
## 1     98 1141                            
## 2     96  357  2       785 106 &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

]
.pull-right[
&lt;img src="jk_img_sandbox/SS3xincrement.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]

???
- Allows testing more than a single coefficient
- E.G


---
# Inference: Joint tests

"additional parameters" could be a set of coefficients for levels of a categorical variable.  
This provides a way of assessing "are there differences in group means?".  

.pull-left[

```r
m1 = lm(y ~ x1, data = df)
m2 = lm(y ~ x1 + species, data = df)
coef(m2)
```

```
## (Intercept)          x1    speciesb    speciesc    speciesd 
##      13.841       1.059      -3.162      -2.943      -3.415
```

```r
anova(m1, m2)
```

```
## Analysis of Variance Table
## 
## Model 1: y ~ x1
## Model 2: y ~ x1 + species
##   Res.Df  RSS Df Sum of Sq    F  Pr(&gt;F)    
## 1     98 1141                              
## 2     95  950  3       192 6.39 0.00055 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

]
.pull-right[
&lt;img src="jk_img_sandbox/SSblankq.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]

???
- because CATEGORICAL predictors are inputted as a **set** of coefficients
- we can test this set as a whole
- "does the grouping make a difference?"


---
exclude: true
# Inference: Joint tests

This is kind of where traditional analysis of variance sits.  
Think of it as testing the addition of each variable entered in to the model,

.pull-left[

```r
m2 = lm(y ~ x1 + species, data = df)
anova(m2)
```

```
## Analysis of Variance Table
## 
## Response: y
##           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## x1         1     91    90.6    9.06 0.00334 ** 
## species    3    192    63.9    6.39 0.00055 ***
## Residuals 95    950    10.0                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

]
.pull-right[
&lt;img src="jk_img_sandbox/SStype1.png" width="400px" height="350px" style="display: block; margin: auto;" /&gt;
]

???
this is where "traditional ANOVA" sits. 
we have p categorical predictors with k levels and we assess the reduction in RSS due to inclusion grouping by those k levels


---
# Assumptions

Our model:  

`\(\color{red}{y} = \color{blue}{\mathbf{X \boldsymbol \beta}} + \varepsilon \qquad \text{where } \boldsymbol \varepsilon \sim N(0, \sigma) \text{ independently}\)`


Our ability to generalise from the model we fit on sample data to the wider population requires making some _assumptions._

???
- all inference relies on model meeting assumptions


--

- assumptions about the nature of the **model** .tr[
(linear)
]

--

- assumptions about the nature of the **errors** .tr[
(normal)
]


.footnote[
You can also phrase the linear model as: `\(\color{red}{\boldsymbol  y} \sim Normal(\color{blue}{\mathbf{X \boldsymbol \beta}}, \sigma)\)`
]



---
# Assumptions: The Broad Idea

All our work here is in
aim of making **models of the world**.  

- Models are models. They are simplifications. They are therefore wrong.  

.pull-left[]
.pull-right[
![](jk_img_sandbox/joeymap.jpg)
]


---
count:false
# Assumptions: The Broad Idea

All our work here is in
aim of making **models of the world**.  

- Models are models. They are simplifications. They are therefore wrong.  

- Our residuals ( `\(y - \hat{y}\)` ) reflect everything that we **don't** account for in our model.  


--

- In an ideal world, our model accounts for _all_ the systematic relationships. The leftovers (our residuals) are just random noise.  

--

  - If our model is mis-specified, or we don't measure some systematic relationship, then our residuals will reflect this.

--

- We check by examining how much "like randomness" the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d ("independent and identically distributed")
    - _these ideas tend to get referred to as our "assumptions"_

--

- We will **never** know whether our residuals contain only randomness - we can never observe everything! 


---
# Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  


]
.pull-right[

![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-41-1.svg)&lt;!-- --&gt;

]

---
# Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- __mean of the residuals = zero across the predicted values on the linear predictor.__    

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  


]
.pull-right[


![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-42-1.svg)&lt;!-- --&gt;

]

---
# Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- __spread of residuals is normally distributed and constant across the predicted values on the linear predictor.__  


]
.pull-right[

![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-43-1.svg)&lt;!-- --&gt;
]

---
# Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  



]
.pull-right[
__`plot(model)`__




```r
my_model &lt;- lm(y ~ x, data = df)
plot(my_model, which = 1)
```

![](dapr3_2324_01a_lmrefresh_files/figure-html/unnamed-chunk-45-1.svg)&lt;!-- --&gt;

]


---
# Assumptions: Recipe Book
&lt;br&gt;&lt;br&gt;
&lt;center&gt;
&lt;div class="acronym"&gt;
L
&lt;/div&gt; inearity&lt;br&gt;
&lt;div class="acronym"&gt;
I
&lt;/div&gt; ndependence&lt;br&gt;
&lt;div class="acronym"&gt;
N
&lt;/div&gt; ormality&lt;br&gt;
&lt;div class="acronym"&gt;
E
&lt;/div&gt; qual variance&lt;br&gt;
&lt;/center&gt;
.footnote["Line without N is a Lie!" (Umberto)]

---
# What if our model doesn't meet assumptions?

- is our model mis-specified?  
  - is the relationship non-linear? higher order terms? (e.g. `\(y \sim x + x^2\)`)
  - is there an omitted variable or interaction term? 
  
  
--

- transform the outcome variable?
  - makes things look more "normal"
  - but can make things more tricky to interpret:  
    `lm(y ~ x)` and `lm(log(y) ~ x)` are quite different models

--

- bootstrap\*
  - do many times: resample (w/ replacement) your data, and refit your model.
  - obtain a distribution of parameter estimate of interest. 
  - compute a confidence interval for estimate
  - celebrate

.footnote[\* not great with small samples.]

--

__looking ahead:__ these don't help if we have violated our assumption of independence...

---
# Summary

- we can fit a linear regression model which takes the form `\(\color{red}{y} = \color{blue}{\mathbf{X} \boldsymbol{\beta}} + \boldsymbol{\varepsilon}\)`  

- in R, we fit this with `lm(y ~ x1 + .... xk, data = mydata)`.  

- we can extend this to different link functions to model outcome variables which follow different distributions.  

- when drawing inferences from a fitted model to the broader population, we rely on certain assumptions.  

  - one of these is that the errors are independent.


---
class: inverse, center, middle, animated, rotateInDownLeft

# End



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="jk_libs/macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
