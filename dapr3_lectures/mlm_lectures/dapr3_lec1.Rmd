---
title: "<b>WEEK 1<br>Linear Models and Clustered Data</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: "AY 2021-2022"
output:
  xaringan::moon_reader:
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)
xaringanExtra::use_share_again()
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)
knitr::opts_chunk$set(
  dev = "png",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
themedapr3 = function(){
  theme_minimal() + 
    theme(text = element_text(size=20))
}
source("jk_source/jk_presfuncs.R")
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  # base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  code_font_size = "0.8rem",
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)
```

---
class: inverse, center, middle

# Part 1<br>Linear Regression Refresh

---

# deterministic vs statistical model


y = mx + c  
$y = \alpha + \beta x + \varepsilon$  

.br3.pa2.f3.white.bg-gray[
$$ \textrm{outcome} = (\textrm{model}) + \textrm{error} $$
]

---
# The Linear Model

.br3.pa2.f2[
$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error} \\
\color{red}{y_i} & = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_i} + \varepsilon_i \\
\text{where } \\
\varepsilon & \sim N(0, \sigma) \text{ independently} \\
\end{align}
$$
]

---
# Model structure

.flex.items-top[
.w-50.pa2[
Our proposed model of the world:

$\color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_i} + \varepsilon_i$  
  
Our model _fitted_ to some data (note the $\widehat{\textrm{hats}}$):  

$\hat{y}_i = \color{blue}{\hat \beta_0 \cdot{} 1 + \hat \beta_1 \cdot{} x_i}$  

{{content}}
]
.w-50.pa2[
```{r bb, echo=F, fig.asp=.6}
x <- tibble(x=c(-1,4))
f <- function(x) {5+2*x}
p1 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=f,size=1,colour="blue") +
  geom_segment(aes(x=0,xend=0,y=0,yend=f(0)),colour="blue", lty="dotted") +
  geom_segment(aes(x=0,xend=1,y=f(0),yend=f(0)),colour="blue",linetype="dotted") +
  geom_segment(aes(x=1,y=f(0),xend=1,yend=f(1)),colour="blue",linetype="dotted") +
  annotate("text",x=.5,y=2.5,label=expression(paste(beta[0], " (intercept)")),
           size=5,parse=TRUE,colour="blue") +
  annotate("text",x=1.4,y=6,label=expression(paste(beta[1], " (slope)")),
           size=5,parse=TRUE,colour="blue") +
    ggtitle(expression(paste(beta[0]," = 5, ",beta[1]," = 2")))+
  scale_y_continuous(breaks=0:13)+
  scale_x_continuous(limits = c(-0.3, 4), breaks=0:4)
p1 +
  ggtitle("")+
  scale_y_continuous("y",labels=NULL)+
  scale_x_continuous(limits=c(-0.3,4), breaks=c(0,1), labels=c("0","1"))+
  themedapr3()
  
```
]]

--

For the $i^{th}$ observation:
  - $\color{red}{y_i}$ is the value we observe for $x_i$   
  - $\hat{y}_i$ is the value the model _predicts_ for $x_i$   
  - $\color{red}{y_i} = \hat{y}_i + \varepsilon_i$  


---
# An Example



.flex.items-top[
.w-50.pa2[

$\color{red}{y_i} = \color{blue}{5 \cdot{} 1 + 2 \cdot{} x_i} + \varepsilon_i$  
  
{{content}}
]
.w-50.pa2[
```{r errplot,fig.asp=.6,echo=FALSE}
xX <-1.2
yY <- 9.9
p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  geom_point(aes(x=xX,y=yY),size=3,colour="red") +
  geom_segment(aes(x=xX,xend=xX,y=f(xX),yend=yY),linetype="dotted",colour="black") +
  annotate("text",.8,8.6,label=expression(paste(epsilon[i]," (residual)")),colour="black",size=5)+
  #scale_y_continuous(labels=NULL)+
  themedapr3()
```
]]

--

__e.g.__   
for the observation $x_i = 1.2, y_i = 9.9$:

$$
\begin{align}
\color{red}{9.9} & = \color{blue}{5 \cdot{}} 1 + \color{blue}{2 \cdot{}} 1.2 + \varepsilon_i \\
& = 7.4 + \varepsilon_i \\
& = 7.4 + 2.5 \\
\end{align}
$$

---
# Extending the linear model
## Categorical Predictors

.pull-left[
```{r echo=FALSE}
set.seed(993)
tibble(
  x = sample(c("Category0","Category1"), size = 30, replace = T),
  y = 5 + 2*(x == "Category1") + rnorm(30,0,1) %>% round(2)
) %>% select(y,x) -> df
df %>% sample_n(6) %>% rbind(., c("...","...")) %>% kable()
```
]
.pull-right[
```{r echo=FALSE, fig.asp=.8}

ggplot(df,aes(x=as.numeric(x=="Category1"),y=y))+
  geom_point()+
  stat_summary(geom="point")+
  stat_summary(geom="path", aes(group=1))+
  scale_x_continuous(name="isCategory1",breaks=c(0,1),
                     labels=c("0\n(FALSE)","1\n(TRUE)"))+
  geom_segment(x=0,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category0"]),
               lty="dashed",col="blue")+
  geom_segment(x=1,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category1"]),
               lty="dashed",col="blue")+
  annotate("text",x=0.9,y=6,label=expression(paste(beta[1], " (slope)")),
           size=5,parse=TRUE,colour="blue")+
  labs(title="y ~ x", subtitle="(x is categorical)")+
  themedapr3()
```
]



---

# Extending the linear model
## Multiple predictors

.pull-left[
```{r echo=FALSE, fig.asp=.8}
mwdata <- read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,main="y~x1+x2")
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

```
]

.pull-right[
```{r echo=FALSE, fig.asp=.8}
fit<-lm(wellbeing~outdoor_time+social_int+routine, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int, routine = "Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,70), main="y~x1+x2+x3\n(x3 is categorical)")
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int-2, routine = "No Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)
y <- wellbeing
par(new=TRUE)
persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,80), axes=F)


```
]

---

# Extending the linear model
## Interactions

.pull-left[
```{r echo=FALSE, fig.asp=.8}
scs_study <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
fit<-lm(dass ~ scs*zn, data = scs_study)
steps=20
scs <- with(scs_study, seq(min(scs),max(scs),length=steps))
zn <- with(scs_study, seq(min(zn),max(zn),length=steps))
newdat <- expand.grid(scs=scs, zn=zn)
dass <- matrix(predict(fit, newdat), steps, steps)
x1 <- scs
x2 <- zn
y <- dass
p <- persp(x2,x1,y, theta = -89,phi=10, col = NA, main = "y~x1+x2+x1:x2")
```
]
.pull-right[
```{r echo=FALSE, fig.asp=.8}
mwdata2<-read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv") %>% mutate(
  isRural = factor(ifelse(location=="rural","rural","notrural"))
)
par(mfrow=c(1,2))
fit<-lm(wellbeing~outdoor_time+isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2], col="red")

fit<-lm(wellbeing~outdoor_time*isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2+x1:x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2]+coef(fit)[4], col="red")
par(mfrow=c(1,1))
```
]



---
# Notation

$\begin{align} \color{red}{y} \;\;\;\; & = \;\;\;\;\; \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_1 + ... + \beta_k \cdot x_k} & + & \;\;\;\varepsilon \\ \qquad \\ \color{red}{\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ \vdots \\ y_n \end{bmatrix}} & = \color{blue}{\begin{bmatrix} 1 & x_{11} & x_{21} & \dots & x_{k1} \\ 1 & x_{12} & x_{22} &  & x_{k2} \\ 1 & x_{13} & x_{23} &  & x_{k3} \\ 1 & x_{14} & x_{24} &  & x_{k4} \\ 1 & x_{15} & x_{25} &  & x_{k5} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{1n} & x_{2n} & \dots & x_{kn} \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix}} & + & \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \varepsilon_4 \\ \varepsilon_5 \\ \vdots \\ \varepsilon_n \end{bmatrix} \\ \qquad \\ \\\color{red}{\boldsymbol y} \;\;\;\;\; & = \qquad \qquad \;\;\; \mathbf{\color{blue}{X \qquad \qquad \qquad \;\;\;\: \boldsymbol \beta}} & + & \;\;\; \boldsymbol \varepsilon \\ \end{align}$

---

# Extending the linear model
## Link functions

$$
\begin{align}
\color{red}{y} = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & (-\infty, \infty) \\
\color{red}{ln \left( \frac{p}{1-p} \right) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & [0,1] \\
\end{align}
$$  

---
# Linear Models in R

```{r eval=FALSE, echo=TRUE}
linear_model <- lm(y ~ x1 + x2 + x3*x4, data = df)
```

```{r eval=FALSE, echo=TRUE}
logistic_model <- glm(y ~ x1 + x2 + x3*x4, data = df, family=binomial(link="logit"))
```

---
# Inference for the linear model

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum1.png")
```

---
# Inference for the linear model

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum2.png")
```

---
# Inference for the linear model

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum3.png")
```

---
# Inference for the linear model

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum4.png")
```

---
# Assumptions

Our model:  

$\color{red}{y} = \color{blue}{\mathbf{X \boldsymbol \beta}} + \varepsilon \\ \text{where } \boldsymbol \varepsilon \sim N(0, \sigma) \text{ independently}$

--

Or put another way:  

$\color{red}{\boldsymbol  y} \sim Normal(\color{blue}{\mathbf{X \boldsymbol \beta}}, \sigma)$

--

Recall, our ability to generalise from our model fitted on sample data to the wider population requires making some _assumptions_

--

- assumptions about the nature of the **model** .tr[
(linear)
]

--

- assumptions about the nature of the **errors** .tr[
(normal)
]

---

# The Broader Idea


All our work here is in aim of making models of the world.

- Models are models. They are simplifications, and so wrong/imperfect.  
- Our residuals ( $y - \hat{y}$ ) reflect everything that we don't account for in our model
- In an ideal world, our model accounts for _all_ the systematic relationships. What is left over (our residuals) is just randomness. 
    - If our model is mis-specified, or misses out something systematic, then our residuals will reflect this.
- We check by examining how much "like randomness" the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d ("independent and identically distributed")
    - _this tends to get referred to as our "assumptions"_
- We will never know whether our residuals contain only randomness - we can never observe everything! 
---

# Checking Assumptions

.pull-left[

residuals should be "zero mean and constant variance".  
what does this look like? 

well, we want the mean of the residuals to be zero, and we want a close to normal distribution.
furthermore, we want that spread to be constant across the fitted values. 

]
.pull-right[

```{r echo=FALSE,fig.height=4}
library(tidyverse)
df<-tibble(x=rnorm(1000),y=2*x+rnorm(1000))
lm(y~x,df) %>% plot(which=1)
```

]


---

# Checking Assumptions
## (the "recipe book" way)

<div class="acronym">
L
</div> inearity<br>
<div class="acronym">
I
</div> ndependence<br>
<div class="acronym">
N
</div> ormality<br>
<div class="acronym">
E
</div> qual variance<br>

.footnote["Line without N is a Lie!" (Umberto)]



---

# What if our model doesn't meet assumptions?

- bootstrap!

what about independence? 

---
# Summary

- we can fit a linear regression model which takes the form $\color{red}{y} = \color{blue}{\mathbf{X} \boldsymbol{\beta}} + \boldsymbol{\varepsilon}$
- in R, we fit this with `lm(y ~ x1 + .... xk, data = mydata)`.
- we can extend this to different link functions to model outcome variables which follow different distributions.
- when drawing inferences from a fitted model to the broader population, we rely on certain assumptions.
- one of these is that the errors are independent.


---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

# Part 2<br>Clustered Data

---

# What is clustered data?

- children within schools
- patients within clinics
- observations within individuals

terms: "levels","hierarchical"

- children within classrooms within schools within districts etc...  

---
# Why is it relevant?  

- clustering will likely result in measurement on observational units within a given cluster being more similar to those of other clusters.  
  - e.g. our measured outcome for children in a given class will tend to be more similar to one another (because of class specific things such as the teacher) than to children in other classes

- clustering is expressed in terms of the correlation among the measurements within the same cluster

---
# ICC (intra-class correlation coefficient)

- various forms
- variance within / total variance  
- $\rho = \frac{\sigma^2_u}{\sigma^2_u + \sigma^2_\varepsilon}$


---
# Why is clustered data a problem for lm?

$$\varepsilon \sim N(0, \sigma) \textbf{ independently}$$ 

- clustering is something systematic that our model should (arguably) take into account. 
- "independence" assumption.  

---
# HOW is clustered data a problem for lm?

.pull-left[
#### Standard errors

We saw:
$$SE(\hat \beta_1) = \sqrt{\frac{ SS_{Residual}/(n-k-1)}{\sum(x_i - \bar{x})^2}}$$

suppose that $\rho = 1$. i.e., all the variation we see is due to the clustering. 

if we ignore clustering, $SS_{Residual}$ 
]
.pull-right[
#### Not always the effect of interest

e.g. simpsons pdox


]


---
# Various values of $\rho$

```{r echo=FALSE, fig.asp=.9}
iccgen <- function(j,n,e,icc,coef=0){
  v = (icc*e)/(1-icc)
  es = e/(v+e)
  v = if(is.infinite(v)){v=e}else{v/(v+e)}
  npj = n/j
  tibble(
    j = letters[1:j],
    zeta_j = rnorm(j,0,sqrt(v))
  ) %>%
    mutate(
      e_ij = map(j, ~rnorm(npj, 0, sqrt(es)))
    ) %>% unnest() %>%
    mutate(
      x = rnorm(n, 10, 5),
      y = 5 + coef*x + zeta_j + e_ij
    )
}
set.seed(3406)
sims = map_dfr(set_names(c(0,.5,.75,.95,.99,1)), 
        ~iccgen(j=10,n=100,e=1,icc=.,coef=0), .id="icc") %>%
  group_by(icc, j) %>%
  mutate(
    m = mean(y)
  ) %>% ungroup

ggplot(sims, aes(x=j, y=y))+
  geom_jitter(height=0, size=2,aes(col=j))+
  scale_y_continuous(NULL, labels=NULL)+
  stat_summary(geom="errorbar",aes(x=j,y=m,col=j),lwd=1)+
  facet_grid(icc~.)+
  guides(col=F)+
  themedapr3() +
  labs(x="cluster")+
  theme(strip.text.y = element_blank()) -> p1

ggplot(sims, aes(x=0, y=y))+
  see::geom_violinhalf(aes(x=.5))+
  geom_jitter(height=0,width=.5, size=2,aes(col=j), alpha=.5)+
  scale_y_continuous(NULL, labels=NULL)+
  scale_x_continuous(NULL, labels=NULL)+
  #stat_summary(geom="errorbar",aes(x=j,y=m,col=j),lwd=1)+
  facet_grid(icc~.)+
  guides(col=F)+
  themedapr3() -> p2

p1 + p2 + plot_layout(widths=c(8,1))
```

---

# Various values of $\rho$

```{r echo=FALSE, fig.asp=.9}
set.seed(875)
sims = map_dfr(set_names(c(0,.5,.75,.95,.99,1)), 
        ~iccgen(j=10,n=100,e=1,icc=.,coef=.1), .id="icc")

ggplot(sims, aes(x=x, y=y))+
  geom_point(aes(col=j))+
  geom_line(aes(col=j))+
  facet_grid(icc~.)+
  themedapr3()+
  guides(col=F)+
  scale_y_continuous(NULL, labels=NULL)+
  scale_x_continuous(NULL, labels=NULL)+
  labs(x = "X")
```

---

# Summary

- clustering can take many forms, and exist at many levels
- clustering is something systematic that we would want our model to take into account
- typically assessed using intra-class correlation coefficient (ICC) - the ratio of variance within clusters to the total variance $\rho = \frac{\sigma^2_u}{\sigma^2_u + \sigma^2_\varepsilon}$


---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 2

---
class: inverse, center, middle

# Part 3
## Possible solutions

```{r include=FALSE}
set.seed(nchar("worms")^6)

tibble(
  gardenid = paste0("garden",1:12),
  gardenmeanx = rnorm(12, 6, .7),
  coeffbirdx = rnorm(12, 3, 2.5),
  birdx = map(gardenmeanx, ~rnorm(10, .,1)),
  birdt = map(gardenmeanx, ~sample(c("wren","blackbird"), 
                                   size = 10, replace = TRUE, 
                                   prob = c(.x/(max(.x)*2.5), 1-(.x/(max(.x)*2.5)))))
) %>%
  unnest(cols = c(birdx, birdt)) %>% # punny! 
  mutate(
    nworms = round(-40 - coeffbirdx*birdx + 15*gardenmeanx -1*(birdt=="blackbird") + .5*(birdt=="blackbird")*birdx + rnorm(n(),0,1)),
    nworms = pmax(0,nworms),
    arrival_time = round(birdx,2)
  ) %>% select(gardenid, arrival_time, nworms, birdt) -> df

df <- df %>% mutate(x = arrival_time, y = nworms, x2 = birdt, birdid=1:n(), cluster_var=gsub("garden","cluster",gardenid))

df <- df %>% filter(!(birdid %in% c(31,33,34,35,37)))
df$y[df$birdid==40]<-37
df$nworms[df$birdid==40]<-37
df$y[df$birdid==36]<-19
df$nworms[df$birdid==36]<-19
worms_data <- df %>% select(gardenid, birdid, arrival_time, nworms, birdt) %>% rename(arrivalt = arrival_time)
```

---
# Some toy data

.pull-left[

> Sample of 115 birds from 12 gardens, information captured on the arrival time (hours past midnight) and number of worms caught by the end of the day. 

```{r}
# worms_data <- read_csv(".......")
head(worms_data)
```

]

.pull-right[

```{r fig.align="center", fig.asp=.7}
ggplot(worms_data, aes(x=arrivalt,y=nworms))+
  geom_point(size=4)+
  geom_smooth(method="lm")+
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()
```

]

---
count:false
# Some toy data: plotting each group

.pull-left[
```{r eval=FALSE}
ggplot(worms_data, aes(x=arrivalt,y=nworms, col=gardenid))+
  geom_point(size=4)+
  geom_smooth(method="lm", se=FALSE)+
  facet_wrap(~gardenid)+
  guides(col=FALSE)+
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()
```

]
.pull-right[
```{r echo=FALSE, fig.align="center", fig.asp=.8}
ggplot(worms_data, aes(x=arrivalt,y=nworms, col=gardenid))+
  geom_point(size=4)+
  geom_smooth(method="lm", se=FALSE)+
  facet_wrap(~gardenid)+
  guides(col=FALSE)+
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()
```
]


---
# 1. Ignore it

.pull-left[
```{r echo=FALSE}
model <- lm(y ~ x, data = df)
df %>% mutate(
  f = fitted(model)
) %>%
ggplot(.,aes(x=x,y=y))+geom_point(size=4)+
  geom_smooth(method="lm")+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.2) + 
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()
```
]
.pull-right[
__(Complete pooling)__  

+ `lm(y ~ 1 + x, data = df)`
+ Information from all clusters is pooled together to estimate over x  

```{r}
model <- lm(nworms ~ arrivalt, data = worms_data)
```
```{r echo=FALSE, out.width="300px"}
.pp(summary(model),l=list(c(10:12)))
```

]

---

# 1. Ignore it 

.pull-left[
```{r echo=FALSE}
library(ggfx)
library(ggforce)
df %>% mutate(
  f = fitted(model)
) -> pdat 

ggplot(pdat,aes(x=x,y=y))+
  with_blur(geom_point(aes(col=cluster_var),size=4,alpha=.2), sigma = unit(0.7, 'mm')) + 
  geom_point(data = filter(pdat,cluster_var %in% c("cluster9","cluster4")),aes(col=cluster_var),size=4) + 
  
  geom_mark_ellipse(aes(label = gardenid, filter = cluster_var == "cluster4"),
                    con.colour  = "#526A83", con.cap = 0, 
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) + 
  geom_mark_ellipse(aes(label = gardenid, filter = cluster_var == "cluster9"),
                    con.colour  = "#526A83", con.cap = 0, 
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) + 
  
  
  guides(col=FALSE, alpha=FALSE)+
  geom_smooth(method="lm",se=F)+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.2) + 
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()
```
]
.pull-right[

__(Complete pooling)__  

+ `lm(y ~ 1 + x, data = df)`
+ Information from all clusters is pooled together to estimate over x  

```{r}
model <- lm(nworms ~ arrivalt, data = worms_data)
```
```{r echo=FALSE, out.width="300px"}
.pp(summary(model),l=list(c(10:12)))
```

But different clusters show different patterns. 
Residuals are not independent.  
]

---
# 2. Lesser used solutions  

<!-- https://rlbarter.github.io/Practical-Statistics/2017/05/10/generalized-estimating-equations-gee/ -->

Don't include clustering as part of the model directly, but incorporate the dependency into our residuals term. 

$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error}^* \\
\end{align}
$$

.footnote[Where errors are correlated with clusters/with the error previously/etc.]


- independence (observations over time are independent)
- exchangeable (all observations over time have the same correlation)
- AR(1) (correlation decreases as a power of how many timepoints apart two observations are)
- unstructured (correlation between all timepoints may be different)


---

# 2. Lesser used solutions  

.pull-left[
__Cluster Robust Standard Errors__

```{r}
library(plm)
clm <- plm(nworms ~ 1 + arrivalt, data=worms_data, 
           model="pooling", index="gardenid")
```
```{r echo=FALSE}
.pp(summary(clm),l=list(c(13:16)))
```
```{r}
sqrt(diag(vcovHC(clm, 
                 method='arellano', 
                 cluster='group')))
```
]
.pull-right[
__Generalised Estimating Equations (GEE)__  

```{r}
library(geepack)
# needs to be arranged by cluster, and for cluster to be numeric
worms_data <- 
  worms_data %>% arrange(gardenid) %>%
  mutate(
    cluster_id = as.numeric(as.factor(gardenid))
  )

geemod  = geeglm(nworms ~ 1 + arrivalt, 
                 data = worms_data, 
                 corstr = 'independence', 
                 id = cluster_id)
```
```{r echo=FALSE}
.pp(summary(geemod),l=list(c(5:8)))
```

]

---
# 3. Fixed effects

.pull-left[

__(No pooling)__  

- `lm(y ~ x * cluster, data = df)`
- Information from a cluster contributes to estimate *for that cluster*, but information is not pooled to estimate an overall effect. 

```{r}
model <- lm(nworms ~ 1 + arrivalt * gardenid, 
            data = worms_data)
```

]
.pull-right[
```{r echo=FALSE}
model1 <- lm(y~x*cluster_var,df)
df %>% mutate(
  f = fitted(model1)
) %>%
ggplot(.,aes(x=x,y=y,col=cluster_var))+
  geom_point()+
  geom_line(aes(y=f,group=cluster_var))+
  guides(col=FALSE)+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.2) + 
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()
```
]

---

# 3. Fixed effects 

+ Lots of estimates (separate for each cluster). 
+ Variance estimates constructed based on information *only* within each cluster. 
+ No overall estimate of effect over x. 

```{r echo=FALSE}
.pp(summary(model),l=list(c(10:34)))
```

---
# 4. Random effects (LMM)

.pull-left[
__(Partial Pooling)__

- `lmer(y ~ 1 + x + (1 + x| cluster), data = df)`
- cluster-level variance in intercepts and slopes is modeled as randomly distributed around fixed parameters.
- effects are free to vary by cluster, but information from clusters contributes (according to cluster $n$ and outlyingness of cluster) to an overall fixed parameter. 

```{r}
library(lme4)
model <- lmer(nworms ~ 1 + arrivalt + 
                (1 + arrivalt | gardenid),
              data = worms_data)
summary(model)$coefficients
```


]
.pull-right[

```{r echo=FALSE}
model = lmer(y~x+(1+x|cluster_var),df)
df %>% mutate(
  fit = fitted(model)
) %>%
ggplot(., aes(x=x, y=y))+
  geom_line(aes(y=fit, group=cluster_var, col=cluster_var))+
  geom_abline(intercept=fixef(model)[1], slope=fixef(model)[2],lwd=2)+
  geom_point(aes(col=cluster_var),alpha=.3)+
  geom_segment(aes(y=y, yend = fit, x = x, xend = x, 
                   col=cluster_var), alpha=.3)+
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()+
  guides(col=FALSE) +
  scale_x_continuous(limits=c(0,9), breaks=0:9)+
  scale_y_continuous(limits=c(0,70))
```
]

---
# 4. Random effects (LMM)

.pull-left[
__(Partial Pooling)__

- `lmer(y ~ 1 + x + (1 + x| cluster), data = df)`
- cluster-level variance in intercepts and slopes is modeled as randomly distributed around fixed parameters.
- effects are free to vary by cluster, but information from clusters contributes (according to cluster $n$ and outlyingness of cluster) to an overall fixed parameter. 

```{r}
library(lme4)
model <- lmer(nworms ~ 1 + arrivalt + 
                (1 + arrivalt | gardenid),
              data = worms_data)
summary(model)$coefficients
```


]
.pull-right[

```{r echo=FALSE}
library(ggforce)
library(ggfx)
model = lmer(y~x+(1+x|cluster_var),df)
dfm <- 
  df %>% group_by(cluster_var) %>%
  summarise(x = 0) %>% 
  mutate(mf = predict(model, newdata=.),
         ff = fixef(model)[1] + x*fixef(model)[2],
         cv = ifelse(mf>ff, -.2, .2))

dfi <- bind_rows(df,
                 df %>% group_by(cluster_var) %>% summarise(x = 0)) %>%
  mutate(fit2 = predict(model, newdata=.))


df %>% mutate(
  fit = fitted(model)
) -> pdat

dfs <- tibble(
  cluster_var = c("cluster5","cluster12"),
  yf = c(
    coef(model)$cluster_var["cluster5","(Intercept)"]+(3*fixef(model)[2]),
    coef(model)$cluster_var["cluster12","(Intercept)"]+(3*fixef(model)[2])
    ),
  yg = c(
    coef(model)$cluster_var["cluster5","(Intercept)"]+(3*coef(model)$cluster_var["cluster5","x"]), 
    coef(model)$cluster_var["cluster12","(Intercept)"]+(2.7*coef(model)$cluster_var["cluster12","x"])
  )
)

filter(pdat, cluster_var %in% c("cluster12","cluster5")) %>%
  group_by(cluster_var) %>%
  mutate(
    fitmc = mean(fit),
    xgc = x-mean(x),
    fix = fitmc + fixef(model)[2]*xgc,
  ) -> counterf


ggplot(pdat, aes(x=x, y=y))+
  geom_abline(intercept=fixef(model)[1], slope=fixef(model)[2],lwd=2)+
  with_blur(geom_line(aes(y=fit, group=cluster_var, col=cluster_var), alpha=.5), sigma = 2)+
  with_blur(geom_point(aes(col=cluster_var), alpha=.5), sigma = 2)+
  with_blur(geom_segment(aes(y=y, yend = fit, x = x, xend = x, col=cluster_var), alpha=.2), sigma = 2)+
  with_blur(geom_curve(data=dfm[dfm$mf>dfm$ff,], 
             aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),
             curvature = -.5, alpha=.5), sigma = 2)+
  with_blur(geom_curve(data=dfm[dfm$mf<dfm$ff,],
             aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),
             curvature = .5, alpha=.5), sigma = 2)+
  with_blur(geom_line(data=dfi, aes(y=fit2, group=cluster_var, col=cluster_var),
            lty = "dashed", alpha=.5), sigma = 2)+
  
  geom_line(data = filter(pdat, cluster_var %in% c("cluster12","cluster5")), aes(y=fit, group=cluster_var, col=cluster_var))+
  geom_point(data = filter(pdat, cluster_var %in% c("cluster12","cluster5")), aes(col=cluster_var))+
  geom_segment(data = filter(pdat, cluster_var %in% c("cluster12","cluster5")), aes(y=y, yend = fit, x = x, xend = x, col=cluster_var), alpha=.7)+
  geom_line(data=filter(dfi, cluster_var %in% c("cluster12","cluster5")), aes(y=fit2, group=cluster_var, col=cluster_var),lty = "dashed")+
  geom_curve(data=filter(dfm[dfm$mf>dfm$ff,], cluster_var %in% c("cluster12","cluster5")),aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),curvature = -.5)+
  geom_curve(data=filter(dfm[dfm$mf<dfm$ff,], cluster_var %in% c("cluster12","cluster5")),aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),curvature = .5)+
  
  geom_line(data=counterf, aes(y=fix,group=cluster_var), alpha=.5)+
  # geom_abline(intercept=coef(model)$cluster_var["cluster5","(Intercept)"], slope=fixef(model)[2],
  #             lwd=1,alpha=.2)+
  # geom_curve(data = dfs[1,], aes(x=3,xend=3,y=yf, yend=yg), lwd=.5, curvature=.5, alpha=.3)+
  # geom_abline(intercept=coef(model)$cluster_var["cluster12","(Intercept)"], slope=fixef(model)[2],
  #             lwd=1,alpha=.2)+
  # geom_curve(data = dfs[2,], aes(x=3,xend=2.7,y=yf, yend=yg), lwd=.5, curvature=-.5, alpha=.3)+
  
  
  guides(col = FALSE)+
  labs(x="x = arrival at garden\n(hrs past midnight)",y="y = number of worms caught")+
  themedapr3() + 
  geom_mark_ellipse(aes(label = gardenid, filter = cluster_var == "cluster5"),
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) +
  geom_mark_ellipse(aes(label = gardenid, filter = cluster_var == "cluster12"),
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) +
  scale_x_continuous(limits=c(0,9), breaks=0:9)+
  scale_y_continuous(limits=c(0,70))+
  NULL

```
]

---

# Summary


---
class: inverse, center, middle, animated, rotateInDownLeft

# End

