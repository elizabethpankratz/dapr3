<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>WEEK 1 Linear Models and Clustered Data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josiah King" />
    <script src="jk_libs/libs/header-attrs/header-attrs.js"></script>
    <link href="jk_libs/libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="jk_libs/libs/tile-view/tile-view.js"></script>
    <link href="jk_libs/libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="jk_libs/libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="jk_libs/libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="jk_libs/tweaks.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <b>WEEK 1<br>Linear Models and Clustered Data</b>
## Data Analysis for Psychology in R 3
### Josiah King
### Department of Psychology<br/>The University of Edinburgh
### AY 2020-2021

---










---
class: inverse, center, middle

# Part 1&lt;br&gt;Linear Regression Refresh

---

# deterministic vs statistical model


y = mx + c  
`\(y = \alpha + \beta x + \epsilon\)`  

.br3.pa2.f3.white.bg-gray[
$$ \textrm{outcome} = (\textrm{model}) + \textrm{error} $$
]

---
# The Linear Model

.br3.pa2.f2[
$$
`\begin{align}
\color{red}{\textrm{outcome}} &amp; = \color{blue}{(\textrm{model})} + \textrm{error} \\
\color{red}{y} &amp; = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x} + \epsilon \\
\text{where } \\
\epsilon &amp; \sim N(0, \sigma) \text{ independently} \\
\end{align}`
$$
]

---
# Model structure

.flex.items-top[
.w-50.pa2[

`\(\color{red}{y} = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x} + \epsilon\)`  
  
so the _fitted_ linear model itself is:  

`\(\hat{y} = \color{blue}{\hat \beta_0 \cdot{} 1 + \hat \beta_1 \cdot{} x}\)`  

{{content}}
]
.w-50.pa2[
![](dapr3_lec1_files/figure-html/bb-1.png)&lt;!-- --&gt;
]]

--

For the `\(i^{th}\)` observation:
  - `\(\color{red}{y_i}\)` is the value we observe for `\(x_i\)`   
  - `\(\hat{y}_i\)` is the value the model _predicts_ for `\(x_i\)`   
  - `\(\color{red}{y_i} = \hat{y}_i + \epsilon_i\)`  


---
# An Example



.flex.items-top[
.w-50.pa2[

`\(\color{red}{y_i} = \color{blue}{5 \cdot{} 1 + 2 \cdot{} x_i} + \epsilon_i\)`  
  
{{content}}
]
.w-50.pa2[
![](dapr3_lec1_files/figure-html/errplot-1.png)&lt;!-- --&gt;
]]

--

__e.g.__   
for the observation `\(x_i = 1.2, y_i = 9.9\)`:

$$
`\begin{align}
\color{red}{9.9} &amp; = \color{blue}{5 \cdot{}} 1 + \color{blue}{2 \cdot{}} 1.2 + \epsilon_i \\
&amp; = 7.4 + \epsilon_i \\
&amp; = 7.4 + 2.5 \\
\end{align}`
$$

---
# Extending the linear model
## Categorical Predictors

.pull-left[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; y &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; x &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 7.99 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4.73 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3.66 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3.41 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5.75 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5.66 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Category0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ... &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ... &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]
.pull-right[
![](dapr3_lec1_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]



---

# Extending the linear model
## Multiple predictors

.pull-left[
![](dapr3_lec1_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
]

.pull-right[
![](dapr3_lec1_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]

---

# Extending the linear model
## Interactions

.pull-left[
![](dapr3_lec1_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]
.pull-right[
![](dapr3_lec1_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]



---
# Notation

`\(\begin{align} \color{red}{y} \;\;\;\; &amp; = \;\;\;\;\; \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_1 + ... + \beta_k \cdot x_k} &amp; + &amp; \;\;\;\epsilon \\ \qquad \\ \color{red}{\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ \vdots \\ y_n \end{bmatrix}} &amp; = \color{blue}{\begin{bmatrix} 1 &amp; x_{11} &amp; x_{21} &amp; \dots &amp; x_{k1} \\ 1 &amp; x_{12} &amp; x_{22} &amp;  &amp; x_{k2} \\ 1 &amp; x_{13} &amp; x_{23} &amp;  &amp; x_{k3} \\ 1 &amp; x_{14} &amp; x_{24} &amp;  &amp; x_{k4} \\ 1 &amp; x_{15} &amp; x_{25} &amp;  &amp; x_{k5} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{1n} &amp; x_{2n} &amp; \dots &amp; x_{kn} \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix}} &amp; + &amp; \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \\ \vdots \\ \epsilon_n \end{bmatrix} \\ \qquad \\ \\\color{red}{y} \;\;\;\;\; &amp; = \qquad \qquad \;\;\; \mathbf{\color{blue}{X \qquad \qquad \qquad \;\;\;\:\beta}} &amp; + &amp; \;\;\;\epsilon \\ \end{align}\)`

---

# Extending the linear model
## Link functions

$$
`\begin{align}
\color{red}{y} = \mathbf{\color{blue}{X \beta} + \epsilon} &amp; \qquad  &amp; (-\infty, \infty) \\
\color{red}{ln \left( \frac{p}{1-p} \right) } = \mathbf{\color{blue}{X \beta} + \epsilon} &amp; \qquad  &amp; [0,1] \\
\end{align}`
$$  

---
# Linear Models in R


```r
linear_model &lt;- lm(y ~ x1 + x2 + x3*x4, data = df)
```


```r
logistic_model &lt;- glm(y ~ x1 + x2 + x3*x4, data = df, family=binomial(link="logit"))
```

---
# Inference for the linear model

&lt;img src="jk_img_sandbox/sum1.png" height="500px" /&gt;

---
# Inference for the linear model

&lt;img src="jk_img_sandbox/sum2.png" height="500px" /&gt;

---
# Inference for the linear model

&lt;img src="jk_img_sandbox/sum3.png" height="500px" /&gt;

---
# Inference for the linear model

&lt;img src="jk_img_sandbox/sum4.png" height="500px" /&gt;

---
# Assumptions

Our model:  

`\(\color{red}{y} = \color{blue}{\mathbf{X \beta}} + \epsilon \\ \text{where } \epsilon \sim N(0, \sigma) \text{ independently}\)`

--

Or put another way:  

`\(\color{red}{y} \sim Normal(\color{blue}{\mathbf{X \beta}}, \sigma)\)`

--

Recall, our ability to generalise from our model fitted on sample data to the wider population requires making some _assumptions_

--

- assumptions about the nature of the **model** .tr[
(linear)
]

--

- assumptions about the nature of the **errors** .tr[
(normal)
]

---

# The Broader Idea


All our work here is in aim of making models of the world.

- Models are models. They are simplifications, and so wrong/imperfect.  
- Our residuals ( `\(y - \hat{y}\)` ) reflect everything that we don't account for in our model
- In an ideal world, our model accounts for _all_ the systematic relationships. What is left over (our residuals) is just randomness. 
    - If our model is mis-specified, or misses out something systematic, then our residuals will reflect this.
- We check by examining how much "like randomness" the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d ("independent and identically distributed")
    - _this tends to get referred to as our "assumptions"_
- We will never know whether our residuals contain only randomness - we can never observe everything! 
---

# Checking Assumptions

.pull-left[

residuals should be "zero mean and constant variance".  
what does this look like? 

well, we want the mean of the residuals to be zero, and we want a close to normal distribution.
furthermore, we want that spread to be constant across the fitted values. 

]
.pull-right[

![](dapr3_lec1_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

]


---

# Checking Assumptions
## (the "recipe book" way)

&lt;div class="acronym"&gt;
L
&lt;/div&gt; inearity&lt;br&gt;
&lt;div class="acronym"&gt;
I
&lt;/div&gt; ndependence&lt;br&gt;
&lt;div class="acronym"&gt;
N
&lt;/div&gt; ormality&lt;br&gt;
&lt;div class="acronym"&gt;
E
&lt;/div&gt; qual variance&lt;br&gt;

.footnote["Line without N is a Lie!" (Umberto)]



---

# What if our model doesn't meet assumptions?

- bootstrap!

what about independence? 




---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

# Part 2&lt;br&gt;Clustered Data

---

# What is clustered data?

- children within schools
- patients within clinics
- observations within individuals

terms: "levels","hierarchical"

- children within classrooms within schools within districts etc...  

---
# Why is it relevant?  

- clustering will likely result in measurement on observational units within a given cluster being more similar to those of other clusters.  
  - e.g. our measured outcome for children in a given class will tend to be more similar to one another (because of class specific things such as the teacher) than to children in other classes

- clustering is expressed in terms of the correlation among the measurements within the same cluster

---
# ICC (intra-class correlation coefficient)

- various forms
- variance within / total variance  
- `\(\rho = \frac{\sigma^2_u}{\sigma^2_u + \sigma^2_\epsilon}\)`


---
# Why is clustered data a problem for lm?

`$$\epsilon \sim N(0, \sigma) \textbf{ independently}$$` 

- clustering is something systematic that our model should (arguably) take into account. 
- "independence" assumption.  

---
# HOW is clustered data a problem for lm?

.pull-left[
#### Standard errors

We saw:
`$$SE(\hat \beta_1) = \sqrt{\frac{ SS_{Residual}/(n-k-1)}{\sum(x_i - \bar{x})^2}}$$`

suppose that `\(\rho = 1\)`. i.e., all the variation we see is due to the clustering. 

if we ignore clustering, `\(SS_{Residual}\)` 
]
.pull-right[
#### Not always the effect of interest

e.g. simpsons pdox


]


---
# Various values of `\(\rho\)`

![](dapr3_lec1_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

---

# Various values of `\(\rho\)`

![](dapr3_lec1_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;

---

# Summary

---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 2

---
class: inverse, center, middle

# Part 3
## Possible solutions



---
# Some toy data

.pull-left[
&lt;img src="dapr3_lec1_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="dapr3_lec1_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;
]

.footnote[You can find this data yourself at LINK]

---
# 1. Ignore it

.pull-left[
![](dapr3_lec1_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;
]
.pull-right[
__(Complete pooling)__  

Information from all clusters is pooled together to estimate over x  


```r
model &lt;- lm(y ~ x, data = df)
```

```
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)    18.08       8.68    2.08    0.040 *
## x               3.29       1.31    2.50    0.014 *
```

]

---

# 1. Ignore it 

.pull-left[
![](dapr3_lec1_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;
]
.pull-right[

__(Complete pooling)__  

Information from all clusters is pooled together to estimate over x  


```r
model &lt;- lm(y ~ x, data = df)
```

```
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)    18.08       8.68    2.08    0.040 *
## x               3.29       1.31    2.50    0.014 *
```

But different clusters show different patterns. 
Residuals are not independent.  
]

---
# 2. Lesser used solutions  

&lt;!-- https://rlbarter.github.io/Practical-Statistics/2017/05/10/generalized-estimating-equations-gee/ --&gt;

Don't include clustering as part of the model directly, but incorporate the dependency into our residuals term. 

$$
`\begin{align}
\color{red}{\textrm{outcome}} &amp; = \color{blue}{(\textrm{model})} + \textrm{error}^* \\
\end{align}`
$$

.footnote[Where errors are correlated with clusters/with the error previously/etc.]


- independence (observations over time are independent)
- exchangeable (all observations over time have the same correlation)
- AR(1) (correlation decreases as a power of how many timepoints apart two observations are)
- unstructured (correlation between all timepoints may be different)


---

# 2. Lesser used solutions  

.pull-left[
### __Cluster Robust Standard Errors__


```r
library(plm)
clm &lt;- plm(y ~ x, data=df, 
           model="pooling", index="cluster_var")
```

```
##             Estimate Std. Error t-value Pr(&gt;|t|)  
## (Intercept)    18.08       8.68    2.08    0.040 *
## x               3.29       1.31    2.50    0.014 *
## ---
```

```r
sqrt(diag(vcovHC(clm, 
                 method='arellano', 
                 cluster='group')))
```

```
## (Intercept)           x 
##      12.890       2.138
```
]
.pull-right[
### __Generalised Estimating Equations (GEE)__  


```r
library(geepack)
# needs to be arranged by cluster, and for cluster to be numeric
df &lt;- df %&gt;% arrange(cluster_var) %&gt;%
  mutate(
    cluster_id = as.numeric(as.factor(cluster_var))
  )
geemod  = geeglm(y ~ x, data=df, 
                 corstr = 'independence', id =  cluster_id)
```

```
##  Coefficients:
##             Estimate Std.err Wald Pr(&gt;|W|)
## (Intercept)    18.08   12.89 1.97     0.16
## x               3.29    2.14 2.36     0.12
```

]

---
# 3. Fixed effects

.pull-left[

__(Complete pooling)__  

Information from a cluster contributes to estimate *for that cluster*, but information is not pooled to estimate an overall effect. 


```r
model &lt;- lm(y ~ x * cluster_var, data = df)
```

]
.pull-right[
![](dapr3_lec1_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;
]

---

# 3. Fixed effects 

+ Lots of estimates (separate for each cluster). 
+ Variance estimates constructed based on information *only* within each cluster. 
+ No overall estimate of effect over x. 


```
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              54.613      6.032    9.05  2.4e-14 ***
## x                        -0.289      0.904   -0.32   0.7502    
## cluster_varcluster10     -5.342      7.608   -0.70   0.4844    
## cluster_varcluster11    -11.088      7.753   -1.43   0.1561    
## cluster_varcluster12     -5.843      8.247   -0.71   0.4804    
## cluster_varcluster2      17.571     10.464    1.68   0.0966 .  
## cluster_varcluster3     -23.058      7.118   -3.24   0.0017 ** 
## cluster_varcluster4     -61.517     14.101   -4.36  3.4e-05 ***
## cluster_varcluster5      11.945      8.158    1.46   0.1466    
## cluster_varcluster6      -1.198      8.057   -0.15   0.8821    
## cluster_varcluster7      12.145      7.577    1.60   0.1124    
## cluster_varcluster8       2.617      7.671    0.34   0.7338    
## cluster_varcluster9      18.480      8.130    2.27   0.0254 *  
## x:cluster_varcluster10   -0.323      1.196   -0.27   0.7881    
## x:cluster_varcluster11   -0.390      1.229   -0.32   0.7517    
## x:cluster_varcluster12   -5.933      1.247   -4.76  7.3e-06 ***
## x:cluster_varcluster2    -2.599      1.459   -1.78   0.0782 .  
## x:cluster_varcluster3    -1.850      1.185   -1.56   0.1219    
## x:cluster_varcluster4     5.156      2.350    2.19   0.0308 *  
## x:cluster_varcluster5    -0.787      1.171   -0.67   0.5035    
## x:cluster_varcluster6    -1.892      1.197   -1.58   0.1173    
## x:cluster_varcluster7    -4.942      1.108   -4.46  2.3e-05 ***
## x:cluster_varcluster8    -3.756      1.157   -3.25   0.0016 ** 
## x:cluster_varcluster9    -0.655      1.175   -0.56   0.5784
```

---
# LMM



```r
library(lme4)

model &lt;- lmer(y ~ x + (1 + x | cluster_var),df)
summary(model)$coefficients
```

```
##             Estimate Std. Error t value
## (Intercept)     52.5      4.957   10.59
## x               -2.1      0.644   -3.26
```


---

# 4. Random effects (LMM)

.pull-left[
__(Partial Pooling)__

].pull-right[

![](dapr3_lec1_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;
]


---

# Summary


---
class: inverse, center, middle, animated, rotateInDownLeft

# End

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="jk_libs/macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
