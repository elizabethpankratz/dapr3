[
  {
    "objectID": "01_regressionrefresh.html",
    "href": "01_regressionrefresh.html",
    "title": "Regression Refresh and Clustered Data",
    "section": "",
    "text": "Exercises: Linear Models & Pooling\n\nData: Wellbeing in Scotland\nIn DAPR2, one of the examples we used in learning about linear regression was in examining the relationship between time spent outdoors and mental wellbeing. In that example researchers had collected data from 32 residents of Edinburgh & Lothians.\nResearchers want to study this relationship across all of Scotland. They contact all the Local Authority Areas (LAAs) and ask them to collect data for them for them, with participants completing the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being, and being asked to estimate the average number of hours they spend outdoors each week.\nTwenty of the Local Authority Areas provided data. It is available at https://uoepsy.github.io/data/LAAwellbeing.csv, and you can read it into your R environment using the code below:\n\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n\nThe dataset contains information on 132 participants. You can see the variables in the table below\n\n\n\n\n\n\n  \n  \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant ID\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nSelf report estimated number of hours per week spent outdoors\n    wellbeing\nWarwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n    density\nLAA Population Density (people per square km)\n  \n  \n  \n\n\n\n\n\n\nRegression Refresh\nRecall that in the DAPR2 course last year we learned all about the linear regression model, which took the form:\n\\[\n\\begin{align}\\\\\n& \\text{for observation }i \\\\\n& \\color{red}{Y_i}\\color{black} = \\color{blue}{\\beta_0 \\cdot{} 1 + \\beta_1 \\cdot{} X_{1i} \\ + \\ ... \\ + \\ \\beta_p \\cdot{} X_{pi}}\\color{black} + \\varepsilon_i \\\\\n\\end{align}\n\\]\nAnd if we wanted to write this more simply, we can express \\(X_1\\) to \\(X_p\\) as an \\(n \\times p\\) matrix (samplesize \\(\\times\\) parameters), and \\(\\beta_0\\) to \\(\\beta_p\\) as a vector of coefficients:\n\\[\n\\begin{align}\n& \\color{red}{\\mathbf{y}}\\color{black} = \\color{blue}{\\boldsymbol{X\\beta}}\\color{black} + \\boldsymbol{\\varepsilon} \\\\\n& \\quad \\\\\n& \\text{where} \\\\\n& \\varepsilon \\sim N(0, \\sigma) \\text{ independently} \\\\\n\\end{align}\n\\]\nIn R, we fitted these models using:\n\nlm(y ~ x1 + x2 + .... xp, data = mydata)  \n\n\n\nQuestion 1\n\n\nRead in the Local Authority data from https://uoepsy.github.io/data/LAAwellbeing.csv and plot the bivariate relationship between wellbeing and time spent outdoors.\nUsing lm(), fit the simple linear model:\n\\[\n\\text{Wellbeing}_i = \\beta_0 + \\beta_1 \\cdot \\text{Hours per week spent outdoors}_i + \\varepsilon_i\n\\]\nThink about the assumptions we make about this model:\n\\[\n\\text{where} \\quad \\varepsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\]\nHave we satisfied this assumption (specifically, the assumption of independence of errors)?\n\n\n\n\n Solution \n\n\n\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\") \n\nggplot(data = scotmw, aes(x = outdoor_time, y = wellbeing))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nsimplemod <- lm(wellbeing ~ outdoor_time, data = scotmw)\nsummary(simplemod)\n\n\nCall:\nlm(formula = wellbeing ~ outdoor_time, data = scotmw)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-27.4395  -7.5658  -0.3175   6.0831  26.7208 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   39.2723     2.6674  14.723   <2e-16 ***\noutdoor_time   0.1603     0.1462   1.096    0.275    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.43 on 130 degrees of freedom\nMultiple R-squared:  0.009155,  Adjusted R-squared:  0.001534 \nF-statistic: 1.201 on 1 and 130 DF,  p-value: 0.2751\n\n\nOur model from the previous question will assume that the residuals for all participants are independent of one another. But is this a reasonable assumption that we can make? Might we not think that the residents of the highlands might have generally higher levels of wellbeing than those living in Glasgow? Additionally, the association between outdoor time and wellbeing might be different depending on where you live?\nThe natural grouping of the people into their respective geographic area introduces a level of dependence which we would be best to account for.\n\n\n\n\nQuestion 2\n\n\nTry running the code below.\n\nggplot(data = scotmw, aes(x = outdoor_time, y = wellbeing))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\nThen try editing the code to include an aesthetic mapping from the LAA to the color in the plot.\nHow do your thoughts about the relationship between outdoor time and wellbeing change?\n\n\n\n\n Solution \n\n\n\nggplot(data = scotmw, aes(x = outdoor_time, y = wellbeing))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\n\n\n\n\n\n\n\n\nggplot(data = scotmw, aes(x = outdoor_time, y = wellbeing, col = laa))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\n\n\n\n\n\n\n\nFrom the second plot, we see a lot of the LAA appear to have a positive relationship (outdoor time is associated with higher wellbeing). There seem to be differences between LAAs in both the general wellbeing level (residents of Na h-Eileanan Siar - the outer hebrides - have high wellbeing), and in how outdoor time is associated with wellbeing (for instance, outdoor time doesn’t seem to help in Glasgow City).\n\n\n\n\nComplete Pooling\nWe can consider the simple regression model (lm(wellbeing ~ outdoor_time, data = scotmw)) to “pool” the information from all observations together. In this ‘Complete Pooling’ approach, we simply ignore the natural clustering of the people into their local authority areas, as if we were unaware of it. The problem is that this assumes the same regression line for all local authority areas, which might not be that appropriate. Additionally, we violate the assumption that our residuals are independent, because all of the residuals from certain groups will be more like one another than they are to the others.\n\n\n\n\n\nComplete pooling can lead to bad fit for certain groups\n\n\n\n\n\n\nNo Pooling\nThere are various ways we could attempt to deal with the problem that our data are in groups (or “clusters”). With the tools you have learned in DAPR2, you may be tempted to try including LAA in the model as another predictor, to allow for some local authority areas being generally better than others:\n\nlm(wellbeing ~ outdoor_time + laa, data = scotmw)\n\nOr even to include an interaction to allow for local authority areas to show different patterns of association between outdoor time and wellbeing:\n\nlm(wellbeing ~ outdoor_time * laa, data = scotmw)\n\nThis approach gets termed the “No Pooling” method, because the information from each cluster contributes only to an estimated parameter for that cluster, and there is no pooling of information across clusters. This is a good start, but it means that a) we are estimating a lot of parameters, and b) we are not necessarily estimating the parameter of interest (the overall effect of practice on reading age). Furthermore, we’ll probably end up having high variance in the estimates at each group.\n\n\nQuestion 3\n\n\nFit a linear model which accounts for the grouping of participants into their different local authorities, but holds the association between outdoor time and wellbeing as constant across LAAs:\n\nmod1 <- lm(wellbeing ~ outdoor_time + laa, data = scotmw)\n\nCan you construct a plot of the fitted values from this model, coloured by LAA?\n\nHint: you might want to use the augment() function from the broom package\n\n\n\n\n\n Solution \n\n\n\nlibrary(broom)\naugment(mod1) %>%\n  ggplot(.,aes(x=outdoor_time, y=.fitted, col=laa))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nWhat happens (to the plot, and to your parameter estimates) when you include the interaction between laa and outdoor_time?\n\n\n\n\n Solution \n\n\n\nmod2 <- lm(wellbeing ~ outdoor_time * laa, data = scotmw)\n\nbroom::augment(mod2) %>%\n  ggplot(.,aes(x=outdoor_time, y=.fitted, col=laa))+\n  geom_line()\n\n\n\n\n\n\n\n\nWe can see now that our model is fitting a different relationship between wellbeing and outdoor time for each LAA. This is good - we’re going to get better estimates for different LAAs (e.g. wellbeing of residents of the Highlands increases with more outdoor time, and wellbeing of residents of Glasgow does not).\nWe can see that this model provides a better fit - it results in a significant reduction in the residual sums of squares:\n\nanova(mod1, mod2)\n\nAnalysis of Variance Table\n\nModel 1: wellbeing ~ outdoor_time + laa\nModel 2: wellbeing ~ outdoor_time * laa\n  Res.Df    RSS Df Sum of Sq   F   Pr(>F)   \n1    111 2826.9                             \n2     92 1864.4 19    962.57 2.5 0.001975 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBut accounting for this heterogeneity over clusters in the effect of interest comes at the expense of not pooling information across groups to get one estimate for “the association between outdoor time and wellbeing”. Additionally, these models will tend to have low statistical power because they are using fewer observations (only those within each cluster) to estimate parameters which only represent within-cluster effects.\n\n\n\n\n\nExercises: Advanced Data Wrangling\nWith more complex data structures comes more in-depth data wrangling in order to get it ready for fitting and estimating our model. Typically, the data we get will not be neat and tidy, and will come in different formats. Often we simply get whatever our experiment/questionnaire software spits out, and we have to work from there. When you are designing a study, you can do work on the front end to minimise the data-wrangling. Try to design an experiment/questionnaire while keeping in mind what the data comes out looking like.\nBelow we have some data from a fake experiment. We’ve tried to make it a bit more difficult to work with - a bit more similar to what we would actually get when doing real-life research.\n\nData: Raising the stakes\nThis data is from a simulated study that aims to investigate how levels of stress are influenced by the size and type of potential rewards.\n30 volunteers from an amateur basketball league participated. Each participant completed 20 trials in which they were tasked with throwing a basketball and scoring a goal in order to score points. The number of points up for grabs varied between trials, ranging from 1 to 20 points, with the order randomised for each participant. If a participant successfully threw the ball in the basket, then their score increased accordingly. If they missed, their score decreased accordingly. Participants were informed of the number of points available prior to each throw.\nTo examine the influence of the type of reward/loss on stress-levels, the study consisted of two conditions. In the monetary condition, (n = 15) participants were informed at the start of the study that they would receive their final score in £ at the end of the study. In the reputation condition, (n = 15) participants were informed that the points would be inputted on to a scoreboard and distributed around the local basketball clubs and in the league newsletter.\nThroughout each trial, participants’ heart rate variability (HRV) was measured via a chest strap. HRV is considered to be indirectly related to levels of stress (i.e., higher HRV = less stress).\nThe data is in stored in two separate files.\n\nInformation on the conditions for each trial for each participant is stored in .csv format at https://uoepsy.github.io/data/basketballconditions.csv.\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nstakes\nNumber of points available to be won/lost based on successful completion of trial\n\n\ncondition\nWhether the final score will be converted to £ or will be placed on a public leader-board\n\n\nsub\nParticipant Identifier\n\n\nthrow\nWhether the participant successfully threw the ball in the basket\n\n\ntrial_no\nTrial number (1 to 20\n\n\n\n\n\n\nInformation on participants’ HRV for each trial is stored in .xlsx format, and can be downloaded from https://uoepsy.github.io/data/basketballhrv.xlsx.\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nsub\nSubject Identifier\n\n\ntrial_1\nHRV average during trial 1\n\n\ntrial_2\nHRV average during trial 2\n\n\ntrial_3\nHRV average during trial 3\n\n\n…\nHRV average during trial …\n\n\n\n\n\n\n\nQuestion 5\n\n\nGet the data into your R session.\nNote: For one of the files, this is a bit different to how we have given you data in previous exercises. You may remember that for a .csv file, you can read directly into R from the link using, read_csv(\"https://uoepsy.......).\nHowever, in reality you are likely to be confronted with data in all sorts of weird formats, such as .xlsx files from MS Excel. Have a look around the internet to try and find any packages/functions/techniques for getting both the datasets in to R.\n\n\n\n\n Solution \n\n\n\nUnfortunately, a few students are getting error messages which we could not solve when trying to read in the .xlsx data. The same data is available at https://uoepsy.github.io/data/basketballhrv.csv so that you can read it in in the normal way.\n\n\nbball <- read_csv(\"https://uoepsy.github.io/data/basketballconditions.csv\")\nhead(bball)\n\n# A tibble: 6 × 5\n  stakes condition   sub throw trial_no\n   <dbl> <chr>     <dbl> <dbl> <chr>   \n1     13 money         1     1 trial_1 \n2     17 money         1     1 trial_2 \n3      7 money         1     1 trial_3 \n4      1 money         1     1 trial_4 \n5      2 money         1     1 trial_5 \n6      8 money         1     1 trial_6 \n\n\nFor the .xlsx data:\n\nStep 1: download the data to your computer\n\nStep 2: load the readxl package.\n\nStep 3: use the read_xlsx() function to read in the data, pointing it to the relevant place on your computer.\n\nYou can actually do all these steps from within R.\n\n# Step 1\ndownload.file(url = \"https://uoepsy.github.io/data/basketballhrv.xlsx\", \n              destfile = \"baskeballhrvdata.xlsx\")\n# Step 2\nlibrary(readxl)\n# Step 3\nbballhrv <- read_xlsx(\"baskeballhrvdata.xlsx\")\nhead(bballhrv)\n\n\n\n# A tibble: 6 × 21\n    sub trial_1 trial_2 trial_3 trial_4 trial_5 trial_6 trial_7 trial_8 trial_9\n  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1     1    5.47    6.69    2.72    4.95    5.96    4.93    4.62    4.70    5.78\n2     2    4.60    6.46    3.77    4.80    6.33    5.15    5.04    5.70    4.66\n3     3    5.14    5.98    4.30    4.40    4.97    5.16    4.71    4.60    5.94\n4     4    5.85    3.74    3.40    4.97    6.46    3.87    5.14    6.26    3.60\n5     5    7.46    5.83    4.83    6.26    3.52    5.92    4.24    4.39    5.75\n6     6    3.53    2.89    2.07    2.20    3.92    4.45    3.19    5.20    3.81\n# … with 11 more variables: trial_10 <dbl>, trial_11 <dbl>, trial_12 <dbl>,\n#   trial_13 <dbl>, trial_14 <dbl>, trial_15 <dbl>, trial_16 <dbl>,\n#   trial_17 <dbl>, trial_18 <dbl>, trial_19 <dbl>, trial_20 <dbl>\n# ℹ Use `colnames()` to see all variable names\n\n\n\n\n\n\nQuestion 6\n\n\nIs each dataset in wide or long format? We want them both in long format, so try to reshape either/both if necessary.\n\nHint: in the tidyverse functions, you can specify all columns between column x and column z by using the colon, x:z.\n\n\n\n\n\n Solution \n\n\nOnly the HRV data is in wide format:\n\nhead(bballhrv)\n\n# A tibble: 6 × 21\n    sub trial_1 trial_2 trial_3 trial_4 trial_5 trial_6 trial_7 trial_8 trial_9\n  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1     1    5.47    6.69    2.72    4.95    5.96    4.93    4.62    4.70    5.78\n2     2    4.60    6.46    3.77    4.80    6.33    5.15    5.04    5.70    4.66\n3     3    5.14    5.98    4.30    4.40    4.97    5.16    4.71    4.60    5.94\n4     4    5.85    3.74    3.40    4.97    6.46    3.87    5.14    6.26    3.60\n5     5    7.46    5.83    4.83    6.26    3.52    5.92    4.24    4.39    5.75\n6     6    3.53    2.89    2.07    2.20    3.92    4.45    3.19    5.20    3.81\n# … with 11 more variables: trial_10 <dbl>, trial_11 <dbl>, trial_12 <dbl>,\n#   trial_13 <dbl>, trial_14 <dbl>, trial_15 <dbl>, trial_16 <dbl>,\n#   trial_17 <dbl>, trial_18 <dbl>, trial_19 <dbl>, trial_20 <dbl>\n# ℹ Use `colnames()` to see all variable names\n\n\n\nbballhrv <-\n  bballhrv %>%\n  pivot_longer(trial_1:trial_20, names_to = \"trial_no\", values_to = \"hrv\")\n\nhead(bballhrv)\n\n# A tibble: 6 × 3\n    sub trial_no   hrv\n  <dbl> <chr>    <dbl>\n1     1 trial_1   5.47\n2     1 trial_2   6.69\n3     1 trial_3   2.72\n4     1 trial_4   4.95\n5     1 trial_5   5.96\n6     1 trial_6   4.93\n\n\n\n\n\n\nPivot!\nOne of the more confusing things to get to grips with is the idea of reshaping a dataframe.\nFor different reasons, you might sometimes want to have data in wide, or in long format.\n\n\n\n\n\nSource: https://fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/\n\n\n\n\nWhen the data is wide, we can make it long using pivot_longer(). When we make data longer, we’re essentially making lots of columns into 2 longer columns. Above, in the animation, the wide variable x, y and z go into a new longer column called name that specifies which (x/y/z) it came from, and the values get put into the val column.\nThe animation takes a shortcut in the code it displays above, but you could also use pivot_longer(c(x,y,z), names_to = \"name\", values_to = \"val\"). To reverse this, and put it back to being wide, we tell R which columns to take the names and values from: pivot_wider(names_from = name, values_from = val).\n\nNow comes a fun bit. You may have noticed that we have two datasets for this study. If we are interested in relationships between the heart rate variability (HRV) of participants during each trial, as well as the experimental manipulations (i.e., the condition of each trial), these are currently in different datasets.\nSolution: we need to join them together!\nProvided that both data-sets contain information on participant number and trial number, which uniquely identify each observation, we can join them together matching on those variables!\n\nQuestion 7\n\n\nJoin the two datasets (both in long format) together.\nNote that the variables we are matching on need to have the information in the same format. For instance, R won’t be able to match \"trial_1\",\"trial_2\",\"trial_3\" with 1, 2, 3 because they are different things. We would need to edit one of them to be in the same format.\n\nHint: You should end up with 600 rows.\n\n\n\n\n\n Solution \n\n\n\nbball <- full_join(bball, bballhrv)\nhead(bball)\n\n# A tibble: 6 × 6\n  stakes condition   sub throw trial_no   hrv\n   <dbl> <chr>     <dbl> <dbl> <chr>    <dbl>\n1     13 money         1     1 trial_1   5.47\n2     17 money         1     1 trial_2   6.69\n3      7 money         1     1 trial_3   2.72\n4      1 money         1     1 trial_4   4.95\n5      2 money         1     1 trial_5   5.96\n6      8 money         1     1 trial_6   4.93\n\n\n\n\n\n\nJoining data\nThere are lots of different ways to join data-sets, depending on whether we want to keep rows from one data-set or the other, or keep only those in both data-sets etc.\n\n\n\n\n\nCheck out the help documentation for them all using ?full_join.\n\n\n\n\n\n\n\nExercises: Clustering\n\nQuestion 8\n\n\nContinuing with our basketball/hrv study, consider the following questions:\nWhat are the units of observations?\nWhat are the groups/clusters?\nWhat varies within these clusters?\nWhat varies between these clusters?\n\n\n\n\n Solution \n\n\nWhat are the units of observations? trials\nWhat are the groups/clusters? participants What varies within these clusters? size of reward\nWhat varies between these clusters? type of reward\n\n\n\n\nQuestion 9\n\n\nCalculate the ICC, using the ICCbare() function from the ICC package.\n\nHint: Remember, you can look up the help for a function by typing a ? followed by the function name in the console.\n\n\n\n\n\n Solution \n\n\n\nlibrary(ICC)\nICCbare(x = sub, y = hrv, data = bball)\n\n[1] 0.3141482\n\n\n\n\n\n\nUnderstanding ICC a bit better\nThink about what ICC represents - the ratio of the variance between the groups to the total variance.\nYou can think of the “variance between the groups” as the group means varying around the overall mean (the black dots around the black line), and the total variance as that plus the addition of the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\nggplot(bball, aes(x=sub, y=hrv))+\n  geom_point(aes(col=sub),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(bball$hrv))+\n  guides(col=FALSE)\n\n\n\n\n\n\n\n\nYou can also think of the ICC as the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\n\n Optional - ICC as the expected correlation between two observations from same group\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n  cluster observation   y\n1 group_1           1   4\n2 group_1           2   2\n3 group_2           1   4\n4 group_2           2   2\n5 group_3           1   7\n6 group_3           2   5\n7     ...         ... ...\n\n\nThe ICC for this data is 0.18:\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  <chr>   <chr> <chr>\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, but grouped.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  <chr>   <chr> <chr>\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen these:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  <chr>   <chr> <chr>\n1 group_1 4     2    \n2 group_2 2     4    \n3 group_3 5     7    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 7     6    \n7 ...     ...   ...  \n\n\nor these:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  <chr>   <chr> <chr>\n1 group_1 4     2    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 7     6    \n7 ...     ...   ...  \n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Optional - Extra difficult. Calculate ICC manually\n\n\nWe have equal group sizes here (there are 2 \\(\\times\\) 15 participants, each with 20 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\nMS_b = & \\textrm{Mean Squares between groups} = \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\nMS_e = & \\textrm{Mean Squares within groups} \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\]\nSo we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a new column which is the overall mean of y. This bit is fairly straightforward.\n\nbball %>% mutate(\n  grand_mean = mean(hrv)\n)\n\n\nWe have seen a lot of the combination of group_by() %>% summarise(), but we can also combine group_by() with mutate()!\n\nTry the following:\n\nbball %>% mutate(\n    grand_mean = mean(hrv)\n  ) %>% \n  group_by(sub) %>%\n  mutate(\n    group_mean = mean(hrv)\n  )\n\n\nThe grouping gets carried forward.\nUsing group_by() can quite easily land you in trouble if you forget that you have grouped the dataframe.\nLook at the output of class() when we have grouped the data. It still mentions something about the grouping.\n\nbball <- bball %>% mutate(\n    grand_mean = mean(hrv)\n  ) %>% \n  group_by(sub) %>%\n  mutate(\n    group_mean = mean(hrv)\n  )\n\nclass(bball)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nTo remove the grouping, we can use ungroup() (we could also just add this to the end of our code sequence above and re-run it):\n\nbball <- ungroup(bball)\nclass(bball)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nbball <- bball %>% \n  mutate(\n    within = (hrv-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(bball$between)\nsswithin = sum(bball$within)\n\nFinally, we divide them by the degrees of freedom.\n\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (600-30)\n\nAnd calculate the ICC!!!\n\n# ICC\n(msb-mse) /(msb + (19*mse))\n\n[1] 0.3141482\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nHow do the size and type of potential reward/loss interact to influence levels of stress?\n\nUsing lm(), we might fit the following model to investigate the research question above.\n\nlm(hrv ~ stakes*condition, data = bball)\n\nHowever, we know that this simply ignores that our observations are not independent - we have repeated measures from the same individuals. Given that roughly 30% of the variation in HRV can be attributed to variability between participants (as found when we calculated ICC), we don’t want to mis-attribute this to differences between experimental conditions.\nUsing the “no pooling” method (i.e., including participant as a fixed effect), becomes difficult because the sub variable (the participant id variable) also uniquely identifies the two conditions. Note that if we fit the following model, some coefficients are not defined.\nTry it and see:\n\nlm(hrv ~ stakes*sub + stakes*condition, data=bball)\n\nThis sort of perfectly balanced design has traditionally been approached with extensions of ANOVA (“repeated measures ANOVA”, “mixed ANOVA”). These methods can partition out variance due to one level of clustering (e.g. subjects), and can examine factorial designs when one factor is within cluster, and the other is between. You can see an example here if you are interested. However, ANOVA has a lot of constraints - it can’t handle multiple levels of clustering (e.g. children in classes in schools), it will likely require treating variables such as time as a factor, and it’s not great with missing data.\nThe multi-level model (MLM) provides a more flexible framework, and this is what we will begin to look at next week."
  },
  {
    "objectID": "02_intromlm.html",
    "href": "02_intromlm.html",
    "title": "Multilevel Models",
    "section": "",
    "text": "A Note on terminology\nThe methods we’re going to learn about in the first five weeks of this course are known by lots of different names: “multilevel models”; “hierarchical linear models”; “mixed-effect models”; “mixed models”; “nested data models”; “random coefficient models”; “random-effects models”; “random parameter models”… and so on).\nWhat the idea boils down to is that model parameters vary at more than one level. This week, we’re going to explore what that means.\nThroughout this course, we will tend to use the terms “mixed effect model”, “linear mixed model (LMM)” and “multilevel model (MLM)” interchangeably."
  },
  {
    "objectID": "02_intromlm.html#introducing-lme4",
    "href": "02_intromlm.html#introducing-lme4",
    "title": "Multilevel Models",
    "section": "Introducing lme4",
    "text": "Introducing lme4\n\nWe’re going to use the lme4 package, and specifically the functions lmer() and glmer().\n“(g)lmer” here stands for “(generalised) linear mixed effects regression”.\nYou will have seen some use of these functions in the lectures. The broad syntax is:\n\n\nlmer(formula, REML = logical, data = dataframe)\n\n\nWe write the first bit of our formula just the same as our old friend the normal linear model y ~ 1 + x + x2 + ..., where y is the name of our outcome variable, 1 is the intercept (which we don’t have to explicitly state as it will be included anyway) and x, x2 etc are the names of our explanatory variables.\nWith lme4, we now have the addition of __random effect terms)), specified in parenthesis with the | operator (the vertical line | is often found to the left of the z key on QWERTY keyboards).\nWe use the | operator to separate the parameters (intercept, slope etc.) on the LHS, from the grouping variable(s) on the RHS, by which we would like to model these parameters as varying.\nRandom Intercept\nLet us suppose that we wish to model our intercept not as a fixed constant, but as varying randomly according to some grouping around a fixed center. We can such a model by allowing the intercept to vary by our grouping variable (g below):\n\n\nlmer(y ~ 1 + x + (1|g), data = df)\n\n\\[\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{Y_{ij}} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot X_{ij}} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\end{align}\n\\]\n\nRandom Slope\nBy extension we can also allow the effect y~x to vary between groups, by including the x on the left hand side of | in the random effects part of the call to lmer().\n\n\nlmer(y ~ 1 + x + (1 + x |g), data = df)\n\n\\[\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "02_intromlm.html#estimation",
    "href": "02_intromlm.html#estimation",
    "title": "Multilevel Models",
    "section": "Estimation",
    "text": "Estimation\n\nMaximum Likelihood (ML)\nRemember back to DAPR2 when we introduced logistic regression, and we briefly discussed Maximum likelihood in an explanation of how models are fitted.\nThe key idea of maximum likelihood estimation (MLE) is that we (well, the computer) iteratively finds the set of estimates for our model which it considers to best reproduce our observed data. Recall our simple linear regression model of how time spent outdoors (hrs per week) is associated with mental wellbeing:\n\\[\n\\color{red}{Wellbeing_i} = \\color{blue}{\\beta_0 \\cdot{} 1 + \\beta_1 \\cdot{} OutdoorTime_{i}} + \\varepsilon_i\n\\]\nThere are values of \\(\\beta_0\\) and \\(\\beta_1\\) and \\(\\sigma_\\varepsilon\\) which maximise the probability of observing the data that we have. For linear regression, these we obtained these same values a different way, via minimising the sums of squares. And we saw that this is not possible for more complex models (e.g., logistic), which is where we turn to MLE.\n\nTo read about the subtle difference between “likelihood” and “probability”, you can find a short explanation here\n\nIf we are estimating just one single parameter (e.g. a mean), then we can imagine the process of maximum likelihood estimation in a one-dimensional world - simply finding the top of the curve:\n\n\n\n\n\nFigure 1: MLE\n\n\n\n\nHowever, our typical models estimate a whole bunch of parameters. The simple regression model above is already having to estimate \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma_\\varepsilon\\), and our multi-level models have far more! With lots of parameters being estimated and all interacting to influence the likelihood, our nice curved line becomes a complex surface (see Left panel of Figure 2). So what we (our computers) need to do is find the maximum, but avoid local maxima and singularities (see Figure 3).\n\n\n\n\n\nFigure 2: MLE for a more complex model\n\n\n\n\n\n\nRestricted Maximum Likelihood (REML)\nWhen it comes to estimating multilevel models, maximum likelihood will consider the fixed effects as unknown values in its estimation of the variance components (the random effect variances). This leads to biased estimates of the variance components, specifically biasing them toward being too small, especially if \\(n_\\textrm{clusters} - n_\\textrm{level 2 predictors} - 1 < 50\\). Restricted Maximum Likelihood (REML), however, separates the estimation of fixed and random parts of the model, leading to unbiased estimates of the variance components.\n\nlmer() models are by default fitted with REML. This is better for small samples.\n\n\nModel Comparisons in MLM\nWhen we compare models that differ in their fixed effects via comparing model deviance (e.g. the likelihood ratio), REML should not be used as only the variance components are included in the likelihood. Functions like anova() will automatically refit your models with ML for you, but it is worth checking.\nWe cannot compare (either with ML or REML) models that differ in both the fixed and random parts.\n\n\n\nModel Convergence\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning. There are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them).\nFor now, if lmer() gives you convergence errors, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model.\n\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))\n\n\n What is a convergence warning??\n\n\nThere are different techniques for maximum likelihood estimation, which we apply by using different ‘optimisers’. Technical problems to do with model convergence and ‘singular fit’ come into play when the optimiser we are using either can’t find a suitable maximum, or gets stuck in a singularity (think of it like a black hole of likelihood, which signifies that there is not enough variation in our data to construct such a complex model).\n\n\n\n\n\nFigure 3: local/global maxima and singularities"
  },
  {
    "objectID": "02_intromlm.html#cross-sectional-wellbeing-across-scotland",
    "href": "02_intromlm.html#cross-sectional-wellbeing-across-scotland",
    "title": "Multilevel Models",
    "section": "Cross-Sectional: Wellbeing Across Scotland",
    "text": "Cross-Sectional: Wellbeing Across Scotland\n\nRecall our dataset from last week, in which we used linear regression to determine how outdoor time (hours per week) is associated with wellbeing in different local authority areas (LAAs) of Scotland. We have data from various LAAs, from Glasgow City, to the Highlands.\n\nscotmw <- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n\n\n\n\n\n\n\n  \n  \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant ID\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nSelf report estimated number of hours per week spent outdoors\n    wellbeing\nWarwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n    density\nLAA Population Density (people per square km)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nUsing lmer() from the lme4 package, fit a model predict wellbeing from outdoor_time, with by-LAA random intercepts.\nPass the model to summary() to see the output.\n\n\n\n\n Solution \n\n\n\nlibrary(lme4)\nri_model <- lmer(wellbeing ~ outdoor_time + (1 | laa), data = scotmw)\nsummary(ri_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: wellbeing ~ outdoor_time + (1 | laa)\n   Data: scotmw\n\nREML criterion at convergence: 866.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2218 -0.7192  0.1217  0.6395  1.8287 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n laa      (Intercept) 106.23   10.307  \n Residual              25.46    5.046  \nNumber of obs: 132, groups:  laa, 20\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  38.18979    2.64778   14.42\noutdoor_time  0.21349    0.07236    2.95\n\nCorrelation of Fixed Effects:\n            (Intr)\noutdoor_tim -0.463\n\n\n\n\n\n\nQuestion 2\n\n\nSometimes the easiest way to start understanding your model is to visualise it.\nLoad the package broom.mixed. Along with some handy functions tidy() and glance() which give us the information we see in summary(), there is a handy function called augment() which returns us the data in the model plus the fitted values, residuals, hat values, Cook’s D etc..\n\nri_model <- lmer(wellbeing ~ outdoor_time + (1 | laa), data = scotmw)\nlibrary(broom.mixed)\naugment(ri_model)\n\n# A tibble: 132 × 14\n   wellbeing outdoor_t…¹ laa   .fitted .resid  .hat .cooksd .fixed   .mu .offset\n       <dbl>       <dbl> <fct>   <dbl>  <dbl> <dbl>   <dbl>  <dbl> <dbl>   <dbl>\n 1        37          20 West…    32.4  4.64  0.139 7.91e-2   42.5  32.4       0\n 2        34          23 Falk…    31.7  2.28  0.192 3.00e-2   43.1  31.7       0\n 3        39          29 Falk…    33.0  6.00  0.195 2.13e-1   44.4  33.0       0\n 4        42          21 Scot…    40.1  1.95  0.163 1.74e-2   42.7  40.1       0\n 5        37          10 Dumf…    37.4 -0.407 0.167 7.84e-4   40.3  37.4       0\n 6        42          19 Argy…    43.9 -1.91  0.122 1.12e-2   42.2  43.9       0\n 7        38          13 Pert…    46.1 -8.06  0.139 2.39e-1   41.0  46.1       0\n 8        44          21 East…    44.4 -0.414 0.168 8.17e-4   42.7  44.4       0\n 9        47          16 Inve…    42.7  4.30  0.193 1.07e-1   41.6  42.7       0\n10        35          12 Midl…    33.1  1.94  0.161 1.69e-2   40.8  33.1       0\n# … with 122 more rows, 4 more variables: .sqrtXwt <dbl>, .sqrtrwt <dbl>,\n#   .weights <dbl>, .wtres <dbl>, and abbreviated variable name ¹​outdoor_time\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nAdd to the code below to plot the model fitted values, and color them according to LAA. (you will need to edit ri_model to be whatever name you assigned to your model).\n\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = ...... \n\n\n\n\n\n Solution \n\n\n\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWe have just fitted the model:\n\\[\n\\begin{align}\n& \\text{For person } j \\text{ in LAA } i \\\\\n& \\color{red}{\\textrm{Wellbeing}_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot \\textrm{Outdoor Time}_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\end{align}\n\\]\nFor our estimates of \\(\\gamma_{00}\\) (the fixed value around which LAA intercepts vary) and \\(\\beta_1\\) (the fixed estimate of the relationship between wellbeing and outdoor time), we can use fixef().\n\nfixef(ri_model)\n\n (Intercept) outdoor_time \n   38.189795     0.213492 \n\n\nCan you add to the plot in the previous question, a thick black line with the intercept and slope given by fixef()?\n\nHint: geom_abline()\n\n\n\n\n\n Solution \n\n\n\naugment(ri_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_abline(intercept = fixef(ri_model)[1], slope = fixef(ri_model)[2], lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nBy now, you should have a plot which looks more or less like the left-hand figure below (we have added on the raw data - the points).\n\n\n\n\n\n\n\nFigure 4: Model fitted values\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Summary model outputlmer(wellbeing~1 + outdoor_time + (1|laa),data = scotmw)\n\n\n\n\n\n  We’re going to map the parts of the plot in Figure 4 to the summary() output of the model in Figure 5. Match the coloured sections Red, Orange, Yellow and Blue in Figure 5 to the descriptions below of Figure 4 A through D.\n\nwhere the black line cuts the y axis\nthe standard deviation of the distances from all the individual LAA lines to the black line\nthe slope of the black line\nthe standard deviation of the distances from all the individual observations to the line for the LAA to which it belongs.\n\nCan you also match those same coloured sections in Figure 5 to the mathematical terms in the model equation:\n\\[\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{Wellbeing_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot OutdoorTime_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\quad \\\\\n& \\text{where} \\\\\n& \\color{orange}{\\zeta_0}\\color{black} \\sim N(0, \\sigma_{\\color{orange}{\\zeta_{0}}}\\color{black})  \\text{ independently} \\\\\n& \\varepsilon \\sim N(0, \\sigma_{\\varepsilon}) \\text{ independently} \\\\\n\\end{align}\n\\]\n\n\n\n\n Solution \n\n\n\nYellow = B = \\(\\sigma_{\\color{orange}{\\zeta_{0}}}\\)\nRed = D = \\(\\sigma_{\\varepsilon}\\)\n\nBlue = A = \\(\\gamma_{00}\\)\n\nOrange = C = \\(\\beta_{1}\\)\n\n\n\n\n\nQuestion 5\n\n\nFit a model which allows also (along with the intercept) the effect of outdoor_time to vary by-LAA.\nThen, using augment() again, plot the model fitted values. What do you think you will see?\nDoes it look like this model better represents the individual LAAs? Take a look at, for instance, Glasgow City.\n\n\n\n\n Solution \n\n\n\nrs_model <- lmer(wellbeing ~ 1 + outdoor_time + (1 + outdoor_time | laa), data = scotmw)\n\naugment(rs_model) %>%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_point(aes(y=wellbeing), alpha=.4)"
  },
  {
    "objectID": "02_intromlm.html#longitudinal-wellbeing-over-time",
    "href": "02_intromlm.html#longitudinal-wellbeing-over-time",
    "title": "Multilevel Models",
    "section": "Longitudinal: Wellbeing Over Time",
    "text": "Longitudinal: Wellbeing Over Time\n\n\n\nAnother very crucial advantage of these methods is that we can use them to study how people change over time.\n\nWellbeing in Work: Longitudinal Data\nThe Wellbeing in Work data (wellbeingwork3) contains information on employees who were randomly assigned to one of three employment conditions:\n\ncontrol: No change to employment. Employees continue at 5 days a week, with standard allocated annual leave quota.\n\nunlimited_leave : Employees were given no limit to their annual leave, but were still expected to meet required targets as specified in their job description.\nfourday_week: Employees worked a 4 day week for no decrease in pay, and were still expected to meet required targets as specified in their job description.\n\nWellbeing was was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post.\nThe researchers had two main questions:\n\nOverall, did the participants’ wellbeing stay the same or did it change?\nDid the employment condition groups differ in the how wellbeing changed over the assessment period?\n\nThe data is available, in .rda format, at https://uoepsy.github.io/data/wellbeingwork3.rda. You can read it directly into your R environment using:\n\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\n\n\n\nQuestion 6\n\n\n\nQ: Overall, did the participants’ wellbeing stay the same or did it change?\n\nEach of our participants have measurements at 4 assessments. We need to think about what this means for the random effects that we will include in our model (our random effect structure). Would we like our models to accommodate individuals to vary in their overall wellbeing, to vary in how they change in wellbeing over the course of the assessment period, or both?\nTo investigate whether wellbeing changed over the course of the assessments, or whether it stayed the same, we could fit and compare 2 models:\n\nThe “null” or “intercept-only” model.\nA model with wellbeing predicted by time point.\n\nAnd we can then compare them in terms of model fit. As discussed in the lecture, there are lots of ways to assess inference in multilevel models.\nOur sample size here (180 participants, each with 4 observations) is reasonably large given the relative simplicity of our model. We might consider running a straightforward Likelihood Ratio Test using anova(restricted_model, full_model) to compare our two models.\n\n\nRemember, we shouldn’t compare models with different random effect structures.\n\n(For now, don’t worry too much about “singular fits”. We’ll talk more about how we might deal with them next week!)\n\n\n\n\n\n\n Solution \n\n\nThis is our null model:\n\nm.null <- lmer(Wellbeing ~ 1 + (1 | ID), data=wellbeingwork3)\nsummary(m.null)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Wellbeing ~ 1 + (1 | ID)\n   Data: wellbeingwork3\n\nREML criterion at convergence: 4395.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5195 -0.6051 -0.0456  0.5895  3.5966 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept)  4.82    2.195   \n Residual             22.48    4.741   \nNumber of obs: 720, groups:  ID, 180\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  40.0431     0.2408   166.3\n\n\nWe can see the 4.82 / (4.82 + 22.48), or 0.18 of the total variance is attributable to participant-level variation.\nNow lets suppose we want to compare this null model with a model with an effect of TimePoint (to assess whether there is overall change over time). Which model should we compare m.null to?\n\nmodA <- lmer(Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID), data=wellbeingwork3)\nmodB <- lmer(Wellbeing ~ 1 + TimePoint + (1 | ID), data=wellbeingwork3)\n\nA comparison between these m.null and modA will not be assessing the influence of only the fixed effect of TimePoint.\nRemember, we shouldn’t compare models with different random effect structures.\nHowever, modB doesn’t include our by-participant random effects of timepoint, so comparing this to m.null is potentially going to mis-attribute random deviations in participants’ change to being an overall effect of timepoint.\nIf we want to conduct a model comparison to isolate the effect of overall change over time (a fixed effect of TimePoint), we might want to compare these two models:\n\nm.base0 <- lmer(Wellbeing ~ 1 + (1 + TimePoint | ID), data=wellbeingwork3)\nm.base <- lmer(Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID), data=wellbeingwork3)\n\nThe first of these models is a bit weird to think about - how can we have by-participant random deviations of TimePoint if we don’t have a fixed effect of TimePoint? That makes very little sense. What it is actually fitting is a model where there is assumed to be no overall effect of TimePoint. So the fixed effect is 0.\n\n# Straightforward LRT\nanova(m.base0, m.base)\n\nData: wellbeingwork3\nModels:\nm.base0: Wellbeing ~ 1 + (1 + TimePoint | ID)\nm.base: Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID)\n        npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm.base0    5 4202.4 4225.2 -2096.2   4192.4                         \nm.base     6 4171.7 4199.2 -2079.8   4159.7 32.649  1  1.104e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nQuestion 7\n\n\n\nQ: Did the employment condition groups differ in the how wellbeing changed over the assessment period?\n\n\nHint: It helps to break it down. There are two questions here:\n\ndo groups differ overall?\n\ndo groups differ over time?\n\nWe can begin to see that we’re asking two questions about the Condition variable here: “is there an effect of Condition?” and “Is there an interaction between TimePoint and Condition?”.\nTry fitting two more models which incrementally build these levels of complexity, and compare them (perhaps to one another, perhaps to models from the previous question - think about what each comparison is testing!)\n\n\n\n\n\n Solution \n\n\n\nm.int <- lmer(Wellbeing ~ 1 + TimePoint + Condition + (1 + TimePoint | ID), \n              data=wellbeingwork3)\nm.full <- lmer(Wellbeing ~ 1+ TimePoint*Condition + (1 + TimePoint | ID), \n               data=wellbeingwork3)\n\nWe’re going to compare each model to the previous one to examine the improvement in fit due to inclusion of each parameter. We could do this quickly with\n\nanova(m.base0, m.base, m.int, m.full)\n\nData: wellbeingwork3\nModels:\nm.base0: Wellbeing ~ 1 + (1 + TimePoint | ID)\nm.base: Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID)\nm.int: Wellbeing ~ 1 + TimePoint + Condition + (1 + TimePoint | ID)\nm.full: Wellbeing ~ 1 + TimePoint * Condition + (1 + TimePoint | ID)\n        npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm.base0    5 4202.4 4225.2 -2096.2   4192.4                         \nm.base     6 4171.7 4199.2 -2079.8   4159.7 32.649  1  1.104e-08 ***\nm.int      8 4164.3 4200.9 -2074.2   4148.3 11.393  2   0.003358 ** \nm.full    10 4144.6 4190.4 -2062.3   4124.6 23.711  2  7.098e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nConditions differed overall in wellbeing change \\(\\chi^2(2)=11.39, p = .003\\)\nConditions differed in change over assessment period \\(\\chi^2(2)=23.71, p < .001\\)\n\n\n\n\n\nQuestion 8\n\n\n\nExamine the parameter estimates and interpret them (i.e., what does each parameter represent?)\nMake a graph of the model fit and the observed data.\n\n\nHints:\n\nWe can get the fixed effects using fixef(model), and we can also use tidy(model) from the broom.mixed package, and similar to lm models in DAPR2, we can pull out the bit of the summary() using summary(model)$coefficients.\n\nThere are lots of ways you can visualise the model, try a couple:\n\nUsing the effects package, does this help: as.data.frame(effect(\"TimePoint:Condition\", model))\n\nUsing fitted(model)\nUsing augment() from the broom.mixed package.\n\nsjPlot, as we used in DAPR2\n\n\n\n\n\n\n\n Solution \n\n\n\n\n                                   Estimate Std. Error t value\n(Intercept)                          38.352      0.398  96.456\nTimePoint                            -0.023      0.325  -0.072\nConditionunlimited_leave             -0.018      0.562  -0.033\nConditionfourday_week                -0.260      0.562  -0.462\nTimePoint:Conditionunlimited_leave    1.357      0.460   2.951\nTimePoint:Conditionfourday_week       2.282      0.460   4.963\n\n\n\n(Intercept) ==> Wellbeing at baseline in ‘control’ group\nTimePoint ==> Slope of welleing change in ‘control’ group\nConditionunlimited_leave ==> baseline wellbeing difference from ‘unlimited_leave’ group relative to ‘control’ group\nConditionfourday_week ==> baseline wellbeing difference from ‘fourday_week’ group relative to ‘control’ group\nTimePoint:Conditionunlimited_leave ==> slope of wellbeing change in ‘unlimited_leave’ group relative to ‘control’ group\nTimePoint:Conditionfourday_week ==> slope of wellbeing change in ‘fourday_week’ group relative to ‘control’ group\n\n\nCompared to the control group, wellbeing increased by 1.35 points/year more for employees with unlimited leave, and by 2.28 points/year for employees on the 4 day week.\n\nTo visualise the model fitted values and observed data, there are various options to choose from.\n\nUsing the effect() function (and then adding the means and SEs from the original data):\n\n\nef <- as.data.frame(effect(\"TimePoint:Condition\", m.full))\n\nggplot(ef, aes(TimePoint, fit, color=Condition)) + \n  geom_line() +\n  stat_summary(data=wellbeingwork3, aes(y=Wellbeing), \n               fun.data=mean_se, geom=\"pointrange\", size=1) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nUsing the fitted() function to extract and plot fitted values from the model:\n\n\nggplot(wellbeingwork3, aes(TimePoint, Wellbeing, color=Condition)) + \n  stat_summary(fun.data=mean_se, geom=\"pointrange\", size=1) + \n  stat_summary(aes(y=fitted(m.full)), fun=mean, geom=\"line\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nOr using augment():\n\n\naugment(m.full) %>%\nggplot(., aes(TimePoint, Wellbeing, color=Condition)) + \n  stat_summary(fun.data=mean_se, geom=\"pointrange\", size=1) + \n  stat_summary(aes(y=.fitted), fun=mean, geom=\"line\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nfinally, sjPlot can give us the model fitted values, but it’s trickier to add on the observed means. We can add the raw data using show.data=TRUE, but that will make it a bit messier\n\n\nlibrary(sjPlot)\nplot_model(m.full, type=\"int\")"
  },
  {
    "objectID": "02_intromlm.html#repeated-measures-basketballhrv",
    "href": "02_intromlm.html#repeated-measures-basketballhrv",
    "title": "Multilevel Models",
    "section": "Repeated Measures: Basketball/HRV",
    "text": "Repeated Measures: Basketball/HRV\nWhile the wellbeing example considers the groupings or ‘clusters’ of different LAAs, a more relate-able grouping in psychological research is that of several observations belonging to the same individual. One obvious benefit of this is that we can collect many more observations with fewer participants, and account for the resulting dependency of observations.\n\nRecall the data from the previous week, from an experiment in which heart rate variability (HRV) was measured for amateur basketball players when tasked with scoring a goal with varying levels and type of potential loss/reward.\nA separate group of researchers conducted a replication of this experiment with 15 participants. There were some issues with the HRV measurements resulting in some missing data, and one participant being excluded completely (meaning a slightly unbalanced design in that 8 participants were in one condition and only 7 in the other).\nYou can find the data at: https://uoepsy.github.io/data/bball_replication.csv It contains the following variables:\n\n\n\n\n\n\n  \n  \n    \n      variable\n      description\n    \n  \n  \n    stakes\nSize of reward (points to be won/lost on a given trial). Ranges 1 to 20\n    condition\nExperimental Condition: Whether the participant was playing for monetary reward ('money') or for a place on the scoreboard ('kudos')\n    sub\nParticipant Identifier\n    throw\nWhether the participant successfully completed the trial\n    trial_no\nTrial Number (1 to 20)\n    hrv\nAverage Heart Rate Variability over the 10 seconds prior to throwing\n  \n  \n  \n\n\n\n\n\n\nQuestion 9\n\n\nRecall that the research question was concerned with how the size and type of potential reward influence stress levels (as measured by heart rate variability):\n\nHow do the size and type of potential reward/loss interact to influence levels of stress?\n\nFit a multi-level model to examine the effects of size and type of reward on HRV, and their interaction.\n\nRemember to think about:\n\nwhat is our outcome variable of interest?\nwhat are our predictor variables that we are interested in?\n\nthese should be in the fixed effects part.\n\n\nwhat is the clustering?\n\nthis should be the random effects (1 | cluster) part\n\ndoes size of reward (stakes) vary within clusters, or between?\n\nif so, we might be able to fit a random slope of stakes | cluster. if not, then it doesn’t make sense to do so.\n\n\ndoes type of reward (condition) vary within clusters, or between? - if so, we might be able to fit a random slope of condition | cluster. if not, then it doesn’t make sense to do so.\n\nIf you get an error about model convergence, consider changing the optimiser (see above)\n\n\n\n\n\n Solution \n\n\n\nmod <- lmer(hrv ~ stakes * condition + \n              (1 + stakes | sub), data = bballrep,\n            control = lmerControl(optimizer=\"bobyqa\"))\nsummary(mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: hrv ~ stakes * condition + (1 + stakes | sub)\n   Data: bballrep\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 867\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6415 -0.6182 -0.0401  0.6038  2.7335 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n sub      (Intercept) 2.86473  1.6926        \n          stakes      0.01661  0.1289   -0.85\n Residual             0.87768  0.9368        \nNumber of obs: 288, groups:  sub, 15\n\nFixed effects:\n                      Estimate Std. Error t value\n(Intercept)            4.76028    0.61973   7.681\nstakes                 0.03215    0.04749   0.677\nconditionmoney        -0.24945    0.90583  -0.275\nstakes:conditionmoney -0.05519    0.06944  -0.795\n\nCorrelation of Fixed Effects:\n            (Intr) stakes cndtnm\nstakes      -0.851              \nconditinmny -0.684  0.582       \nstks:cndtnm  0.582 -0.684 -0.850\n\n\n\n\n\nWe now have a model, but we don’t have any p-values, confidence intervals, or inferential criteria on which to draw conclusions.\nIn the longitudinal study of wellbeing over time, we did a series of model comparisons, performing tests of the incremental inclusion of additional parameters. In the Basketball/HRV example we went straight for the full model. This is in part because the two research aims of the longitudinal example can be matched two models (one for the “overall” trajectory, and one looking at differences between groups), whereas the Basketball/HRV research question simply is interested in the theorised stakes*condition interaction.\nThere are some options here for us to choose from for this model: we can either perform tests against the null that certain parameter estimates are equal to zero (i.e. testing that the interaction is zero), or ca wen fit a reduced model and conduct model comparisons between that and the full model (thereby isolating and testing the improvement in the model due to including the interaction term).\nThere are different methods of implementing these in R, as detailed in the table below. Standard likelihood ratio tests require models to be fitted with ML, and can be less reliable when samples are small (at any level). Often, approximations of the degrees of freedom are preferable, in part because these allow models to be fitted with REML. The more computationally expensive bootstrapping approach is perhaps the most recommended approach as it can provide more accurate p-values for the likelihood ratio test, as well as confidence intervals for our estimates, but for larger models it can take a lot of time to compute. Additionally, when performing the bootstrap it is important to watch out for issues with convergence in the bootstrap iterations - it may indicate your model needs simplification.\n\n\n\n\n\n\n\n\nMethod\nModel Comparison\nParameter Estimation\n\n\n\n\nApproximations to \\(ddf\\)\nKenward-Rogers: KRmodcomp(model2, model1) from the pbkrtest package\nSatterthwaite: load the lmerTest package and re-fit your model (the summary output will then have p-values)\n\n\nLikelihood Ratio Test\nanova(model1, model2)\n\n\n\nParametric Bootstrap\nPBmodcomp(model2, model1) from the pbkrtest package\nconfint(model, method=\"boot\")\n\n\n\n\nQuestion 10\n\n\nThis study is interested in whether the previously reported interaction between size (stakes) and type (condition) of reward on stress levels - measured by heart-rate variability (hrv) - replicates in their new sample.\nPick a method of your choosing and perform a test of/provide an interval for the relevant effect of interest.\n\n\n\n\n Solution \n\n\nIn this case we have \\(n=15\\) participants (our level 2 sample size), and each participant has approximately 20 observations (but some have missingness). The sample size might be a bit small for standard likelihood ratio tests (i.e. comparing models fitted with ML rather than REML). We would be better off using models fitted with REML because they will provide more accurate estimates of the variance components (the 1+stakes|sub bit), and so better estimates of the standard errors of the fixed effects.\nWe’ll go through each approach here so you can see what it looks like. There’s no right answer here.\nStandard LRT\nIf we choose a model comparison approach, we need to isolate the interaction term, because that’s what we’re interested in:\n\nmod_res <- lmer(hrv ~ stakes + condition + (1 + stakes | sub), data = bballrep,\n            control = lmerControl(optimizer=\"bobyqa\"))\nmod_full <- lmer(hrv ~ stakes * condition + (1 + stakes | sub), data = bballrep,\n            control = lmerControl(optimizer=\"bobyqa\"))\n\nThis needs models fitted with ML rather than REML, but it will automatically re-fit them for you:\n\nanova(mod_res, mod_full)\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: bballrep\nModels:\nmod_res: hrv ~ stakes + condition + (1 + stakes | sub)\nmod_full: hrv ~ stakes * condition + (1 + stakes | sub)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nmod_res     7 872.33 897.97 -429.17   858.33                     \nmod_full    8 873.62 902.93 -428.81   857.62 0.7111  1     0.3991\n\n\n\nInclusion of the interaction between size and type of reward was not found to improve model fit, as indicated by a likelihood ratio test (\\(\\chi^2(1) = 0.71, p = .399\\)).\n\ndf approximations\nIf instead we choose the approximation for degrees of freedom, then generally speaking the Kenward Rogers approach is preferable as it is a little more conservative.\n\nlibrary(pbkrtest)\nKRmodcomp(mod_full, mod_res)\n\nlarge : hrv ~ stakes * condition + (1 + stakes | sub)\nsmall : hrv ~ stakes + condition + (1 + stakes | sub)\n         stat     ndf     ddf F.scaling p.value\nFtest  0.6317  1.0000 12.9927         1   0.441\n\n\n\nInclusion of the interaction between size and type of reward was not found to improve model fit (\\(F(1, 13^*) = 0.63, p = .441\\)).\n\\(\\textrm{ }^*\\): denominator degrees of freedom approximated using Kenward-Rogers method.\n\nAlternatively, we can perform the tests on the fixed effects themselves. This is more like what you will remember from DAPR2, where we get a table of effects and we can interpret each one in turn. For now we’ll just focus on the interaction term as that is the main one of interest.\n\nlibrary(lmerTest)\nmod_full <- lmer(hrv ~ stakes * condition + (1 + stakes| sub), data = bballrep,\n            control = lmerControl(optimizer=\"bobyqa\"))\nsummary(mod_full)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: hrv ~ stakes * condition + (1 + stakes | sub)\n   Data: bballrep\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 867\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6415 -0.6182 -0.0401  0.6038  2.7335 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n sub      (Intercept) 2.86473  1.6926        \n          stakes      0.01661  0.1289   -0.85\n Residual             0.87768  0.9368        \nNumber of obs: 288, groups:  sub, 15\n\nFixed effects:\n                      Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)            4.76028    0.61973 13.08438   7.681 3.34e-06 ***\nstakes                 0.03215    0.04749 13.09616   0.677    0.510    \nconditionmoney        -0.24945    0.90583 13.00709  -0.275    0.787    \nstakes:conditionmoney -0.05519    0.06944 13.04372  -0.795    0.441    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) stakes cndtnm\nstakes      -0.851              \nconditinmny -0.684  0.582       \nstks:cndtnm  0.582 -0.684 -0.850\n\n\n\nThere was no significant interaction between size and type of reward (\\(\\beta = -0.06, SE = 0.07, t(13.04^*) = -0.8, p = .441\\)).\n\\(\\textrm{ }^*\\): denominator degrees of freedom approximated using Satterthwaite method.\n\nparametric bootstrapping\nWe could instead perform the parametric bootstrap for the likelihood ratio test instead. This requires us to remove any incomplete cases from the dataset first, and then re-fit the models.\n\n\n\nThis will also (just like the standard LRT) refit the models for us with ML rather than REML (it just won’t tell us that it’s doing so!)\n\nlibrary(pbkrtest)\nbballrep2 = na.omit(bballrep)\nmod_res <- lmer(hrv ~ stakes + condition + (1 + stakes | sub), data = bballrep2,\n            control = lmerControl(optimizer=\"bobyqa\"))\nmod_full <- lmer(hrv ~ stakes * condition + (1 + stakes | sub), data = bballrep2,\n            control = lmerControl(optimizer=\"bobyqa\"))\nPBmodcomp(mod_full, mod_res)\n\nBootstrap test; time: 129.97 sec; samples: 1000; extremes: 468;\nlarge : hrv ~ stakes * condition + (1 + stakes | sub)\nhrv ~ stakes + condition + (1 + stakes | sub)\n         stat df p.value\nLRT    0.6122  1  0.4340\nPBtest 0.6122     0.4685\n\n\n\nInclusion of the interaction between size and type of reward was not found to improve model fit, as indicated by a parametric bootstrapped (\\(k=1000\\)) likelihood ratio test (\\(\\Delta 2loglik = 0.612, p = .469\\)).\n\nLastly, we could also opt to construct parametric bootstrapped confidence intervals around our fixed effect estimates:\n\n\n\n\nconfint(mod_full, method=\"boot\")\n\n                            2.5 %      97.5 %\n.sig01                 1.01777802  2.41524811\n.sig02                -0.96252286 -0.61207542\n.sig03                 0.07907304  0.18089815\n.sigma                 0.86358573  1.02589783\n(Intercept)            3.50865914  6.14629755\nstakes                -0.06090850  0.12560977\nconditionmoney        -2.07035051  1.50523773\nstakes:conditionmoney -0.19634219  0.06820011\n\n\n\nThere was no significant interaction between size and type of reward (\\(\\beta = -0.06, \\text{ parametric bootstrapped 95\\% CI: } [-0.196, 0.068]\\))."
  },
  {
    "objectID": "03_assumptranef.html",
    "href": "03_assumptranef.html",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "",
    "text": "Preliminaries\n\nCreate a new RMarkdown document or R script (whichever you like) for this week."
  },
  {
    "objectID": "03_assumptranef.html#singular-fits",
    "href": "03_assumptranef.html#singular-fits",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "Singular fits",
    "text": "Singular fits\nYou may have noticed that some of our models over the last few weeks have been giving a warning: boundary (singular) fit: see ?isSingular.\nUp to now, we’ve been largely ignoring these warnings. However, this week we’re going to look at how to deal with this issue.\n\nboundary (singular) fit: see ?isSingular\n\nThe warning is telling us that our model has resulted in a ‘singular fit’. Singular fits often indicate that the model is ‘overfitted’ - that is, the random effects structure which we have specified is too complex to be supported by the data.\nPerhaps the most intuitive advice would be remove the most complex part of the random effects structure (i.e. random slopes). This leads to a simpler model that is not over-fitted. In other words, start simplying from the top (where the most complexity is) to the bottom (where the lowest complexity is). Additionally, when variance estimates are very low for a specific random effect term, this indicates that the model is not estimating this parameter to differ much between the levels of your grouping variable. It might, in some experimental designs, be perfectly acceptable to remove this or simply include it as a fixed effect.\nA key point here is that when fitting a mixed model, we should think about how the data are generated. Asking yourself questions such as “do we have good reason to assume subjects might vary over time, or to assume that they will have different starting points (i.e., different intercepts)?” can help you in specifying your random effect structure\nYou can read in depth about what this means by reading the help documentation for ?isSingular. For our purposes, a relevant section is copied below:\n… intercept-only models, or 2-dimensional random effects such as intercept + slope models, singularity is relatively easy to detect because it leads to random-effect variance estimates of (nearly) zero, or estimates of correlations that are (almost) exactly -1 or 1."
  },
  {
    "objectID": "03_assumptranef.html#convergence-warnings",
    "href": "03_assumptranef.html#convergence-warnings",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "Convergence warnings",
    "text": "Convergence warnings\nIssues of non-convergence can be caused by many things. If you’re model doesn’t converge, it does not necessarily mean the fit is incorrect, however it is is cause for concern, and should be addressed, else you may end up reporting inferences which do not hold.\nThere are lots of different things which you could do which might help your model to converge. A select few are detailed below:\n\ndouble-check the model specification and the data\nadjust stopping (convergence) tolerances for the nonlinear optimizer, using the optCtrl argument to [g]lmerControl. (see ?convergence for convergence controls).\n\nWhat is “tolerance”? Remember that our optimizer is the the method by which the computer finds the best fitting model, by iteratively assessing and trying to maximise the likelihood (or minimise the loss).\n\n\n\n\n\n\n\nFigure 1: An optimizer will stop after a certain number of iterations, or when it meets a tolerance threshold\n\n\n\n\n\ncenter and scale continuous predictor variables (e.g. with scale)\nChange the optimization method (for example, here we change it to bobyqa): lmer(..., control = lmerControl(optimizer=\"bobyqa\"))\nglmer(..., control = glmerControl(optimizer=\"bobyqa\"))\nIncrease the number of optimization steps: lmer(..., control = lmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=50000))\nglmer(..., control = glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=50000))\nUse allFit() to try the fit with all available optimizers. This will of course be slow, but is considered ‘the gold standard’; “if all optimizers converge to values that are practically equivalent, then we would consider the convergence warnings to be false positives.”\nConsider simplifying your model, for example by removing random effects with the smallest variance (but be careful to not simplify more than necessary, and ensure that your write up details these changes)"
  },
  {
    "objectID": "03_assumptranef.html#crossed-ranefs",
    "href": "03_assumptranef.html#crossed-ranefs",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "Crossed Ranefs",
    "text": "Crossed Ranefs\n\nData: Test-enhanced learning\nAn experiment was run to conceptually replicate “test-enhanced learning” (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (StudyStudy), the other group studied the material once then did a test (StudyTest). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (Test_word). One of the researchers’ questions concerned how test-enhanced learning influences time-to-recall.\nThe critical (replication) prediction is that the StudyStudy group should perform somewhat better on the immediate recall test, but the StudyTest group will retain the material better and thus perform better on the 1-week follow-up test.\n\n\n\n\n \n  \n    variable \n    description \n  \n \n\n  \n    Subject_ID \n    Unique Participant Identifier \n  \n  \n    Group \n    Group denoting whether the participant studied the material twice (StudyStudy), or studied it once then did a test (StudyTest) \n  \n  \n    Delay \n    Time of recall test ('min' = Immediate, 'week' = One week later) \n  \n  \n    Test_word \n    Word being recalled (175 different test words) \n  \n  \n    Correct \n    Whether or not the word was correctly recalled \n  \n  \n    Rtime \n    Time to recall word (milliseconds) \n  \n\n\n\n\n\nThe following code loads the data into your R environment by creating a variable called tel:\n\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n\n\n\nQuestion 5\n\n\nLoad and plot the data.\nFor this week, we’ll use Reaction Time as our proxy for the test performance, so you’ll probably want that variable on the y-axis.\nDoes it look like the effect was replicated?\n\n\n\n\n Solution \n\n\n\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n\nYou can make use of stat_summary()!\n\nggplot(tel, aes(Delay, Rtime, col=Group)) + \n  stat_summary(fun.data=mean_se, geom=\"pointrange\")+\n  theme_light()\n\nIt’s more work, but some people might rather calculate the numbers and then plot them directly. It does just the same thing:\n\ntel %>% \n  group_by(Delay, Group) %>%\n  summarise(\n    mean = mean(Rtime),\n    se = sd(Rtime)/sqrt(n())\n  ) %>%\n  ggplot(., aes(x=Delay, col = Group)) +\n  geom_pointrange(aes(y=mean, ymin=mean-se, ymax=mean+se))+\n  theme_light() +\n  labs(y = \"Response Time (ms)\")\n\n\n\n\n\n\n\n\nThat looks like test-enhanced learning to me!\n\n\n\n\nQuestion 6\n\n\n\nThe critical (replication) prediction is that the StudyStudy group should perform somewhat better on the immediate recall test, but the StudyTest group will retain the material better and thus perform better on the 1-week follow-up test.\n\nTest the critical hypothesis using a multi-level model.\nTry to fit the maximally complex random effect structure that is supported by the experimental design.\nNOTE: Your model probably won’t converge. We’ll deal with that in the next question\n\nHints:\n\nWe can expect variability across subjects (some people are better at learning than others) and across items (some of the recall items are harder than others). How should this be represented in the random effects?\nIf a model takes ages to fit, you might want to cancel it by pressing the escape key. It is normal for complex models to take time, but for the purposes of this task, give up after a couple of minutes, and try simplifying your model.\n\n\n\n\n\n\n Solution \n\n\nWe know that we are interested in the Rtime ~ Delay * Group interaction, because we want to see how people perform at one week vs one minute (the Delay variable) and whether this is different between those in each condition (the Group variable, specifying whether participants are in the ‘StudyStudy’ condition or the ‘StudyTest’ condition).\nWe want to include Subject random effects and Item random effects, and these groupings are crossed, so we’re going to have (1 + ??? | Subject_ID) + (1 + ??? | Test_word). Each subject is only in one group, so we can only have (1 + Delay | Subject_ID), but the Test_word items are seen by subjects in both groups, and at both timepoints, so we can have (1 + Delay * Group | Test_word).\nThis one will probably take a little bit of time:\n\nm <- lmer(Rtime ~ Delay * Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay * Group | Test_word),\n           data=tel, control=lmerControl(optimizer = \"bobyqa\"))\n\n\n\n\n\nQuestion 7\n\n\nOften, models with maximal random effect structures will not converge, or will obtain a singular fit. One suggested approach here is to simplify the model until you achieve convergence (Barr et al., 2013).\nIncrementally simplify your model from the previous question until you obtain a model that converges and is not a singular fit.\n\nHint: you can look at the variance estimates and correlations easily by using the VarCorr() function. What jumps out?\n\n\n\n\n\n Solution \n\n\nThere are very high correlations with the by-item random effects of the interaction Delay:Group. We might expect that because it’s an interaction term, but it is quite a complex bit of the model, so let’s remove it:\n\nVarCorr(m)\n\n Groups     Name                     Std.Dev. Corr                \n Test_word  (Intercept)               17.8642                     \n            Delayweek                 13.2793 -0.251              \n            GroupStudyTest            18.0270 -0.797 -0.385       \n            Delayweek:GroupStudyTest  13.1273  0.972 -0.016 -0.917\n Subject_ID (Intercept)               40.5201                     \n            Delayweek                  7.4486 -0.038              \n Residual                            240.3113                     \n\n\n\nm1 <- lmer(Rtime ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay + Group | Test_word),\n           data=tel, control=lmerControl(optimizer = \"bobyqa\"))\nVarCorr(m1)\n\n Groups     Name           Std.Dev. Corr         \n Test_word  (Intercept)     14.5561              \n            Delayweek       14.6914  0.152       \n            GroupStudyTest  12.3210 -0.612 -0.875\n Subject_ID (Intercept)     40.5190              \n            Delayweek        7.4365 -0.038       \n Residual                  240.3436              \n\nisSingular(m1)\n\n[1] TRUE\n\n\nWe still have a singular fit here, and we still have quite high1 correlations between by-testword random effects. Thinking about the study, if we are going to remove one of the by-testword random effects (Delay or Group), which one do we consider to be more theoretically justified? Is the effect of Delay likely to vary by test-words? More so than the effect of group is likely to vary by test-words? Quite possibly - there’s no obvious reason for certain words to be more memorable for people in one group vs another. But there is reason for words to vary in the effect that delay of one week has - how familiar a word is will likely influence the amount to which a week’s delay has on recall.\nLet’s remove the by-testword random effect of group.\n\nm2 <- lmer(Rtime ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay | Test_word),\n           data=tel, control=lmerControl(optimizer = \"bobyqa\"))\nisSingular(m2)\n\n[1] FALSE\n\nVarCorr(m2)\n\n Groups     Name        Std.Dev. Corr  \n Test_word  (Intercept)  11.6972       \n            Delayweek    13.5689 -0.236\n Subject_ID (Intercept)  40.5156       \n            Delayweek     7.4001 -0.037\n Residual               240.4432       \n\n\nHooray, the model converged!\n\nsummary(m2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Rtime ~ Delay * Group + (1 + Delay | Subject_ID) + (1 + Delay |  \n    Test_word)\n   Data: tel\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 241671.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8200 -0.6685 -0.0096  0.6760  4.0783 \n\nRandom effects:\n Groups     Name        Variance Std.Dev. Corr \n Test_word  (Intercept)   136.82  11.70        \n            Delayweek     184.12  13.57   -0.24\n Subject_ID (Intercept)  1641.52  40.52        \n            Delayweek      54.76   7.40   -0.04\n Residual               57812.93 240.44        \nNumber of obs: 17498, groups:  Test_word, 175; Subject_ID, 50\n\nFixed effects:\n                         Estimate Std. Error t value\n(Intercept)               744.047      8.925  83.363\nDelayweek                  26.766      5.448   4.913\nGroupStudyTest            -18.032     12.560  -1.436\nDelayweek:GroupStudyTest  -17.647      7.566  -2.332\n\nCorrelation of Fixed Effects:\n            (Intr) Delywk GrpStT\nDelayweek   -0.285              \nGropStdyTst -0.704  0.200       \nDlywk:GrpST  0.202 -0.694 -0.288\n\n\nLet’s quickly visualise the interaction. Remember, lower reaction times are better here. It looks like we have replicated the hypothesised effect:\n\nlibrary(sjPlot)\nplot_model(m2, type=\"int\")"
  },
  {
    "objectID": "03_assumptranef.html#nested-random-effects",
    "href": "03_assumptranef.html#nested-random-effects",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "Nested Random Effects",
    "text": "Nested Random Effects\n\nData: Naming\n74 children from 10 schools were administered the full Boston Naming Test (BNT-60) on a yearly basis for 5 years to examine development of word retrieval. Five of the schools taught lessons in a bilingual setting with English as one of the languages, and the remaining five schools taught in monolingual English.\nThe data is available at https://uoepsy.github.io/data/bntmono.csv.\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nchild_id\nunique child identifier\n\n\nschool_id\nunique school identifier\n\n\nBNT60\nscore on the Boston Naming Test-60. Scores range from 0 to 60\n\n\nschoolyear\nYear of school\n\n\nmlhome\nMono/Bi-lingual School. 0 = Bilingual, 1 = Monolingual\n\n\n\n\n\n\n\nQuestion 8\n\n\nLet’s start by thinking about our clustering - we’d like to know how much of the variance in BNT60 scores is due to the clustering of data within children, who are themselves within schools. One easy way of assessing this is to fit an intercept only model, which has the appropriate random effect structure.\nUsing the model below, calculate the proportion of variance attributable to the clustering of data within children within schools.\n\nbnt_null <- lmer(BNT60 ~ 1 +  (1 | school_id/child_id), data = bnt)\n\n\nHint: the random intercept variances are the building blocks here. There are no predictors in this model, so all the variance in the outcome gets attributed to either school-level nesting, child-level nesting, or else is lumped into the residual.\n\n\n\n\n\n Solution \n\n\nAs we can see from summary(bnt_null), the random intercept variances are 36.52 for child-level, 28.76 for school-level, and the residual variance is 99.11.\nSo the nesting of data within children within schools accounts for \\(\\frac{36.52 + 28.76}{36.52 + 28.76 + 99.11} = 0.397\\) of the variance in the outcome BNT60.\nWe can calculate this directly using the model estimates if we want, but sometimes doing it by hand is more straightforward.\n\nas.data.frame(VarCorr(bnt_null)) %>%\n  select(grp, vcov) %>% \n  mutate(\n    prop_var = vcov / sum(vcov),\n    prop_var2 = cumsum(prop_var)\n  )\n\n                 grp     vcov  prop_var prop_var2\n1 child_id:school_id 36.52302 0.2221657 0.2221657\n2          school_id 28.76157 0.1749536 0.3971194\n3           Residual 99.11079 0.6028806 1.0000000\n\n\n\n\n\n\nQuestion 9\n\n\nFit a model examining the interaction between the effects of school year and mono/bilingual teaching on word retrieval, with random intercepts only for children and schools.\n\nHint: make sure your variables are of the right type first - e.g. numeric, factor etc\n\nExamine the fit and consider your model assumptions, and assess what might be done to improve the model in order to make better statistical inferences.\n\n\n\n\n Solution \n\n\nThis is a quick way to make a set of variables factors:\n\nbnt <- bnt %>% mutate(across(c(mlhome, school_id, child_id), factor))\n\nAnd now let’s fit our model:\n\nbntm0 <- lmer(BNT60 ~ schoolyear * mlhome + (1 | school_id/child_id), data = bnt)\n\nResiduals don’t look zero mean:\n\nplot(bntm0, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nIt looks a little like, compared to our model (black lines below) the children’s scores (coloured lines) are more closely clustered together when they start school, and then they are more spread out by the end of the study. The fact that we’re fitting the same slope for each child is restricting us here, so we should try fitting random effects of schoolyear.\n\naugment(bntm0) %>%\n  ggplot(aes(x=schoolyear, col=child_id)) + \n  geom_point(aes(y = BNT60))+\n  geom_path(aes(y = BNT60))+\n  geom_path(aes(y = .fitted), col=\"black\", alpha=.3)+\n  guides(col=\"none\")+\n  facet_wrap(~school_id)\n\n\n\n\n\n\n\n\n\nbntm1 <- lmer(BNT60 ~ schoolyear * mlhome + (1 + schoolyear | school_id/child_id), data = bnt)\nplot(bntm1, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nMuch better!\nLet’s do some quick diagnostic checks for influence:\n\ninf1 <- hlm_influence(bntm1, level=1)\ndotplot_diag(inf1$cooksd, cutoff = \"internal\")\n\n\n\n\n\n\n\n\nIf you check in the help for dotplot_diag(), it tells you that\n\nwe can add an index for the labels, and\nthe coordinates (x,y) are flipped. We’re telling R to change the limits of the y axis, but actually it is the x axis. This is just because we want to see the label for that point out to the right.\n\n\ninfchild <- hlm_influence(bntm1, level=\"child_id:school_id\")\ndotplot_diag(infchild$cooksd, cutoff = \"internal\", index = infchild$`child_id:school_id`) + \n  scale_y_continuous(limits=c(0,.05))\n\n\n\n\n\n\n\n\nAnd then we can examine the effects to the fixed effects and our standard errors when we remove this child:\n\ndel94 <- case_delete(bntm1, level=\"child_id:school_id\", delete = \"ID94:SC9\")\ncbind(del94$fixef.original, del94$fixef.delete)\n\n                        [,1]       [,2]\n(Intercept)         6.265626  6.2627962\nschoolyear          6.371168  6.3639335\nmlhome1             0.138711 -0.3052998\nschoolyear:mlhome1 -2.603763 -2.3701181\n\n\n\n Optional: Case deletion influence on standard errors\n\n\nWe can examine the influence that deleting a case has on the standard errors. The standard errors are the square-root of the diagonal of the model-implied variance-covariance matrix:\n\ncbind( \n  sqrt(diag(del94$vcov.original)),\n  sqrt(diag(del94$vcov.delete))\n)\n\n                        [,1]      [,2]\n(Intercept)        1.0413285 0.9557645\nschoolyear         0.7719321 0.6717837\nmlhome1            1.4683927 1.3525492\nschoolyear:mlhome1 1.0875897 0.9498207\n\n\n\n\n\n\ninfschool <- hlm_influence(bntm1, level=\"school_id\")\ndotplot_diag(infschool$cooksd, cutoff = \"internal\", index = infschool$school_id)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nUsing a method of your choosing, conduct inferences (i.e. obtain p-values or confidence intervals) from your final model and write up the results.\n\n\n\n\n Solution \n\n\n\n\nWe’ll use case-based bootstrapping for a demonstration, but other methods would be appropriate here. We have a large sample of children (74), each with 5 observations. However, we only have 10 schools. A standard likelihood ratio test using anova(model1, model2) might not be preferable here.\nThis took quite a while to run:\n\nlibrary(lmeresampler)\nbntm1BS <- bootstrap(bntm1, .f=fixef, type = \"case\", B = 2000, resample = c(FALSE,TRUE,FALSE))\nconfint(bntm1BS, type = \"perc\")\n\n\n\n# A tibble: 4 × 6\n  term               estimate lower upper type  level\n  <chr>                 <dbl> <dbl> <dbl> <chr> <dbl>\n1 (Intercept)           6.27   5.43  7.12 perc   0.95\n2 schoolyear            6.37   5.72  7.02 perc   0.95\n3 mlhome1               0.139 -1.12  1.40 perc   0.95\n4 schoolyear:mlhome1   -2.60  -3.51 -1.59 perc   0.95\n\n\n\nMultilevel level linear regression was used to investigate childrens’ development of word retrieval over 5 years of school, and whether development was dependent upon the school teaching classes monolingually or bilingually. Initial evaluation of the intercept-only model indicated that the clustering of multiple observations from children within schools accounted for 39.7% of the variance in scores on the Boston Naming Task (BNT60, range 0 to 60). BNT60 scores were modelled with fixed effects of school year (1-5) and monolingual teaching (monolingual vs bilingual, treatment coded with monolingual as the reference level). Random intercepts and slopes of school year were included for schools and for children nested within schools. The model was fitting with maximum likelihood estimation using the default optimiser from the lme4 package (Bates et al., 2015).\n95% Confidence for fixed effect estimates were constructed by case-based bootstrapping with 2000 bootstraps in which children, (but neither observations within children nor the schools within which children were nested) were resampled. Results indicated that children’s scores on the BNT60 increased over the 5 years in which they were studied, with children from bilingual schools increasing in scores by 6.37 ([5.72 – 7.02]) every school year. There was a significant interaction between mono/bilingual schools and changes over the school year, with children from monolingual schools increasing -2.6 ([-3.51 – -1.59]) less than those from bilingual schools for every additional year of school. Full model results can be found in Table 1.\n\nTable 1\n::: {.cell layout-align=“center”}\n\n\n\n\n\n \n\n\n\nBNT 60\n\n\n\n\n\nPredictors\n\n\n\nEstimates\n\n\n\n95% CIbootstrap\n\n\n\n\n\nIntercept\n\n\n6.27\n\n\n5.43 – 7.12\n\n\n\n\n\nSchool Year\n\n\n6.37\n\n\n4.85 – 7.89\n\n\n\n\n\nMonolingualSchool [1]\n\n\n0.14\n\n\n-2.75 – 3.03\n\n\n\n\n\nSchoolYear:MonolingualSchool[1]\n\n\n-2.60\n\n\n-4.74 – -0.46\n\n\n\n\nRandom Effects\n\n\n\n\nσ2\n\n\n8.64\n\n\n\n\nτ00 child_id:school_id\n\n\n1.77\n\n\n\nτ00 school_id\n\n\n3.83\n\n\n\nτ11 child_id:school_id.schoolyear\n\n\n6.83\n\n\n\nτ11 school_id.schoolyear\n\n\n1.89\n\n\n\nρ01 child_id:school_id\n\n\n-0.42\n\n\n\nρ01 school_id\n\n\n-0.39\n\n\n\nICC\n\n\n0.91\n\n\n\nN child_id\n\n\n74\n\n\n\nN school_id\n\n\n10\n\n\n\nObservations\n\n\n370\n\n\n\n\nMarginal R2 / Conditional R2\n\n\n0.420 / 0.947\n\n\n\n\n:::\n\nlibrary(effects)\nas.data.frame(effect(\"schoolyear:mlhome\",bntm1)) %>%\n  ggplot(., aes(x=schoolyear,y=fit,col=mlhome))+\n  geom_pointrange(aes(ymin=lower,ymax=upper))+\n  scale_color_manual(NULL,labels=c(\"Bilingual\",\"Monolingual\"),values=c(\"tomato1\",\"navyblue\"))+\n  labs(x=\"- School Year -\", y=\"BNT-60\")"
  },
  {
    "objectID": "04_centerglmer.html",
    "href": "04_centerglmer.html",
    "title": "Centering in MLM | Logistic MLM",
    "section": "",
    "text": "Centering & Scaling in LM\nWe have some data from a study investigating how perceived persuasiveness of a speaker is influenced by the rate at which they speak (you may remember this from the first report in DAPR2 last year!).\n\ndap2 <- read_csv(\"https://uoepsy.github.io/data/dapr2_2122_report1.csv\")\n\nWe can fit a simple linear regression (one predictor) to evaluate how speech rate (variable sp_rate in the dataset) influences perceived persuasiveness (variable persuasive in the dataset). There are various ways in which we can transform the predictor variable sp_rate, which in turn can alter the interpretation of some of our estimates:\n\n\n\nRaw X\n\nm1 <- lm(persuasive ~ sp_rate, data = dap2)\nsummary(m1)$coefficients\n\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 55.532060  6.4016670  8.674625 6.848945e-15\nsp_rate     -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\nThe intercept and the coefficient for neuroticism are interpreted as:\n\n(Intercept): A audio clip of someone speaking at zero phones per second is estimated as having an average persuasive rating of 55.53.\n\nsp_rate: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nMean-Centered X\nWe can mean center our predictor and fit the model again:\n\ndap2 <- dap2 %>% mutate(sp_rate_mc = sp_rate - mean(sp_rate))\nm2 <- lm(persuasive ~ sp_rate_mc, data = dap2)\nsummary(m2)$coefficients\n\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 52.874667  1.3519418 39.110165 6.429541e-80\nsp_rate_mc  -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_mc: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nStandardised X\nWe can standardise our predictor and fit the model yet again:\n\ndap2 <- dap2 %>% mutate(sp_rate_z = scale(sp_rate))\nm3 <- lm(persuasive ~ sp_rate_z, data = dap2)\nsummary(m3)$coefficients\n\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 52.874667   1.351942 39.110165 6.429541e-80\nsp_rate_z   -0.576077   1.356471 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_z: For every increase of one standard deviation in phones per second, perceived persuasiveness is estimated to decrease by -0.58.\n\nRemember that the scale(sp_rate) is subtracting the mean from each value, then dividing those by the standard deviation. The standard deviation of dap2$sp_rate is:\n\nsd(dap2$sp_rate)\n\n[1] 3.016315\n\n\nso in our variable dap2$sp_rate_z, a change of 3.02 gets scaled to be a change of 1 (because we are dividing by sd(dap2$sp_rate)).\n\ncoef(m1)[2] * sd(dap2$sp_rate)\n\n  sp_rate \n-0.576077 \n\ncoef(m3)[2]\n\nsp_rate_z \n-0.576077 \n\n\n\n\n\nNote that these models are identical. When we conduct a model comparison between the 3 models, the residual sums of squares is identical for all models:\n\nanova(m1,m2,m3)\n\nAnalysis of Variance Table\n\nModel 1: persuasive ~ sp_rate\nModel 2: persuasive ~ sp_rate_mc\nModel 3: persuasive ~ sp_rate_z\n  Res.Df   RSS Df Sum of Sq F Pr(>F)\n1    148 40576                      \n2    148 40576  0         0         \n3    148 40576  0         0         \n\n\n\nWhat changes when you center or scale a predictor in a standard regression model (one fitted with lm())?\n\nThe variance explained by the predictor remains exactly the same\nThe intercept will change to be the estimated mean outcome where that predictor is “0”. Scaling and centering changes what “0” represents, thereby changing this estimate (the significance test will therefore also change because the intercept now has a different meaning)\nThe slope of the predictor will change according to any scaling (e.g. if you divide your predictor by 10, the slope will multiply by 10).\nThe test of the slope of the predictor remains exactly the same.\n\n\n\n\nExercises: Centering in the MLM\n\nData: Hangry\nThe study is interested in evaluating whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nThe data are available at: https://uoepsy.github.io/data/hangry.csv.\n\n\n\n\n \n  \n    variable \n    description \n  \n \n\n  \n    q_irritability \n    Score on irritability questionnaire (0:100) \n  \n  \n    q_hunger \n    Score on hunger questionnaire (0:100) \n  \n  \n    ppt \n    Participant \n  \n  \n    fivetwo \n    Whether the participant follows the five-two diet \n  \n\n\n\n\n\n\n\nQuestion 1\n\n\nRead carefully the description of the study above, and try to write out (in lmer syntax) an appropriate model to test the research aims.\ne.g.:\noutcome ~ explanatory variables + (???? | grouping)\nTry to think about the maximal random effect structure (i.e. everything that can vary by-grouping is estimated as doing so).\nTo help you think through the steps to get from a description of a research study to a model specification, think about your answers to the following questions.\nQ: What is our outcome variable?\n\nHint: The research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet.\n\nQ: What are our explanatory variables?\n\nHint: The research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet.\n\nQ: Is there any grouping (or “clustering”) of our data that we consider to be a random sample? If so, what are the groups?\n\nHint: We can split our data in to groups of each participant. We can also split it into groups of each diet. Which of these groups have we randomly sampled? Do we have a random sample of participants? Do we have a random sample of diets? Another way to think of this is “if i repeated the experiment, what these groups be different?”\n\n\n\n\n\n Solution \n\n\nOur outcome is irritability here, because it is the thing that we are trying to explain through peoples’ hunger levels and diets.\n\nlmer(irritability ~  explanatory variables + (???? | grouping))\n\nWe are interested in the effect of hunger on irritability, and whether this effect is different for the five-two diet. So we are interested in the interaction:\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | grouping))\n\n(remember that hunger + diet + hunger:diet is just a more explicit way of writing hunger*diet).\nIf we did this experiment again, would we have different participants?\nYes. If we did this experiment again, would we have different diets? No, because we’re interested in the specific differences between the five-two diet and no dieting. This means we will likely want to by-participant random deviations (e.g. the ( ... | participant) bit in lmer). But we won’t have by-diet random effects (1 | diet) because the diet differences are the specific differences that we wish to test.\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | participant))\n\nThinking about what can be modelled as randomly varying between participants, we have some options:\n\nparticipants vary in how irritable they are on average\n(the intercept, 1 | participant)\nparticipants vary in how much hunger influences their irritability\n(the effect of hunger, hunger | participant)\nparticipants vary in how much diet influences irritability\n(the effect of diet, diet | participant)\nparticipants vary in how much diet effects hunger’s influence on irritability\n(the interaction between diet and hunger, diet:hunger | participant)\n\nWe can vary 1 and 2, but not 3 and 4. This is because each participant is either following the five-two diet or they are not. So for a single participant, we can’t assess “the effect diet has” on anything, because we haven’t seen that participant under different diets. if we try to plot a single participants’ data, we can see that it is impossible for us to assess “the effect of diet”:\n\n\n\n\n\n\n\n\n\nBy contrast, we can vary the intercept and the effect of hunger, because each participant has multiple values of irritability, and multiple different observations of hunger. We can think about a single participant’s “effect of hunger on irritability” and how we might fit a line to their data:\n\n\n\n\n\n\n\n\n\n\nlmer(irritability ~  hunger + diet + hunger:diet + (1 + hunger | participant))\n\n\n\n\n\nTotal, Within, Between\nRecall our research aim:\n\n… whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting.\n\nForgetting about any differences due to diet, let’s just think about the relationship between irritability and hunger. How should we interpret this research aim?\nWas it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a composite of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that the ‘total effect’ isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it about how person i changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are.).\n\n\nQuestion 2\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score.\n\n\nHint: You’ll find group_by() %>% mutate() very useful here.\n\n\n\n\n\n Solution \n\n\n\nhangry <- \n    hangry %>% group_by(ppt) %>%\n        mutate(\n            avg_hunger = mean(q_hunger),\n            hunger_gc = q_hunger - avg_hunger\n        )\nhead(hangry)\n\n# A tibble: 6 × 6\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   fivetwo avg_hunger hunger_gc\n           <dbl>    <dbl> <chr> <fct>        <dbl>     <dbl>\n1             17       30 N1p1  1             26.6     3.4  \n2             19       27 N1p1  1             26.6     0.400\n3             19       29 N1p1  1             26.6     2.4  \n4             20       33 N1p1  1             26.6     6.4  \n5             24       14 N1p1  1             26.6   -12.6  \n6             30       28 N1p2  1             32.6    -4.6  \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people?\n\nDoes it look like when people are more hungry than normal, they are more irritable?\n\n\n\n\n\n Solution \n\n\nWe might find it easier to look at a plot where each participant is represented as their mean plus an indication of their range of irritability scores:\n\nggplot(hangry,aes(x=avg_hunger,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nThere appears to be a slight positive relationship between a persons’ average hunger and their irritability scores.\nIt is harder to tell what the relationship is between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data:\n\nggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt))+\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2)\n\n\n\n\n\n\n\n\nI think there might be a positive trend in here, in that participants tend to be higher irritability when they are higher (for them) on the hunger score.\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation level deviations from those averages), that represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables as predictors, instead of the raw hunger scores.\n\nHints:\n\nhunger * diet could be replaced by (hunger1 + hunger2) * diet, thereby allowing each aspect of hunger to interact with diet.\nWe can only put one of these variables in the random effects (1 + hunger | participant). Recall that above we discussed how we cannot have (diet | participant), because “an effect of diet” makes no sense for a single participant (they are either on the diet or they are not, so there is no ‘effect’). Similarly, each participant has only one value for their average hungriness.\n\n\n\n\n\n\n Solution \n\n\n\nlibrary(lme4)\nhangrywb <- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                control = lmerControl(optimizer=\"bobyqa\"))\n\n\n\n\n\nQuestion 5\n\n\nHopefully, you have fitted a model similar to the below:\n\nhangrywb <- lmer(q_irritability ~ (avg_hunger + hunger_gc) * fivetwo + \n                (1 + hunger_gc | ppt), data = hangry,\n            control = lmerControl(optimizer=\"bobyqa\"))\n\nBelow, we have obtained p-values using the Satterthwaite Approximation of \\(df\\) for the test of whether the fixed effects are zero, so we can see the significance of each estimate.\nProvide an answer for each of these questions:\n\nFor those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\nDo people following no diet tend to be more irritable when they are more hungry than they usually are?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\n(Trickier:) What does the fivetwo coefficient represent?\n\n\n\n\n\n\n\n  \n  \n    \n      term\n      Estimate\n      Std. Error\n      df\n      t value\n      Pr(>|t|)\n    \n  \n  \n    (Intercept)\n17.131\n5.15\n77\n3.33\n0.0013\n    avg_hunger\n0.004\n0.11\n77\n0.04\n0.9708\n    hunger_gc\n0.186\n0.08\n65.4\n2.46\n0.0167\n    fivetwo1\n-10.855\n6.54\n77\n-1.66\n0.1008\n    avg_hunger:fivetwo1\n0.466\n0.13\n77\n3.49\n<0.001\n    hunger_gc:fivetwo1\n0.381\n0.1\n68.5\n3.76\n<0.001\n  \n  \n  \n\n\n\n\n\n\n\n\n Solution \n\n\n1: For those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nA: ‘No diet’ is the reference level of the five-two variable, and because we have an interaction, that means the avg_hunger coefficient will provide the relevant estimate. There is no evidence (\\(p>.05\\)) to suggest that when not dieting, hungrier people are more irritable than less hungry people.\n2: Is there evidence to suggest that this is different for those following the five-two diet? In what way?\nA: This is the interaction between avg_hunger:fivetwo1. We can see that, for every increase of 1 in average hunger, irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet.\nThese units are still in terms of the original scale (i.e. 0 to 100).\n3: Do people following no diet tend to be more irritable when they are more hungry than they usually are? A: This is the estimate for the coefficient of hunger_gc. For people following no diet, there is an estimated 0.19 increase in irritability for every 1 unit more hungry they become.\n4: Is there evidence to suggest that this is different for those following the five-two diet? In what way? A: This effect of a 1 unit change on within-person hunger increasing irritability is increased for those who are following the five-two diet by an additional 0.38\n5: What does the fivetwo1 coefficient represent? A: This represents the group difference of irritability between those on the five-two diet vs those not dieting, for someone who has an average hunger score of 0.\n\n\n\n\nQuestion 6\n\n\nConstruct two plots showing the two model estimated interactions. Think about your answers to the previous question, and check that they match with what you are seeing in the plots (do not underestimate the utility of this activity for helping understanding!).\n\nHint: This isn’t as difficult as it sounds. the sjPlot package can do it in one line of code!\n\n\n\n\n\n Solution \n\n\n\nlibrary(sjPlot)\nplot_model(hangrywb, type = \"int\")[[1]]\n\n\n\n\n\n\n\n\nWe saw in the model coefficients that for the reference level of fivetwo, the “No Diet” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. We also saw the interaction avg_hunger:fivetwo1 indicates that irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\nplot_model(hangrywb, type = \"int\")[[2]]\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_gc we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re on “No Diet”). This is the slope of the red line. The interaction hunger_gc:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive and significant, which matches with the fact that the blue line is clearly steeper in this plot.\n\n\n\n\nQuestion 7\n\n\nLoad the lmerTest package and fit the model again. Take a look at the summary - you should now have the \\(df\\), \\(t\\)-value, and \\(p\\)-value for each estimate.\nWrite-up the results.\n\n\n\n\n Solution \n\n\n\nlibrary(lmerTest)\nhangrywb2 <- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                REML = TRUE,\n                control = lmerControl(optimizer=\"bobyqa\"))\nsummary(hangrywb2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nq_irritability ~ (avg_hunger + hunger_gc) * fivetwo + (1 + hunger_gc |  \n    ppt)\n   Data: hangry\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 2734.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.41378 -0.59064 -0.04539  0.54264  2.39536 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ppt      (Intercept) 48.0827  6.9342        \n          hunger_gc    0.1451  0.3809   -0.01\n Residual             23.3050  4.8275        \nNumber of obs: 405, groups:  ppt, 81\n\nFixed effects:\n                      Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)          17.130959   5.146478  76.999477   3.329 0.001341 ** \navg_hunger            0.003863   0.105327  76.998524   0.037 0.970835    \nhunger_gc             0.185773   0.075601  65.409225   2.457 0.016659 *  \nfivetwo1            -10.854713   6.535684  76.999696  -1.661 0.100813    \navg_hunger:fivetwo1   0.465897   0.133539  76.998690   3.489 0.000806 ***\nhunger_gc:fivetwo1    0.381412   0.101393  68.486392   3.762 0.000352 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) avg_hn hngr_g fivtw1 avg_:1\navg_hunger  -0.971                            \nhunger_gc   -0.002  0.000                     \nfivetwo1    -0.787  0.765  0.002              \navg_hngr:f1  0.766 -0.789  0.000 -0.968       \nhngr_gc:fv1  0.002  0.000 -0.746 -0.002  0.000\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on a restricted diet such as the five-two, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes of relative-hunger level were included for participants. The model was fitting with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser from the lme4. \\(P\\)-values were obtained using the Satterthwaite approximation for degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(\\beta = 0.186,\\ SE = 0.08,\\ t(65.4) = 2.46,\\ p = 0.0167\\)), and that this was increased for those following the five-two diet (\\(\\beta = 0.381,\\ SE = 0.1,\\ t(68.5) = 3.76,\\ p <0.001\\)). Although for those not on a specific diet there was no evidence for an association between irritability and being generally a more hungry person (\\(p = 0.9708\\)), there a significant interaction was found between average hunger and being on the five-two diet (\\(\\beta = 0.466,\\ SE = 0.13,\\ t(77) = 3.49,\\ p <0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association for both between-hunger and within-hunger with irritability.\n\n\n\n\nOther within-group transformations\nAs well as within-group mean centering a predictor (like we have done above). There are quite a few similar things we can do, for which the logic is the same.\nFor instance, we can within-group standardise a predictor. This would disagregate within and between effects, but interpretation would of the within effect would be the estimated change in \\(y\\) associated with being 1 standard deviation higher in \\(x\\) for that group.\nWe can also do within-group transformations on our outcome variable. This allows to address questions such as:\n“Are people more irritable than they usually are (\\(y\\) is group-mean centered) if they are, for them, more hungry than they usually are (\\(x\\) is group-mean centered)?”\n\n\n\nOptional Exercises: Logistic MLM\n\nDon’t forget to look back at other materials!\nBack in DAPR2, we introduced logistic regression in semester 2, week 8. The lab contained some simulated data based on a hypothetical study about inattentional blindness. That content will provide a lot of the groundwork for this week, so we recommend revisiting it if you feel like it might be useful.\n\n\nlmer() >> glmer()\nRemember how we simply used glm() and could specify the family = \"binomial\" in order to fit a logistic regression? Well it’s much the same thing for multi-level models!\n\nGaussian model: lmer(y ~ x1 + x2 + (1 | g), data = data)\n\nBinomial model: glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial(link='logit'))\n\nor just glmer(y ~ x1 + x2 + (1 | g), data = data, family = \"binomial\")\nor glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial)\n\n\n\n\n Binary? Binomial?\n\n\nFor binary regression, all the data in our outcome variable has to be a 0 or a 1.\nFor example, the correct variable below:\n\n\n\n\n\n\n  \n  \n    \n      participant\n      question\n      correct\n    \n  \n  \n    1\n1\n1\n    1\n2\n0\n    1\n3\n1\n    ...\n...\n...\n  \n  \n  \n\n\n\n\nBut we can re-express this information in a different way, when we know the total number of questions asked.\n\n\n\n\n\n\n  \n  \n    \n      participant\n      questions_correct\n      questions_incorrect\n    \n  \n  \n    1\n2\n1\n    2\n1\n2\n    3\n3\n0\n    ...\n...\n...\n  \n  \n  \n\n\n\n\nTo model data when it is in this form, we can express our outcome as cbind(questions_correct, questions_incorrect)\n\n\n\n\nMemory Recall & Finger Tapping\n\nResearch Question: After accounting for effects of sentence length, does the rhythmic tapping of fingers aid memory recall?\n\nResearchers recruited 40 participants. Each participant was tasked with studying and then recalling 10 randomly generated sentences between 1 and 14 words long. For 5 of these sentences, participants were asked to tap their fingers along with speaking the sentence in both the study period and in the recall period. For the remaining 5 sentences, participants were asked to sit still.\nThe data are available at https://uoepsy.github.io/data/memorytap.csv, and contains information on the length (in words) of each sentence, the condition (static vs tapping) under which it was studied and recalled, and whether the participant was correct in recalling it.\n\n\n\n\n\n\n  \n  \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier (n=40)\n    slength\nNumber of words in sentence\n    condition\nCondition under which sentence is studied and recalled ('static' = sitting still, 'tap' = tapping fingers along to sentence)\n    correct\nWhether or not the sentence was correctly recalled\n  \n  \n  \n\n\n\n\n\n\nQuestion 8 (Optional)\n\n\n\nResearch Question: After accounting for effects of sentence length, does the rhythmic tapping of fingers aid memory recall?\n\nFit an appropriate model to answer the research question.\n\nHint:\n\nour outcome is conceptually ‘memory recall’, and it’s been measured by “Whether or not a sentence was correctly recalled”. This is a binary variable.\n\nwe have multiple observations for each ?????\nThis will define our (  | ??? ) bit\n\n\n\n\n\n\n Solution \n\n\n\nmemtap <- read_csv(\"https://uoepsy.github.io/data/memorytap.csv\")\n\nWhen we fit the maximal model, note that we obtain a singular fit. The variance of the slength effect between participants is quite small relative to the others, and there is a correlation between it and the random intercepts.\n\ntapmod <- glmer(correct ~ 1 + slength + condition + \n                  (1 + slength + condition | ppt),\n      data = memtap,\n      family = binomial)\nisSingular(tapmod)\n\n[1] TRUE\n\nVarCorr(tapmod)\n\n Groups Name         Std.Dev. Corr         \n ppt    (Intercept)  1.032849              \n        slength      0.070307 -1.000       \n        conditiontap 0.665626  0.590 -0.590\n\n\nlet’s remove the random effect of slength | ppt.\n\ntapmod2 <- glmer(correct ~ 1 + slength + condition + \n                  (1 + condition | ppt),\n      data = memtap,\n      family = binomial)\n\nthe model now looks a bit better (not a singular fit):\n\nsummary(tapmod2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: correct ~ 1 + slength + condition + (1 + condition | ppt)\n   Data: memtap\n\n     AIC      BIC   logLik deviance df.resid \n   537.6    561.5   -262.8    525.6      394 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8949 -0.8955  0.4483  0.7962  1.6862 \n\nRandom effects:\n Groups Name         Variance Std.Dev. Corr\n ppt    (Intercept)  0.2755   0.5249       \n        conditiontap 0.4207   0.6486   0.66\nNumber of obs: 400, groups:  ppt, 40\n\nFixed effects:\n             Estimate Std. Error z value Pr(>|z|)  \n(Intercept)   0.76140    0.37077   2.054   0.0400 *\nslength      -0.12086    0.04721  -2.560   0.0105 *\nconditiontap  0.50945    0.24317   2.095   0.0362 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) slngth\nslength     -0.890       \nconditiontp -0.154 -0.029\n\n\n\n\n\n\nTake some time to remind yourself from DAPR2 of the interpretation of logistic regression coefficients.\nIn family = binomial(link='logit'), we are modelling the log-odds. We can obtain estimates on this scale using:\n\nfixef(model)\nsummary(model)$coefficients\ntidy(model) from broom.mixed\n\n(there are probably more ways, but I can’t think of them right now!)\n\nWe can use exp(), to get these back into odds and odds ratios.\n\n\nQuestion 9 (Optional)\n\n\nInterpret each of the fixed effect estimates from your model.\n\n\n\n\n Solution \n\n\n\nfixef(tapmod2)\n\n (Intercept)      slength conditiontap \n   0.7613976   -0.1208591    0.5094460 \n\nexp(fixef(tapmod2))\n\n (Intercept)      slength conditiontap \n   2.1412669    0.8861589    1.6643689 \n\n\n\n(Intercept): For an sentence with zero words, when sitting statically, the odds of correctly recalling the sentence are 2.14. This is equivalent to a \\(\\frac{2.14}{1 + 2.14} = 0.6815287\\) probability of getting it correct.\n\nslength: After accounting for differences due to tapping/not-tapping during study & recall, for every 1 word longer a sentence is, the odds of correctly recalling the sentence is decreased by 0.89.\nconditiontap: After accounting for differences in recall due to sentence length, finger tapping during the study and recall of sentences was associated with 1.66 increased odds correct recall in comparison to sitting still.\n\n\n\n\n\nQuestion 10 (Optional)\n\n\nChecking the assumptions in non-gaussian models in general (i.e. those where we set the family to some other error distribution) can be a bit tricky, and this is especially true for multilevel models.\nFor the logistic MLM, the standard assumptions of normality etc for our Level 1 residuals residuals(model) do not hold. However, it is still useful to quickly plot the residuals and check that \\(|residuals|\\leq 2\\) (or \\(|residuals|\\leq 3\\) if you’re more relaxed). We don’t need to worry too much about the pattern though.\nWhile we’re more relaxed about Level 1 residuals, we do still want our random effects ranef(model) to look fairly normally distributed.\n\nPlot the level 1 residuals and check whether any are greater than 3 in magnitude\nPlot the random effects (the level 2 residuals) and assess the normality.\n\n\nfor beyond DAPR3\n\nThe HLMdiag package doesn’t support diagnosing influential points/clusters for glmer, but there is a package called influence.me which might help: https://journal.r-project.org/archive/2012/RJ-2012-011/RJ-2012-011.pdf\nThere are packages which aim to create more interpretable residual plots for these models via simulation, such as the DHARMa package: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\n\n\n\n\n\n Solution \n\n\n\nplot(tapmod2)\n\n\n\n\n\n\n\nsum(abs(resid(tapmod2))>3)\n\n[1] 0\n\n\nAll residuals are between -3 and 3.\nThe random effects look okay here. Not perfect, but bear in mind we have only 40 participants.\n\nqqnorm(ranef(tapmod2)$ppt[, 1], main = \"Random intercept\")\nqqline(ranef(tapmod2)$ppt[, 1])\nqqnorm(ranef(tapmod2)$ppt[, 2], main = \"Random slope of condition\")\nqqline(ranef(tapmod2)$ppt[, 2])\nhist(ranef(tapmod2)$ppt[, 1])\nhist(ranef(tapmod2)$ppt[, 2])"
  },
  {
    "objectID": "05_recap.html",
    "href": "05_recap.html",
    "title": "Recap of multilevel models",
    "section": "",
    "text": "Flashcards: lm to lmer\n\n\n\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\n\n\n Simple Linear Regression\n\n\n\nFormula:\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Clustered (multi-level) data\n\n\nWhen our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nIf we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Random intercepts\n\n\nBy including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nWe can now see that the intercept estimate \\(\\beta_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has an adjustment for each subject.\nEach subject’s line has been moved up or down accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Shrinkage\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different:\n\n\n\n\n\n\n\n\n\nWhy? One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters.\n\n\n\n\n Random slopes\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(\\beta_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nAs with the intercept \\(\\beta_{0i}\\), the slope of the predictor \\(\\beta_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Fixed effects\n\n\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the groups all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\nThese are the overall intercept and slope.\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n405.7897675  -0.6722654 \n\n\n\n\n\n\n Random effects\n\n\nThe plots below show the fitted values for each subject from each model that we have gone through in these expandable boxes (simple linear regression, random intercept, and random intercept & slope):\n\n\n\n\n\n\n\n\n\nIn the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_0\\). The standard deviation (and variance, which is \\(\\sigma_0^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function).\n\n\n\n\n\n\n\n\n\nIn the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. We can extract the deviations for each group from the fixed effect estimates using the ranef() function.\nThese are the deviations from the overall intercept (\\(\\widehat \\gamma_{00} = 405.79\\)) and slope (\\(\\widehat \\gamma_{10} = -0.672\\)) for each subject \\(i\\).\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)          x1\nsub_308   31.327291 -1.43995253\nsub_309  -28.832219  0.41839420\nsub_310    2.711822  0.05993766\nsub_330   59.398971  0.38526670\nsub_331   74.958481  0.17391602\nsub_332   91.086535 -0.23461836\nsub_333   97.852988 -0.19057838\nsub_334  -54.185688 -0.55846794\nsub_335  -16.902018  0.92071637\nsub_337   52.217859 -1.16602280\nsub_349  -67.760246 -0.68438960\nsub_350   -5.821271 -1.23788002\nsub_351   61.198823  0.05499816\nsub_352   -7.905596 -0.66495059\nsub_369  -47.636645 -0.46810258\nsub_370  -33.121093 -1.11001234\nsub_371   77.576205 -0.20402571\nsub_372  -36.389281 -0.45829505\nsub_373 -197.579562  1.79897904\nsub_374  -52.195357  4.60508775\n\nwith conditional variances for \"subject\" \n\n\n\n\n\n\n Group-level coefficients\n\n\nWe can also see the actual intercept and slope for each subject \\(i\\) directly, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)         x1\nsub_308    437.1171 -2.1122179\nsub_309    376.9575 -0.2538712\nsub_310    408.5016 -0.6123277\nsub_330    465.1887 -0.2869987\nsub_331    480.7482 -0.4983494\nsub_332    496.8763 -0.9068837\nsub_333    503.6428 -0.8628438\nsub_334    351.6041 -1.2307333\nsub_335    388.8877  0.2484510\nsub_337    458.0076 -1.8382882\nsub_349    338.0295 -1.3566550\nsub_350    399.9685 -1.9101454\nsub_351    466.9886 -0.6172672\nsub_352    397.8842 -1.3372160\nsub_369    358.1531 -1.1403680\nsub_370    372.6687 -1.7822777\nsub_371    483.3660 -0.8762911\nsub_372    369.4005 -1.1305604\nsub_373    208.2102  1.1267137\nsub_374    353.5944  3.9328224\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nNotice that the above are the fixed effects + random effects estimates, i.e. the overall intercept and slope + deviations for each subject.\n\ncoef(random_intercept_model)\n\n$subject\n        (Intercept)         x1\nsub_308    384.0955 -0.9135829\nsub_309    406.5426 -0.9135829\nsub_310    421.8658 -0.9135829\nsub_330    492.0476 -0.9135829\nsub_331    498.0868 -0.9135829\nsub_332    496.0130 -0.9135829\nsub_333    504.6193 -0.9135829\nsub_334    338.5855 -0.9135829\nsub_335    440.3964 -0.9135829\nsub_337    416.7346 -0.9135829\nsub_349    319.6674 -0.9135829\nsub_350    356.3696 -0.9135829\nsub_351    479.2943 -0.9135829\nsub_352    379.5162 -0.9135829\nsub_369    349.0152 -0.9135829\nsub_370    335.0869 -0.9135829\nsub_371    484.0427 -0.9135829\nsub_372    360.5322 -0.9135829\nsub_373    293.6168 -0.9135829\nsub_374    511.3440 -0.9135829\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\n\n\n\n\n Visualising Model Fitted values\n\n\nThe model fitted (or “model predicted”) values can be obtained using predict() (returning just the values) or broom.mixed::augment() (returning the values attached to the data that is inputted to the model).\nTo plot, them, we would typically like to plot the fitted values for each group (e.g. subject)\n\nlibrary(broom.mixed)\naugment(random_slopes_model) %>%\n  ggplot(.,aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n Visualising Fixed Effects\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. Packages like sjPlot make it incredibly easy (but sometimes too easy), so a nice option is to use the effects package to construct a dataframe of the linear prediction accross the values of a predictor, plus standard errors and confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\nlibrary(effects)\nef <- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n Plotting random effects\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms <- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Nested and Crossed structures\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\nModel Structure & Notation\nMLM allows us to model effects in the linear model as varying between groups. Our coefficients we remember from simple linear models (the \\(\\beta\\)’s) are modelled as a distribution that has an overall mean around which our groups vary. We can see this in Figure 1, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 1 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\nFigure 1: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(\\beta\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is \\(\\rho = \\text{Cor}(\\zeta_{0i}, \\zeta_{1i}) = \\frac{\\sigma_{01}}{\\sigma_0 \\sigma_1}\\):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\n\n\nMLM in R\nWe can fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\n\n\n\nlibrary(lme4)\nm1 <- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 637.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.49449 -0.57223 -0.01353  0.62544  2.39122 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.2616   1.5038       \n          x           0.7958   0.8921   0.55\n Residual             4.3672   2.0898       \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   1.7261     0.9673   1.785\nx             1.1506     0.2968   3.877\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\n\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)\n\n\n\n\nFlashcards: Getting p-values/CIs for Multilevel Models\n\nInference for multilevel models is tricky, and there are lots of different approaches.\nYou might find it helpful to think of the majority of these techniques as either:\nA. Fitting a full model and a reduced model. These differ only with respect to the relevant effect. Comparing the overall fit of these models by some metric allows us to isolate the improvement due to our effect of interest. B. Fitting the full model and - using some method of approximating the degrees of freedom for the tests of whether a coefficient is zero. - constructing some confidence intervals (e.g. via bootstrapping) in order to gain a range of plausible values for the estimate (typically we then ask whether zero is within the interval)\nNeither of these is correct or incorrect, but they each have different advantages. In brief:\n\nSatterthwaite \\(df\\) = Very easy to implement, can fit with REML\nKenward-Rogers \\(df\\) = Good when working with small samples, can fit with REML\nLikelihood Ratio Tests = better with bigger samples (of groups, and of observations within groups), requires REML = FALSE\nParametric Bootstrap = takes time to run, but in general a reliable approach. , requires REML = FALSE if doing comparisons, but not for confidence intervals\nNon-Parametric Bootstrap = takes time, needs careful thought about which levels to resample, but means we can relax distributional assumptions (e.g. about normality of residuals).\n\nFor the examples below, we’re investigating how “hours per week studied” influences a child’s reading age. We have data from 132 children from 20 schools, and we have fitted the model:\n\n\n\n\nfull_model <- lmer(reading_age ~ 1 + hrs_week + (1 + hrs_week | school), \n                   data = childread)\n\n\n\n Use a normal approximation (not advisable)\n\n\nRemember that the \\(t\\) distribution starts to look more and more like the \\(z\\) (“normal”) distribution when degrees of freedom increase? We could just assume we have infinite degrees of freedom in our test statistics, and pretend that the \\(t\\)-values we get are actually \\(z\\)-values. This is “anti-conservative” inasmuch as it is not a very cautious approach, and we are likely to have a higher false positive rate (e.g. more chance of saying “there is an effect!” when there actually isn’t.)\n\ncoefs <- as.data.frame(summary(full_model)$coefficients)\ncoefs$p.z <- 2 * (1 - pnorm(abs(coefs[,3])))\ncoefs\n\n            Estimate Std. Error  t value          p.z\n(Intercept) 1.726120  0.9672824 1.784504 0.0743417714\nhrs_week    1.150637  0.2968219 3.876525 0.0001059589\n\n\n\n\n\n\n Satterthwaite df approximation\n\n\nThere have been a couple of methods proposed to estimate the degrees of freedom in order to provide a better approximation to the null distribution of our tests. The way the Satterthwaite method has been implemented in R will just add a column for p-values to your summary(model) output).\nLoad the lmerTest package, refit the model, and voila!\n\nlibrary(lmerTest)\nfull_model <- lmer(reading_age ~ 1 + hrs_week + (1 + hrs_week | school), \n                   data = childread, REML = TRUE)\nsummary(full_model)$coefficients\n\n            Estimate Std. Error       df  t value    Pr(>|t|)\n(Intercept) 1.726120  0.9672824 16.23589 1.784504 0.093038229\nhrs_week    1.150637  0.2968219 17.50762 3.876525 0.001155763\n\n\n\nReporting\nTo account for the extra uncertainty brought by the inclusion of random effects in the model, the degrees of freedom in the coefficients tests have been corrected via Satterthwaite’s method.\n…\n…\nWeekly hours of reading practice was associated increased reading age (\\(\\beta = 1.15,\\ SE = 0.30,\\ t(17.51^*) = 3.88,\\ p = .001\\)).\n\nNote: if you have the lmerTest package loaded, then all the models you fit with lmer() will show p-values! If you want to stop this, then you will have to detach/unload the package, and refit the model.\n\ndetach(\"package:lmerTest\", unload=TRUE)\n\n\n\n\n\n Kenward Rogers df approximations\n\n\nThe Kenward-Rogers approach is slightly more conservative than the Satterthwaite method, and has been implemented for model comparison between a full model and a restricted model (a model without the parameter of interest), using the KR adjustment for the denominator degrees of freedom in the \\(F\\)-test.\nFor this, models must be fitted with REML, not ML. The function KRmodcomp() will take care of this and re-fit them for you.\n\nlibrary(pbkrtest)\nrestricted_model <- lmer(reading_age ~ 1 + (1 + hrs_week | school), \n                   data = childread, REML = TRUE)\nfull_model <- lmer(reading_age ~ 1 + hrs_week + (1 + hrs_week | school), \n                   data = childread, REML = TRUE)\nKRmodcomp(full_model, restricted_model)\n\nlarge : reading_age ~ 1 + hrs_week + (1 + hrs_week | school)\nsmall : reading_age ~ 1 + (1 + hrs_week | school)\n        stat    ndf    ddf F.scaling p.value   \nFtest 14.710  1.000 17.741         1 0.00124 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nReporting\nTo account for the extra uncertainty brought by the inclusion of random effects in the model, the denominator degrees of freedom in have been corrected via Kenward-Rogers’ method.\n…\n…\nInclusion of weekly hours of reading practice as a predictor was associated with an improvement in model fit (\\(F(1,17.74^*) = 14.71,\\ p = .001\\)).\n\n\n\n\n\n Likelihood Ratio Test (LRT)\n\n\nConduct a model comparison between your model and a restricted model (a model without the parameter of interest), evaluating the change in log-likelihood.\n\nLikelihood\n“likelihood” is a function that associates to a parameter the probability (or probability density) of observing the given sample data.\nIn simpler terms, the likelihood is the probability of the model given that we have this data.\nThe intuition behind likelihood:\n\nI toss a coin 10 time and observed 8 Heads.\n\nWe can think of a ‘model’ of the process that governs the coin’s behaviour in terms of just one number: a parameter that indicates the probability of the coin landing on heads.\nI have two models:\n\n\nModel 1: The coin will land on heads 20% of the time. \\(P(Heads)=0.2\\)\n\nModel 2: The coin will land on heads 70% of the time. \\(P(Heads)=0.7\\)\n\n\n\nGiven the data I observe (see 1, above), we can (hopefully) intuit that Model 2 is more likely than Model 1.\n\nFor a (slightly) more detailed explanation, see here.\n\nThis method assumes that the ratio of two likelihoods will (as sample size increases) become closer to being \\(\\chi^2\\) distributed, and so may be unreliable for small samples.\nModels must be fitted with ML, not REML. The function anova() will re-fit them for you.\n\nrestricted_model <- lmer(reading_age ~ 1 + (1 + hrs_week | school), \n                   data = childread, REML = FALSE)\nfull_model <- lmer(reading_age ~ 1 + hrs_week + (1 + hrs_week | school), \n                   data = childread, REML = FALSE)\nanova(restricted_model, full_model, test = \"Chisq\")\n\nData: childread\nModels:\nrestricted_model: reading_age ~ 1 + (1 + hrs_week | school)\nfull_model: reading_age ~ 1 + hrs_week + (1 + hrs_week | school)\n                 npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nrestricted_model    5 660.52 674.93 -325.26   650.52                         \nfull_model          6 650.66 667.96 -319.33   638.66 11.857  1  0.0005744 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nReporting\nA likelihood ratio test indicated that the inclusion of weekly hours of reading practice as a predictor was associated with an improvement in model fit (\\(\\chi^2(1) = 11.86, p < .001\\)).\n\n\n\n\n\n Parametric Bootstrap LRT\n\n\nThere are also various “bootstrapping” methods which it is worth looking into. Think back to USMR when we first learned about hypothesis testing. Remember that we did lots of simulating data, so that we can compare what we actually observe with what we would expect if the null hypothesis were true? By doing this, we were essentially creating a null distribution, so that our calculating a p-value can become an issue of summarising data (e.g. calculate the proportion of our simulated null distribution that is more extreme than our observed statistic)\nInstead of assuming that the likelihood ratio test statistics are \\(\\chi^2\\)-distributed, we can bootstrap this test instead. This approach simulates data from the simpler model, fits both the simple model and the complex model and evaluates the change in log-likelihood. By doing this over and over again, we build a distribution of what changes in log-likelihood we would be likely to see if the more complex model is not any better. In this way it actually constructs a distribution reflecting our null hypothesis, against which we can then compare our actual observed effect\n\n\n\n\nlibrary(pbkrtest)\nrestricted_model <- lmer(reading_age ~ 1 + (1 + hrs_week | school), \n                   data = childread, REML = FALSE)\nfull_model <- lmer(reading_age ~ 1 + hrs_week + (1 + hrs_week | school), \n                   data = childread, REML = FALSE)\nPBmodcomp(full_model, restricted_model, nsim=1000)\n\n\nReporting\nA parametric bootstrap likelihood ratio test (R = 1000) indicated that the inclusion of weekly hours of reading practice as a predictor was associated with an improvement in model fit (\\(LRT = 11.84, p = .002\\)).\n\n\n\n\n\n Parametric Bootstrap Confidence Intervals\n\n\nMuch the same as above, but with just one model we simulate data many times and refit the model, so that we get an empirical distribution that we can use to construct confidence intervals for our effects.\n\nfull_model <- lmer(reading_age ~ 1 + hrs_week + (1 + hrs_week | school), \n                   data = childread, REML = TRUE)\nconfint(full_model, method=\"boot\")\n\n                 2.5 %   97.5 %\n.sig01       0.0000000 3.913368\n.sig02      -0.5647829 1.000000\n.sig03       0.3927015 1.376779\n.sigma       1.7921405 2.349858\n(Intercept) -0.1214629 3.370657\nhrs_week     0.5567827 1.752359\n\n\n\nReporting\n95% Confidence Intervals were obtained via parametric bootstrapping with 1000 iterations.\n…\n…\nWeekly hours of reading practice was associated increased reading age (\\(\\beta = 1.15,\\ 95%\\ CI\\ [0.63 -- 1.73]\\)).\n\n\n\n\n\n Non-Parametric Bootstrap Confidence Intervals\n\n\nIt’s worth noting that there are many different types of bootstrapping that we can conduct. Different methods of bootstrapping vary with respect to the assumptions we will have to make when using them for drawing inferences. For instance, the parametric bootstrap discussed above assumes that explanatory variables are fixed and that model specification and the distributions such as \\(\\zeta_i \\sim N(0,\\sigma_{\\zeta})\\) and \\(\\varepsilon_i \\sim N(0,\\sigma_{\\varepsilon})\\) are correct.\nAn alternative is to generate a distribution by resampling with replacement from our data, fitting our model to the resample, and then repeating this over and over. This doesn’t have to rely on assumptions about the shape of the distributions of \\(\\zeta_i\\) and \\(\\varepsilon_i\\) - we just need to ensure that we correctly specify the hierarchical dependency of data. It does, however, require the decision of at which levels to resample.\n\n\n\n\nfull_model <- lmer(reading_age ~ 1 + hrs_week + (1 + hrs_week | school), \n                   data = childread, REML = TRUE)\nlibrary(lmeresampler)\nfullmod_bs <- \n  bootstrap(full_model, # this is the model\n            .f = fixef, # we want the fixef from each bootstrap\n            type = \"case\", # case based bootstrap\n            B=2000, # do 2000 bootstraps\n            resample = c(TRUE, TRUE) # resample both schools and individual children\n  ) \nconfint(fullmod_bs, type = \"perc\")\n\n# A tibble: 2 × 6\n  term        estimate  lower upper type  level\n  <chr>          <dbl>  <dbl> <dbl> <chr> <dbl>\n1 (Intercept)     1.73 -1.44   3.57 perc   0.95\n2 hrs_week        1.15  0.681  2.02 perc   0.95\n\n\n\n\n\n\nReporting\n95% Confidence Intervals were obtained via case-based bootstrapping (resampling both toy types and individual toys) with 2000 iterations.\n…\n…\nWeekly hours of reading practice was associated increased reading age (\\(\\beta\\) = 1.1506375, 95% CI [0.6811553 – 2.0204023])."
  },
  {
    "objectID": "example2_01_EFA.html",
    "href": "example2_01_EFA.html",
    "title": "Analysis Example: Exploratory Factor Analysis",
    "section": "",
    "text": "Intro\nThis is a quick demonstration of one way of dealing with these tasks. It is by no means the only correct way. There is a substantial level of subjectivity in Exploratory Factor Analysis and the method involves repeated evaluation and re-evaluation of the model in light of the extracted factors and their conceptual relationships with the analysed items. In other words, a good EFA requires you to get your hands dirty.\n\nData: Work Pressures Survey\nThe Work Pressures Survey (WPS) data is available at the following link: https://uoepsy.github.io/data/WPS_data.csv.\nThe data contains responses from 946 workers from a variety of companies to the Work Pressures Survey. You can look at the survey taken by the study participants at the following link: https://uoepsy.github.io/data/WPS_data_codebook.pdf\nWe will be performing a factor analysis of the main section of this survey (Job1 to Job50).\n\n\n\nRead in Data\nRead the WPS data into R. Make sure to take a look at the variable names and data structure.\n\n\nCode\nlibrary(tidyverse)\ndf <- read.csv(\"https://uoepsy.github.io/data/WPS_data.csv\")\nhead(df)\n\n\n  job_yrs job_mths worktype cont_hrs act_hrs gender dobm doby status youngdep\n1       1        6        1       45      45      1    7   83      1        1\n2       5        8        1       36      36      2    1   58      2        2\n3       2        8        1       40      40      2    2   82      1        1\n4       4        3        1       35      35      1    4   78      1        1\n5       7        1        1       40      41      1    2   73      2        2\n6       3       11        1       32      32      1    8   75      2        2\n  elderdep qualif exercise cigs alcohol time_rel when_rel job1 job2 job3 job4\n1        1      5        4    1       1        2        1    5    4    5    4\n2        1      5        2    2       0        2        2    5    7    4    3\n3        1      4        6    2       0        1        1    1    4    7    1\n4        2      5        3    1       2        2        1    6    4    5    5\n5        1      4        2    1       3        2        1    1    1    5    5\n6        2      5        6    1       4        5        2    3    4    5    3\n  job5 job6 job7 job8 job9 job10 job11 job12 job13 job14 job15 job16 job17\n1    6    4    7    7    4     7     4     4     1     4     4     6     4\n2    6    2    1    4    5     6     1     5     4     5     7     5     1\n3    1    1    7    7    3     1     7     4     1     7     3     1     1\n4    6    4    5    5    5     6     5     3     4     2     7     2     4\n5    1    2    2    2    5     2     3     3     4     6     2     2     2\n6    4    3    4    3    5     4     3     3     4     1     6     3     5\n  job18 job19 job20 job21 job22 job23 job24 job25 job26 job27 job28 job29 job30\n1     7     7     3     4     3     5     4     4     7     2     3     3     4\n2     7     1     5     1     1     1     4     5     6     4     6     3     3\n3     7     7     7     7     1     2     5     6     7     1     7     5     6\n4     6     7     5     5     2     4     4     5     3     2     5     7     5\n5     3     2     1     1     4     6     2     3     6     6     2     5     1\n6     5     6     3     3     1     3     4     2     5     5     5     6     2\n  job31 job32 job33 job34 job35 job36 job37 job38 job39 job40 job41 job42 job43\n1     5     5     6     4     4     4     4     4     5     4     6     6     6\n2     1     7     4     6     6     1     5     5     7     3     5     4     1\n3     5     3     6     5     3     2     1     5     5     5     3     5     7\n4     3     2     5     7     5     3     3     2     7     3     5     4     3\n5     4     2     4     4     4     2     2     2     2     4     1     5     4\n6     5     1     3     5     1     5     3     1     5     3     1     3     5\n  job44 job45 job46 job47 job48 job49 job50\n1     6     6     3     3     3     6     3\n2     4     6     1     4     7     5     5\n3     6     7     2     5     7     5     1\n4     5     7     3     5     7     6     3\n5     2     6     6     1     6     2     1\n6     3     3     6     3     4     2     7\n\n\nVariable names - Option 1:\n\n\nCode\nnames(df)\n\n\n [1] \"job_yrs\"  \"job_mths\" \"worktype\" \"cont_hrs\" \"act_hrs\"  \"gender\"  \n [7] \"dobm\"     \"doby\"     \"status\"   \"youngdep\" \"elderdep\" \"qualif\"  \n[13] \"exercise\" \"cigs\"     \"alcohol\"  \"time_rel\" \"when_rel\" \"job1\"    \n[19] \"job2\"     \"job3\"     \"job4\"     \"job5\"     \"job6\"     \"job7\"    \n[25] \"job8\"     \"job9\"     \"job10\"    \"job11\"    \"job12\"    \"job13\"   \n[31] \"job14\"    \"job15\"    \"job16\"    \"job17\"    \"job18\"    \"job19\"   \n[37] \"job20\"    \"job21\"    \"job22\"    \"job23\"    \"job24\"    \"job25\"   \n[43] \"job26\"    \"job27\"    \"job28\"    \"job29\"    \"job30\"    \"job31\"   \n[49] \"job32\"    \"job33\"    \"job34\"    \"job35\"    \"job36\"    \"job37\"   \n[55] \"job38\"    \"job39\"    \"job40\"    \"job41\"    \"job42\"    \"job43\"   \n[61] \"job44\"    \"job45\"    \"job46\"    \"job47\"    \"job48\"    \"job49\"   \n[67] \"job50\"   \n\n\nVariable names - Option 2:\n\n\nCode\ncolnames(df)\n\n\nData structure - Option 1:\n\n\nCode\nstr(df)\n\n\n'data.frame':   946 obs. of  67 variables:\n $ job_yrs : int  1 5 2 4 7 3 4 4 1 0 ...\n $ job_mths: num  6 8 8 3 1 11 2 0 1 11 ...\n $ worktype: int  1 1 1 1 1 1 1 1 1 1 ...\n $ cont_hrs: num  45 36 40 35 40 32 30 54 37 37 ...\n $ act_hrs : num  45 36 40 35 41 32 30 54 37 37 ...\n $ gender  : int  1 2 2 1 1 1 2 1 2 2 ...\n $ dobm    : int  7 1 2 4 2 8 3 5 12 11 ...\n $ doby    : int  83 58 82 78 73 75 72 85 70 70 ...\n $ status  : int  1 2 1 1 2 2 2 2 2 2 ...\n $ youngdep: int  1 2 1 1 2 2 2 2 2 2 ...\n $ elderdep: int  1 1 1 2 1 2 1 1 1 1 ...\n $ qualif  : int  5 5 4 5 4 5 4 5 2 2 ...\n $ exercise: int  4 2 6 3 2 6 4 3 1 3 ...\n $ cigs    : int  1 2 2 1 1 1 2 2 1 1 ...\n $ alcohol : num  1 0 0 2 3 4 0 0 1 4 ...\n $ time_rel: int  2 2 1 2 2 5 1 3 1 2 ...\n $ when_rel: int  1 2 1 1 1 2 1 2 2 2 ...\n $ job1    : int  5 5 1 6 1 3 5 4 6 6 ...\n $ job2    : int  4 7 4 4 1 4 4 1 2 6 ...\n $ job3    : int  5 4 7 5 5 5 6 7 4 7 ...\n $ job4    : int  4 3 1 5 5 3 7 7 5 6 ...\n $ job5    : int  6 6 1 6 1 4 5 7 4 7 ...\n $ job6    : int  4 2 1 4 2 3 5 1 2 7 ...\n $ job7    : int  7 1 7 5 2 4 4 5 4 6 ...\n $ job8    : int  7 4 7 5 2 3 5 5 6 7 ...\n $ job9    : int  4 5 3 5 5 5 6 7 2 7 ...\n $ job10   : int  7 6 1 6 2 4 5 7 4 7 ...\n $ job11   : int  4 1 7 5 3 3 6 1 5 7 ...\n $ job12   : int  4 5 4 3 3 3 3 1 4 7 ...\n $ job13   : int  1 4 1 4 4 4 4 1 5 7 ...\n $ job14   : int  4 5 7 2 6 1 2 1 1 7 ...\n $ job15   : int  4 7 3 7 2 6 6 7 5 5 ...\n $ job16   : int  6 5 1 2 2 3 3 7 4 5 ...\n $ job17   : int  4 1 1 4 2 5 3 7 3 7 ...\n $ job18   : int  7 7 7 6 3 5 5 3 6 2 ...\n $ job19   : int  7 1 7 7 2 6 7 3 6 2 ...\n $ job20   : int  3 5 7 5 1 3 7 6 3 3 ...\n $ job21   : int  4 1 7 5 1 3 3 1 5 6 ...\n $ job22   : int  3 1 1 2 4 1 2 1 2 6 ...\n $ job23   : int  5 1 2 4 6 3 6 1 5 7 ...\n $ job24   : int  4 4 5 4 2 4 4 4 5 6 ...\n $ job25   : int  4 5 6 5 3 2 3 7 5 4 ...\n $ job26   : int  7 6 7 3 6 5 6 7 6 4 ...\n $ job27   : int  2 4 1 2 6 5 1 1 3 7 ...\n $ job28   : int  3 6 7 5 2 5 6 1 4 7 ...\n $ job29   : int  3 3 5 7 5 6 5 1 3 7 ...\n $ job30   : int  4 3 6 5 1 2 3 6 6 4 ...\n $ job31   : int  5 1 5 3 4 5 4 1 5 5 ...\n $ job32   : int  5 7 3 2 2 1 3 2 6 5 ...\n $ job33   : int  6 4 6 5 4 3 7 4 6 4 ...\n $ job34   : int  4 6 5 7 4 5 6 1 5 5 ...\n $ job35   : int  4 6 3 5 4 1 3 1 5 6 ...\n $ job36   : int  4 1 2 3 2 5 2 1 5 6 ...\n $ job37   : int  4 5 1 3 2 3 3 7 3 6 ...\n $ job38   : int  4 5 5 2 2 1 3 1 3 7 ...\n $ job39   : int  5 7 5 7 2 5 6 7 6 5 ...\n $ job40   : int  4 3 5 3 4 3 3 7 3 6 ...\n $ job41   : int  6 5 3 5 1 1 3 1 5 7 ...\n $ job42   : int  6 4 5 4 5 3 4 1 4 5 ...\n $ job43   : int  6 1 7 3 4 5 1 2 3 7 ...\n $ job44   : int  6 4 6 5 2 3 7 1 5 7 ...\n $ job45   : int  6 6 7 7 6 3 6 7 6 7 ...\n $ job46   : int  3 1 2 3 6 6 3 7 3 5 ...\n $ job47   : int  3 4 5 5 1 3 3 3 3 6 ...\n $ job48   : int  3 7 7 7 6 4 6 7 5 6 ...\n $ job49   : int  6 5 5 6 2 2 5 2 6 7 ...\n $ job50   : int  3 5 1 3 1 7 4 1 5 6 ...\n\n\nData structure - Option 2:\n\n\nCode\nglimpse(df)\n\n\nRows: 946\nColumns: 67\n$ job_yrs  <int> 1, 5, 2, 4, 7, 3, 4, 4, 1, 0, 2, 2, 11, 4, 1, 2, 1, 0, 1, 0, …\n$ job_mths <dbl> 6, 8, 8, 3, 1, 11, 2, 0, 1, 11, 0, 5, 1, 2, 10, 10, 4, 3, 0, …\n$ worktype <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1…\n$ cont_hrs <dbl> 45.0, 36.0, 40.0, 35.0, 40.0, 32.0, 30.0, 54.0, 37.0, 37.0, 3…\n$ act_hrs  <dbl> 45.0, 36.0, 40.0, 35.0, 41.0, 32.0, 30.0, 54.0, 37.0, 37.0, 3…\n$ gender   <int> 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1…\n$ dobm     <int> 7, 1, 2, 4, 2, 8, 3, 5, 12, 11, 1, 12, 11, 3, 6, 6, 3, 5, 3, …\n$ doby     <int> 83, 58, 82, 78, 73, 75, 72, 85, 70, 70, 77, 77, 74, 68, 88, 8…\n$ status   <int> 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2…\n$ youngdep <int> 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2…\n$ elderdep <int> 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ qualif   <int> 5, 5, 4, 5, 4, 5, 4, 5, 2, 2, 2, 3, 2, 5, 2, 1, 1, 2, 3, 5, 2…\n$ exercise <int> 4, 2, 6, 3, 2, 6, 4, 3, 1, 3, 3, 2, 4, 1, 4, 3, 3, 4, 5, 2, 4…\n$ cigs     <int> 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 4…\n$ alcohol  <dbl> 1, 0, 0, 2, 3, 4, 0, 0, 1, 4, 0, 10, 40, 30, 10, 0, 15, 0, 15…\n$ time_rel <int> 2, 2, 1, 2, 2, 5, 1, 3, 1, 2, 3, 2, 2, 1, 2, 4, 2, 3, 1, 2, 3…\n$ when_rel <int> 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 4, 2, 1, 2, 1, 1…\n$ job1     <int> 5, 5, 1, 6, 1, 3, 5, 4, 6, 6, 5, 5, 4, 6, 4, 5, 5, 5, 4, 5, 5…\n$ job2     <int> 4, 7, 4, 4, 1, 4, 4, 1, 2, 6, 3, 2, 3, 2, 1, 2, 4, 6, 4, 5, 3…\n$ job3     <int> 5, 4, 7, 5, 5, 5, 6, 7, 4, 7, 1, 4, 3, 2, 4, 3, 1, 6, 3, 5, 6…\n$ job4     <int> 4, 3, 1, 5, 5, 3, 7, 7, 5, 6, 7, 7, 3, 6, 6, 6, 6, 6, 4, 3, 2…\n$ job5     <int> 6, 6, 1, 6, 1, 4, 5, 7, 4, 7, 6, 6, 6, 6, 6, 6, 4, 6, 4, 6, 5…\n$ job6     <int> 4, 2, 1, 4, 2, 3, 5, 1, 2, 7, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1…\n$ job7     <int> 7, 1, 7, 5, 2, 4, 4, 5, 4, 6, 5, 6, 5, 7, 4, 4, 7, 5, 6, 7, 7…\n$ job8     <int> 7, 4, 7, 5, 2, 3, 5, 5, 6, 7, 7, 6, 4, 6, 5, 4, 6, 6, 5, 3, 6…\n$ job9     <int> 4, 5, 3, 5, 5, 5, 6, 7, 2, 7, 3, 4, 6, 2, 1, 2, 1, 5, 2, 6, 3…\n$ job10    <int> 7, 6, 1, 6, 2, 4, 5, 7, 4, 7, 6, 6, 6, 6, 6, 4, 4, 5, 5, 6, 4…\n$ job11    <int> 4, 1, 7, 5, 3, 3, 6, 1, 5, 7, 3, 5, 5, 6, 6, 4, 5, 5, 6, 4, 6…\n$ job12    <int> 4, 5, 4, 3, 3, 3, 3, 1, 4, 7, 4, 2, 1, 4, 2, 2, 2, 2, 2, 2, 2…\n$ job13    <int> 1, 4, 1, 4, 4, 4, 4, 1, 5, 7, 3, 4, 2, 7, 1, 4, 2, 4, 4, 6, 4…\n$ job14    <int> 4, 5, 7, 2, 6, 1, 2, 1, 1, 7, 7, 6, 4, 6, 4, 4, 7, 4, 6, 6, 4…\n$ job15    <int> 4, 7, 3, 7, 2, 6, 6, 7, 5, 5, 4, 4, 7, 6, 3, 5, 5, 4, 4, 3, 5…\n$ job16    <int> 6, 5, 1, 2, 2, 3, 3, 7, 4, 5, 6, 6, 6, 6, 5, 4, 4, 6, 5, 6, 3…\n$ job17    <int> 4, 1, 1, 4, 2, 5, 3, 7, 3, 7, 2, 2, 3, 2, 1, 2, 2, 2, 1, 2, 3…\n$ job18    <int> 7, 7, 7, 6, 3, 5, 5, 3, 6, 2, 3, 5, 5, 6, 5, 5, 6, 6, 4, 5, 3…\n$ job19    <int> 7, 1, 7, 7, 2, 6, 7, 3, 6, 2, 1, 2, 2, 6, 1, 5, 5, 6, 2, 2, 2…\n$ job20    <int> 3, 5, 7, 5, 1, 3, 7, 6, 3, 3, 4, 3, 3, 4, 3, 6, 1, 5, 3, 6, 5…\n$ job21    <int> 4, 1, 7, 5, 1, 3, 3, 1, 5, 6, 3, 5, 5, 6, 4, 4, 3, 5, 4, 2, 5…\n$ job22    <int> 3, 1, 1, 2, 4, 1, 2, 1, 2, 6, 2, 5, 2, 2, 1, 6, 1, 5, 3, 5, 3…\n$ job23    <int> 5, 1, 2, 4, 6, 3, 6, 1, 5, 7, 2, 5, 6, 4, 4, 2, 4, 5, 5, 6, 6…\n$ job24    <int> 4, 4, 5, 4, 2, 4, 4, 4, 5, 6, 4, 6, 4, 2, 3, 6, 5, 6, 5, 4, 2…\n$ job25    <int> 4, 5, 6, 5, 3, 2, 3, 7, 5, 4, 5, 5, 5, 3, 4, 6, 4, 6, 4, 6, 3…\n$ job26    <int> 7, 6, 7, 3, 6, 5, 6, 7, 6, 4, 1, 5, 6, 6, 4, 6, 4, 4, 5, 4, 4…\n$ job27    <int> 2, 4, 1, 2, 6, 5, 1, 1, 3, 7, 6, 3, 1, 1, 1, 2, 2, 4, 6, 6, 2…\n$ job28    <int> 3, 6, 7, 5, 2, 5, 6, 1, 4, 7, 3, 3, 5, 1, 3, 4, 1, 3, 2, 6, 5…\n$ job29    <int> 3, 3, 5, 7, 5, 6, 5, 1, 3, 7, 4, 4, 3, 2, 2, 4, 3, 3, 5, 5, 3…\n$ job30    <int> 4, 3, 6, 5, 1, 2, 3, 6, 6, 4, 7, 6, 4, 4, 3, 4, 4, 6, 4, 2, 3…\n$ job31    <int> 5, 1, 5, 3, 4, 5, 4, 1, 5, 5, 5, 3, 4, 6, 5, 6, 6, 6, 5, 4, 2…\n$ job32    <int> 5, 7, 3, 2, 2, 1, 3, 2, 6, 5, 5, 5, 4, 7, 5, 5, 5, 6, 5, 5, 5…\n$ job33    <int> 6, 4, 6, 5, 4, 3, 7, 4, 6, 4, 6, 3, 4, 2, 3, 6, 6, 6, 4, 2, 2…\n$ job34    <int> 4, 6, 5, 7, 4, 5, 6, 1, 5, 5, 2, 2, 2, 2, 3, 5, 5, 5, 1, 2, 4…\n$ job35    <int> 4, 6, 3, 5, 4, 1, 3, 1, 5, 6, 5, 5, 4, 6, 5, 4, 5, 5, 7, 5, 6…\n$ job36    <int> 4, 1, 2, 3, 2, 5, 2, 1, 5, 6, 5, 3, 4, 6, 5, 6, 6, 5, 7, 5, 5…\n$ job37    <int> 4, 5, 1, 3, 2, 3, 3, 7, 3, 6, 7, 4, 6, 7, 4, 5, 6, 5, 6, 5, 5…\n$ job38    <int> 4, 5, 5, 2, 2, 1, 3, 1, 3, 7, 7, 5, 6, 7, 3, 6, 6, 5, 6, 5, 5…\n$ job39    <int> 5, 7, 5, 7, 2, 5, 6, 7, 6, 5, 4, 5, 6, 7, 3, 5, 5, 5, 2, 1, 4…\n$ job40    <int> 4, 3, 5, 3, 4, 3, 3, 7, 3, 6, 3, 5, 6, 7, 4, 4, 6, 6, 4, 6, 1…\n$ job41    <int> 6, 5, 3, 5, 1, 1, 3, 1, 5, 7, 3, 5, 5, 6, 4, 5, 7, 6, 5, 6, 5…\n$ job42    <int> 6, 4, 5, 4, 5, 3, 4, 1, 4, 5, 4, 6, 5, 7, 4, 5, 5, 6, 4, 3, 5…\n$ job43    <int> 6, 1, 7, 3, 4, 5, 1, 2, 3, 7, 2, 6, 4, 6, 5, 4, 7, 6, 2, 6, 4…\n$ job44    <int> 6, 4, 6, 5, 2, 3, 7, 1, 5, 7, 7, 6, 4, 5, 6, 5, 6, 6, 6, 5, 3…\n$ job45    <int> 6, 6, 7, 7, 6, 3, 6, 7, 6, 7, 5, 4, 6, 6, 5, 6, 4, 6, 4, 2, 6…\n$ job46    <int> 3, 1, 2, 3, 6, 6, 3, 7, 3, 5, 5, 2, 2, 2, 1, 2, 2, 5, 1, 1, 2…\n$ job47    <int> 3, 4, 5, 5, 1, 3, 3, 3, 3, 6, 5, 5, 4, 4, 3, 6, 5, 5, 4, 4, 3…\n$ job48    <int> 3, 7, 7, 7, 6, 4, 6, 7, 5, 6, 5, 3, 6, 3, 4, 5, 4, 5, 3, 1, 6…\n$ job49    <int> 6, 5, 5, 6, 2, 2, 5, 2, 6, 7, 5, 3, 6, 6, 7, 6, 7, 6, 6, 6, 7…\n$ job50    <int> 3, 5, 1, 3, 1, 7, 4, 1, 5, 6, 5, 4, 4, 5, 1, 2, 5, 3, 4, 5, 7…\n\n\n\n\nSanity Checks\nProduce a table of summary statistics for the variables in the data.\n\n\nCode\ndf %>%\n    summarise(across(everything(), \n                     list(M = mean, SD = sd, MIN = min, MAX = max))) %>%\n    pivot_longer(everything())\n\n\n# A tibble: 268 × 2\n   name          value\n   <chr>         <dbl>\n 1 job_yrs_M     4.03 \n 2 job_yrs_SD    5.86 \n 3 job_yrs_MIN  -1    \n 4 job_yrs_MAX  37    \n 5 job_mths_M   NA    \n 6 job_mths_SD  NA    \n 7 job_mths_MIN NA    \n 8 job_mths_MAX NA    \n 9 worktype_M    1.24 \n10 worktype_SD   0.427\n# … with 258 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe can see that there are a few missing values in some variables.\nIf you were to analyse this data for a research project hopefully leading to a paper, you would probably want to perform sanity check on the variables, such as check if everyone is an adult (assuming this was a requirement for partaking of the study).\nCheck whether all participants in the study are adults.\n\n\nCode\nunique(df$doby)\n\n\n [1]   83   58   82   78   73   75   72   85   70   77   74   68   88   81   90\n[16]   59   89   87   80   86   84   67   79   55   71   56   57   62   61   91\n[31]   52   63   69   60   53   76   50   49   64   NA   54    0   66   35   65\n[46]   47   48   45   51   46 1979 1982 1980 1975 1969 1981 1973 1946   -1   43\n[61]   44   42 1971 1976 1983 1965 1985 1955 1950 1974 1984\n\n\nThe data look like a bit of a mess.. Some participants have the full year of birth, some only the last 2 digits. Let’s only extract the last 2 digits from all rows then.\nLet’s use the str_sub() function to take only the last 2 characters.\n\n\nCode\ndf <- df %>%\n    mutate(doby = str_sub(doby, -2, -1))\n\n\nIt seems like it’s now a character rather than a number:\n\n\nCode\nclass(df$doby)\n\n\n[1] \"character\"\n\n\nLet’s make it a number again:\n\n\nCode\ndf$doby <- as.numeric(df$doby)\n\n\nVisualise the distribution of birth year. Do we notice anything strange?\n\n\nCode\nggplot(df, aes(x = doby)) + \n    geom_histogram(color = 'white')\n\n\n\n\n\n\n\n\n\nOr a dotplot if you prefer:\n\n\nCode\nggplot(df, aes(x = doby)) + \n    geom_dotplot(dotsize = 0.6, binwidth = 1, fill = 'dodgerblue', color = NA)\n\n\n\n\n\n\n\n\n\nA year of birth equal to −1 doesn’t make any sense and, since we only want to keep adults, we will remove the rows in the data set having a year of birth equal to -1.\nIn the meantime, we will also remove those participants who don’t have a value for doby.\n\n\nCode\ndf <- df %>%\n    filter(!is.na(doby) | doby > 0)\n\nhist(df$doby)\n\n\n\n\n\n\n\n\n\nThat looks much better!\nNormally, you’d want to check other variables too. For now, because we are focusing on EFA, we’ll just assume that the other variables are okay.\n\n\nSubset\nRemember that the only variables we are interested in for our EFA are the job1 to job50 variables. We need to subset the data set to only include those variables.\n\n\nCode\ndf <- df %>%\n    select(job1:job50)\n\n\nCreate a table of descriptive summary statistics for each variable.\n\n\nCode\nlibrary(psych)\ndescribe(df)\n\n\n      vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\njob1     1 943 4.20 2.43      4    4.18 1.48  -1  55    56  9.62   201.28 0.08\njob2     2 943 3.76 2.00      4    3.71 2.97  -1   7     8  0.07    -1.29 0.06\njob3     3 943 4.33 2.05      5    4.41 2.97  -1   7     8 -0.24    -1.24 0.07\njob4     4 943 4.05 2.04      4    4.06 2.97  -1   7     8 -0.08    -1.29 0.07\njob5     5 943 4.77 2.11      5    4.89 1.48   0  36    36  3.05    49.63 0.07\njob6     6 943 3.38 2.00      3    3.25 2.97   1   7     6  0.34    -1.20 0.07\njob7     7 943 3.79 2.13      4    3.74 2.97  -1   7     8  0.04    -1.38 0.07\njob8     8 943 4.47 1.72      5    4.55 1.48  -1   7     8 -0.47    -0.66 0.06\njob9     9 943 4.06 1.99      4    4.08 2.97  -1   7     8 -0.06    -1.18 0.06\njob10   10 943 4.52 1.80      5    4.63 1.48   0   7     7 -0.44    -0.81 0.06\njob11   11 943 4.68 1.78      5    4.81 1.48   1   7     6 -0.60    -0.69 0.06\njob12   12 943 3.67 2.03      4    3.59 2.97   0   7     7  0.21    -1.25 0.07\njob13   13 943 3.76 1.94      4    3.71 2.97  -1   7     8  0.10    -1.15 0.06\njob14   14 943 4.77 2.45      5    4.88 1.48   1  52    51  7.34   144.84 0.08\njob15   15 943 3.91 1.91      4    3.89 2.97  -1  14    15  0.08    -0.34 0.06\njob16   16 943 4.11 1.77      4    4.17 1.48  -1   7     8 -0.23    -0.97 0.06\njob17   17 943 3.73 1.97      3    3.66 2.97   1   7     6  0.18    -1.27 0.06\njob18   18 943 4.20 1.79      5    4.26 1.48  -1   7     8 -0.29    -0.97 0.06\njob19   19 943 2.83 1.89      2    2.61 1.48  -1   7     8  0.73    -0.67 0.06\njob20   20 943 4.02 1.98      4    4.03 2.97  -1   7     8 -0.07    -1.15 0.06\njob21   21 943 4.35 1.66      5    4.42 1.48  -1   7     8 -0.40    -0.50 0.05\njob22   22 943 3.60 1.90      3    3.54 1.48  -1   7     8  0.24    -1.11 0.06\njob23   23 943 4.06 1.88      4    4.08 2.97  -1   7     8 -0.10    -1.15 0.06\njob24   24 943 3.83 1.69      4    3.86 1.48  -1   7     8 -0.11    -0.76 0.06\njob25   25 943 3.97 1.77      4    3.99 1.48  -1   7     8 -0.15    -0.65 0.06\njob26   26 943 4.13 1.81      4    4.17 2.97   0   7     7 -0.20    -1.07 0.06\njob27   27 943 3.84 2.22      4    3.80 2.97   1   7     6  0.11    -1.45 0.07\njob28   28 943 3.79 2.02      4    3.74 2.97   1   7     6  0.16    -1.26 0.07\njob29   29 943 4.55 1.76      5    4.63 1.48  -1   7     8 -0.38    -0.80 0.06\njob30   30 943 3.99 1.92      4    3.98 2.97   1   7     6 -0.10    -1.16 0.06\njob31   31 943 4.20 2.00      4    4.25 2.97  -1   7     8 -0.22    -1.21 0.07\njob32   32 943 5.14 1.47      5    5.29 1.48   0   7     7 -0.78     0.24 0.05\njob33   33 943 3.59 1.80      4    3.55 2.97  -1   7     8  0.13    -0.99 0.06\njob34   34 943 3.56 1.86      4    3.50 2.97   0   7     7  0.14    -1.13 0.06\njob35   35 943 4.89 1.51      5    5.02 1.48  -1   7     8 -0.84     0.51 0.05\njob36   36 943 4.52 1.81      5    4.63 1.48  -1   7     8 -0.46    -0.79 0.06\njob37   37 943 4.32 1.96      5    4.40 1.48   1   7     6 -0.33    -1.13 0.06\njob38   38 943 4.44 1.87      5    4.54 1.48  -1   7     8 -0.45    -0.93 0.06\njob39   39 943 3.83 1.85      4    3.83 1.48  -1   7     8 -0.07    -1.08 0.06\njob40   40 943 4.29 1.86      4    4.36 2.97  -1   7     8 -0.30    -0.96 0.06\njob41   41 943 4.73 1.55      5    4.84 1.48   1   7     6 -0.60    -0.17 0.05\njob42   42 943 4.20 1.59      5    4.27 1.48   1   8     7 -0.36    -0.72 0.05\njob43   43 943 3.52 1.96      3    3.43 2.97  -1   7     8  0.21    -1.16 0.06\njob44   44 943 3.75 1.88      4    3.71 2.97  -1   7     8  0.07    -1.06 0.06\njob45   45 943 4.50 1.77      5    4.60 1.48  -1   7     8 -0.50    -0.60 0.06\njob46   46 943 3.46 2.05      3    3.36 2.97  -1   7     8  0.23    -1.29 0.07\njob47   47 943 3.71 1.61      4    3.74 1.48  -1   7     8 -0.08    -0.81 0.05\njob48   48 943 4.38 1.94      5    4.48 1.48  -1   7     8 -0.36    -1.09 0.06\njob49   49 943 5.68 1.33      6    5.88 1.48  -1   7     8 -1.34     1.93 0.04\njob50   50 943 4.18 1.97      5    4.23 1.48  -1   7     8 -0.27    -1.17 0.06\n\n\nSome of the variables appear to have values of 0, −1, as well as values larger than 7, even though all the questionnaire items are on a 7-point Likert scale.\nGet rid of infeasible values:\n\n\nCode\ndf[df < 1 | df > 7] <- NA\n\n\n\n\nDescriptives & Visualising\nLet’s now look at the score distributions per item and the correlations between pairs of items.\nSometimes easier to use subsets of ten variables each time. For example, look at the pairwise plots of the first 10 variables, then the next 10, and so on.\n\n\nCode\npairs.panels(df[, 1:10])\n\n\n\n\n\n\n\n\n\nCode\npairs.panels(df[, 11:20])\n\n\n\n\n\n\n\n\n\nCode\npairs.panels(df[, 21:30])\n\n\n\n\n\n\n\n\n\nCode\npairs.panels(df[, 31:40])\n\n\n\n\n\n\n\n\n\nCode\npairs.panels(df[, 41:50])\n\n\n\n\n\n\n\n\n\nAs you can see, while some of the items have pretty much bell-shaped distributions, some others are massively skewed (looking at you job49) or close to uniform (job9). At this stage, you’d want to have a closer look at the wording of these troublesome items and see if you can spot any methodological issues that might account for these distributions. If the items look fine, you might want to consider alternative correlation coefficients (e.g., polychoric correlations) that might be more suitable to items with weird distributions. For now, let’s stick to Pearson’s correlation (r). Since we have NAs in the data, let’s just use complete observations.\nCompute the correlation matrix of the variables.\nInstead of looking at the \\(50 \\times 50\\) matrix of correlations, look at the distribution of correlation coefficients from the lower triangular part of the matrix.\nOption 1:\n\n\nCode\nR <- cor(df, use = \"complete.obs\")\n\n\nOption 2:\n\n\nCode\nR <- cor(na.omit(df))\n\n\n\n\nCode\nhist(R[lower.tri(R)])\n\n\n\n\n\n\n\n\n\nIf you want to be a little fancier, you can categorise the coefficients into negligible, weak, moderate, and strong correlations and plot a bar plot like this:\n\n\nCode\nRc <- cut(abs(R), \n          breaks = c(0, .2, .5, .7, 1), \n          labels = c(\"negligible\", \"weak\", \"moderate\", \"strong\"))\n\nbarplot(table(Rc[lower.tri(Rc)]))\n\n\n\n\n\n\n\n\n\nAs we can see, most of the correlations are negligible and many are weak. There are some moderate and strong relationships in the data. This suggests that there might be multiple independent factors.\n\n\nSuitability for FA\nCheck if the correlations are sufficient for EFA with Bartlett’s test of sphericity and if the sample was adequate with KMO.\n\n\nCode\ncortest.bartlett(R, \n                 n = sum(complete.cases(df))) # we have 917 complete cases\n\n\n$chisq\n[1] 18702.7\n\n$p.value\n[1] 0\n\n$df\n[1] 1225\n\n\n\n\nCode\nKMO(R)\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = R)\nOverall MSA =  0.89\nMSA for each item = \n job1  job2  job3  job4  job5  job6  job7  job8  job9 job10 job11 job12 job13 \n 0.95  0.77  0.76  0.88  0.88  0.81  0.91  0.93  0.77  0.84  0.74  0.93  0.80 \njob14 job15 job16 job17 job18 job19 job20 job21 job22 job23 job24 job25 job26 \n 0.76  0.93  0.90  0.76  0.92  0.87  0.87  0.89  0.92  0.79  0.93  0.95  0.93 \njob27 job28 job29 job30 job31 job32 job33 job34 job35 job36 job37 job38 job39 \n 0.92  0.85  0.78  0.95  0.80  0.79  0.90  0.88  0.82  0.76  0.76  0.83  0.91 \njob40 job41 job42 job43 job44 job45 job46 job47 job48 job49 job50 \n 0.93  0.84  0.96  0.91  0.91  0.95  0.81  0.94  0.94  0.85  0.90 \n\n\nA significant Bartlett’s test of sphericity means that our correlation matrix is not proportional to an identity matrix (a matrix with only 1s on the diagonal and 0s everywhere else). This is exactly what we want, so we’re happy!\nLikewise, the sampling adequacy is pretty good. All items have a measure of sampling adequacy (MSA) in the \\(>.7\\) “middling” region and the overall KMO is bordering on the \\(>.9\\) “marvellous” level (I kid you not).\nGiven these results, we can merrily factor-analyse!\n\n\nNumber of Factors\nIn order to decide how many factors to use, look at the suggestions given by parallel analysis and MAP.\n\n\nCode\nfa.parallel(df, fa = 'fa')\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  11  and the number of components =  NA \n\n\n\n\nCode\nVSS(df)\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.68  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.78  with  5  factors\n\nThe Velicer MAP achieves a minimum of 0.01  with  7  factors \nBIC achieves a minimum of  -2604.78  with  8  factors\nSample Size adjusted BIC achieves a minimum of  104.3  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2    map  dof chisq     prob sqresid  fit RMSEA   BIC SABIC complex\n1 0.63 0.00 0.0149 1175 11124  0.0e+00      59 0.63 0.095  3076  6808     1.0\n2 0.68 0.71 0.0136 1126  9339  0.0e+00      47 0.71 0.088  1627  5203     1.1\n3 0.59 0.75 0.0119 1078  7703  0.0e+00      37 0.77 0.081   320  3743     1.4\n4 0.51 0.76 0.0103 1031  6309  0.0e+00      30 0.81 0.074  -753  2522     1.6\n5 0.51 0.78 0.0085  985  4953  0.0e+00      25 0.85 0.065 -1793  1335     1.7\n6 0.50 0.75 0.0086  940  4248  0.0e+00      22 0.86 0.061 -2191   795     1.9\n7 0.46 0.72 0.0084  896  3660  0.0e+00      20 0.87 0.057 -2477   369     2.0\n8 0.46 0.70 0.0086  853  3237 1.3e-273      19 0.88 0.054 -2605   104     2.0\n  eChisq  SRMR eCRMS  eBIC\n1  23561 0.101 0.103 15513\n2  16220 0.084 0.087  8508\n3  10627 0.068 0.072  3243\n4   7270 0.056 0.061   208\n5   4414 0.044 0.049 -2332\n6   3488 0.039 0.044 -2950\n7   2746 0.034 0.040 -3391\n8   2167 0.031 0.037 -3675\n\n\nSince parallel analysis (suggesting 11 factor) tends to overextract and MAP (suggesting 6-8 factors) can sometimes underextract, it is reasonable to look at solutions with 7-10 factors. However, looking at the scree plot, it might be reasonable to cast a glance on a 5- or 6-factor solution.\n\n\nPerform EFA\nFit a factor analysis model to the data using 10 factors. Since there is no good reason to expect the factors to be uncorrelated (orthogonal), let’s use the oblimin rotation.\n\n\nCode\nm_10f <- fa(df, nfactors = 10, rotate = \"oblimin\", fm = \"ml\")\n\n\nPrint loadings sorted according to loadings\n\n\nCode\nfa.sort(m_10f)\n\n\nFactor Analysis using method =  ml\nCall: fa(r = df, nfactors = 10, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n        ML1   ML3   ML2   ML4   ML5   ML6   ML7   ML9   ML8  ML10   h2   u2 com\njob42  0.72 -0.10  0.02 -0.02  0.10 -0.06 -0.15 -0.02  0.15  0.03 0.59 0.41 1.3\njob25  0.68 -0.09  0.11  0.01  0.00 -0.17  0.00 -0.15  0.03  0.07 0.53 0.47 1.4\njob45  0.66  0.08 -0.09 -0.08  0.09 -0.21  0.13  0.03 -0.11 -0.01 0.54 0.46 1.5\njob18  0.65 -0.13  0.13 -0.06  0.11 -0.02 -0.20  0.11  0.33 -0.06 0.64 0.36 2.1\njob15  0.65  0.11 -0.08 -0.13  0.15 -0.37  0.19 -0.03 -0.09 -0.05 0.67 0.33 2.2\njob47  0.63  0.02  0.01 -0.13  0.07  0.00 -0.12 -0.16  0.12 -0.02 0.48 0.52 1.4\njob39  0.61  0.17 -0.13 -0.12  0.17 -0.27  0.17 -0.10 -0.09  0.05 0.58 0.42 2.3\njob24  0.60 -0.07  0.32 -0.10  0.04  0.15 -0.09  0.01 -0.12  0.01 0.53 0.47 2.0\njob1   0.60 -0.11  0.16 -0.10  0.04 -0.11 -0.05 -0.06  0.15  0.22 0.50 0.50 1.8\njob26  0.59  0.11 -0.18 -0.22  0.11 -0.04 -0.03 -0.12  0.10 -0.05 0.49 0.51 1.9\njob8   0.59 -0.12  0.05  0.08  0.09  0.04 -0.16  0.14  0.25 -0.06 0.49 0.51 1.9\njob38  0.58  0.22 -0.48  0.05 -0.19  0.18 -0.15 -0.11 -0.08  0.15 0.75 0.25 3.3\njob48  0.55  0.05 -0.01  0.00 -0.02 -0.11  0.10 -0.13 -0.03  0.03 0.35 0.65 1.3\njob33  0.55 -0.04  0.41 -0.16  0.12  0.19 -0.07  0.15 -0.26 -0.04 0.65 0.35 3.3\njob44  0.55  0.03  0.42 -0.05  0.05  0.24 -0.06  0.14 -0.26 -0.04 0.63 0.37 3.1\njob30  0.51 -0.09  0.15 -0.07  0.09 -0.07 -0.13  0.05  0.04  0.21 0.38 0.62 2.0\njob12 -0.50  0.29  0.12  0.04  0.13  0.26 -0.10  0.01  0.13  0.05 0.47 0.53 2.9\njob40  0.50  0.01 -0.27  0.03 -0.06  0.00 -0.07 -0.07  0.02 -0.06 0.34 0.66 1.7\njob16  0.47 -0.06  0.17  0.03 -0.31  0.09  0.25 -0.11  0.19 -0.01 0.47 0.53 3.4\njob5   0.47 -0.06  0.30  0.04 -0.35  0.21  0.34 -0.04  0.09  0.00 0.61 0.39 4.2\njob43  0.46 -0.03  0.23 -0.03 -0.06  0.04 -0.21 -0.07  0.19  0.12 0.37 0.63 2.7\njob22 -0.46  0.07  0.13 -0.03 -0.10  0.27 -0.10  0.01  0.07  0.16 0.35 0.65 2.4\njob10  0.45 -0.02  0.23  0.13 -0.42  0.26  0.35 -0.01  0.18  0.00 0.67 0.33 4.7\njob27 -0.44 -0.04 -0.08  0.00  0.14  0.26 -0.05 -0.02  0.19  0.08 0.33 0.67 2.5\njob34  0.37  0.22 -0.07 -0.18  0.29 -0.20  0.24  0.04 -0.03 -0.05 0.41 0.59 4.9\njob19  0.36 -0.01  0.29 -0.32  0.18  0.04 -0.13  0.04  0.17 -0.08 0.40 0.60 4.4\njob7   0.33  0.00  0.16 -0.06  0.17  0.07 -0.12 -0.03  0.09  0.08 0.20 0.80 2.9\njob21  0.33  0.22 -0.14  0.12  0.20 -0.02  0.17  0.18  0.06  0.02 0.30 0.70 4.9\njob50 -0.33  0.18  0.01  0.11  0.02 -0.06  0.03  0.00 -0.01  0.09 0.16 0.84 2.2\njob20 -0.28  0.14 -0.26  0.12  0.08 -0.01  0.11  0.00  0.21 -0.13 0.26 0.74 4.9\njob3   0.05  0.59  0.26  0.09 -0.32 -0.22 -0.15 -0.01  0.06 -0.03 0.61 0.39 2.6\njob28 -0.06  0.58  0.18  0.03 -0.08 -0.16 -0.06  0.08  0.03 -0.13 0.44 0.56 1.6\njob13 -0.12  0.53  0.25  0.17 -0.27 -0.18 -0.16  0.04 -0.01  0.09 0.53 0.47 3.0\njob9   0.11  0.53  0.26  0.13 -0.32 -0.25 -0.06  0.05  0.00 -0.04 0.55 0.45 3.1\njob2  -0.07  0.49  0.05  0.03 -0.19 -0.10 -0.16  0.12  0.09 -0.23 0.40 0.60 2.5\njob46 -0.10  0.49  0.17 -0.15  0.29  0.18  0.14 -0.17  0.04  0.01 0.47 0.53 3.2\njob6  -0.22  0.46  0.29 -0.06  0.34  0.25  0.05 -0.09  0.04 -0.01 0.53 0.47 4.1\njob17 -0.22  0.44  0.27 -0.14  0.42  0.22  0.14 -0.22  0.09  0.01 0.63 0.37 4.9\njob29 -0.06  0.34  0.18 -0.02  0.25  0.05  0.04  0.04 -0.01  0.21 0.26 0.74 3.4\njob37  0.48  0.28 -0.62 -0.11 -0.08  0.31 -0.04  0.01 -0.06 -0.08 0.82 0.18 3.1\njob4   0.45  0.01  0.48  0.02 -0.13  0.16 -0.02  0.11 -0.29  0.03 0.57 0.43 3.2\njob36  0.26 -0.09  0.11  0.71  0.14  0.04 -0.08 -0.29 -0.07 -0.17 0.74 0.26 2.0\njob35  0.23  0.11 -0.10  0.55  0.24  0.00  0.08  0.21  0.02  0.18 0.51 0.49 2.7\njob32  0.21  0.10 -0.13  0.49  0.12  0.00  0.09  0.23  0.04  0.24 0.45 0.55 3.0\njob41  0.26  0.10 -0.20  0.48  0.24  0.01  0.09  0.17  0.02  0.09 0.45 0.55 3.2\njob31  0.28 -0.19  0.19  0.42  0.10  0.03 -0.14 -0.25 -0.08 -0.13 0.44 0.56 4.4\njob49  0.30 -0.02 -0.15  0.40  0.05  0.02  0.03  0.22  0.04 -0.04 0.33 0.67 2.9\njob14 -0.02  0.23 -0.03  0.39  0.14  0.00  0.01 -0.07 -0.08  0.18 0.27 0.73 2.6\njob11  0.21  0.05 -0.08  0.17  0.22  0.12  0.14  0.33  0.11 -0.29 0.37 0.63 5.5\njob23  0.21 -0.01  0.07  0.05  0.21  0.08  0.01  0.24  0.12 -0.15 0.19 0.81 4.8\n\n                       ML1  ML3  ML2  ML4  ML5  ML6  ML7  ML9  ML8 ML10\nSS loadings           9.48 2.90 2.56 2.28 1.86 1.32 0.96 0.86 0.85 0.66\nProportion Var        0.19 0.06 0.05 0.05 0.04 0.03 0.02 0.02 0.02 0.01\nCumulative Var        0.19 0.25 0.30 0.34 0.38 0.41 0.43 0.44 0.46 0.47\nProportion Explained  0.40 0.12 0.11 0.10 0.08 0.06 0.04 0.04 0.04 0.03\nCumulative Proportion 0.40 0.52 0.63 0.73 0.80 0.86 0.90 0.94 0.97 1.00\n\nMean item complexity =  2.9\nTest of the hypothesis that 10 factors are sufficient.\n\nThe degrees of freedom for the null model are  1225  and the objective function was  20.66 with Chi Square of  19096.72\nThe degrees of freedom for the model are 770  and the objective function was  2.45 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic number of observations is  940 with the empirical chi square  1420.85  with prob <  3e-41 \nThe total number of observations was  943  with Likelihood Chi Square =  2249.02  with prob <  1.2e-144 \n\nTucker Lewis Index of factoring reliability =  0.867\nRMSEA index =  0.045  and the 90 % confidence intervals are  0.043 0.047\nBIC =  -3024.76\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   ML1  ML3  ML2  ML4  ML5  ML6\nCorrelation of (regression) scores with factors   0.98 0.93 0.94 0.92 0.90 0.88\nMultiple R square of scores with factors          0.96 0.86 0.88 0.84 0.81 0.77\nMinimum correlation of possible factor scores     0.91 0.73 0.76 0.68 0.61 0.54\n                                                   ML7  ML9  ML8 ML10\nCorrelation of (regression) scores with factors   0.83 0.80 0.81 0.74\nMultiple R square of scores with factors          0.68 0.64 0.66 0.55\nMinimum correlation of possible factor scores     0.37 0.28 0.31 0.11\n\n\nOK, 10 factors looks like way too many as the last 2 have very few substantive loadings (>.33). Let’s look at a smaller solution, e.g. 9 or 8 factors and see if it’s still the case…\n\n\nCode\nm_9f <- fa(df, nfactors = 9, rotate = \"oblimin\", fm = \"ml\")\nm_8f <- fa(df, nfactors = 8, rotate = \"oblimin\", fm = \"ml\")\n\n\nIt was the case and even the 9-factor solution has only 1 substantive loading on the last factor. These are however quite large so let’s look at this solution a little closer. We can see that a few items don’t have any loadings larger than our .33 cut-off: We can see that a few items don’t have any loadings larger than our .33 cut-off:\n\n\nCode\n# get loadings\nx <- m_9f$loadings\nwhich(rowSums(x < .33) == 9)\n\n\njob11 job12 job20 job22 job23 job27 job31 job50 \n   11    12    20    22    23    27    31    50 \n\n\nIt was the case and even the 8-factor solution has only 2 substantive loadings on the last factor. These are however quite large so let’s look at this solution a little closer. We can see that a few items don’t have any loadings larger than our .33 cut-off:\n\n\nCode\n# get loadings\nx <- m_8f$loadings\nwhich(rowSums(x < .33) == 8)\n\n\n job7 job11 job14 job20 job22 job23 job27 job31 job50 \n    7    11    14    20    22    23    27    31    50 \n\n\nHere is when we would go back to the item wordings and try to see why these items might not really correlate with any other items. For instance, job11 (“I regularly discuss problems at work with my colleagues.”) might be ambiguous: does it mean that there are often problems or that if there are problems, I discuss them regularly?\nFor argument’s sake, let’s say, all of these identified items are deemed problematic so we should remove them:\n\n\nCode\ncols_to_remove <- names(which(rowSums(x < .33) == 8))\ndf2 <- df[ , !names(df) %in% cols_to_remove]\n\n\nCheck parallel analysis and MAP again.\n\n\nCode\nfa.parallel(df2, fa = 'fa')\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  9  and the number of components =  NA \n\n\n\n\nCode\nVSS(df2)\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.7  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.81  with  5  factors\n\nThe Velicer MAP achieves a minimum of 0.01  with  8  factors \nBIC achieves a minimum of  -1789.88  with  8  factors\nSample Size adjusted BIC achieves a minimum of  -138.4  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq     prob sqresid  fit RMSEA   BIC SABIC complex\n1 0.64 0.00 0.018 779  9036  0.0e+00      46 0.64 0.106  3701  6175     1.0\n2 0.70 0.73 0.016 739  7329  0.0e+00      34 0.73 0.097  2267  4614     1.1\n3 0.58 0.77 0.014 700  5910  0.0e+00      27 0.79 0.089  1116  3339     1.4\n4 0.52 0.78 0.013 662  4724  0.0e+00      21 0.83 0.081   190  2292     1.7\n5 0.55 0.81 0.010 625  3568  0.0e+00      17 0.87 0.071  -713  1272     1.6\n6 0.55 0.80 0.011 589  2958 5.5e-311      15 0.88 0.065 -1076   795     1.7\n7 0.50 0.74 0.010 554  2316 2.0e-213      14 0.89 0.058 -1478   281     1.7\n8 0.46 0.73 0.010 520  1772 4.4e-136      12 0.90 0.051 -1790  -138     1.8\n  eChisq  SRMR eCRMS  eBIC\n1  18464 0.109 0.112 13129\n2  11631 0.087 0.091  6569\n3   7521 0.070 0.075  2726\n4   4842 0.056 0.062   308\n5   2772 0.042 0.048 -1509\n6   2045 0.036 0.043 -1989\n7   1429 0.030 0.037 -2365\n8   1013 0.026 0.032 -2548\n\n\nFit an 8-factor model to the new dataset.\n\n\nCode\nm_8f <- fa(df2, nfactors = 8, rotate = \"oblimin\", fm = \"ml\")\nfa.sort(m_8f)\n\n\nFactor Analysis using method =  ml\nCall: fa(r = df2, nfactors = 8, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n        ML1   ML2   ML3   ML4   ML5   ML6   ML7   ML8   h2    u2 com\njob37  0.82 -0.51  0.05  0.01 -0.10  0.02 -0.02 -0.01 0.94 0.056 1.7\njob38  0.74 -0.20  0.01  0.15  0.05 -0.01 -0.06  0.03 0.61 0.388 1.3\njob26  0.61  0.10  0.02 -0.22  0.07 -0.14  0.02  0.14 0.48 0.520 1.6\njob42  0.60  0.40 -0.13 -0.09  0.11 -0.01 -0.13  0.19 0.60 0.397 2.4\njob45  0.60  0.26  0.00 -0.11  0.21 -0.11  0.09 -0.15 0.52 0.477 2.1\njob39  0.58  0.21  0.06 -0.21  0.30 -0.18  0.18 -0.15 0.60 0.398 2.9\njob15  0.57  0.29  0.02 -0.19  0.32 -0.26  0.16 -0.18 0.68 0.322 3.5\njob47  0.55  0.30 -0.01 -0.14  0.01 -0.08 -0.05  0.19 0.46 0.539 2.0\njob40  0.54  0.01 -0.12  0.06  0.09 -0.06 -0.03  0.05 0.32 0.677 1.2\njob25  0.51  0.46 -0.08 -0.01  0.11 -0.10  0.03  0.05 0.51 0.492 2.2\njob18  0.51  0.43 -0.10 -0.09  0.02  0.02 -0.19  0.32 0.60 0.397 3.2\njob48  0.47  0.27  0.01  0.00  0.12 -0.08  0.13  0.00 0.33 0.667 2.0\njob8   0.47  0.34 -0.11  0.01  0.06  0.12 -0.17  0.24 0.45 0.545 3.0\njob12 -0.41 -0.25  0.34 -0.06 -0.10  0.19 -0.07  0.14 0.41 0.586 3.7\njob16  0.37  0.34 -0.01  0.27 -0.17 -0.06  0.30  0.13 0.47 0.528 4.6\njob34  0.36  0.12  0.15 -0.31  0.27 -0.10  0.20 -0.13 0.40 0.602 4.7\njob21  0.34  0.03  0.12 -0.04  0.29  0.15  0.11 -0.03 0.25 0.746 3.0\njob4   0.27  0.52  0.19  0.12 -0.28  0.13 -0.12 -0.29 0.58 0.420 3.7\njob24  0.43  0.51  0.04 -0.09 -0.20  0.09 -0.11 -0.06 0.52 0.479 2.6\njob44  0.39  0.51  0.19 -0.06 -0.27  0.22 -0.18 -0.26 0.68 0.325 4.4\njob33  0.38  0.51  0.12 -0.18 -0.26  0.15 -0.16 -0.21 0.61 0.393 3.9\njob1   0.44  0.46 -0.08 -0.08  0.05 -0.08 -0.01  0.15 0.45 0.547 2.4\njob5   0.34  0.42  0.04  0.32 -0.30  0.03  0.34  0.02 0.61 0.390 4.8\njob30  0.37  0.40 -0.06 -0.10  0.05  0.00 -0.13  0.05 0.33 0.669 2.5\njob43  0.33  0.39  0.04  0.03 -0.08 -0.04 -0.14  0.22 0.34 0.661 3.1\njob19  0.25  0.37  0.09 -0.31 -0.16 -0.09 -0.11  0.15 0.37 0.629 4.2\njob3   0.02  0.10  0.62  0.33  0.12 -0.27 -0.13  0.06 0.61 0.388 2.2\njob28 -0.03 -0.02  0.59  0.10  0.15 -0.16 -0.08  0.00 0.42 0.583 1.4\njob6  -0.20 -0.04  0.57 -0.32 -0.06  0.27  0.08  0.12 0.57 0.432 2.6\njob13 -0.14  0.04  0.56  0.32  0.16 -0.15 -0.15 -0.02 0.51 0.486 2.3\njob9   0.06  0.15  0.56  0.35  0.15 -0.23 -0.07 -0.03 0.53 0.466 2.5\njob46 -0.04 -0.08  0.54 -0.32 -0.04  0.12  0.20  0.08 0.47 0.531 2.2\njob17 -0.21 -0.03  0.54 -0.42 -0.05  0.21  0.22  0.15 0.63 0.374 3.2\njob2   0.00 -0.12  0.47  0.19  0.08 -0.18 -0.18  0.04 0.34 0.659 2.3\njob29 -0.06  0.03  0.37 -0.19  0.08  0.16  0.02  0.01 0.21 0.789 2.1\njob10  0.35  0.34  0.06  0.44 -0.28  0.10  0.35  0.11 0.66 0.341 5.0\njob41  0.28 -0.01 -0.02  0.13  0.43  0.42  0.01 -0.02 0.46 0.542 2.9\njob35  0.20  0.06  0.04  0.19  0.47  0.50  0.00 -0.02 0.55 0.453 2.7\njob32  0.20  0.03  0.00  0.24  0.40  0.42  0.01 -0.01 0.43 0.567 3.1\njob36  0.12  0.22 -0.05  0.22  0.22  0.35 -0.03  0.03 0.28 0.716 3.7\njob49  0.27  0.06 -0.09  0.23  0.26  0.28 -0.04  0.00 0.29 0.712 4.3\n\n                       ML1  ML2  ML3  ML4  ML5  ML6  ML7  ML8\nSS loadings           6.73 3.65 2.97 1.86 1.75 1.53 0.88 0.75\nProportion Var        0.16 0.09 0.07 0.05 0.04 0.04 0.02 0.02\nCumulative Var        0.16 0.25 0.33 0.37 0.41 0.45 0.47 0.49\nProportion Explained  0.33 0.18 0.15 0.09 0.09 0.08 0.04 0.04\nCumulative Proportion 0.33 0.52 0.66 0.76 0.84 0.92 0.96 1.00\n\nMean item complexity =  2.9\nTest of the hypothesis that 8 factors are sufficient.\n\nThe degrees of freedom for the null model are  820  and the objective function was  17.47 with Chi Square of  16202.67\nThe degrees of freedom for the model are 520  and the objective function was  1.89 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic number of observations is  940 with the empirical chi square  1052.5  with prob <  2.3e-38 \nThe total number of observations was  943  with Likelihood Chi Square =  1745.24  with prob <  4.8e-132 \n\nTucker Lewis Index of factoring reliability =  0.874\nRMSEA index =  0.05  and the 90 % confidence intervals are  0.047 0.053\nBIC =  -1816.28\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML3  ML4  ML5  ML6\nCorrelation of (regression) scores with factors   0.98 0.96 0.93 0.90 0.89 0.87\nMultiple R square of scores with factors          0.96 0.92 0.86 0.80 0.80 0.76\nMinimum correlation of possible factor scores     0.92 0.85 0.72 0.61 0.59 0.51\n                                                   ML7  ML8\nCorrelation of (regression) scores with factors   0.82 0.79\nMultiple R square of scores with factors          0.67 0.63\nMinimum correlation of possible factor scores     0.34 0.26\n\n\nWe still only get 2 substantive loadings on the last factor, while we want at least 3. This also happens with the 7- and 6-factor solutions. But once we hit the 5-factor solution, we find a better structure:\n\n\nCode\nm_5f <- fa(df2, nfactors = 5, rotate = \"oblimin\", fm = \"ml\")\nfa.sort(m_5f)\n\n\nFactor Analysis using method =  ml\nCall: fa(r = df2, nfactors = 5, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n        ML1   ML2   ML3   ML4   ML5   h2   u2 com\njob42  0.73 -0.08 -0.01  0.04  0.00 0.54 0.46 1.0\njob25  0.69 -0.02 -0.03 -0.05 -0.05 0.48 0.52 1.0\njob15  0.66  0.03  0.14  0.19 -0.24 0.55 0.45 1.6\njob45  0.66  0.01  0.13  0.13 -0.13 0.48 0.52 1.2\njob18  0.66 -0.04 -0.11  0.00  0.05 0.45 0.55 1.1\njob47  0.64  0.02 -0.05  0.11 -0.12 0.44 0.56 1.1\njob1   0.63 -0.01 -0.12 -0.01 -0.02 0.41 0.59 1.1\njob24  0.62  0.11 -0.32 -0.05  0.14 0.52 0.48 1.7\njob39  0.62  0.05  0.18  0.26 -0.22 0.53 0.47 1.9\njob26  0.59  0.00  0.06  0.25 -0.24 0.47 0.53 1.7\njob8   0.58 -0.07  0.00 -0.04  0.13 0.36 0.64 1.1\njob33  0.57  0.17 -0.38  0.01  0.20 0.54 0.46 2.3\njob44  0.56  0.24 -0.32 -0.07  0.26 0.55 0.45 2.6\njob48  0.55  0.03  0.08  0.03 -0.09 0.32 0.68 1.1\njob30  0.53  0.00 -0.11  0.00  0.06 0.30 0.70 1.1\njob38  0.51 -0.07  0.33  0.01 -0.15 0.39 0.61 2.0\njob12 -0.50  0.30 -0.03  0.12  0.15 0.38 0.62 2.0\njob40  0.48 -0.14  0.20  0.01 -0.14 0.31 0.69 1.8\njob43  0.47  0.10 -0.16 -0.13  0.01 0.28 0.72 1.5\njob16  0.47  0.03 -0.09 -0.24 -0.02 0.28 0.72 1.6\njob5   0.46  0.08 -0.18 -0.30  0.08 0.34 0.66 2.2\njob4   0.46  0.25 -0.31 -0.26  0.22 0.49 0.51 3.7\njob10  0.42  0.09 -0.08 -0.34  0.12 0.32 0.68 2.3\njob37  0.39 -0.09  0.30  0.18 -0.20 0.33 0.67 3.1\njob19  0.39  0.14 -0.36  0.14 -0.05 0.32 0.68 2.6\njob34  0.38  0.13  0.13  0.37 -0.15 0.34 0.66 2.8\njob21  0.32  0.09  0.31  0.20  0.09 0.26 0.74 3.0\njob3   0.05  0.65  0.19 -0.32 -0.21 0.60 0.40 1.9\njob28 -0.06  0.59  0.18 -0.06 -0.15 0.41 0.59 1.4\njob9   0.10  0.59  0.21 -0.33 -0.16 0.53 0.47 2.1\njob13 -0.11  0.58  0.21 -0.30 -0.08 0.49 0.51 1.9\njob6  -0.22  0.55 -0.12  0.37  0.22 0.55 0.45 2.6\njob17 -0.21  0.50 -0.15  0.48  0.14 0.57 0.43 2.7\njob46 -0.10  0.50 -0.05  0.41  0.04 0.43 0.57 2.0\njob2  -0.08  0.45  0.21 -0.15 -0.20 0.31 0.69 2.2\njob29 -0.05  0.37  0.00  0.24  0.15 0.22 0.78 2.1\njob35  0.22  0.03  0.50  0.04  0.49 0.54 0.46 2.4\njob41  0.26 -0.04  0.49  0.09  0.38 0.46 0.54 2.5\njob32  0.20 -0.01  0.48 -0.03  0.41 0.44 0.56 2.4\njob49  0.28 -0.10  0.34 -0.10  0.27 0.29 0.71 3.3\njob36  0.23 -0.02  0.21 -0.14  0.40 0.28 0.72 2.5\n\n                       ML1  ML2  ML3  ML4  ML5\nSS loadings           8.56 2.99 2.24 1.75 1.57\nProportion Var        0.21 0.07 0.05 0.04 0.04\nCumulative Var        0.21 0.28 0.34 0.38 0.42\nProportion Explained  0.50 0.17 0.13 0.10 0.09\nCumulative Proportion 0.50 0.67 0.81 0.91 1.00\n\nMean item complexity =  2\nTest of the hypothesis that 5 factors are sufficient.\n\nThe degrees of freedom for the null model are  820  and the objective function was  17.47 with Chi Square of  16202.67\nThe degrees of freedom for the model are 625  and the objective function was  3.84 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic number of observations is  940 with the empirical chi square  2822.5  with prob <  1.7e-275 \nThe total number of observations was  943  with Likelihood Chi Square =  3545.78  with prob <  0 \n\nTucker Lewis Index of factoring reliability =  0.75\nRMSEA index =  0.07  and the 90 % confidence intervals are  0.068 0.073\nBIC =  -734.88\nFit based upon off diagonal values = 0.97\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML3  ML4  ML5\nCorrelation of (regression) scores with factors   0.97 0.93 0.90 0.88 0.86\nMultiple R square of scores with factors          0.94 0.86 0.80 0.77 0.74\nMinimum correlation of possible factor scores     0.88 0.71 0.61 0.53 0.49\n\n\nAlso notice that the 8-factor solution accounted for 49% of the variance and the 5-factor one explains 41%. That is not a huge drop considering that, by choosing the 5-factor over the 8-factor solution, we reduce the dimensionality of the data (number of variables we have to deal with) by further 3 dimensions!\nLooking at the loadings, we can see that only 4 items have substantial cross-loadings (on exactly 2 factors), which is not terrible.\nGlance at the factor correlations of this final model, we see that only factor 1 and 4 are weakly-to-moderately correlated, which is not too bad! It allows us to claim that the factors (except for one) are largely independent of each other. This model accounts for about 39% of the common variance.\nAt this stage, we would go to the individual items, look at which factors load on which items, and try to figure out what is the common theme linking these items. For instance, let’s look at the factor ML5. I would start by looking at the items with the highest loadings, i.e., items 44, 4, 33, and 24. They all have three things in common: they address fairness, openness, and promotions/pay rises. Since we have multiple themes going on here, let’s look at the items with loadings in the .4-.6 range. A stronger theme of fair acknowledgement of performance emerges. Not all of the lower-loading items chime with this theme terribly well, but those that do not tend to have cross-loadings with other factor. I would therefore be reasonable confident that the factor taps into something that could be called “Fair recognition” (apparently this is referred to in the OrgPsych jargon as “Procedural justice”)."
  },
  {
    "objectID": "example_00_anova.html",
    "href": "example_00_anova.html",
    "title": "Analysis Example: Rpt & Mixed ANOVA",
    "section": "",
    "text": "Data\nThe data is from a simulated experiment in which heart rate variability (HRV) was measured for amateur basketball players when tasked with scoring a goal with varying levels and type of potential loss/reward.\nThe data was split over two files. The code below will read in both datasets and join them for you:\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\ndownload.file(url = \"https://uoepsy.github.io/data/basketballhrv.xlsx\", \n              destfile = \"baskeballhrvdata.xlsx\")\n\nbball <- \n  left_join(\n    read_csv(\"https://uoepsy.github.io/data/basketballconditions.csv\"),\n    read_xlsx(\"baskeballhrvdata.xlsx\") %>%\n      pivot_longer(trial_1:trial_20, names_to = \"trial_no\", values_to = \"hrv\")\n  ) %>%\n  mutate(sub = factor(sub))\n\n\n\n\n\n\n\nOne-Way Repeated Measures ANOVA\nFor a repeated measures ANOVA, our independent variables are within groups.\nFollowing from the example study above, we might consider using it to answer the question below.\n\nQuestion: What is the effect of the size of reward on stress levels (as measured by HRV)?\n\nThe easiest way to conduct a repeated measures ANOVA in R is to use the ez package.\nIt comes with some handy functions to visualise the experimental design.\nWe can see from below that every participant completed a trial for each value of reward-size (1-20 points):\n\n\nCode\nlibrary(ez)\nezDesign(data = bball, x = sub, y = stakes)\n\n\n\n\n\n\n\n\n\nThe ezANOVA() function takes a few arguments.\nThe ones you will need for this are:\n\ndata the name of the dataframe\ndv the column name for the dependent variable\nwid the column name for the participant id variable\nwithin the column name(s) for the predictor variable(s) that vary within participants\nbetween the column name(s) for any predictor variable(s) that vary between participants\n\nFit a repeated measures ANOVA to examine the effect of the size of reward on HRV.\n\n\nCode\nezANOVA(data=bball, dv=hrv, wid = sub, within = stakes)\n\n\n$ANOVA\n  Effect DFn DFd        F         p p<.05        ges\n1 stakes   1  29 1.254585 0.2718695       0.04146761\n\n\n\n\nMixed ANOVA\nMixed ANOVA can be used to investigate effects of independent variables that are at two different levels, i.e. some are within clusters and some are between.\n\nQuestion: Does the influence of the size of reward/loss on stress levels differ depending upon whether it is money vs reputation at stake?\n\nLook at the two lines below. Can you work out what the plots will look like before you run them?\n\n\nCode\nezDesign(data = bball, x = condition, y = sub)\nezDesign(data = bball, x = condition, y = stakes)\n\n\nParticipants 1-15 are in one condition, and 16-30 are in another.\nThis should look like a two big blocks on the diagonal.\n\n\nCode\nezDesign(data = bball, x = condition, y = sub)\n\n\n\n\n\n\n\n\n\nIn each condition, the full set of stakes (1-20 points) were observed in the same number of trials. This should be a full grid:\n\n\nCode\nezDesign(data = bball, x = condition, y = stakes)\n\n\n\n\n\n\n\n\n\nFit a mixed ANOVA to examine the interaction between size and type of reward on HRV.\n\n\nCode\nezANOVA(data=bball, dv=hrv, wid = sub, within = stakes, between = condition)\n\n\n$ANOVA\n            Effect DFn DFd        F          p p<.05        ges\n1        condition   1  28 3.021712 0.09314393       0.06041768\n2           stakes   1  28 1.260498 0.27109204       0.01786900\n3 condition:stakes   1  28 1.136668 0.29546463       0.01614191\n\n\nThe ez package also contains some easy plotting functions for factorial experiments, such as ezPlot(). It takes similar arguments to the ezANOVA() function.\n\nlook up the help documentation for ezPlot().\nlet’s use ezPlot() to make a nice plot (Note: we may need to make sure that the stakes variable is a factor).\n\n\n\nCode\nbball <- bball %>% mutate(stakes = factor(stakes))\nezPlot(data=bball, dv=hrv, wid = sub, within = stakes, between = condition, x = stakes, split = condition)"
  },
  {
    "objectID": "example_01_repeated_measures.html",
    "href": "example_01_repeated_measures.html",
    "title": "Analysis Example: Repeated-measures",
    "section": "",
    "text": "Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:\n\nFirst the data and research context is introduced. For the purpose of these tutorials, we will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated.\nSecond, we go through any tidying of the data that is required, before creating some brief descriptives and visualizations of the raw data.\nThen, we conduct an analysis. Where possible, we translate the research questions into formal equations prior to fitting the models in lme4. Model comparisons are conducted, along with checks of distributional assumptions on our model residuals.\nFinally, we visualize and interpret our analysis.\n\nPlease note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ug.ppls.stats@ed.ac.uk."
  },
  {
    "objectID": "example_01_repeated_measures.html#equations",
    "href": "example_01_repeated_measures.html#equations",
    "title": "Analysis Example: Repeated-measures",
    "section": "Equations",
    "text": "Equations\nWe’re going to fit this model, and examine the change in dv associated with moving from time-point 1 to each subsequent time-point.\nRecall that because iv is categorical with 3 levels, we’re going to be estimating 2 (\\(3-1\\)) coefficients.\n\\[\n\\begin{aligned}\n  \\operatorname{dv}_{i[j]} &= \\beta_{0i} + \\beta_1(\\operatorname{iv}_{\\operatorname{T2}_j}) + \\beta_2(\\operatorname{iv}_{\\operatorname{T3}_j}) + \\varepsilon_{i[j]} \\\\\n    \\beta_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n    & \\text{for }\\operatorname{pid}\\text{ i = 1,} \\dots \\text{,I}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "example_01_repeated_measures.html#fitting-the-models",
    "href": "example_01_repeated_measures.html#fitting-the-models",
    "title": "Analysis Example: Repeated-measures",
    "section": "Fitting the models",
    "text": "Fitting the models\n\n\nCode\nlibrary(lme4)\n\n\nHere we run an empty model so that we have something to compare our model which includes our iv. Other than to give us a reference model, we do not have a huge amount of interest in this.\n\n\nCode\nm0 <- lmer(dv ~ 1 + (1 | pid), data = simRPT)\n\n\nNext, we specify our model. Here we include a fixed effect of our predictor (group membership, iv), and a random effect of participant (iv) to take account of the fact we have three measurements per person.\n\n\nCode\nm1 <- lmer(dv ~ 1 + iv + (1 | pid), data = simRPT)\nsummary(m1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: dv ~ 1 + iv + (1 | pid)\n   Data: simRPT\n\nREML criterion at convergence: 1144.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.52990 -0.57636 -0.02015  0.56073  2.50816 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n pid      (Intercept) 74.66    8.641   \n Residual             84.60    9.198   \nNumber of obs: 150, groups:  pid, 50\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   38.842      1.785   21.76\nivT2          11.976      1.840    6.51\nivT3          28.964      1.840   15.74\n\nCorrelation of Fixed Effects:\n     (Intr) ivT2  \nivT2 -0.515       \nivT3 -0.515  0.500\n\n\nAnd we can compare our models.\n\n\nCode\nlibrary(pbkrtest)\nPBmodcomp(m1, m0)\n\n\nBootstrap test; time: 41.77 sec; samples: 1000; extremes: 0;\nlarge : dv ~ 1 + iv + (1 | pid)\ndv ~ 1 + (1 | pid)\n         stat df   p.value    \nLRT    126.84  2 < 2.2e-16 ***\nPBtest 126.84     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nanova(m0,m1)\n\n\nData: simRPT\nModels:\nm0: dv ~ 1 + (1 | pid)\nm1: dv ~ 1 + iv + (1 | pid)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm0    3 1285.9 1294.9 -639.94   1279.9                         \nm1    5 1163.0 1178.1 -576.52   1153.0 126.83  2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOK, so we can see that we appear to have a significant effect of our repeated factor here. Our parametric bootstrap LRT is in agreement here.\n\nComparison to aov()\nUsing anova() to compare multilevel models will not give you a typical ANOVA output.\nFor piece of mind, it can be useful to compare how we might do this in aov()\n\n\nCode\nm2 <- aov(dv ~ iv + Error(pid), data = simRPT)\n\n\nHere the term Error(pid) is specifying the within person error, or residual. This is what we are doing with our random effect (1 | pid) in lmer()\nAnd we can compare the model sums of squares from both approaches to see the equivalence:\n\n\nCode\nsummary(m2)\n\n\n\nError: pid\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 49  15121   308.6               \n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(>F)    \niv         2  21183   10591   125.2 <2e-16 ***\nResiduals 98   8291      85                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nanova(m1)\n\n\nAnalysis of Variance Table\n   npar Sum Sq Mean Sq F value\niv    2  21183   10591  125.19"
  },
  {
    "objectID": "example_01_repeated_measures.html#check-model",
    "href": "example_01_repeated_measures.html#check-model",
    "title": "Analysis Example: Repeated-measures",
    "section": "Check model",
    "text": "Check model\nThe residuals look reasonably normally distributed, and there seems to be fairly constant variance across the linear predictor. We might be a little concerned about the potential tails of the plot below, at which residuals don’t appear to have a mean of zero\n\n\nCode\nplot(m1, type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\nCode\nlibrary(lattice)\nqqmath(m1)\n\n\n\n\n\n\n\n\n\nRandom effects are (roughly) normally distributed:\n\n\nCode\nrans <- as.data.frame(ranef(m1)$pid)\nggplot(rans, aes(sample = `(Intercept)`)) + \n  stat_qq() + stat_qq_line() +\n  labs(title=\"random intercept\")"
  },
  {
    "objectID": "example_02_intervention.html",
    "href": "example_02_intervention.html",
    "title": "Analysis Example: Intervention",
    "section": "",
    "text": "Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:\n\nFirst the data and research context is introduced. For the purpose of these tutorials, we will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated.\nSecond, we go through any tidying of the data that is required, before creating some brief descriptives and visualizations of the raw data.\nThen, we conduct an analysis. Where possible, we translate the research questions into formal equations prior to fitting the models in lme4. Model comparisons are conducted, along with checks of distributional assumptions on our model residuals.\nFinally, we visualize and interpret our analysis.\n\nPlease note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ug.ppls.stats@ed.ac.uk."
  },
  {
    "objectID": "example_02_intervention.html#equations",
    "href": "example_02_intervention.html#equations",
    "title": "Analysis Example: Intervention",
    "section": "Equations",
    "text": "Equations\n\\[\n\\begin{aligned}\n  \\operatorname{stress}_{i[j]}  &= \\beta_{0i} + \\beta_{1i}(\\operatorname{timeDuring}_j) + \\beta_{2i}(\\operatorname{timePost}_j) + \\varepsilon_{i[j]} \\\\\n  \\beta_{0i} &= \\gamma_{00} + \\gamma_{01}(\\operatorname{groupTreatment}_i) + \\zeta_{0i} \\\\\n  \\beta_{1i} &= \\gamma_{10} + \\gamma_{11}(\\operatorname{groupTreatment}_i) \\\\\n  \\beta_{2i} &= \\gamma_{20} + \\gamma_{21}(\\operatorname{groupTreatment}_i) \\\\\n  & \\text{for ppt i = 1,} \\dots \\text{, I}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "example_02_intervention.html#fitting-the-models",
    "href": "example_02_intervention.html#fitting-the-models",
    "title": "Analysis Example: Intervention",
    "section": "Fitting the models",
    "text": "Fitting the models\n\n\nCode\nlibrary(lme4)\n\n\nBase model:\n\n\nCode\nm0 <- lmer(stress ~ 1 +\n             (1 | ppt), data = simMIX)\nsummary(m0)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1286.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.02099 -0.49670  0.01511  0.49445  1.96548 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 179.7    13.40   \n Residual             209.7    14.48   \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   42.220      2.234    18.9\n\n\nMain effects:\n\n\nCode\nm1 <- lmer(stress ~ 1 + time + group +\n             (1 | ppt), data = simMIX)\nsummary(m1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + time + group + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1185.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.69666 -0.62649  0.01574  0.59245  1.92387 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 166.57   12.91   \n Residual              98.01    9.90   \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)      61.100      3.046  20.061\ntimeDuring      -13.760      1.980  -6.950\ntimePost        -20.980      1.980 -10.596\ngroupTreatment  -14.600      3.992  -3.657\n\nCorrelation of Fixed Effects:\n            (Intr) tmDrng timPst\ntimeDuring  -0.325              \ntimePost    -0.325  0.500       \ngroupTrtmnt -0.655  0.000  0.000\n\n\nInteraction:\n\n\nCode\nm2 <- lmer(stress ~ 1 + time + group + time*group +\n             (1 | ppt), data = simMIX)\nsummary(m2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + time + group + time * group + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1096.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.83946 -0.53326 -0.05639  0.53766  2.31546 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 184.82   13.595  \n Residual              43.26    6.577  \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)                 52.840      3.020  17.494\ntimeDuring                  -3.200      1.860  -1.720\ntimePost                    -6.760      1.860  -3.634\ngroupTreatment               1.920      4.272   0.449\ntimeDuring:groupTreatment  -21.120      2.631  -8.028\ntimePost:groupTreatment    -28.440      2.631 -10.810\n\nCorrelation of Fixed Effects:\n            (Intr) tmDrng timPst grpTrt tmDr:T\ntimeDuring  -0.308                            \ntimePost    -0.308  0.500                     \ngroupTrtmnt -0.707  0.218  0.218              \ntmDrng:grpT  0.218 -0.707 -0.354 -0.308       \ntmPst:grpTr  0.218 -0.354 -0.707 -0.308  0.500\n\n\n\nComparison with aov()\nAs we did with the simple repeated measures, we can also compare the sums of squares breakdown for the LMM (m2) by calling anova() on the lmer() model.\nFirst with aov():\n\n\nCode\nm3 <- aov(stress ~ time + group + time*group +\n            Error(ppt), data = simMIX)\nsummary(m3)\n\n\n\nError: ppt\n          Df Sum Sq Mean Sq F value   Pr(>F)    \ngroup      1   7993    7993   13.37 0.000633 ***\nResiduals 48  28691     598                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n           Df Sum Sq Mean Sq F value Pr(>F)    \ntime        2  11360    5680  131.31 <2e-16 ***\ntime:group  2   5452    2726   63.01 <2e-16 ***\nResiduals  96   4153      43                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd then summarise:\n\n\nCode\nanova(m2)\n\n\nAnalysis of Variance Table\n           npar  Sum Sq Mean Sq F value\ntime          2 11360.4  5680.2 131.305\ngroup         1   578.5   578.5  13.373\ntime:group    2  5452.0  2726.0  63.014\n\n\n\nFor ease, lets compare all models with a parametric bootstrap likelihood ratio test:\n\n\nCode\nlibrary(pbkrtest)\nPBmodcomp(m1, m0)\nPBmodcomp(m2, m1)\n\n\n\n\nBootstrap test; time: 50.95 sec; samples: 1000; extremes: 0;\nlarge : stress ~ 1 + time + group + (1 | ppt)\nstress ~ 1 + (1 | ppt)\n         stat df   p.value    \nLRT    90.348  3 < 2.2e-16 ***\nPBtest 90.348     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBootstrap test; time: 54.92 sec; samples: 1000; extremes: 0;\nlarge : stress ~ 1 + time + group + time * group + (1 | ppt)\nstress ~ 1 + time + group + (1 | ppt)\n         stat df   p.value    \nLRT    83.853  2 < 2.2e-16 ***\nPBtest 83.853     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd extract some bootstrap 95% CIs\n\n\nCode\nconfint(m2,method=\"boot\")\n\n\n                               2.5 %      97.5 %\n.sig01                     11.063915  16.8989137\n.sigma                      5.570576   7.3318954\n(Intercept)                46.958215  58.9035049\ntimeDuring                 -6.598935  -0.1161317\ntimePost                  -10.473009  -3.1302747\ngroupTreatment             -7.001190  10.0517531\ntimeDuring:groupTreatment -26.395306 -16.1101659\ntimePost:groupTreatment   -33.665532 -23.2588741"
  },
  {
    "objectID": "example_02_intervention.html#check-model",
    "href": "example_02_intervention.html#check-model",
    "title": "Analysis Example: Intervention",
    "section": "Check Model",
    "text": "Check Model\nThe residuals look reasonably normally distributed, and there seems to be fairly constant variance across the linear predictor. We might be a little concerned about the potential tails of the plot below, at which residuals don’t appear to have a mean of zero\n\n\nCode\nplot(m2, type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\nCode\nplot(m2, sqrt(abs(resid(.)))~fitted(.))\n\n\n\n\n\n\n\n\n\nCode\nlibrary(lattice)\nqqmath(m2)\n\n\n\n\n\n\n\n\n\nRandom effects are (roughly) normally distributed:\n\n\nCode\nrans <- as.data.frame(ranef(m2)$ppt)\nggplot(rans, aes(sample = `(Intercept)`)) + \n  stat_qq() + stat_qq_line() +\n  labs(title=\"random intercept\")"
  },
  {
    "objectID": "example_03_many_trials.html",
    "href": "example_03_many_trials.html",
    "title": "Analysis Example 3: Many trials",
    "section": "",
    "text": "Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:\n\nFirst the data and research context is introduced. For the purpose of these tutorials, we will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated.\nSecond, we go through any tidying of the data that is required, before creating some brief descriptives and visualizations of the raw data.\nThen, we conduct an analysis. Where possible, we translate the research questions into formal equations prior to fitting the models in lme4. Model comparisons are conducted, along with checks of distributional assumptions on our model residuals.\nFinally, we visualize and interpret our analysis.\n\nPlease note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ug.ppls.stats@ed.ac.uk."
  },
  {
    "objectID": "example_03_many_trials.html#equations",
    "href": "example_03_many_trials.html#equations",
    "title": "Analysis Example 3: Many trials",
    "section": "Equations",
    "text": "Equations\n\nWhy item as a fixed effect and not by-item random intercepts (e.g.(1 | item))? It is debatable how you decide to treat item here. It could be argued that the different item types (dress/top/bottom, etc) represent a sample from some broader population of garment types, and as such should be modeled as random effects (especially given that we are not interested in specific parameter estimates for differences between item types).\nHowever, we only have 3 different types of item, and it is important to remember that having (1|item) will involve estimating variance components based on these. Variance estimates will be less stable with such a small number of groups. There’s no hard rule to follow here about “how many groups is enough” (some people suggest 5 or 6 at least), but personally I would be inclined to use item as a fixed effect.\nWe should also consider the possibility that certain items should be less desirable to purchase when in a smaller/bigger size. It may be that size differences are more salient, or more influential on our outcome variable, for certain types of item over others. This would involve adding the interaction between size and item type\n\n$$\n\\[\\begin{aligned}\n  \\operatorname{price}_{i[j]}  =& \\beta_{0i} +\n  \\beta_{1i}(\\operatorname{item-size}_j) +\n  \\beta_{1}(\\operatorname{item-type}_j) + \\\\\n  & \\beta_{2}(\\operatorname{item-size}_j \\times \\operatorname{item-type}_j) + \\\\\n  & \\beta_{3i}(\\operatorname{matches-actual-size}_j) + \\\\\n  & \\beta_{4}(\\operatorname{matches-ideal-size}_j) + \\varepsilon_{i[j]}\\\\\n  \\qquad \\\\\n  \n  \\beta_{0i} &= \\gamma_{00} + \\gamma_{01}(\\operatorname{thin-idealisation-score}_i) + \\zeta_{0i} \\\\\n  \\beta_{1i} &= \\gamma_{10} + \\zeta_{1i}\\\\\n  \\beta_{3i} &= \\gamma_{30} + \\gamma_{31}(\\operatorname{thin-idealisation-score}_i)\\\\\n  & \\text{for participant i = 1, } \\dots \\text{, I} \\\\\n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "example_03_many_trials.html#fitting-the-models",
    "href": "example_03_many_trials.html#fitting-the-models",
    "title": "Analysis Example 3: Many trials",
    "section": "Fitting the models",
    "text": "Fitting the models\nWe’re going to want to do lots of model comparisons, and for those we need all our models to be fitted on the same data. We’re going to use these variables:\n\n\nCode\ndf_analysis <-\n  df_analysis %>%\n  filter(\n    !is.na(size),\n    !is.na(item),\n    !is.na(c.match),\n    !is.na(i.match),\n    !is.na(tii_scorez),\n    !is.na(id),\n    !is.na(price)\n  )\n\n\nFirst, the effect of size. We’re going to treat size as a numeric variable here, in part because it makes our models less complex. It also allows us to model by-participant random effects of size (you can think of this as modeling participants as varying in the amount to which size influences the amount that they are prepared to pay).\n\n\nCode\nm0 <- lmer(price ~ 1 + item + \n             (1+size|id),\n           data = df_analysis)\nm1 <- lmer(price ~ 1 + item + size +\n             (1 + size|id),\n           data = df_analysis)\nsummary(m1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: price ~ 1 + item + size + (1 + size | id)\n   Data: df_analysis\n\nREML criterion at convergence: 14045.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.5003 -0.5885 -0.0396  0.5259  5.1435 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 62.130   7.882         \n          size         2.371   1.540    -0.07\n Residual             33.570   5.794         \nNumber of obs: 2130, groups:  id, 120\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  24.3845     0.7517  32.439\nitemdress     2.6468     0.3081   8.590\nitemtop      -5.3352     0.3073 -17.360\nsize         -0.4893     0.1887  -2.593\n\nCorrelation of Fixed Effects:\n          (Intr) itmdrs itemtp\nitemdress -0.204              \nitemtop   -0.205  0.500       \nsize      -0.051  0.000  0.001\n\n\n\n\nCode\nanova(m0, m1)\n\n\nData: df_analysis\nModels:\nm0: price ~ 1 + item + (1 + size | id)\nm1: price ~ 1 + item + size + (1 + size | id)\n   npar   AIC   BIC  logLik deviance  Chisq Df Pr(>Chisq)  \nm0    7 14065 14104 -7025.3    14051                       \nm1    8 14060 14105 -7022.0    14044 6.6082  1    0.01015 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe interaction between size and item:\n\n\nCode\nm2 <- lmer(price ~ 1 +  item * size +\n             (1+size|id),\n           data = df_analysis)\nsummary(m2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: price ~ 1 + item * size + (1 + size | id)\n   Data: df_analysis\n\nREML criterion at convergence: 14026.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.5043 -0.5804 -0.0377  0.5095  5.1433 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 62.135   7.883         \n          size         2.392   1.547    -0.07\n Residual             33.240   5.765         \nNumber of obs: 2130, groups:  id, 120\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)     24.3870     0.7514  32.454\nitemdress        2.6449     0.3066   8.626\nitemtop         -5.3370     0.3058 -17.451\nsize            -1.1062     0.2587  -4.277\nitemdress:size   0.4740     0.3071   1.543\nitemtop:size     1.3689     0.3058   4.477\n\nCorrelation of Fixed Effects:\n            (Intr) itmdrs itemtp size   itmdr:\nitemdress   -0.203                            \nitemtop     -0.204  0.500                     \nsize        -0.038  0.002  0.002              \nitemdrss:sz  0.001 -0.003 -0.002 -0.591       \nitemtop:siz  0.001 -0.002 -0.002 -0.594  0.500\n\n\n\n\nCode\nanova(m1,m2)\n\n\nData: df_analysis\nModels:\nm1: price ~ 1 + item + size + (1 + size | id)\nm2: price ~ 1 + item * size + (1 + size | id)\n   npar   AIC   BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm1    8 14060 14105 -7022.0    14044                         \nm2   10 14043 14100 -7011.7    14023 20.613  2  3.341e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat about the effect of item size matching your current size:\n\n\nCode\nm3 <- lmer(price ~ 1 +  item * size + c.match +\n             (1+size|id),\n           data = df_analysis)\nsummary(m3)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: price ~ 1 + item * size + c.match + (1 + size | id)\n   Data: df_analysis\n\nREML criterion at convergence: 14023.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.4754 -0.5995 -0.0312  0.5075  5.1647 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 62.145   7.883         \n          size         2.317   1.522    -0.08\n Residual             33.233   5.765         \nNumber of obs: 2130, groups:  id, 120\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)     24.2782     0.7537  32.214\nitemdress        2.6431     0.3066   8.621\nitemtop         -5.3374     0.3058 -17.454\nsize            -1.0747     0.2580  -4.166\nc.match1         0.6620     0.3494   1.895\nitemdress:size   0.4736     0.3071   1.542\nitemtop:size     1.3689     0.3058   4.477\n\nCorrelation of Fixed Effects:\n            (Intr) itmdrs itemtp size   c.mtc1 itmdr:\nitemdress   -0.202                                   \nitemtop     -0.203  0.500                            \nsize        -0.048  0.002  0.002                     \nc.match1    -0.076 -0.003 -0.001  0.064              \nitemdrss:sz  0.001 -0.003 -0.002 -0.592  0.000       \nitemtop:siz  0.001 -0.002 -0.002 -0.595  0.000  0.500\n\n\nCompare models for current size match:\n\n\nCode\nanova(m2,m3)\n\n\nData: df_analysis\nModels:\nm2: price ~ 1 + item * size + (1 + size | id)\nm3: price ~ 1 + item * size + c.match + (1 + size | id)\n   npar   AIC   BIC  logLik deviance  Chisq Df Pr(>Chisq)  \nm2   10 14043 14100 -7011.7    14023                       \nm3   11 14042 14104 -7009.9    14020 3.5856  1    0.05828 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr your ideal size:\n\n\nCode\nm4 <- lmer(price ~ 1 + item * size + c.match + i.match +\n             (1+size|id),\n           data = df_analysis)\nsummary(m4)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: price ~ 1 + item * size + c.match + i.match + (1 + size | id)\n   Data: df_analysis\n\nREML criterion at convergence: 14021.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.4705 -0.5911 -0.0355  0.5049  5.1828 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 62.171   7.885         \n          size         2.281   1.510    -0.08\n Residual             33.244   5.766         \nNumber of obs: 2130, groups:  id, 120\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)     24.2167     0.7556  32.050\nitemdress        2.6439     0.3066   8.622\nitemtop         -5.3369     0.3058 -17.450\nsize            -1.0264     0.2606  -3.938\nc.match1         0.5972     0.3539   1.688\ni.match1         0.4308     0.3650   1.180\nitemdress:size   0.4736     0.3071   1.542\nitemtop:size     1.3685     0.3058   4.475\n\nCorrelation of Fixed Effects:\n            (Intr) itmdrs itemtp size   c.mtc1 i.mtc1 itmdr:\nitemdress   -0.202                                          \nitemtop     -0.203  0.500                                   \nsize        -0.058  0.002  0.002                            \nc.match1    -0.064 -0.003 -0.001  0.038                     \ni.match1    -0.069  0.002  0.001  0.156 -0.160              \nitemdrss:sz  0.001 -0.003 -0.002 -0.586 -0.001  0.000       \nitemtop:siz  0.001 -0.002 -0.002 -0.589  0.000 -0.001  0.500\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00383895 (tol = 0.002, component 1)\n\n\nCompare models for ideal size match:\n\n\nCode\nanova(m3,m4)\n\n\nData: df_analysis\nModels:\nm3: price ~ 1 + item * size + c.match + (1 + size | id)\nm4: price ~ 1 + item * size + c.match + i.match + (1 + size | id)\n   npar   AIC   BIC  logLik deviance Chisq Df Pr(>Chisq)\nm3   11 14042 14104 -7009.9    14020                    \nm4   12 14042 14110 -7009.2    14018 1.399  1     0.2369\n\n\nFixed effects for thin ideals:\n\n\nCode\nm5 <- lmer(price ~ 1 + item * size + c.match + i.match + tii_scorez + \n             (1+size|id),\n           data = df_analysis)\nsummary(m5)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: price ~ 1 + item * size + c.match + i.match + tii_scorez + (1 +  \n    size | id)\n   Data: df_analysis\n\nREML criterion at convergence: 14020.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.4719 -0.5907 -0.0357  0.5059  5.1829 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 62.695   7.918         \n          size         2.279   1.510    -0.08\n Residual             33.245   5.766         \nNumber of obs: 2130, groups:  id, 120\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)    24.21667    0.75848  31.928\nitemdress       2.64389    0.30664   8.622\nitemtop        -5.33684    0.30584 -17.450\nsize           -1.02641    0.26060  -3.939\nc.match1        0.59704    0.35388   1.687\ni.match1        0.43085    0.36497   1.181\ntii_scorez     -0.07229    0.73246  -0.099\nitemdress:size  0.47367    0.30714   1.542\nitemtop:size    1.36846    0.30582   4.475\n\nCorrelation of Fixed Effects:\n            (Intr) itmdrs itemtp size   c.mtc1 i.mtc1 t_scrz itmdr:\nitemdress   -0.201                                                 \nitemtop     -0.202  0.500                                          \nsize        -0.058  0.002  0.002                                   \nc.match1    -0.064 -0.003 -0.001  0.038                            \ni.match1    -0.068  0.002  0.001  0.156 -0.160                     \ntii_scorez   0.000  0.000 -0.001  0.001  0.003 -0.001              \nitemdrss:sz  0.001 -0.003 -0.002 -0.587 -0.001  0.000  0.000       \nitemtop:siz  0.001 -0.002 -0.002 -0.589  0.000 -0.001  0.000  0.500\n\n\nCompare models for thin ideal:\n\n\nCode\nanova(m4,m5)\n\n\nData: df_analysis\nModels:\nm4: price ~ 1 + item * size + c.match + i.match + (1 + size | id)\nm5: price ~ 1 + item * size + c.match + i.match + tii_scorez + (1 + size | id)\n   npar   AIC   BIC  logLik deviance  Chisq Df Pr(>Chisq)\nm4   12 14042 14110 -7009.2    14018                     \nm5   13 14044 14118 -7009.2    14018 0.0099  1     0.9208\n\n\nDo they interact?\n\n\nCode\nm6 <- lmer(price ~ 1 + item * size + c.match + i.match + tii_scorez + c.match*tii_scorez + \n             (1+size|id),\n           data = df_analysis)\nsummary(m6)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: price ~ 1 + item * size + c.match + i.match + tii_scorez + c.match *  \n    tii_scorez + (1 + size | id)\n   Data: df_analysis\n\nREML criterion at convergence: 14019.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.4672 -0.5966 -0.0323  0.5021  5.1910 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 62.679   7.917         \n          size         2.281   1.510    -0.08\n Residual             33.243   5.766         \nNumber of obs: 2130, groups:  id, 120\n\nFixed effects:\n                    Estimate Std. Error t value\n(Intercept)         24.20683    0.75845  31.916\nitemdress            2.64336    0.30663   8.621\nitemtop             -5.33608    0.30584 -17.448\nsize                -1.02537    0.26062  -3.934\nc.match1             0.56650    0.35506   1.595\ni.match1             0.51739    0.37448   1.382\ntii_scorez          -0.01563    0.73458  -0.021\nitemdress:size       0.47423    0.30713   1.544\nitemtop:size         1.36867    0.30581   4.476\nc.match1:tii_scorez -0.37423    0.36191  -1.034\n\nCorrelation of Fixed Effects:\n            (Intr) itmdrs itemtp size   c.mtc1 i.mtc1 t_scrz itmdr: itmtp:\nitemdress   -0.201                                                        \nitemtop     -0.202  0.500                                                 \nsize        -0.056  0.002  0.002                                          \nc.match1    -0.063 -0.003 -0.001  0.037                                   \ni.match1    -0.070  0.002  0.002  0.153 -0.173                            \ntii_scorez  -0.001  0.000  0.000  0.001 -0.003  0.016                     \nitemdrss:sz  0.001 -0.003 -0.002 -0.586 -0.001  0.001  0.000              \nitemtop:siz  0.001 -0.002 -0.002 -0.589  0.000 -0.001  0.000  0.500       \nc.mtch1:t_s  0.013  0.002 -0.002 -0.004  0.082 -0.224 -0.076 -0.002 -0.001\n\n\nAnd let’s just compare all at once, just for fun:\n\n\nCode\nanova(m0,m1,m2,m3,m4,m5,m6)\n\n\nData: df_analysis\nModels:\nm0: price ~ 1 + item + (1 + size | id)\nm1: price ~ 1 + item + size + (1 + size | id)\nm2: price ~ 1 + item * size + (1 + size | id)\nm3: price ~ 1 + item * size + c.match + (1 + size | id)\nm4: price ~ 1 + item * size + c.match + i.match + (1 + size | id)\nm5: price ~ 1 + item * size + c.match + i.match + tii_scorez + (1 + size | id)\nm6: price ~ 1 + item * size + c.match + i.match + tii_scorez + c.match * tii_scorez + (1 + size | id)\n   npar   AIC   BIC  logLik deviance   Chisq Df Pr(>Chisq)    \nm0    7 14065 14104 -7025.3    14051                          \nm1    8 14060 14105 -7022.0    14044  6.6082  1    0.01015 *  \nm2   10 14043 14100 -7011.7    14023 20.6131  2  3.341e-05 ***\nm3   11 14042 14104 -7009.9    14020  3.5856  1    0.05828 .  \nm4   12 14042 14110 -7009.2    14018  1.3990  1    0.23690    \nm5   13 14044 14118 -7009.2    14018  0.0099  1    0.92076    \nm6   14 14045 14125 -7008.7    14017  1.0720  1    0.30048    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "example_03_many_trials.html#check-models",
    "href": "example_03_many_trials.html#check-models",
    "title": "Analysis Example 3: Many trials",
    "section": "Check model(s)",
    "text": "Check model(s)\n\n\nCode\nlibrary(lattice)\nlibrary(gridExtra)\ngrid.arrange(\n  plot(m1, sqrt(abs(resid(.)))~fitted(.), type = c(\"p\",\"smooth\")),\n  plot(m2, sqrt(abs(resid(.)))~fitted(.), type = c(\"p\",\"smooth\")),\n  plot(m3, sqrt(abs(resid(.)))~fitted(.), type = c(\"p\",\"smooth\")),\n  plot(m4, sqrt(abs(resid(.)))~fitted(.), type = c(\"p\",\"smooth\")),\n  plot(m5, sqrt(abs(resid(.)))~fitted(.), type = c(\"p\",\"smooth\")),\n  plot(m6, sqrt(abs(resid(.)))~fitted(.), type = c(\"p\",\"smooth\")),\n  ncol=3\n)\n\n\n\n\n\n\n\n\n\nCode\ngrid.arrange(\n  qqmath(m1),\n  qqmath(m2),\n  qqmath(m3),\n  qqmath(m4),\n  qqmath(m5),\n  qqmath(m6),\n  ncol=3\n)\n\n\n\n\n\n\n\n\n\nI might be a little concerned about our assumptions here. The QQplots look a little off in the tails, however, the histograms of residuals make it look okay:\n\n\n\n\n\n\n\n\n\nFor peace of mind, we could re-do the model comparisons with parametric bootstrapped model comparison for more reliable conclusions. This will assume that the model specified distribution of residuals (\\(\\varepsilon \\sim N(0, \\sigma_\\varepsilon)\\)) holds in the population\n\n\n\n\n\nCode\nlibrary(pbkrtest)\nPBmodcomp(m1, m0)\n\n\n\n\n           stat df    p.value\nLRT    6.605558  1 0.01016610\nPBtest 6.605558 NA 0.01098901\n\n\n\n\nCode\nPBmodcomp(m2, m1)\n\n\n\n\n           stat df      p.value\nLRT    20.61468  2 3.338708e-05\nPBtest 20.61468 NA 9.990010e-04\n\n\n\n\nCode\nPBmodcomp(m3, m2)\n\n\n\n\n           stat df    p.value\nLRT    3.585967  1 0.05826951\nPBtest 3.585967 NA 0.07092907\n\n\n\n\nCode\nPBmodcomp(m4, m3)\n\n\n\n\n           stat df   p.value\nLRT    1.399085  1 0.2368768\nPBtest 1.399085 NA 0.2317682\n\n\n\n\nCode\nPBmodcomp(m5, m4)\n\n\n\n\n             stat df   p.value\nLRT    0.00153228  1 0.9687753\nPBtest 0.00153228 NA 0.9670330\n\n\n\n\nCode\nPBmodcomp(m6, m5)\n\n\n\n\n           stat df   p.value\nLRT    1.072739  1 0.3003275\nPBtest 1.072739 NA 0.2857143"
  },
  {
    "objectID": "example_04_cross_sectional_multilevel.html",
    "href": "example_04_cross_sectional_multilevel.html",
    "title": "Analysis Example: Cross-sectional multi-level",
    "section": "",
    "text": "Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:\n\nFirst the data and research context is introduced. For the purpose of these tutorials, we will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated.\nSecond, we go through any tidying of the data that is required, before creating some brief descriptives and visualizations of the raw data.\nThen, we conduct an analysis. Where possible, we translate the research questions into formal equations prior to fitting the models in lme4. Model comparisons are conducted, along with checks of distributional assumptions on our model residuals.\nFinally, we visualize and interpret our analysis.\n\nPlease note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ug.ppls.stats@ed.ac.uk."
  },
  {
    "objectID": "example_04_cross_sectional_multilevel.html#equations",
    "href": "example_04_cross_sectional_multilevel.html#equations",
    "title": "Analysis Example: Cross-sectional multi-level",
    "section": "Equations",
    "text": "Equations\nOur model will take the structure specified below. However, we may also want to discuss whether public/private moderates the within effect of motivation, or the between effect of motivation, in which case we would need to separate out the group mean motivation and the group-mean centered motivation.\n\\[\n\\begin{align}\n\\operatorname{Perform}_{ij} &= \\beta_{0i} + \\beta_{1i}(\\operatorname{Motivation}_j) + \\varepsilon_{ij}\\\\\n\\beta_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\beta_{1i} &= \\gamma_{10} + \\gamma_{11}(\\operatorname{isPrivate}_i) + \\zeta_{0i} \\\\\n& \\text{for OrgID i = 1,} \\dots \\text{,I}\n\\end{align}\n\\]"
  },
  {
    "objectID": "example_04_cross_sectional_multilevel.html#fitting-the-models",
    "href": "example_04_cross_sectional_multilevel.html#fitting-the-models",
    "title": "Analysis Example: Cross-sectional multi-level",
    "section": "Fitting the models",
    "text": "Fitting the models\nAs we have in many of our examples, let’s run an empty model, and look at the ICC for the grouping variable organisation.\n\n\nCode\nlibrary(lme4)\n# Empty model\nm0 <- lmer(Perform ~ 1 + \n             (1 | OrgID),\n           data = perform)\nsummary(m0)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Perform ~ 1 + (1 | OrgID)\n   Data: perform\n\nREML criterion at convergence: 3560\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.69138 -0.71156 -0.03565  0.73487  2.68864 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n OrgID    (Intercept) 25.05    5.005   \n Residual             80.34    8.964   \nNumber of obs: 487, groups:  OrgID, 23\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   50.613      1.134   44.64\n\n\n\n\nCode\n# ICC\nvarM0 <- as.data.frame(VarCorr(m0))\nvarM0\n\n\n       grp        var1 var2     vcov    sdcor\n1    OrgID (Intercept) <NA> 25.05319 5.005316\n2 Residual        <NA> <NA> 80.34482 8.963527\n\n\n\n\nCode\nround((varM0[1,4]/sum(varM0[,4]))*100,2)\n\n\n[1] 23.77\n\n\nSo, around 24% of the variance in our data is at the grouping level, meaning around 76% is at the person level. So clearly we have scope here for explanatory variables of both types.\nTo begin, we add motivation as a fixed effect:\n\n\nCode\n# Fixed effect for motivation\nm1 <- lmer(Perform ~ 1 + Mot +\n             (1 | OrgID),\n           data = perform)\nsummary(m1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Perform ~ 1 + Mot + (1 | OrgID)\n   Data: perform\n\nREML criterion at convergence: 3493\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6341 -0.6969 -0.0137  0.6438  3.2444 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n OrgID    (Intercept) 20.30    4.506   \n Residual             70.25    8.381   \nNumber of obs: 487, groups:  OrgID, 23\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   46.035      1.159  39.731\nMot            2.453      0.288   8.517\n\nCorrelation of Fixed Effects:\n    (Intr)\nMot -0.464\n\n\nFrom this, we can calculate the reduction in the residual variance, or the PRV:\n\n\nCode\n# PRV - how much residual variance does motivation account for\nm1_sum <- summary(m1)\nvarM1 <- as.data.frame(m1_sum$varcor)\nvarM1\n\n\n       grp        var1 var2     vcov    sdcor\n1    OrgID (Intercept) <NA> 20.30283 4.505866\n2 Residual        <NA> <NA> 70.24540 8.381253\n\n\n\n\nCode\nround(((varM0[2,4]-varM1[2,4])/varM0[2,4])*100,2)\n\n\n[1] 12.57\n\n\nNot bad, so there is a 12.5% reduction in level 1 residual variance from the inclusion of motivation. The coefficient is positive, which is what we would hope to see, motivated employees have better performance.\nWe will do all of our model comparisons in one go at the end of our model building, so let’s continue. Next, is a random slope (as evidenced by our plots above) for motivation.\n\n\nCode\n# Random slope\nm2 <- lmer(Perform ~ 1 + Mot +\n             (1 + Mot | OrgID),\n           data = perform)\nsummary(m2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Perform ~ 1 + Mot + (1 + Mot | OrgID)\n   Data: perform\n\nREML criterion at convergence: 3393.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.27286 -0.69631 -0.00381  0.66058  2.82123 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n OrgID    (Intercept) 62.10    7.881         \n          Mot         18.06    4.250    -0.84\n Residual             50.88    7.133         \nNumber of obs: 487, groups:  OrgID, 23\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  46.0257     1.7571  26.195\nMot           2.0031     0.9351   2.142\n\nCorrelation of Fixed Effects:\n    (Intr)\nMot -0.834\n\n\nAnd now we can look to compare the set of models before adding the predictors:\n\n\nCode\nanova(m0, m1, m2)\n\n\nData: perform\nModels:\nm0: Perform ~ 1 + (1 | OrgID)\nm1: Perform ~ 1 + Mot + (1 | OrgID)\nm2: Perform ~ 1 + Mot + (1 + Mot | OrgID)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm0    3 3568.1 3580.7 -1781.0   3562.1                         \nm1    4 3502.2 3518.9 -1747.1   3494.2 67.894  1  < 2.2e-16 ***\nm2    6 3409.1 3434.2 -1698.5   3397.1 97.087  2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn each case, adding the additional parameters has improved our model, so we will keep them all.\nOur big interest here is in whether organisations being public or private changes the relationship of motivation to performance. For this, we will want to include the cross level interaction between Mot and PubPri.\n\n\nCode\nm3 <- lmer(Perform ~ 1 + Mot + PubPri + Mot*PubPri +\n             (1 + Mot | OrgID),\n           data = perform)\nsummary(m3)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Perform ~ 1 + Mot + PubPri + Mot * PubPri + (1 + Mot | OrgID)\n   Data: perform\n\nREML criterion at convergence: 3384.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.29012 -0.69613 -0.02848  0.65620  2.76816 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n OrgID    (Intercept) 63.44    7.965         \n          Mot         18.62    4.315    -0.86\n Residual             50.91    7.135         \nNumber of obs: 487, groups:  OrgID, 23\n\nFixed effects:\n                 Estimate Std. Error t value\n(Intercept)       45.2718     2.1765  20.800\nMot                1.7909     1.1737   1.526\nPubPripublic       2.4448     3.7569   0.651\nMot:PubPripublic   0.5623     1.9904   0.283\n\nCorrelation of Fixed Effects:\n            (Intr) Mot    PbPrpb\nMot         -0.853              \nPubPripublc -0.579  0.494       \nMt:PbPrpblc  0.503 -0.590 -0.848\n\n\nThe t-values here are pretty small. This doesn’t look like the added effects have contributed much to our model.\n\n\nCode\n# model comparison\nanova(m2,m3)\n\n\nData: perform\nModels:\nm2: Perform ~ 1 + Mot + (1 + Mot | OrgID)\nm3: Perform ~ 1 + Mot + PubPri + Mot * PubPri + (1 + Mot | OrgID)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nm2    6 3409.1 3434.2 -1698.5   3397.1                     \nm3    8 3410.1 3443.6 -1697.1   3394.1 2.9908  2     0.2242\n\n\nAnd indeed our model comparison suggests not. But wait a minute, we forgot about something….\n…we need to center our predictors.\nLet’s create variables which are the group mean motivation, and the group-mean centered motivation.\n\n\nCode\nperform <- \n  perform %>%\n  group_by(OrgID) %>%\n  mutate(\n    Mot_m = mean(Mot, na.rm=T),\n    Mot_c = Mot - mean(Mot, na.rm=T)\n  ) %>% ungroup\n\n\nAnd re-run the models:\n\n\nCode\nm4 <- lmer(Perform ~ 1 + Mot_m + Mot_c +\n             (1 + Mot_c | OrgID),\n           data = perform)\nm5 <- lmer(Perform ~ 1 + Mot_m + Mot_c + PubPri + Mot_c*PubPri +\n             (1 + Mot_c | OrgID),\n           data = perform)\nanova(m4,m5)\n\n\nData: perform\nModels:\nm4: Perform ~ 1 + Mot_m + Mot_c + (1 + Mot_c | OrgID)\nm5: Perform ~ 1 + Mot_m + Mot_c + PubPri + Mot_c * PubPri + (1 + Mot_c | OrgID)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nm4    7 3413.8 3443.2 -1699.9   3399.8                     \nm5    9 3414.6 3452.3 -1698.3   3396.6 3.1947  2     0.2024\n\n\nCode\nsummary(m5)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Perform ~ 1 + Mot_m + Mot_c + PubPri + Mot_c * PubPri + (1 +  \n    Mot_c | OrgID)\n   Data: perform\n\nREML criterion at convergence: 3383.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.38702 -0.70580 -0.02145  0.64741  2.79873 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n OrgID    (Intercept) 19.96    4.467         \n          Mot_c       18.62    4.315    -0.09\n Residual             51.01    7.142         \nNumber of obs: 487, groups:  OrgID, 23\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         45.2647     3.2271  14.027\nMot_m                2.1373     1.8199   1.174\nMot_c                1.7695     1.1752   1.506\nPubPripublic         4.1109     2.4320   1.690\nMot_c:PubPripublic   0.6012     1.9967   0.301\n\nCorrelation of Fixed Effects:\n            (Intr) Mot_m  Mot_c  PbPrpb\nMot_m       -0.925                     \nMot_c       -0.028 -0.001              \nPubPripublc  0.269 -0.497  0.039       \nMt_c:PbPrpb  0.020 -0.003 -0.589 -0.064\n\n\nWhat do you notice about the results? Spend a little bit of time comparing the effects (fixed and random) and see the changes that result from centering a variable. Remember that our interpretation of the conditional main effects is the effect of a predictor on the outcome when the interacting variable = 0. Zero for Mot_c is average motivation, and zero for PubPri is public sector. So for every unit increase in motivation within a public centre organisation, performance increases by 1.77 units."
  },
  {
    "objectID": "explainer_3leveleq.html",
    "href": "explainer_3leveleq.html",
    "title": "3-level multilevel model equations",
    "section": "",
    "text": "It varies in textbooks whether the specification of levels is \\(i\\)>\\(j\\)>\\(k\\) or \\(i\\)<\\(j\\)<\\(k\\).\nIn complex multilevel equations, it becomes a little easier sometimes (for me) to use the latter.\nEither way, we need to be clear about what each index refers to.\n\\[\n\\begin{align}\nk &= \\textrm{Schools}\\\\\nj &= \\textrm{Children}\\\\\ni &= \\textrm{observations}\n\\end{align}\n\\]\nThe outcome, aggressive behaviour score, occurs at the observation level \\(ABS_{ijk}\\).\nLevel of routine occurs at the observation level \\(CRQ_{ijk}\\).\nFamily income occurs at the child level \\(Income_{jk}\\).\nSchool funding occurs at the school level \\(Funding_k\\).\n\nIf we just ignore all of the clustering:\n\\[\nABS = \\beta_0 + \\beta_1(CRQ) + \\beta_2(Income) + \\beta_3(Funding) + \\varepsilon\n\\]\n\n\n\nIf we put random intercepts for schools and children:\n\\[\n\\begin{align}\n\\textrm{Level 1:} & \\\\\n& ABS_{ijk} = \\beta_{0jk} + \\beta_1(CRQ_{ijk}) + \\beta_2(Income_{jk}) + \\beta_3(Funding_k) + \\varepsilon_{ijk}\\\\\n\\textrm{Level 2:} & \\\\\n&\\beta_{0jk} = \\gamma_{00k} +\\zeta_{0jk} \\\\\n\\textrm{Level 3:} & \\\\\n&\\gamma_{00k} = \\pi_{000} + u_{00k}\n\\end{align}\n\\]\n\n\n\nWe can also move the relevant effects to the corresponding level at which they exist. While we’re at it, we can change the symbol to match the other effects at each level:\n\\[\n\\begin{align}\n\\textrm{Level 1:} & \\\\\n& ABS_{ijk} = \\beta_{0jk} + \\beta_{1}(CRQ_{ijk}) + \\varepsilon_{ijk}\\\\\n\\textrm{Level 2:} & \\\\\n&\\beta_{0jk} = \\gamma_{00k} + \\gamma_{010}(Income_{jk}) +\\zeta_{0jk} \\\\\n\\textrm{Level 3:} & \\\\\n&\\gamma_{00k}= \\pi_{000} + \\pi_{001}(Funding_k) + u_{00k}\n\\end{align}\n\\]\n\n\n\nAdding in a random slope of routine for each child:\n\\[\n\\begin{align}\n\\textrm{Level 1:} & \\\\\n& ABS_{ijk} = \\beta_{0jk} + \\beta_{1jk}(CRQ_{ijk}) + \\varepsilon_{ijk}\\\\\n\\textrm{Level 2:} & \\\\\n&\\beta_{0jk} = \\gamma_{00k} + \\gamma_{010}(Income_{jk}) +\\zeta_{0jk} \\\\\n&\\beta_{1jk} = \\gamma_{100} +\\zeta_{1jk} \\\\\n\\textrm{Level 3:} & \\\\\n&\\gamma_{00k}= \\pi_{000} + \\pi_{001}(Funding_k) + u_{00k}\n\\end{align}\n\\]\nAdding in a random slope of routine for each school:\n\\[\n\\begin{align}\n\\textrm{Level 1:} & \\\\\n& ABS_{ijk} = \\beta_{0jk} + \\beta_{1jk}(CRQ_{ijk}) + \\varepsilon_{ijk}\\\\\n\\textrm{Level 2:} & \\\\\n&\\beta_{0jk} = \\gamma_{00k} + \\gamma_{010}(Income_{jk}) +\\zeta_{0jk} \\\\\n&\\beta_{1jk} = \\gamma_{10k} +\\zeta_{1jk} \\\\\n\\textrm{Level 3:} & \\\\\n&\\gamma_{00k}= \\pi_{000} + \\pi_{001}(Funding_k) + u_{00k}\\\\\n&\\gamma_{10k} = \\pi_{100} + u_{10k}\n\\end{align}\n\\]\nAdding in a random slope of income for each school:\n\\[\n\\begin{align}\n\\textrm{Level 1:} & \\\\\n& ABS_{ijk} = \\beta_{0jk} + \\beta_{1jk}(CRQ_{ijk}) + \\varepsilon_{ijk}\\\\\n\\textrm{Level 2:} & \\\\\n&\\beta_{0jk} = \\gamma_{00k} + \\gamma_{01k}(Income_{jk}) +\\zeta_{0jk} \\\\\n&\\beta_{1jk} = \\gamma_{10k} +\\zeta_{1jk} \\\\\n\\textrm{Level 3:} & \\\\\n&\\gamma_{00k}= \\pi_{000} + \\pi_{001}(Funding_k) + u_{00k}\\\\\n&\\gamma_{10k} = \\pi_{100} + u_{10k} \\\\\n& \\gamma_{01k} = \\pi_{010} + u_{01k}\n\\end{align}\n\\]\n\n\n\ncollapsing it all back down in to a “mixed” equation:\n\\[\n\\begin{align}\nABS_{ijk}= &(\\pi_{000} + u_{00k} + \\zeta_{0jk}) + & \\\\\n& (\\pi_{100} + u_{10k} + \\zeta_{1jk})(CRQ_{ijk}) + &\\\\\n&(\\pi_{010} + u_{01k})(Income_{jk}) + &\\\\\n&\\pi_{001}(Funding_k) + &\\\\\n& \\varepsilon_{ijk}& \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "explainer_pcavar.html",
    "href": "explainer_pcavar.html",
    "title": "PCA and unequal variances",
    "section": "",
    "text": "We’re including this code if you want to create some data and play around with it yourself, but do not worry about understanding it! In brief, what it does is 1) create a covariance matrix 2) generate data based on the covariance matrix and 3) rename the columns to “item1”, “item2”.. etc.\n\n\nCode\nlibrary(tidyverse)\nset.seed(993)\nnitem <- 5  \nA <- matrix(runif(nitem^2)*2-1, ncol=nitem) \nscor <- t(A) %*% A\ndf <- MASS::mvrnorm(n=200,mu=rep(0,5),Sigma = scor) %>% as_tibble()\nnames(df)<-paste0(\"item\",1:5)\n\n\nThe data we created has 5 items, all on similar scales:\n\n\nCode\nlibrary(psych)\nlibrary(knitr)\nkable(describe(df)[,c(3:4)])\n\n\n\n\n\n\nmean\nsd\n\n\n\n\nitem1\n-0.096\n1.48\n\n\nitem2\n0.027\n1.98\n\n\nitem3\n-0.080\n1.22\n\n\nitem4\n-0.047\n1.66\n\n\nitem5\n0.140\n1.65"
  },
  {
    "objectID": "explainer_pcavar.html#pca-on-the-covariance-matrix",
    "href": "explainer_pcavar.html#pca-on-the-covariance-matrix",
    "title": "PCA and unequal variances",
    "section": "PCA on the covariance matrix",
    "text": "PCA on the covariance matrix\nIf we use the covariance matrix, we get slightly different results, because the loadings are proportional to the scale of the variance for each item.\n\n\nCode\nprincipal(cov(df), nfactors = 1, covar = TRUE)$loadings\n\n\n\nLoadings:\n      PC1   \nitem1  0.995\nitem2  1.689\nitem3  0.403\nitem4  1.644\nitem5 -0.613\n\n                PC1\nSS loadings    7.08\nProportion Var 1.42\n\n\n\n\n\n\n \n  \n      \n    variance of item \n    loadings cor PCA \n    loadings cov PCA \n  \n \n\n  \n    item1 \n    2.20 \n    0.662 \n    0.995 \n  \n  \n    item2 \n    3.92 \n    0.658 \n    1.689 \n  \n  \n    item3 \n    1.48 \n    0.576 \n    0.403 \n  \n  \n    item4 \n    2.75 \n    0.957 \n    1.644 \n  \n  \n    item5 \n    2.72 \n    -0.630 \n    -0.613 \n  \n\n\n\n\n\nThis means that if the items are measured on very different scales, using the covariance matrix will lead to the components being dominated by the items with the largest variance.\nLet’s make another dataset in which item2 is just measured on a completely different scale\n\n\nCode\ndfb <- df %>% mutate(item2 = item2*20)\nkable(describe(dfb)[,c(3:4)])\n\n\n\n\n \n  \n      \n    mean \n    sd \n  \n \n\n  \n    item1 \n    -0.096 \n    1.48 \n  \n  \n    item2 \n    0.545 \n    39.58 \n  \n  \n    item3 \n    -0.080 \n    1.22 \n  \n  \n    item4 \n    -0.047 \n    1.66 \n  \n  \n    item5 \n    0.140 \n    1.65 \n  \n\n\n\n\n\n\n\n\n\n \n  \n      \n    variance of item \n    loadings cor PCA \n    loadings cov PCA \n  \n \n\n  \n    item1 \n    2.20 \n    0.662 \n    0.546 \n  \n  \n    item2 \n    1566.53 \n    0.658 \n    39.579 \n  \n  \n    item3 \n    1.48 \n    0.576 \n    0.020 \n  \n  \n    item4 \n    2.75 \n    0.957 \n    1.337 \n  \n  \n    item5 \n    2.72 \n    -0.630 \n    0.069"
  },
  {
    "objectID": "explainer_pcavar.html#use-of-covar..",
    "href": "explainer_pcavar.html#use-of-covar..",
    "title": "PCA and unequal variances",
    "section": "Use of covar=..",
    "text": "Use of covar=..\nThe covar=TRUE/FALSE argument of principal() only makes a difference if you give the function a covariance matrix.\nIf you give the principal() function the raw data, then it will automatically conduct the PCA on the correlation matrix regardless of whether you put covar=TRUE or covar=FALSE\n\n\n\n\n \n  \n    principal(dfb, nfactors = 1, covar = FALSE) \n    dfb, nfactors = 1, covar = TRUE) \n    principal(cor(dfb), nfactors = 1) \n    principal(cov(dfb), nfactors = 1, covar = FALSE) \n    principal(cov(dfb), nfactors = 1, covar = TRUE) \n  \n \n\n  \n    0.662 \n    0.662 \n    0.662 \n    0.662 \n    0.546 \n  \n  \n    0.658 \n    0.658 \n    0.658 \n    0.658 \n    39.579 \n  \n  \n    0.576 \n    0.576 \n    0.576 \n    0.576 \n    0.020 \n  \n  \n    0.957 \n    0.957 \n    0.957 \n    0.957 \n    1.337 \n  \n  \n    -0.630 \n    -0.630 \n    -0.630 \n    -0.630 \n    0.069"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DAPR3",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 3 (DAPR3) lab workbook. Using the menu above, you can find lab materials for each week. These include sets of exercises along with walkthrough readings in which we introduce some of the more important R code. It is strongly recommended that students have taken Data Analysis for Psychology in R 1 and 2 (DAPR1 & DAPR2)."
  },
  {
    "objectID": "index.html#solutions",
    "href": "index.html#solutions",
    "title": "Welcome to DAPR3",
    "section": "Solutions",
    "text": "Solutions\nSolutions will be made available immediately below each exercise, in the week after they are set, so make sure to re-visit the labs to check your answers."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Welcome to DAPR3",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Welcome to DAPR3",
    "section": "Tips on googling statistics and R",
    "text": "Tips on googling statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Welcome to DAPR3",
    "section": "Feedback on labs",
    "text": "Feedback on labs\nIf you wish to make suggestions for improvements to these workbooks, please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin toss. For a fair coin, the chance of getting a heads/tails for any given toss is 0.5.\nWe can simulate the number of “heads” in a single fair coin toss with the following code (because it is a single toss, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 0\n\n\nWe can simulate the number of “heads” in 8 fair coin tosses with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 4\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin tosses? Answer: 4! Doing another 8 tosses:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 1\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 2\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin tosses, but to do 1000 sets of 8 tosses:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  3  31 114 203 266 220 125  35   3"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nWe can get to the probability of observing \\(k\\) heads in 8 tosses of a fair coin using dbinom().\nLet’s calculate the probability of observing 2 heads in 8 tosses.\nAs coin tosses are independent, we can calculate probability using the product rule (“\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent). So the probability of observing 2 heads in 2 tosses is \\(0.5 \\cdot 0.5 = 0.25\\):\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 tosses, those two heads could occur in various ways:\n\n\n# A tibble: 9 × 1\n  `Ways to get 2 heads in 8 tosses`\n  <chr>                            \n1 TTTTTHHT                         \n2 HTTTTTTH                         \n3 HHTTTTTT                         \n4 HTTTTHTT                         \n5 HTTTTTHT                         \n6 HTTTHTTT                         \n7 HTHTTTTT                         \n8 THTTTTHT                         \n9 ...                              \n\n\nIn fact there are 28 different ways this could happen:\n\ndim(combn(8, 2))\n\n[1]  2 28\n\n\nThe probability of getting 2 heads in 8 tosses of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin tosses (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given toss)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin tosses is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nFor likelihood, we are interested in hypotheses about our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?).\nTo consider these hypotheses, we need to observe some data, and so we need to have a given number of tosses, and a given number of heads. Whereas above we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given toss, for the likelihood we fix the number of heads observed, and can make statements about different possible parameters that might govern the coins behaviour.\nFor example, if we did observe 2 heads in 8 tosses, what is the likelihood of this data given various parameters?\nOur parameter can take any real number between from 0 to 1, but let’s do it for a selection:\n\nposs_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, poss_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin tosses, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). You can visualise this as below:\n\n\n\n\n\n\n\n\n\nFormalizing the intuition We have a stochastic process that takes discrete values (i.e. outcomes of tossing a coin 10 times). We calculated the probability of observing a particular set of outcomes (8 correct predictions) by making assumptions about the underlying stochastic process, that is, the probability that our test subject can correctly predict the outcome of the coin toss is (p) (e.g. 0.8). We also assumed implicitly that the coin tosses are independent."
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\). Given that we know that if we have a specific value for \\(\\theta\\), then \\(P(d | \\theta)\\) gives us the probability of observing \\(d\\), it follows that we would like to figure out what values of \\(\\theta\\) maximise \\(\\mathcal{L}(\\theta \\vert d) = P(d \\vert \\theta)\\), where \\(\\mathcal{L}(\\theta \\vert d)\\) is the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  }
]