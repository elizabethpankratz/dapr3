[
  {
    "objectID": "01_regressionrefresh.html",
    "href": "01_regressionrefresh.html",
    "title": "Regression Refresh and Clustered Data",
    "section": "",
    "text": "Preliminaries\n\nOpen Rstudio!\n\nCreate a new RMarkdown document or R script (whichever you like) for this week.\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already.\n\n\ntidyverse : for organising data\npatchwork: for organising plots\nICC : for quickly calculating intraclass correlation coefficient\nlme4 : for fitting generalised linear mixed effects models\nparameters : inference!\npbkrTest : more inference!\nHLMdiag : for examining case diagnostics at multiple levels\nlmeresampler : for bootstrapping!\neffects : for tables/plots\nsjPlot : for tables/plots\nbroom.mixed : tidying methods for mixed models\n\nYou can install all of these at once using:\n\ninstall.packages(c(\"tidyverse\",\"ICC\",\"lme4\",\"parameters\",\"pbkrTest\",\"effects\",\"broom.mixed\",\"sjPlot\",\"HLMdiag\"))\n# the lmeresampler package has had some recent updates. better to install the most recent version:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"aloy/lmeresampler\")"
  },
  {
    "objectID": "01_regressionrefresh.html#footnotes",
    "href": "01_regressionrefresh.html#footnotes",
    "title": "Regression Refresh and Clustered Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nANOVA is just a special case of the linear model↩︎"
  },
  {
    "objectID": "02_intromlm.html",
    "href": "02_intromlm.html",
    "title": "Multilevel Models",
    "section": "",
    "text": "A Note on terminology\n\n\n\n\n\nThe methods we’re going to learn about in the first five weeks of this course are known by lots of different names: “multilevel models”; “hierarchical linear models”; “mixed-effect models”; “mixed models”; “nested data models”; “random coefficient models”; “random-effects models”; “random parameter models”… and so on).\nWhat the idea boils down to is that model parameters vary at more than one level. This week, we’re going to explore what that means.\nThroughout this course, we will tend to use the terms “mixed effect model”, “linear mixed model (LMM)” and “multilevel model (MLM)” interchangeably."
  },
  {
    "objectID": "02_intromlm.html#footnotes",
    "href": "02_intromlm.html#footnotes",
    "title": "Multilevel Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“(g)lmer” here stands for “(generalised) linear mixed effects regression”.↩︎"
  },
  {
    "objectID": "03_assumptranef.html",
    "href": "03_assumptranef.html",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "",
    "text": "Data: Wellbeing Across Scotland\nFor these next set of exercises we continue with our recurring study in which researchers want to look at the relationship between time spent outdoors and mental wellbeing, across all of Scotland. Data is collected from 20 of the Local Authority Areas and is accessible at https://uoepsy.github.io/data/LAAwellbeing.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant ID\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nSelf report estimated number of hours per week spent outdoors\n    wellbeing\nWarwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n    density\nLAA Population Density (people per square km)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nThe code below will read in the data and fit the model with by-LAA random intercepts and slopes of outdoor time.\n\nlibrary(tidyverse)\nlibrary(lme4)\nscotmw &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\nrs_model &lt;- lmer(wellbeing ~ 1 + outdoor_time + (1 + outdoor_time | laa), data = scotmw)\n\n\nPlot the residuals vs fitted model, and assess the extend to which the assumption holds that the residuals are zero mean.\nConstruct a scale-location plot. This is where the square-root of the absolute value of the standardised residuals is plotted against the fitted values, and allows you to more easily assess the assumption of constant variance.\n\n\nOptional: can you create the same plot using ggplot, starting with the augment() function from the broom.mixed package?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nplot(model) will give you this plot, but you might want to play with the type = c(......) argument to get the smoothing line\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nplot(rs_model, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nAs we can see, the mean value of the residuals is quite close to zero, right the way across the fitted values. This is good.\n\nplot(rs_model,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nIn this plot we can see that the variance of the residuals is fairly constant across the fitted values. There is a slight dip at the lower end. We can see this in the previous plot too - all the points at the LHS of the plot are slightly more tightly grouped around the line. This is not enough to worry me, personally.\n\nlibrary(broom.mixed)\naugment(rs_model) %&gt;%\n  mutate(\n    sqrtr = sqrt(abs(.resid))\n  ) %&gt;%\n  ggplot(aes(x=.fitted, y=sqrtr)) + \n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nExamine the normality of both the level 1 and level 2 residuals.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nUse hist() if you like, or qqnorm(residuals) followed by qqline(residuals)\nExtracting the level 2 residuals (the random effects) can be difficult. ranef(model) will get you some of the way.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nLevel 1\n\nhist(resid(rs_model))\n\n\n\n\n\n\n\nqqnorm(resid(rs_model))\nqqline(resid(rs_model))\n\n\n\n\n\n\n\n\nLevel 2\n\nqqnorm(ranef(rs_model)$laa[, 1], main = \"Random intercept\")\nqqline(ranef(rs_model)$laa[, 1])\n\n\n\n\n\n\n\nqqnorm(ranef(rs_model)$laa[, 2], main = \"Random slope\")\nqqline(ranef(rs_model)$laa[, 2])\n\n\n\n\n\n\n\n\nThe normality of the residuals at both levels looks pretty decent here. This is especially good given that we only actually have 20 clusters (the LAAs). We have quite a small sample at this level.\n\n\n\n\nQuestion 3\n\n\n\nWhich person in the dataset has the greatest influence on our model?\n\nFor which person is the model fit the worst (i.e., who has the highest residual?)\nWhich LAA has the greatest influence on our model?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nas well as hlm_influence() in the HLMdiag package there is another nice function, hlm_augment()\nwe can often end up in confusion because the \\(i^{th}\\) observation inputted to our model (and therefore the \\(i^{th}\\) observation of hlm_influence() output) might not be the \\(i^{th}\\) observation in our original dataset - there may be missing data! (Luckily, we have no missing data in this dataset).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(HLMdiag)\nl1_inf &lt;- hlm_influence(rs_model,level=1)\ndotplot_diag(l1_inf$cooksd, cutoff=\"internal\")+\n  ylim(0,.15)\n\n\n\n\n\n\n\n\nGreatest influence:\n\nhlm_augment(rs_model, level=1) %&gt;% arrange(desc(cooksd))\n\n# A tibble: 132 × 15\n      id wellbeing outdoor_time laa          .resid .fitted .ls.resid .ls.fitted\n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1    74        35           33 Scottish Bo…  -5.15    40.2    -3.73        38.7\n 2   129        60            5 City of Edi…   8.89    51.1     6.43        53.6\n 3   109        31            8 Inverclyde    -9.22    40.2    -3.03        34.0\n 4    59        32            7 Scottish Bo…  -6.43    38.4    -7.45        39.5\n 5    31        35            7 Moray         -3.44    38.4     0.198       34.8\n 6    90        70           34 Na h-Eilean…  -3.16    73.2    -1.19        71.2\n 7    87        54           29 Highland      -5.33    59.3    -5.66        59.7\n 8    62        26           21 Midlothian    -4.77    30.8     0.214       25.8\n 9    67        37            7 East Ayrshi…   4.58    32.4     4.62        32.4\n10    64        46           18 City of Edi… -10.5     56.5   -10.1         56.1\n# ℹ 122 more rows\n# ℹ 7 more variables: .mar.resid &lt;dbl&gt;, .mar.fitted &lt;dbl&gt;, cooksd &lt;dbl&gt;,\n#   mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;, covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\nscotmw[74, ]\n\n# A tibble: 1 × 6\n  ppt   name                 laa              outdoor_time wellbeing density\n  &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ID46  Groundskeeper Willie Scottish Borders           33        35      29\n\n\n\n\n\n\n\n\n\n\n\nHighest residual:\n\nhlm_augment(rs_model, level=1) %&gt;% arrange(desc(abs(.resid)))\n\n# A tibble: 132 × 15\n      id wellbeing outdoor_time laa          .resid .fitted .ls.resid .ls.fitted\n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1    64        46           18 City of Edi… -10.5     56.5    -10.1        56.1\n 2   107        22           12 East Ayrshi… -10.3     32.3    -10.1        32.1\n 3    72        22           24 West Lothian -10.0     32.0     -9.24       31.2\n 4   109        31            8 Inverclyde    -9.22    40.2     -3.03       34.0\n 5   130        22           16 West Dunbar…  -8.98    31.0     -8.61       30.6\n 6   129        60            5 City of Edi…   8.89    51.1      6.43       53.6\n 7    93        65           18 City of Edi…   8.51    56.5      8.90       56.1\n 8    85        47           15 City of Edi…  -8.25    55.2     -8.52       55.5\n 9     7        38           13 Perth and K…  -7.84    45.8     -5.28       43.3\n10   121        31           16 Dumfries an…  -7.78    38.8     -7.70       38.7\n# ℹ 122 more rows\n# ℹ 7 more variables: .mar.resid &lt;dbl&gt;, .mar.fitted &lt;dbl&gt;, cooksd &lt;dbl&gt;,\n#   mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;, covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\nscotmw[64, ]\n\n# A tibble: 1 × 6\n  ppt   name            laa               outdoor_time wellbeing density\n  &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ID37  Nicola Sturgeon City of Edinburgh           18        46    1958\n\n\nMost influential LAA:\n\nhlm_augment(rs_model, level=\"laa\") %&gt;% arrange(desc(cooksd))\n\n# A tibble: 20 × 10\n   laa       .ranef.intercept .ranef.outdoor_time .ls.intercept .ls.outdoor_time\n   &lt;chr&gt;                &lt;dbl&gt;               &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n 1 Midlothi…           -1.76             -0.486           8.75           -1.26  \n 2 Na h-Eil…           14.1               0.399          21.4             0.0553\n 3 Glasgow …          -10.9              -0.464          -9.45           -0.593 \n 4 City of …           10.8               0.199          16.2            -0.144 \n 5 Stirling             6.51             -0.0758         13.3            -0.500 \n 6 Shetland…            4.73              0.389           3.69            0.424 \n 7 Angus               -4.86              0.0959         -6.82            0.265 \n 8 West Lot…           -3.13             -0.344           4.13           -0.725 \n 9 Falkirk             -7.45             -0.157         -12.3            -0.0360\n10 Invercly…           -1.30              0.197         -14.5             1.17  \n11 Highland             6.73              0.281           8.95            0.155 \n12 West Dun…           -6.41             -0.267          -4.87           -0.395 \n13 Moray               -2.22              0.133          -5.81            0.267 \n14 Perth an…            1.49              0.257         -20.4             1.76  \n15 East Ayr…           -5.67             -0.233          -3.63           -0.391 \n16 Orkney I…            0.179             0.155          -0.816           0.212 \n17 Scottish…           -0.264            -0.148           3.28           -0.367 \n18 Dumfries…           -2.69             -0.0120         -7.27            0.261 \n19 Argyll a…            1.65             -0.00518         4.67           -0.187 \n20 East Ren…            0.502             0.0849          1.40            0.0247\n# ℹ 5 more variables: cooksd &lt;dbl&gt;, mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;,\n#   covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\n\n\n\n\n\nQuestion 4\n\n\n\nLooking at the random effects, which LAA shows the least improvement in wellbeing as outdoor time increases, and which shows the greatest improvement?\n\nWhat is the estimated wellbeing for people from City of Edinburgh with zero hours of outdoor time per week, and what is their associated increases in wellbeing for every hour per week increase in outdoor time?\n\n\n\n\n\n\nSolution\n\n\n\nIt looks like the residents of Midlothian have the least improvement, and the Western Isles (Na h-Eileanan Siar) show the most increases of wellbeing with outdoor time. We can see this from the LAA-random slopes of outdoor time:\n\nranef(rs_model)\n\n$laa\n                      (Intercept) outdoor_time\nAngus                  -4.8568066  0.095850303\nArgyll and Bute         1.6488121 -0.005181049\nCity of Edinburgh      10.8163125  0.199034174\nDumfries and Galloway  -2.6893688 -0.012005965\nEast Ayrshire          -5.6749200 -0.232750990\nEast Renfrewshire       0.5024800  0.084907037\nFalkirk                -7.4525578 -0.156694328\nGlasgow City          -10.9101439 -0.464232183\nHighland                6.7315989  0.280992008\nInverclyde             -1.2966048  0.197142062\nMidlothian             -1.7585791 -0.485961786\nMoray                  -2.2165380  0.133392034\nNa h-Eileanan Siar     14.0595006  0.399493656\nOrkney Islands          0.1789928  0.154590585\nPerth and Kinross       1.4894924  0.256689754\nScottish Borders       -0.2638474 -0.148174460\nShetland Islands        4.7262680  0.388873631\nStirling                6.5060959 -0.075781592\nWest Dunbartonshire    -6.4127140 -0.266515096\nWest Lothian           -3.1274727 -0.343667797\n\nwith conditional variances for \"laa\" \n\n\nWe can get the cluster-specific coefficients either by adding the fixef() and ranef() together, or using coef():\n\ncoef(rs_model)\n\n$laa\n                      (Intercept) outdoor_time\nAngus                    33.36700   0.31050188\nArgyll and Bute          39.87261   0.20947052\nCity of Edinburgh        49.04011   0.41368575\nDumfries and Galloway    35.53443   0.20264561\nEast Ayrshire            32.54888  -0.01809942\nEast Renfrewshire        38.72628   0.29955861\nFalkirk                  30.77124   0.05795725\nGlasgow City             27.31366  -0.24958061\nHighland                 44.95540   0.49564358\nInverclyde               36.92720   0.41179364\nMidlothian               36.46522  -0.27131021\nMoray                    36.00726   0.34804361\nNa h-Eileanan Siar       52.28330   0.61414523\nOrkney Islands           38.40280   0.36924216\nPerth and Kinross        39.71329   0.47134133\nScottish Borders         37.95995   0.06647711\nShetland Islands         42.95007   0.60352520\nStirling                 44.72990   0.13886998\nWest Dunbartonshire      31.81109  -0.05186352\nWest Lothian             35.09633  -0.12901622\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\n\ncoef(rs_model)$laa[\"City of Edinburgh\",]\n\n                  (Intercept) outdoor_time\nCity of Edinburgh    49.04011    0.4136857"
  },
  {
    "objectID": "03_assumptranef.html#footnotes",
    "href": "03_assumptranef.html#footnotes",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s always going to be debateable about what is ‘too high’ because in certain situations you might expect correlations close to 1. It’s best to think through whether it is a feasible value given the study itself↩︎"
  },
  {
    "objectID": "04_centerglmer.html",
    "href": "04_centerglmer.html",
    "title": "Centering in MLM | Logistic MLM",
    "section": "",
    "text": "Centering & Scaling in LM\n\n\n\n\n\nWe have some data from a study investigating how perceived persuasiveness of a speaker is influenced by the rate at which they speak.\n\ndap2 &lt;- read_csv(\"https://uoepsy.github.io/data/dapr2_2122_report1.csv\")\n\nWe can fit a simple linear regression (one predictor) to evaluate how speech rate (variable sp_rate in the dataset) influences perceived persuasiveness (variable persuasive in the dataset). There are various ways in which we can transform the predictor variable sp_rate, which in turn can alter the interpretation of some of our estimates:\n\n\nRaw X\n\nm1 &lt;- lm(persuasive ~ sp_rate, data = dap2)\nsummary(m1)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 55.532060  6.4016670  8.674625 6.848945e-15\nsp_rate     -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\nThe intercept and the coefficient for neuroticism are interpreted as:\n\n(Intercept): A audio clip of someone speaking at zero phones per second is estimated as having an average persuasive rating of 55.53.\n\nsp_rate: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nMean-Centered X\nWe can mean center our predictor and fit the model again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_mc = sp_rate - mean(sp_rate))\nm2 &lt;- lm(persuasive ~ sp_rate_mc, data = dap2)\nsummary(m2)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667  1.3519418 39.110165 6.429541e-80\nsp_rate_mc  -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_mc: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nStandardised X\nWe can standardise our predictor and fit the model yet again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_z = scale(sp_rate))\nm3 &lt;- lm(persuasive ~ sp_rate_z, data = dap2)\nsummary(m3)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667   1.351942 39.110165 6.429541e-80\nsp_rate_z   -0.576077   1.356471 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_z: For every increase of one standard deviation in phones per second, perceived persuasiveness is estimated to decrease by -0.58.\n\nRemember that the scale(sp_rate) is subtracting the mean from each value, then dividing those by the standard deviation. The standard deviation of dap2$sp_rate is:\n\nsd(dap2$sp_rate)\n\n[1] 3.016315\n\n\nso in our variable dap2$sp_rate_z, a change of 3.02 gets scaled to be a change of 1 (because we are dividing by sd(dap2$sp_rate)).\n\ncoef(m1)[2] * sd(dap2$sp_rate)\n\n  sp_rate \n-0.576077 \n\ncoef(m3)[2]\n\nsp_rate_z \n-0.576077 \n\n\n\n\nNote that these models are identical. When we conduct a model comparison between the 3 models, the residual sums of squares is identical for all models:\n\nanova(m1,m2,m3)\n\nAnalysis of Variance Table\n\nModel 1: persuasive ~ sp_rate\nModel 2: persuasive ~ sp_rate_mc\nModel 3: persuasive ~ sp_rate_z\n  Res.Df   RSS Df Sum of Sq F Pr(&gt;F)\n1    148 40576                      \n2    148 40576  0         0         \n3    148 40576  0         0         \n\n\nWhat changes when you center or scale a predictor in a standard regression model (one fitted with lm())?\n\nThe variance explained by the predictor remains exactly the same\nThe intercept will change to be the estimated mean outcome where that predictor is “0”. Scaling and centering changes what “0” represents, thereby changing this estimate (the significance test will therefore also change because the intercept now has a different meaning)\nThe slope of the predictor will change according to any scaling (e.g. if you divide your predictor by 10, the slope will multiply by 10).\nThe test of the slope of the predictor remains exactly the same.\n\n\n\n\n\n\nExercises: Centering in the MLM\n\nData: Hangry\nThe study is interested in evaluating whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nThe data are available at: https://uoepsy.github.io/data/hangry.csv.\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant\n\n\nfivetwo\nWhether the participant follows the five-two diet\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead carefully the description of the study above, and try to write out (in lmer syntax) an appropriate model to test the research aims.\ne.g.:\noutcome ~ explanatory variables + (???? | grouping)\nTry to think about the maximal random effect structure (i.e. everything that can vary by-grouping is estimated as doing so).\nTo help you think through the steps to get from a description of a research study to a model specification, think about your answers to the following questions.\nQ: What is our outcome variable?\nQ: What are our explanatory variables?\nQ: Is there any grouping (or “clustering”) of our data that we consider to be a random sample? If so, what are the groups?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet.\nWe can split our data in to groups of each participant. We can also split it into groups of each diet. Which of these groups have we randomly sampled? Do we have a random sample of participants? Do we have a random sample of diets? Another way to think of this is “if i repeated the experiment, what these groups be different?”\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nOur outcome is irritability here, because it is the thing that we are trying to explain through peoples’ hunger levels and diets.\n\nlmer(irritability ~  explanatory variables + (???? | grouping))\n\nWe are interested in the effect of hunger on irritability, and whether this effect is different for the five-two diet. So we are interested in the interaction:\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | grouping))\n\n(remember that hunger + diet + hunger:diet is just a more explicit way of writing hunger*diet).\nIf we did this experiment again, would we have different participants?\nYes. If we did this experiment again, would we have different diets? No, because we’re interested in the specific differences between the five-two diet and no dieting. This means we will likely want to by-participant random deviations (e.g. the ( ... | participant) bit in lmer). But we won’t have by-diet random effects (1 | diet) because the diet differences are the specific differences that we wish to test.\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | participant))\n\nThinking about what can be modelled as randomly varying between participants, we have some options:\n\nparticipants vary in how irritable they are on average\n(the intercept, 1 | participant)\nparticipants vary in how much hunger influences their irritability\n(the effect of hunger, hunger | participant)\nparticipants vary in how much diet influences irritability\n(the effect of diet, diet | participant)\nparticipants vary in how much diet effects hunger’s influence on irritability\n(the interaction between diet and hunger, diet:hunger | participant)\n\nWe can vary 1 and 2, but not 3 and 4. This is because each participant is either following the five-two diet or they are not. So for a single participant, we can’t assess “the effect diet has” on anything, because we haven’t seen that participant under different diets. if we try to plot a single participants’ data, we can see that it is impossible for us to assess “the effect of diet”:\n\n\n\n\n\n\n\n\n\nBy contrast, we can vary the intercept and the effect of hunger, because each participant has multiple values of irritability, and multiple different observations of hunger. We can think about a single participant’s “effect of hunger on irritability” and how we might fit a line to their data:\n\n\n\n\n\n\n\n\n\n\nlmer(irritability ~  hunger + diet + hunger:diet + (1 + hunger | participant))\n\n\n\n\n\nTotal, Within, Between\nRecall our research aim:\n\n… whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting.\n\nForgetting about any differences due to diet, let’s just think about the relationship between irritability and hunger. How should we interpret this research aim?\nWas it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a composite of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that the ‘total effect’ isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it about how person i changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are.).\n\n\nQuestion 2\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll find group_by() %&gt;% mutate() very useful here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nhangry &lt;- \n    hangry %&gt;% group_by(ppt) %&gt;%\n        mutate(\n            avg_hunger = mean(q_hunger),\n            hunger_gc = q_hunger - avg_hunger\n        )\nhead(hangry)\n\n# A tibble: 6 × 6\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   fivetwo avg_hunger hunger_gc\n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1             17       30 N1p1  1             26.6     3.4  \n2             19       27 N1p1  1             26.6     0.400\n3             19       29 N1p1  1             26.6     2.4  \n4             20       33 N1p1  1             26.6     6.4  \n5             24       14 N1p1  1             26.6   -12.6  \n6             30       28 N1p2  1             32.6    -4.6  \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people?\n\nDoes it look like when people are more hungry than normal, they are more irritable?\n\n\n\n\n\n\nSolution\n\n\n\nWe might find it easier to look at a plot where each participant is represented as their mean plus an indication of their range of irritability scores:\n\nggplot(hangry,aes(x=avg_hunger,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nThere appears to be a slight positive relationship between a persons’ average hunger and their irritability scores.\nIt is harder to tell what the relationship is between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data:\n\nggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt))+\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2)\n\n\n\n\n\n\n\n\nI think there might be a positive trend in here, in that participants tend to be higher irritability when they are higher (for them) on the hunger score.\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation level deviations from those averages), that represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables as predictors, instead of the raw hunger scores.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nhunger * diet could be replaced by (hunger1 + hunger2) * diet, thereby allowing each aspect of hunger to interact with diet.\nWe can only put one of these variables in the random effects (1 + hunger | participant). Recall that above we discussed how we cannot have (diet | participant), because “an effect of diet” makes no sense for a single participant (they are either on the diet or they are not, so there is no ‘effect’). Similarly, each participant has only one value for their average hungriness.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(lme4)\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                control = lmerControl(optimizer=\"bobyqa\"))\n\n\n\n\n\nQuestion 5\n\n\nHopefully, you have fitted a model similar to the below:\n\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc) * fivetwo + \n                (1 + hunger_gc | ppt), data = hangry,\n            control = lmerControl(optimizer=\"bobyqa\"))\n\nBelow, we have obtained p-values using the Kenward Rogers Approximation of \\(df\\) for the test of whether the fixed effects are zero, so we can see the significance of each estimate.\nProvide an answer for each of these questions:\n\nFor those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\nDo people following no diet tend to be more irritable when they are more hungry than they usually are?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\n(Trickier:) What does the fivetwo coefficient represent?\n\n\n\n\n\n\n\n  \n    \n      Model Summary\n    \n    \n    \n      Parameter\n      Coefficient\n      SE\n      95% CI\n      t\n      df\n      p\n    \n  \n  \n    \n      Fixed Effects \n    \n    (Intercept)\n17.13\n5.21\n(6.75, 27.51)\n3.29\n77.00\n0.002 \n    avg hunger\n3.86e-03\n0.11\n(-0.21, 0.22)\n0.04\n77.00\n0.971 \n    hunger gc\n0.19\n0.08\n(0.03, 0.34)\n2.45\n70.40\n0.017 \n    fivetwo (1)\n-10.85\n6.62\n(-24.03, 2.32)\n-1.64\n77.00\n0.105 \n    avg hunger × fivetwo (1)\n0.47\n0.14\n(0.20, 0.74)\n3.44\n77.00\n&lt; .001\n    hunger gc × fivetwo (1)\n0.38\n0.10\n(0.18, 0.58)\n3.75\n73.64\n&lt; .001\n    \n      Random Effects \n    \n    SD (Intercept: ppt)\n6.93\n\n\n\n\n\n    SD (hunger_gc: ppt)\n0.38\n\n\n\n\n\n    Cor (Intercept~hunger_gc: ppt)\n-0.01\n\n\n\n\n\n    SD (Residual)\n4.83\n\n\n\n\n\n  \n  \n    \n      \n    \n  \n  \n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n1: For those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nA: ‘No diet’ is the reference level of the five-two variable, and because we have an interaction, that means the avg_hunger coefficient will provide the relevant estimate. There is no evidence (\\(p&gt;.05\\)) to suggest that when not dieting, hungrier people are more irritable than less hungry people.\n2: Is there evidence to suggest that this is different for those following the five-two diet? In what way?\nA: This is the interaction between avg_hunger:fivetwo1. We can see that, for every increase of 1 in average hunger, irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet.\nThese units are still in terms of the original scale (i.e. 0 to 100).\n3: Do people following no diet tend to be more irritable when they are more hungry than they usually are? A: This is the estimate for the coefficient of hunger_gc. For people following no diet, there is an estimated 0.19 increase in irritability for every 1 unit more hungry they become.\n4: Is there evidence to suggest that this is different for those following the five-two diet? In what way? A: This effect of a 1 unit change on within-person hunger increasing irritability is increased for those who are following the five-two diet by an additional 0.38\n5: What does the fivetwo1 coefficient represent? A: This represents the group difference of irritability between those on the five-two diet vs those not dieting, for someone who has an average hunger score of 0.\n\n\n\n\nQuestion 6\n\n\nConstruct two plots showing the two model estimated interactions. Think about your answers to the previous question, and check that they match with what you are seeing in the plots (do not underestimate the utility of this activity for helping understanding!).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis isn’t as difficult as it sounds. the sjPlot package can do it in one line of code!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(sjPlot)\nplot_model(hangrywb, type = \"int\")[[1]]\n\n\n\n\n\n\n\n\nWe saw in the model coefficients that for the reference level of fivetwo, the “No Diet” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. We also saw the interaction avg_hunger:fivetwo1 indicates that irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\nplot_model(hangrywb, type = \"int\")[[2]]\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_gc we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re on “No Diet”). This is the slope of the red line. The interaction hunger_gc:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive and significant, which matches with the fact that the blue line is clearly steeper in this plot.\n\n\n\n\nQuestion 7\n\n\nProvide tests or confidence intervals for the parameters of interest, and write-up the results.\n\n\n\n\n\n\nRemember: some options for inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\n\n\n\n\ntests or CIs for model parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\n\n\nmodel comparison(different fixed effects, same random effects)\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(parameters)\nmodel_parameters(hangrywb, ci_method = \"kr\", ci_random = FALSE)\n\n# Fixed Effects\n\nParameter                | Coefficient |   SE |          95% CI |     t |    df |      p\n----------------------------------------------------------------------------------------\n(Intercept)              |       17.13 | 5.21 | [  6.75, 27.51] |  3.29 | 77.00 | 0.002 \navg hunger               |    3.86e-03 | 0.11 | [ -0.21,  0.22] |  0.04 | 77.00 | 0.971 \nhunger gc                |        0.19 | 0.08 | [  0.03,  0.34] |  2.45 | 70.40 | 0.017 \nfivetwo [1]              |      -10.85 | 6.62 | [-24.03,  2.32] | -1.64 | 77.00 | 0.105 \navg hunger × fivetwo [1] |        0.47 | 0.14 | [  0.20,  0.74] |  3.44 | 77.00 | &lt; .001\nhunger gc × fivetwo [1]  |        0.38 | 0.10 | [  0.18,  0.58] |  3.75 | 73.64 | &lt; .001\n\n# Random Effects\n\nParameter                      | Coefficient\n--------------------------------------------\nSD (Intercept: ppt)            |        6.93\nSD (hunger_gc: ppt)            |        0.38\nCor (Intercept~hunger_gc: ppt) |       -0.01\nSD (Residual)                  |        4.83\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on a restricted diet such as the five-two, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes of relative-hunger level were included for participants. The model was fitting with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser from the lme4. \\(P\\)-values were obtained using Wald tests with Kenward-Roger approximation of denominator degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(\\beta = 0.19,\\ SE = 0.08,\\ t(2.45) = 70.4,\\ p=0.017\\)), and that this was increased for those following the five-two diet (\\(\\beta = 0.38,\\ SE = 0.1,\\ t(3.75) = 73.64,\\ p&lt;0.001\\)). Although for those not on a specific diet there was no evidence for an association between irritability and being generally a more hungry person (\\(p=0.971\\)), there a significant interaction was found between average hunger and being on the five-two diet (\\(\\beta = 0.47,\\ SE = 0.14,\\ t(3.44) = 77,\\ p&lt;0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association of both between-person hunger and within-person hunger with irritability.\n\n\n\n\nOther within-group transformations\nAs well as within-group mean centering a predictor (like we have done above), we can within-group standardise a predictor. This would disagregate within and between effects, but interpretation would of the within effect would be the estimated change in \\(y\\) associated with being 1 standard deviation higher in \\(x\\) for that group.\n\n\n\n\nOptional Exercises: Logistic MLM\n\nDon’t forget to look back at other materials!\nBack in DAPR2, we introduced logistic regression in semester 2, week 8. The lab contained some simulated data based on a hypothetical study about inattentional blindness. That content will provide a lot of the groundwork for this week, so we recommend revisiting it if you feel like it might be useful.\n\n\n\n\n\n\n\nFrom lmer() to glmer()\n\n\n\n\n\nRemember how we simply used glm() and could specify the family = \"binomial\" in order to fit a logistic regression? Well it’s much the same thing for multi-level models!\n\nGaussian model: lmer(y ~ x1 + x2 + (1 | g), data = data)\n\nBinomial model: glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial(link='logit'))\n\nor just glmer(y ~ x1 + x2 + (1 | g), data = data, family = \"binomial\")\nor glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData: Memory Recall & Finger Tapping\n\nResearch Question: After accounting for effects of sentence length, does the rhythmic tapping of fingers aid memory recall?\n\nResearchers recruited 40 participants. Each participant was tasked with studying and then recalling 10 randomly generated sentences between 1 and 14 words long. For 5 of these sentences, participants were asked to tap their fingers along with speaking the sentence in both the study period and in the recall period. For the remaining 5 sentences, participants were asked to sit still.\nThe data are available at https://uoepsy.github.io/data/memorytap.csv, and contains information on the length (in words) of each sentence, the condition (static vs tapping) under which it was studied and recalled, and whether the participant was correct in recalling it.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier (n=40)\n    slength\nNumber of words in sentence\n    condition\nCondition under which sentence is studied and recalled ('static' = sitting still, 'tap' = tapping fingers along to sentence)\n    correct\nWhether or not the sentence was correctly recalled\n  \n  \n  \n\n\n\n\n\n\nQuestion Optional\n\n\n\nResearch Question: After accounting for effects of sentence length, does the rhythmic tapping of fingers aid memory recall?\n\nFit an appropriate model to answer the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nour outcome is conceptually ‘memory recall’, and it’s been measured by “Whether or not a sentence was correctly recalled”. This is a binary variable.\n\nwe have multiple observations for each ?????\nThis will define our (  | ??? ) bit\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmemtap &lt;- read_csv(\"https://uoepsy.github.io/data/memorytap.csv\")\n\nWhen we fit the maximal model, note that we obtain a singular fit. The variance of the slength effect between participants is quite small relative to the others, and there is a correlation between it and the random intercepts.\n\ntapmod &lt;- glmer(correct ~ 1 + slength + condition + \n                  (1 + slength + condition | ppt),\n      data = memtap,\n      family = binomial)\nisSingular(tapmod)\n\n[1] TRUE\n\nVarCorr(tapmod)\n\n Groups Name         Std.Dev. Corr         \n ppt    (Intercept)  1.032849              \n        slength      0.070307 -1.000       \n        conditiontap 0.665626  0.590 -0.590\n\n\nlet’s remove the random effect of slength | ppt.\n\ntapmod2 &lt;- glmer(correct ~ 1 + slength + condition + \n                  (1 + condition | ppt),\n      data = memtap,\n      family = binomial)\n\nthe model now looks a bit better (not a singular fit):\n\nsummary(tapmod2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: correct ~ 1 + slength + condition + (1 + condition | ppt)\n   Data: memtap\n\n     AIC      BIC   logLik deviance df.resid \n   537.6    561.5   -262.8    525.6      394 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8949 -0.8955  0.4483  0.7962  1.6862 \n\nRandom effects:\n Groups Name         Variance Std.Dev. Corr\n ppt    (Intercept)  0.2755   0.5249       \n        conditiontap 0.4207   0.6486   0.66\nNumber of obs: 400, groups:  ppt, 40\n\nFixed effects:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   0.76140    0.37077   2.054   0.0400 *\nslength      -0.12086    0.04721  -2.560   0.0105 *\nconditiontap  0.50945    0.24317   2.095   0.0362 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) slngth\nslength     -0.890       \nconditiontp -0.154 -0.029\n\n\n\n\n\n\n\n\n\n\n\nFrom Log odds to odds ratios\n\n\n\n\n\nTake some time to remind yourself from DAPR2 of the interpretation of logistic regression coefficients.\nIn family = binomial(link='logit'), we are modelling the log-odds. We can obtain estimates on this scale using:\n\nfixef(model)\nsummary(model)$coefficients\ntidy(model) from broom.mixed\n\n(there are probably more ways, but I can’t think of them right now!)\n\nWe can use exp(), to get these back into odds and odds ratios.\n\n\n\n\nQuestion Optional\n\n\nInterpret each of the fixed effect estimates from your model.\n\n\n\n\n\nSolution\n\n\n\n\nfixef(tapmod2)\n\n (Intercept)      slength conditiontap \n   0.7613976   -0.1208591    0.5094460 \n\nexp(fixef(tapmod2))\n\n (Intercept)      slength conditiontap \n   2.1412669    0.8861589    1.6643689 \n\n\n\n(Intercept): For an sentence with zero words, when sitting statically, the odds of correctly recalling the sentence are 2.14. This is equivalent to a \\(\\frac{2.14}{1 + 2.14} = 0.6815287\\) probability of getting it correct.\n\nslength: After accounting for differences due to tapping/not-tapping during study & recall, for every 1 word longer a sentence is, the odds of correctly recalling the sentence is decreased by 0.89.\nconditiontap: After accounting for differences in recall due to sentence length, finger tapping during the study and recall of sentences was associated with 1.66 increased odds correct recall in comparison to sitting still.\n\n\n\n\n\nQuestion Optional\n\n\nChecking the assumptions in non-gaussian models in general (i.e. those where we set the family to some other error distribution) can be a bit tricky, and this is especially true for multilevel models.\nFor the logistic MLM, the standard assumptions of normality etc for our Level 1 residuals residuals(model) do not hold. However, it is still useful to quickly plot the residuals and check that \\(|residuals|\\leq 2\\) (or \\(|residuals|\\leq 3\\) if you’re more relaxed). We don’t need to worry too much about the pattern though.\nWhile we’re more relaxed about Level 1 residuals, we do still want our random effects ranef(model) to look fairly normally distributed.\n\nPlot the level 1 residuals and check whether any are greater than 3 in magnitude\nPlot the random effects (the level 2 residuals) and assess the normality.\n\n\n\n\n\n\nSolution\n\n\n\n\nplot(tapmod2)\n\n\n\n\n\n\n\nsum(abs(resid(tapmod2))&gt;3)\n\n[1] 0\n\n\nAll residuals are between -3 and 3.\nThe random effects look okay here. Not perfect, but bear in mind we have only 40 participants.\n\nqqnorm(ranef(tapmod2)$ppt[, 1], main = \"Random intercept\")\nqqline(ranef(tapmod2)$ppt[, 1])\nqqnorm(ranef(tapmod2)$ppt[, 2], main = \"Random slope of condition\")\nqqline(ranef(tapmod2)$ppt[, 2])\nhist(ranef(tapmod2)$ppt[, 1])\nhist(ranef(tapmod2)$ppt[, 2])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor beyond DAPR3\n\n\n\n\n\n\nThe HLMdiag package doesn’t support diagnosing influential points/clusters for glmer, but there is a package called influence.me which might help: https://journal.r-project.org/archive/2012/RJ-2012-011/RJ-2012-011.pdf\nThere are packages which aim to create more interpretable residual plots for these models via simulation, such as the DHARMa package: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html"
  },
  {
    "objectID": "05_recap.html",
    "href": "05_recap.html",
    "title": "Recap of multilevel models",
    "section": "",
    "text": "Flashcards: lm to lmer\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\n\n\n\n\n\n\n\nSimple Linear Regression”\n\n\n\n\n\n\nFormula:\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustered (multi-level) data\n\n\n\n\n\nWhen our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nIf we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nBy including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nWe can now see that the intercept estimate \\(\\beta_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has an adjustment for each subject.\nEach subject’s line has been moved up or down accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage\n\n\n\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different:\n\n\n\n\n\n\n\n\n\nWhy? One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters.\n\n\n\n\n\n\n\n\n\nRandom slopes\n\n\n\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(\\beta_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nAs with the intercept \\(\\beta_{0i}\\), the slope of the predictor \\(\\beta_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed effects\n\n\n\n\n\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the groups all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\nThese are the overall intercept and slope.\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n405.7897675  -0.6722654 \n\n\n\n\n\n\n\n\n\n\n\nRandom effects\n\n\n\n\n\nThe plots below show the fitted values for each subject from each model that we have gone through in these expandable boxes (simple linear regression, random intercept, and random intercept & slope):\n\n\n\n\n\n\n\n\n\nIn the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_0\\). The standard deviation (and variance, which is \\(\\sigma_0^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function).\n\n\n\n\n\n\n\n\n\nIn the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. We can extract the deviations for each group from the fixed effect estimates using the ranef() function.\nThese are the deviations from the overall intercept (\\(\\widehat \\gamma_{00} = 405.79\\)) and slope (\\(\\widehat \\gamma_{10} = -0.672\\)) for each subject \\(i\\).\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)          x1\nsub_308   31.327291 -1.43995253\nsub_309  -28.832219  0.41839420\nsub_310    2.711822  0.05993766\nsub_330   59.398971  0.38526670\nsub_331   74.958481  0.17391602\nsub_332   91.086535 -0.23461836\nsub_333   97.852988 -0.19057838\nsub_334  -54.185688 -0.55846794\nsub_335  -16.902018  0.92071637\nsub_337   52.217859 -1.16602280\nsub_349  -67.760246 -0.68438960\nsub_350   -5.821271 -1.23788002\nsub_351   61.198823  0.05499816\nsub_352   -7.905596 -0.66495059\nsub_369  -47.636645 -0.46810258\nsub_370  -33.121093 -1.11001234\nsub_371   77.576205 -0.20402571\nsub_372  -36.389281 -0.45829505\nsub_373 -197.579562  1.79897904\nsub_374  -52.195357  4.60508775\n\nwith conditional variances for \"subject\" \n\n\n\n\n\n\n\n\n\n\n\nGroup-level coefficients\n\n\n\n\n\nWe can see the estimated intercept and slope for each subject \\(i\\) specifically, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)         x1\nsub_308    437.1171 -2.1122179\nsub_309    376.9575 -0.2538712\nsub_310    408.5016 -0.6123277\nsub_330    465.1887 -0.2869987\nsub_331    480.7482 -0.4983494\nsub_332    496.8763 -0.9068837\nsub_333    503.6428 -0.8628438\nsub_334    351.6041 -1.2307333\nsub_335    388.8877  0.2484510\nsub_337    458.0076 -1.8382882\nsub_349    338.0295 -1.3566550\nsub_350    399.9685 -1.9101454\nsub_351    466.9886 -0.6172672\nsub_352    397.8842 -1.3372160\nsub_369    358.1531 -1.1403680\nsub_370    372.6687 -1.7822777\nsub_371    483.3660 -0.8762911\nsub_372    369.4005 -1.1305604\nsub_373    208.2102  1.1267137\nsub_374    353.5944  3.9328224\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nNotice that the above are the fixed effects + random effects estimates, i.e. the overall intercept and slope + deviations for each subject.\n\ncbind(\n  int = fixef(random_slopes_model)[1] + \n    ranef(random_slopes_model)$subject[,1],\n  slope = fixef(random_slopes_model)[2] + \n    ranef(random_slopes_model)$subject[,2]\n)\n\n           int      slope\n [1,] 437.1171 -2.1122179\n [2,] 376.9575 -0.2538712\n [3,] 408.5016 -0.6123277\n [4,] 465.1887 -0.2869987\n [5,] 480.7482 -0.4983494\n [6,] 496.8763 -0.9068837\n [7,] 503.6428 -0.8628438\n [8,] 351.6041 -1.2307333\n [9,] 388.8877  0.2484510\n[10,] 458.0076 -1.8382882\n[11,] 338.0295 -1.3566550\n[12,] 399.9685 -1.9101454\n[13,] 466.9886 -0.6172672\n[14,] 397.8842 -1.3372160\n[15,] 358.1531 -1.1403680\n[16,] 372.6687 -1.7822777\n[17,] 483.3660 -0.8762911\n[18,] 369.4005 -1.1305604\n[19,] 208.2102  1.1267137\n[20,] 353.5944  3.9328224\n\n\n\n\n\n\n\n\n\n\n\nAssumptions, Influence\n\n\n\n\n\nIn the simple linear model \\(\\color{red}{y} = \\color{blue}{\\beta_0 + \\beta_1(x)} + \\varepsilon\\), we distinguished between the systematic model part \\(\\beta_0 + \\beta_1(x)\\), around which observations randomly vary (the \\(\\varepsilon\\) part) - i.e. \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}} + \\text{error}\\).\nIn the multi-level model, our random effects are another source of random variation - \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}} + \\text{group_error} + \\text{individual_error}\\). As such, random effects are another form of residual, and our assumptions of zero mean constant variance apply at both levels of residuals (see Figure 1).\n\n\n\n\n\nFigure 1: The black dashed lines show our model assumptions.\n\n\n\n\n\nWe can assess these normality of both resid(model) and ranef(model) by constructing plots using functions such as hist(), qqnorm() and qqline().\n\nWe can also use plot(model, type=c(\"p\",\"smooth\")) to give us our residuals vs fitted plot (smooth line should be horizontal at approx zero, showing zero mean).\n\nplot(model, form = sqrt(abs(resid(.))) ~ fitted(.), type = c(\"p\",\"smooth\")) will give us our scale-location plot (smooth line should be horizontal, showing constant variance).\n\nWe can also use the check_model() function from the performance package to get lots of info at once:\n\nlibrary(performance)\ncheck_model(random_slopes_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\ncase-based bootstrap\n\n\n\n\nmodel parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\nlibrary(lmeresampler)bootstrap(model, .f=fixef, type=\"case\", B = 2000, resample = c(??,??))\n\n\nmodel comparison\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\ntakes time, needs careful thought about which levels to resample, but means we can relax distributional assumptions (e.g. about normality of residuals)\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Model Fitted values\n\n\n\n\n\nThe model fitted (or “model predicted”) values can be obtained using predict() (returning just the values) or broom.mixed::augment() (returning the values attached to the data that is inputted to the model).\nTo plot, them, we would typically like to plot the fitted values for each group (e.g. subject)\n\nlibrary(broom.mixed)\naugment(random_slopes_model) %&gt;%\n  ggplot(.,aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Fixed Effects\n\n\n\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. Packages like sjPlot make it incredibly easy (but sometimes too easy), so a nice option is to use the effects package to construct a dataframe of the linear prediction accross the values of a predictor, plus standard errors and confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\n# a quick option:  \nlibrary(sjPlot)\nplot_model(random_slopes_model, type = \"eff\")\n\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting random effects\n\n\n\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms &lt;- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\n\n\n\n\nMLM allows us to model effects in the linear model as varying between groups. Our coefficients we remember from simple linear models (the \\(\\beta\\)’s) are modelled as a distribution that has an overall mean around which our groups vary. We can see this in Figure 2, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 2 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\nFigure 2: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(\\beta\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is \\(\\rho = \\text{Cor}(\\zeta_{0i}, \\zeta_{1i}) = \\frac{\\sigma_{01}}{\\sigma_0 \\sigma_1}\\):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\nWe fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\nlibrary(lme4)\nm1 &lt;- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 637.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.49449 -0.57223 -0.01353  0.62544  2.39122 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.2616   1.5038       \n          x           0.7958   0.8921   0.55\n Residual             4.3672   2.0898       \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   1.7261     0.9673   1.785\nx             1.1506     0.2968   3.877\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)\n\n\n\n\n\n\n\nExercises: Less guided\nTODO"
  },
  {
    "objectID": "07_path1.html",
    "href": "07_path1.html",
    "title": "Path Analysis",
    "section": "",
    "text": "Relevant packages\n\nlavaan\n\nsemPlot or tidySEM\n\n\nBy now, we are getting more comfortable with the regression world, and we can see how it is extended to lots of different types of outcome and data structures. So far in DAPR3 it’s been all about the multiple levels. This has brought so many more potential study designs that we can now consider modelling - pretty much any study where we are interested in explaining some outcome variable, and where we have sampled clusters of observations (or clusters of clusters of clusters of … etc.).\nBut we are still restricted to thinking, similar to how we thought in DAPR2, about one single outcome variable. In fact, if we think about the structure of the fixed effects part of a model (i.e., the bit we’re specifically interested in), then we’re still limited to thinking of the world in terms of “this is my outcome variable, everything else predicts it”.\n\n\n\n\n\n\nRegression as a path diagram\n\n\n\n\n\n\nImagine writing the names of all your variables on a whiteboard\nSpecify which one is your dependent (or “outcome” or “response”) variable.\nSit back and relax, you’re done!\n\nIn terms of a theoretical model of the world, there’s not really much to it. We have few choices in the model we construct beyond specifying which is our outcome variable.\nWe can visualise our multiple regression model like this:\n\n\n\n\n\nFigure 1: In multiple regression, we decide which variable is our outcome variable, and then everything else is done for us\n\n\n\n\nOf course, there are a few other things that are included (an intercept term, the residual error, and the fact that our predictors can be correlated with one another), but the idea remains pretty much the same:\n\n\n\n\n\nFigure 2: Multiple regression with intercept, error, predictor covariances\n\n\n\n\n\n\n\n\n\n\n\n\n\nA model reflects a theory\n\n\n\n\n\nWhat if I my theoretical model of the world doesn’t fit the structure of “one outcome, multiple precictors”?\nLet’s suppose I have 5 variables: Age, Parental Income, Income, Autonomy, and Job Satisfaction. I draw them up on my whiteboard:\n\n\n\n\n\nFigure 3: My variables\n\n\n\n\nMy theoretical understanding of how these things fit together leads me to link my variables to end up with something like that in Figure 4.\n\n\n\n\n\nFigure 4: My theory about my system of variables\n\n\n\n\nIn this diagram, a persons income is influenced by their age, their parental income, and their level of autonomy, and in turn their income predicts their job satisfaction. Job satisfaction is also predicted by a persons age directly, and by their level of autonomy, which is also predicted by age. It’s complicated to look at, but in isolation each bit of this makes theoretical sense.\nTake each arrow in turn and think about what it represents:\n\n\n\n\n\nFigure 5: ?(caption)\n\n\n\n\nIf we think about trying to fit this “model” with the tools that we have, then we might end up wanting to fit three separate regression models, which between them specify all the different arrows in the diagram:\n\\[\n\\begin{align}\n\\textrm{Job Satisfaction} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_3(\\textrm{Income}) + \\varepsilon \\\\\n\\textrm{Income} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_2(\\textrm{Parental Income}) + \\varepsilon \\\\\n\\textrm{Autonomy} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\varepsilon \\\\\n\\end{align}\n\\]\nThis is all well and good, but what if I want to talk about how well my entire model (Figure 4) fits the data we observed?\n\n\n\n\n\n\n\n\n\nIntroducing Path Analysis\n\n\n\n\n\nThe starting point for Path Analysis is to think about our theories in terms of the connections between variables drawn on a whiteboard. By representing a theory as paths to and from different variables, we open up a whole new way of ‘modelling’ the world around us.\nThere are a few conventions to help us understand this sort of diagrammatical way of thinking. By using combinations of rectangles, ovals, single- and double-headed arrows, we can draw all sorts of model structures. In Path Diagrams, we use specific shapes and arrows to represent different things in our model:\nShapes and Arrows in Path Diagrams\n\nObserved variables are represented by squares or rectangles. These are the named variables of interest which exist in our dataset - i.e. the ones which we have measured directly.\nVariances/Covariances are represented by double-headed arrows. In many diagrams these are curved.\nRegressions are shown by single headed arrows (e.g., an arrow from \\(x\\) to \\(y\\) for the path \\(y~x\\)).\n\n\nLatent variables are represented by ovals, and we will return to these in a few weeks time!\n\n\n\n\n\n\n\n\n\n\n\nTerminology refresher\n\nExogenous variables are a bit like what we have been describing with words like “independent variable” or “predictor”. In a path diagram, they have no paths coming from other variables in the system, but have paths going to other variables.\n\nEndogenous variables are more like the “outcome”/“dependent”/“response” variables we are used to. They have some path coming from another variable in the system (and may also - but not necessarily - have paths going out from them).\n\n\n\n\n\n\n\n\n\n\nHow does it work (in brief)?\n\n\n\n\n\nThe logic behind path analysis is to estimate a system of equations that can reproduce the covariance structure that we see in the data.\n\nWe specify our theoretical model of the world as a system of paths between variables\nWe collect data on the relevant variables and we observe a correlation matrix (i.e. how each variable correlates with all others)\nWe fit our model to the data, and evaluate how well our theoretical model (a system of paths) can reproduce the correlation matrix we observed.\n\n\n\n\n\n\n\n\n\n\nOPTIONAL How does it work (less brief)?\n\n\n\n\n\nPath Diagram Tracing\nFor Path Diagrams that meet a certain set of pre-requisites, we can use a cool technique called Path Tracing to estimate the different paths (i.e., the coefficients) from just the covariance matrix of the dataset. For us to be able to do this, a Path Diagram must meet these criteria:\n\nAll our exogenous variables are correlated (unless we specifically assume that their correlation is zero)\nAll models are ‘recursive’ (no two-way causal relations, no feedback loops)\nResiduals are uncorrelated with exogenous variables\nEndogenous variables are not connected by correlations (we would use correlations between residuals here, because the residuals are not endogenous)\nAll ‘causal’ relations are linear and additive\n‘causes’ are unitary (if A -&gt; B and A -&gt; C, then it is presumed that this is the same aspect of A resulting in a change in both B and C, and not two distinct aspects of A, which would be better represented by two correlated variables A1 and A2).\n\n\n\n\n\n\n\nCausal?\n\n\n\n\n\nIt is a slippery slope to start using the word ‘cause’, and personally I am not that comfortable using it here. However, you will likely hear it a lot in resources about path analysis, so it is best to be warned.\nPlease keep in mind that we are using a very broad definition of ‘causal’, simply to reflect the one way nature of the relationship we are modeling. In Figure 6, a change in the variable X1 is associated with a change in Y, but not vice versa.\n\n\n\n\n\nFigure 6: Paths are still just regressions.\n\n\n\n\n\n\n\nTracing Rules\nThanks to Sewal Wright, we can express the correlation between any two variables in the system as the sum of all compound paths between the two variables.\ncompound paths are any paths you can trace between A and B for which there are:\n\nno loops\nno going forward then backward\nmaximum of one curved arrow per path\n\nEXAMPLE\nLet’s consider the example below, for which the paths are all labelled with lower case letters \\(a, b, c, \\text{and } d\\).\n\n\n\n\n\nFigure 7: A multiple regression model as a path diagram\n\n\n\n\nAccording to Wright’s tracing rules above, write out the equations corresponding to the 3 correlations between our observed variables (remember that \\(r_{a,b} = r_{b,a}\\), so it doesn’t matter at which variable we start the paths).\n\n\\(r_{x1,x2} = c\\)\n\n\\(r_{x1,y} = a + bc\\)\n\n\\(r_{x2,y} = b + ac\\)\n\nNow let’s suppose we observed the following correlation matrix:\n\negdat &lt;- read_csv(\"https://uoepsy.github.io/data/patheg.csv\")\negdat &lt;- read_csv(\"../../data/patheg.csv\")\nround(cor(egdat),2)\n\n     x1   x2    y\nx1 1.00 0.36 0.75\nx2 0.36 1.00 0.60\ny  0.75 0.60 1.00\n\n\nWe can plug these into our system of equations:\n\n\\(r_{x1,x2} = c = 0.36\\)\n\n\\(r_{x1,y} = a + bc = 0.75\\)\n\n\\(r_{x2,y} = b + ac = 0.60\\)\n\nAnd with some substituting and rearranging, we can work out the values of \\(a\\), \\(b\\) and \\(c\\).\n\n\n\n\n\n\nClick for answers\n\n\n\n\n\nThis is what I get:\na = 0.61\nb = 0.38\nc = 0.36\n\n\n\nWe can even work out what the path labeled \\(d\\) (the residual variance) is.\nFirst we sum up all the equations for the paths from Y to Y itself.\nThese are:\n\n\\(a^2\\) (from Y to X1 and back)\n\n\\(b^2\\) (from Y to X2 and back)\n\n\\(acb\\) (from Y to X1 to X2 to Y)\n\\(bca\\) (from Y to X2 to X1 to Y)\n\nSumming them all up and solving gives us:\n\\[\n\\begin{align}\nr_{y \\cdot x1, x2} & = a^2 + b^2 + acb + bca\\\\\n& = 0.61^2 + 0.38^2 + 2 \\times(0.61 \\times 0.38 \\times 0.36)\\\\\n& = 0.68 \\\\\n\\end{align}\n\\] We can think of this as the portion of the correlation of Y with itself that occurs via the predictors. Put another way, this is the amount of variance in Y explained jointly by X1 and X2, which sounds an awful lot like an \\(R^2\\)!\nThe path labelled \\(d\\) is simply all that is left in Y after taking out the variance explained by X1 and X2, meaning that the path \\(d = \\sqrt{1-R^2}\\) (i.e., the residual variance!).\nHooray! We’ve just worked out regression coefficients when all we had was the correlation matrix of the variables! It’s important to note that we have been using the correlation matrix, so, somewhat unsurprisingly, our estimates are standardised coefficients.\nBecause we have the data itself, let’s quickly find them with lm()\n\n# model:\nmodel1 &lt;- lm( scale(y) ~ scale(x1) + scale(x2), egdat)\n# extract the coefs\ncoef(model1) %&gt;% round(2)\n\n(Intercept)   scale(x1)   scale(x2) \n       0.00        0.61        0.38 \n\n# extract the r^2\nsummary(model1)$r.squared\n\n[1] 0.688\n\n\n\n\n\n\n\n\n\n\n\nIntroducing lavaan\n\n\n\n\n\nFor the remaining weeks of the course, we’re going to rely heavily on the lavaan (Latent Variable Analysis) package. This is the main package in R for fitting path diagrams (as well as more cool models like factor analysis sructures and structural equation mdoels). There is a huge scope of what this package can do.\nThe first thing to get to grips with is the various new operators which it allows us to use.\nOur old multiple regression formula in R was specified as y ~ x1 + x2 + x3 + ....\nIn lavaan, we continue to fit regressions using the ~ symbol, but we can also specify the construction of latent variables using =~ and residual variances & covariances using ~~.\n\n\n\nformula type\noperator\nmnemonic\n\n\n\n\nlatent variable definition\n=~\n“is measured by”\n\n\nregression\n~\n“is regressed on”\n\n\n(residual) (co)variance\n~~\n“is correlated with”\n\n\nintercept\n~1\n“intercept”\n\n\nnew parameter\n:=\n“is defined as”\n\n\n\n(from https://lavaan.ugent.be/tutorial/syntax1.html)\nIn practice, fitting models in lavaan tends to be a little different from things like lm() and (g)lmer().\nInstead of including the model formula inside the fit function (e.g., lm(y ~ x1 + x2, data = df)), we tend to do it in a step-by-step process. This is because as our models become more complex, our formulas can pretty long!\nWe write the model as a character string (e.g. model &lt;- \"y ~ x1 + x2\") and then we pass that formula along with the data to the relevant lavaan function, which for our purposes will be the sem() function, sem(model, data = mydata).\n\n\n\n\n\n\n\n\n\nFitting a multiple regression model with lavaan\n\n\n\n\n\nYou can see a multiple regression fitted with lavaan below.\n\nlibrary(lavaan)\nscsdat &lt;- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\n\n# the lm() way\nmreg_lm &lt;- lm(dass ~ zo + zc + ze + za + zn + scs, scsdat)\n\n# setting up the model\nmreg_model &lt;- \"\n    #regression\n    dass ~ zo + zc + ze + za + zn + scs\n\"\nmreg_sem &lt;- sem(mreg_model, data=scsdat)\n\nThese are the coefficients from our lm() model:\n\ncoefficients(mreg_lm)\n\n(Intercept)          zo          zc          ze          za          zn \n    62.1243     -0.0307     -0.0378      0.7449      0.2029      1.5209 \n        scs \n    -0.4865 \n\n\nAnd you can see the estimated parameters are the same for our sem() model!\n\nsummary(mreg_sem)\n\nlavaan 0.6.15 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                           656\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  dass ~                                              \n    zo               -0.031    0.242   -0.127    0.899\n    zc               -0.038    0.248   -0.153    0.879\n    ze                0.745    0.377    1.976    0.048\n    za                0.203    0.378    0.537    0.591\n    zn                1.521    0.249    6.097    0.000\n    scs              -0.486    0.071   -6.893    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .dass             40.021    2.210   18.111    0.000\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 1: Model Specification\n\n\n\n\n\nThe first part of estimating a path model involves specifying the model. This means basically writing down the paths that are included in your theoretical model.\nLet’s start by looking at the example about job satisfaction, income, autonomy and age.\nRecall we had this theoretical model:\n\n\n\n\n\n\n\n\n\nAnd now let’s suppose that we collected data on these variables:\n\njobsatpath &lt;- read_csv(\"https://uoepsy.github.io/data/jobsatpath.csv\")\nhead(jobsatpath)\n\n\n\n\n\n\njobsat\nincome\nautonomy\nage\nparentincome\n\n\n\n\n22\n39\n17\n55\n47\n\n\n29\n35\n58\n51\n43\n\n\n69\n38\n45\n52\n49\n\n\n67\n27\n52\n43\n44\n\n\n54\n14\n36\n35\n40\n\n\n25\n25\n39\n48\n44\n\n\n...\n...\n...\n...\n...\n\n\n\n\n\n\n\nRemember we said that we could specify all these paths using three regression models? Well, to specify our path model, we simply write these out like we would do in lm(), but this time we do so all in one character string. We still have to make sure that we use the correct variable names, as when we make R estimate the model, it will look in the data for things like “jobsat”.\n\nmymodel &lt;- \"\njobsat ~ age + autonomy + income\nincome ~ autonomy + age + parentincome\nautonomy ~ age\n\"\n\nThere are some other things which we will automatically be estimated here: all our exogenous variables (the ones with arrows only going from them) will be free to correlate with one another. We can write this explicitly in the model if we like, using the two tildes ~~ between our two exogenous variables age and parentincome. We will also get the variances of all our variables.\nWe can see all the paths here:\n\nlavaanify(mymodel)\n\n\n\n            lhs op          rhs\n1        jobsat  ~          age\n2        jobsat  ~     autonomy\n3        jobsat  ~       income\n4        income  ~     autonomy\n5        income  ~          age\n6        income  ~ parentincome\n7      autonomy  ~          age\n8        jobsat ~~       jobsat\n9        income ~~       income\n10     autonomy ~~     autonomy\n11          age ~~          age\n12          age ~~ parentincome\n13 parentincome ~~ parentincome\n\n\nand even make a nice diagram:\n\nlibrary(semPlot)\nsemPaths(lavaanify(mymodel))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 2: Model Identification\n\n\n\n\n\nYou’ll have heard the term “model fit” many times since the start of DAPR2, when we began model-based thinking. However, there is a crucial difference in what it means when it is used in for path analysis.\nIn things like multiple regression, we have been using “model fit” to be the measure of “how much variance can we explain in y with our set of predictors?”. For a path model, examining “model fit” is more like asking “how well does our model reproduce the characteristics of the data that we observed?”.\nWe can represent the “characteristics of our data” in a covariance matrix, so one way of thinking of “model fit” is as “how well can our model reproduce our observed covariance matrix?”.\n\ncov(jobsatpath)\n\n             jobsat income autonomy    age parentincome\njobsat       341.62   4.47    107.3 -28.81        -6.47\nincome         4.47  50.17     46.7  29.56        14.83\nautonomy     107.30  46.68    365.6  32.94       -10.00\nage          -28.81  29.56     32.9  38.69         1.81\nparentincome  -6.47  14.83    -10.0   1.81        14.18\n\n\nTODO link between\nDegrees of freedom\nWhen we think of “degrees of freedom” for a multiple regression model, in DAPR2 we learned that \\(df = n-k-1\\) (\\(n\\) is the number of observations, \\(k\\) is the number of predictors). These degrees of freedom related to the corresponding \\(F\\) and \\(t\\)-distributions with which we performed our hypothesis tests (e.g. \\(t\\)-tests for a null hypothesis that a coefficient is zero, or \\(F\\)-tests for a null that the reduction in residual sums of squares is zero).\nBut in relation to it’s ability to represent a \\(k \\times k\\) covariance matrix (i.e. the covariance matrix of our \\(k\\) variables), the multiple regression model has zero degrees of freedom. This is because “degrees of freedom” in the this framework corresponds to the number of knowns (observed covariances/variances) minus the number of unknowns (parameters to be estimated by the model). A model is only able to be estimated if it has at least 0 degrees of freedom (if there are as many knowns as unknowns). A model with 0 degrees of freedom is termed just-identified. Under- and Over- identified models correspond to those with \\(&lt;0\\) and \\(&gt;0\\) degrees of freedom respectively.\nSo the multiple regression model is an example of a just-identified model! In multiple regression, everything is allowed to covary with everything else, which means that there is a unique solution for all of the model’s parameters because there are as many paths as there are observed covariances. This means that in this path analysis world, a multiple regression model is “just-identified”.\n\nHow many knowns are there?\nThe number of known covariances in a set of \\(k\\) observed variables is equal to \\(\\frac{k \\cdot (k+1)}{2}\\).\n\nWhen learning about path models, the visualisations can play a key part. It often helps to draw all our variables (both observed and latent) on the whiteboard, and connect them up according to your theoretical model. You can then count the number of paths (arrows) and determine whether the number of knowns &gt; number of unknowns. We can reduce the number of unknowns by fixing parameters to be specific values.\n\nBy constraining some estimated parameter to be some specific value, we free-up a degree of freedom! For instance “the correlation between x1 and x2 is equal to 0.7 (\\(r_{x_1x_2} = .07\\))”. This would turn a previously estimated parameter into a fixed parameter, and this gains us the prize of a lovely degree of freedom!\nBy removing a path altogether, we are constraining it to be zero.\n\n\n\n\n\n\n\n\n\n\nOPTIONAL: multiple regression model is just-identified\n\n\n\n\n\nIf I have two predictors and one outcome variable, then there are 6 variances and covariances available. For instance:\n\ncov(somedata)\n\n         y     x1     x2\ny  45.6481 0.0149 1.3525\nx1  0.0149 1.0455 0.0196\nx2  1.3525 0.0196 1.0000\n\n\nThe multiple regression model will estimate the two variances of the exogenous variables (the predictors), their covariance, the two paths from each exogenous to the endogenous (each predictor to the outcome), and the error variance. This makes up 6 estimated parameters - which is exactly how many known covariances there are.\nCount the number of arrows (both single and double headed) in the diagram:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 3: Model Estimation\n\n\n\n\n\nEstimating the model is relatively straightforward. We pass the formula we have written to the sem() function, along with the data set in which we want it to look for the variables:\n\nmymodel.fit &lt;- sem(mymodel, data = jobsatpath)\n\nWe can then examine the parameter estimates:\n\nsummary(mymodel.fit)\n\nlavaan 0.6.15 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                            50\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.050\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.080\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  jobsat ~                                            \n    age              -1.566    0.482   -3.250    0.001\n    autonomy          0.347    0.131    2.653    0.008\n    income            0.689    0.439    1.572    0.116\n  income ~                                            \n    autonomy          0.099    0.026    3.796    0.000\n    age               0.631    0.081    7.834    0.000\n    parentincome      1.036    0.128    8.099    0.000\n  autonomy ~                                          \n    age               0.851    0.418    2.038    0.042\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .jobsat          251.098   50.220    5.000    0.000\n   .income           11.293    2.259    5.000    0.000\n   .autonomy        330.771   66.154    5.000    0.000\n\n\nWe can now, to “visualise” our model, add the estimates to the diagram:\n\n\n\n\n\nModel estimates\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 4: Model Fit\n\n\n\n\n\nTODO\nThere are too many different indices of model fit for these types of models, and there’s lots of controversy over the various merits and disadvantages and proposed cutoffs of each method. We will return to this more in coming weeks, and the lecture this week contains information on some of them. The important thing to remember: “model fit” and “degrees of freedom” have quite different meanings to those you are likely used to.\nThe simplest metric of fit is a chi-square value that we can compute that reflects reflects the discrepancy between the model-implied covariance matrix and the observed covariance matrix. We can then calculate a p-value for this chi-square statistic by using the chi-square distribution with the degrees of freedom equivalent to that of the model.\nIf we denote the sample covariance matrix as \\(S\\) and the model-implied covariance matrix as \\(\\Sigma(\\theta)\\), then we can think of the null hypothesis here as \\(H_0: S - \\Sigma(\\hat\\theta) = 0\\). In this way our null hypothesis is sort of like saying that our theoretical model is correct (and can therefore perfectly reproduce the covariance matrix).\n\n\n\n\nExercises: Burnout\n\nData: Passion & Burnout\nResearchers are interested in the role of obsessive and harmonious passion in psychological wellbeing. The researchers collect data from 100 participants. Participants respond on sliding scales (0-100) for five measures:\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nworksat\nWork Satisfaction: On a scale of 0-100, how satisfied are you with your work?\n\n\nhp\nHarmonious Passion: On a scale of 0-100, how much do you feel that you freely choose to engage in work outside of working hours?\n\n\nop\nObsessive Passion: On a scale of 0-100, how much do you have uncontrollable urges to work outside of working hours?\n\n\nconflict\nWork Conflict: On a scale of 0-100, how much conflict do you experience in your work?\n\n\nburnout\nWork Burnout: On a scale of 0-100, how close to burnout at work are you?\n\n\n\n\n\n\n\nThe data is available at https://uoepsy.github.io/data/passionpath.csv\n\n\nQuestion 1\n\n\nLoad in the libraries we will use in these exercises:\n\ntidyverse\n\nlavaan\n\nsemPlot\n\nRead in the data.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nburnout &lt;- read_csv(\"https://uoepsy.github.io/data/passionpath.csv\")\n\n\n\n\n\nQuestion 2\n\n\nThe researchers have this theoretical model:\n\n\n\n\n\nFigure 8: Burnout Theory\n\n\n\n\nSpecify this model and store the formula as an object in R\n\n\n\n\n\nSolution\n\n\n\n\nburnoutmod &lt;- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\n\"\n\n\n\n\n\nQuestion 3\n\n\nFit the model to the data using the sem() function.\n\n\n\n\n\nSolution\n\n\n\n\nburnoutfit &lt;- sem(burnoutmod, data=burnout)\n\n\n\n\n\nQuestion 4\n\n\nExamine the parameter estimates\n\n\n\n\n\nSolution\n\n\n\n\nsummary(burnoutfit)\n\nlavaan 0.6.15 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.458\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.243\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  worksat ~                                           \n    hp                0.420    0.088    4.769    0.000\n  burnout ~                                           \n    worksat          -0.207    0.063   -3.304    0.001\n    conflict         -0.147    0.191   -0.772    0.440\n  conflict ~                                          \n    op                0.095    0.038    2.476    0.013\n    hp               -0.010    0.033   -0.308    0.758\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  hp ~~                                               \n    op               61.628   18.529    3.326    0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .worksat         155.762   22.028    7.071    0.000\n   .burnout          75.347   10.656    7.071    0.000\n   .conflict         19.465    2.753    7.071    0.000\n    hp              200.614   28.371    7.071    0.000\n    op              152.210   21.526    7.071    0.000\n\n\n\n\n\n\nQuestion 5\n\n\nProduce a diagram with the estimates on the paths. Can you also produce one which has the standardised estimates?\nTake a look at the help function for semPaths(). What do the arguments what and whatLabels do?\n\n\n\n\n\nSolution\n\n\n\nwhat will weight and colour the paths according something like the estimates. whatLabels will provide labels for the paths:\n\nsemPaths(burnoutfit, whatLabels = \"est\")\n\n\n\n\n\n\n\n\nThis will change them to the standardised estimates:\n\nsemPaths(burnoutfit, what = \"std\", whatLabels = \"std\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nHow many variables do you have in your model?\nHow many knowns are there in the covariance matrix?\n\nHow many unknowns are there in your model?\nHow many degrees of freedom do you therefore have?\n\n\n\n\n\n\nSolution\n\n\n\n\nHow many variables do you have in your model?\n5\nHow many knowns are there in the covariance matrix? \\(\\frac{5 \\times (5 + 1)}{2} = 15\\)\nHow many unknowns are there in your model?\nThere are 6 paths in Figure 8, but we also need to consider the variances of the 5 variables, so we have 11 things being estimated\nHow many degrees of freedom do you therefore have? 15 - 11 = 4\n\n\n\n\n\nQuestion 7\n\n\nTake a look at the summary of the model you fitted. Specifically, examine the bit near the top where it mentions the \\(\\chi^2\\) statistic.\nIs it significant? What do we conclude?\n\n\n\n\n\nSolution\n\n\n\nThe \\(\\chi^2\\) statistic is not significant:\n\npchisq(5.458, df=4, lower.tail=F)\n\n[1] 0.243\n\n\nWe therefore have no evidence to support rejecting our null hypothesis that our model provides a reasonable fit to the data.\n\n\n\n\nQuestion 8\n\n\nTry examing what the other fit measures (RMSEA, SRMR, CLI, TLI: How do they compare with the cutoffs provided in the lecture?\nhint: summary(modelfit, fit.measures = TRUE)\n\n\n\n\n\nSolution\n\n\n\nThe fit statistics for our model:\nComparative Fit Index (CFI) = 0.968 Tucker-Lewis Index (TLI) = 0.921 RMSEA = 0.060 SRMR = 0.063\nOnly CFI meets the criteria given in the lecture slides for “considered good”.\n\nsummary(burnoutfit, fit.measures = TRUE)\n\nlavaan 0.6.15 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.458\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.243\n\nModel Test Baseline Model:\n\n  Test statistic                                56.068\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.968\n  Tucker-Lewis Index (TLI)                       0.921\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1836.113\n  Loglikelihood unrestricted model (H1)      -1833.384\n                                                      \n  Akaike (AIC)                                3694.226\n  Bayesian (BIC)                              3722.883\n  Sample-size adjusted Bayesian (SABIC)       3688.142\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.060\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.172\n  P-value H_0: RMSEA &lt;= 0.050                    0.361\n  P-value H_0: RMSEA &gt;= 0.080                    0.475\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.063\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  worksat ~                                           \n    hp                0.420    0.088    4.769    0.000\n  burnout ~                                           \n    worksat          -0.207    0.063   -3.304    0.001\n    conflict         -0.147    0.191   -0.772    0.440\n  conflict ~                                          \n    op                0.095    0.038    2.476    0.013\n    hp               -0.010    0.033   -0.308    0.758\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  hp ~~                                               \n    op               61.628   18.529    3.326    0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .worksat         155.762   22.028    7.071    0.000\n   .burnout          75.347   10.656    7.071    0.000\n   .conflict         19.465    2.753    7.071    0.000\n    hp              200.614   28.371    7.071    0.000\n    op              152.210   21.526    7.071    0.000\n\n\n\n\n\n\nQuestion Extra: modification indices\n\n\nExamine the modification indices of the model (use the modindices() function).\nPay close attention to the mi column (this is the “modification index”, which is the change in the \\(\\chi^2\\) statistic). The other interesting column is going to be the sepc.all column, which is the estimated parameter value of the proposed path, in a model where all the variables are standardised. This means we can evaluate whether the estimated parameter is relatively small/moderate/large, because these are all standardised correlations between -1 and 1!\nAre there any paths which the modification indices suggest might improve the model? Do they make theoretical sense to include them?\n\n\n\n\n\nSolution\n\n\n\n\nmodindices(burnoutfit)\n\n        lhs op      rhs    mi    epc sepc.lv sepc.all sepc.nox\n12  worksat  ~  burnout 0.095 -0.101  -0.101   -0.067   -0.067\n13  worksat  ~ conflict 1.306 -0.314  -0.314   -0.103   -0.103\n14  worksat  ~       op 1.102 -0.113  -0.113   -0.101   -0.101\n15  burnout  ~       hp 0.268  0.035   0.035    0.054    0.054\n16  burnout  ~       op 3.471  0.137   0.137    0.184    0.184\n17 conflict  ~  worksat 0.842 -0.032  -0.032   -0.099   -0.099\n18 conflict  ~  burnout 0.225 -0.062  -0.062   -0.126   -0.126\n19       hp  ~  worksat 1.102  0.316   0.316    0.309    0.309\n20       hp  ~  burnout 0.096 -0.051  -0.051   -0.033   -0.033\n23       op  ~  worksat 1.102 -0.097  -0.097   -0.109   -0.109\n24       op  ~  burnout 4.058  0.264   0.264    0.196    0.196\n\n\nThere seems to be a suggested reasonably large correlation between burnout and conflict. If we were to fit this model, our fit indices may well improve and meet our cut-offs. But this may well just be overfitting.\n\nburnoutmod2 &lt;- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\nburnout ~~ conflict\n\"\nsem(burnoutmod2, data=burnout)\n\nWe will not start adjusting models based on modification indices today (or indeed in this course at all). As a general rule for dapR3 course, we want you to specify and test a specific model, and not seek to use exploratory modifications."
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n\nSolution\n\n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "example_00_anova.html",
    "href": "example_00_anova.html",
    "title": "Analysis Example: Rpt & Mixed ANOVA",
    "section": "",
    "text": "This is optional for the DAPR3 course, but may be useful for your dissertations should your field/supervisor prefer the ANOVA framework to that of the linear model.\nThis walks briefly through these models with the ez package. There are many other packages available, and many good tutorials online should you desire extra resources in the future:\n\nhttps://www.datanovia.com/en/lessons/repeated-measures-anova-in-r\nhttps://www.r-bloggers.com/2021/04/repeated-measures-of-anova-in-r-complete-tutorial/\nhttps://stats.idre.ucla.edu/r/seminars/repeated-measures-analysis-with-r/\nhttps://www.datanovia.com/en/lessons/mixed-anova-in-r/\n\n\n\nData: Audio interference in executive functioning\nThis data is from a simulated study that aims to investigate the following research questions:\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n\n24 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room.\nThe data is in stored in two separate files - the research administering the tests recorded the SDMT score in one spreadsheet, while details of the audio used in the experiment are held in a separate sheet\n\nInformation about the audio condition for each trial of each participant is stored in .csv format at https://uoepsy.github.io/data/ef_music.csv. The data is in long format (1 row per participant-trial).\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\ntrial_n\nTrial Number (1-15)\n\n\naudio\nAudio heard during the test (‘no_audio’, ‘white_noise’,‘music’)\n\n\nheadphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n\n\n\n\n\n\nInformation on participants’ Symbol Digit Modalities Test (SDMT) for each trial is stored in .xlsx format at https://uoepsy.github.io/data/ef_sdmt.xlsx. The data is in wide format (1 row per participant, 1 column per trial).\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\nTrial_01\nSDMT score in trial 1\n\n\nTrial_02\nSDMT score in trial 2\n\n\nTrial_03\nSDMT score in trial 3\n\n\n…\nSDMT score in trial …\n\n\n…\nSDMT score in trial …\n\n\nTrial_15\nSDMT score in trial 15\n\n\n\n\n\n\nThe code below will read in both datasets and join them for you:\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\ndownload.file(url = \"https://uoepsy.github.io/data/ef_sdmt.xlsx\",\n              destfile = \"ef_sdmt.xlsx\",\n              mode = \"wb\")\nefdata &lt;- \n  left_join(\n    read_csv(\"https://uoepsy.github.io/data/ef_music.csv\"),\n    read_xlsx(\"ef_sdmt.xlsx\") %&gt;%\n      pivot_longer(Trial_01:Trial_15, names_to = \"trial_n\", values_to = \"SDMT\")\n  )\n\n\n\nOne-Way Repeated Measures ANOVA\nFor a repeated measures ANOVA, we have one independent variable that is within group.\nThis would be appropriate if our research question were the following:\n\nHow do different types of audio interfere with executive functioning?\n\nMapping this to the variables in our dataset, our model is going to be SDMT ~ audio, and we want to account for PID differences. So for now we will ignore the headphones variable.\n\n\nCode\nhead(efdata)\n\n\n# A tibble: 6 × 5\n  PID    trial_n  audio       headphones  SDMT\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 PPT_01 Trial_02 no_audio    speakers      31\n2 PPT_01 Trial_08 no_audio    speakers      23\n3 PPT_01 Trial_11 no_audio    speakers      23\n4 PPT_01 Trial_13 no_audio    speakers      24\n5 PPT_01 Trial_15 no_audio    speakers      34\n6 PPT_01 Trial_01 white_noise speakers      38\n\n\nThe easiest way to conduct a repeated measures ANOVA in R is to use the ez package, which comes with some handy functions to visualise the experimental design.\nWe can see from below that every participant completed 5 trials for each type of audio interference:\n\n\nCode\nlibrary(ez)\nezDesign(data = efdata, x = audio, y = PID)\n\n\n\n\n\n\n\n\n\nThe ezANOVA() function takes a few arguments.\nThe ones you will need for this are:\n\ndata the name of the dataframe\ndv the column name for the dependent variable\nwid the column name for the participant id variable\nwithin the column name(s) for the predictor variable(s) that vary within participants\nbetween the column name(s) for any predictor variable(s) that vary between participants\n\nFit a repeated measures ANOVA to examine the effect of the audio type on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio)\n\n\n$ANOVA\n  Effect DFn DFd        F            p p&lt;.05       ges\n2  audio   2  46 44.69618 1.647271e-11     * 0.2534633\n\n$`Mauchly's Test for Sphericity`\n  Effect         W          p p&lt;.05\n2  audio 0.8105961 0.09927715      \n\n$`Sphericity Corrections`\n  Effect       GGe      p[GG] p[GG]&lt;.05      HFe        p[HF] p[HF]&lt;.05\n2  audio 0.8407573 5.0677e-10         * 0.899603 1.427119e-10         *\n\n\n\n\nMixed ANOVA\nMixed ANOVA can be used to investigate effects of independent variables that are at two different levels, i.e. some are within clusters and some are between.\n\nDoes the effect of audio interference on executive functioning differ depending upon whether or not noise-cancelling headphones are used?\n\nLook at the two lines below. Can you work out what the plots will look like before you run them?\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\nezDesign(data = efdata, x = headphones, y = audio)\n\n\nParticipants 1-20 are in one condition, and 21-40 are in another.\nThis should look like a two big blocks on the diagonal.\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\n\n\n\n\n\n\n\n\n\nIn each condition, all different types of audio were observed in the same number of trials. This should be a full grid:\n\n\nCode\nezDesign(data = efdata, x = headphones, y = audio)\n\n\n\n\n\n\n\n\n\nFit a mixed ANOVA to examine the interaction between audio and headphone use on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio, between = headphones)\n\n\n$ANOVA\n            Effect DFn DFd         F            p p&lt;.05        ges\n2       headphones   1  22  9.815545 4.836945e-03     * 0.26784992\n3            audio   2  44 59.615596 2.980503e-13     * 0.32788320\n4 headphones:audio   2  44  8.677316 6.657590e-04     * 0.06629911\n\n$`Mauchly's Test for Sphericity`\n            Effect         W         p p&lt;.05\n3            audio 0.9422531 0.5355001      \n4 headphones:audio 0.9422531 0.5355001      \n\n$`Sphericity Corrections`\n            Effect       GGe        p[GG] p[GG]&lt;.05      HFe        p[HF]\n3            audio 0.9454057 1.196469e-12         * 1.031585 2.980503e-13\n4 headphones:audio 0.9454057 8.648057e-04         * 1.031585 6.657590e-04\n  p[HF]&lt;.05\n3         *\n4         *\n\n\nThe ez package also contains some easy plotting functions for factorial experiments, such as ezPlot(). It takes similar arguments to the ezANOVA() function.\n\nlook up the help documentation for ezPlot().\nlet’s use ezPlot() to make a nice plot\n\n\n\nCode\nezPlot(data = efdata, dv = SDMT, \n       wid = PID, within = audio, between = headphones,\n       x = audio, split = headphones)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same thing in lmer\n\n\n\n\n\n\n\nCode\nlibrary(lme4)\nlibrary(lmerTest)\nmod &lt;- lmer(SDMT ~ 1 + headphones * audio + (1 + audio | PID), \n            data = efdata)\nanova(mod, type=\"III\")\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                 Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nheadphones        325.0  325.04     1    22  9.8155  0.004837 ** \naudio            3212.0 1606.01     2    22 48.4976 8.626e-09 ***\nheadphones:audio  490.1  245.06     2    22  7.4001  0.003486 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nlibrary(sjPlot)\nplot_model(mod, type=\"eff\", terms=c(\"audio\",\"headphones\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DAPR3",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 3 (DAPR3) lab workbook. Using the menu above, you can find lab materials for each week. These include sets of exercises along with walkthrough readings in which we introduce some of the more important R code. It is strongly recommended that students have taken Data Analysis for Psychology in R 1 and 2 (DAPR1 & DAPR2)."
  },
  {
    "objectID": "index.html#solutions",
    "href": "index.html#solutions",
    "title": "Welcome to DAPR3",
    "section": "Solutions",
    "text": "Solutions\nSolutions will be made available immediately below each exercise, in the week after they are set, so make sure to re-visit the labs to check your answers."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Welcome to DAPR3",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Welcome to DAPR3",
    "section": "Tips on googling statistics and R",
    "text": "Tips on googling statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Welcome to DAPR3",
    "section": "Feedback on labs",
    "text": "Feedback on labs\nIf you wish to make suggestions for improvements to these workbooks, please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  }
]