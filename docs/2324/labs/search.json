[
  {
    "objectID": "01_regressionrefresh.html",
    "href": "01_regressionrefresh.html",
    "title": "1. Regression Refresh | Clustered Data",
    "section": "",
    "text": "Preliminaries\n\nOpen Rstudio!\n\nCreate a new RMarkdown document or R script (whichever you like) for this week.\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already.\n\n\ntidyverse : for organising data\npatchwork: for organising plots\nICC : for quickly calculating intraclass correlation coefficient\nlme4 : for fitting generalised linear mixed effects models\nparameters : inference!\npbkrTest : more inference!\nHLMdiag : for examining case diagnostics at multiple levels\nlmeresampler : for bootstrapping!\neffects : for tables/plots\nsjPlot : for tables/plots\nbroom.mixed : tidying methods for mixed models\n\nYou can install all of these at once using:\n\ninstall.packages(c(\"tidyverse\",\"ICC\",\"lme4\",\"parameters\",\"pbkrTest\",\"effects\",\"broom.mixed\",\"sjPlot\",\"HLMdiag\"))\n# the lmeresampler package has had some recent updates. better to install the most recent version:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"aloy/lmeresampler\")"
  },
  {
    "objectID": "01_regressionrefresh.html#footnotes",
    "href": "01_regressionrefresh.html#footnotes",
    "title": "1. Regression Refresh | Clustered Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nANOVA is just a special case of the linear model↩︎"
  },
  {
    "objectID": "02_intromlm.html",
    "href": "02_intromlm.html",
    "title": "2. Multilevel Models",
    "section": "",
    "text": "A Note on terminology\n\n\n\n\n\nThe methods we’re going to learn about in the first five weeks of this course are known by lots of different names: “multilevel models”; “hierarchical linear models”; “mixed-effect models”; “mixed models”; “nested data models”; “random coefficient models”; “random-effects models”; “random parameter models”… and so on).\nWhat the idea boils down to is that model parameters vary at more than one level. This week, we’re going to explore what that means.\nThroughout this course, we will tend to use the terms “mixed effect model”, “linear mixed model (LMM)” and “multilevel model (MLM)” interchangeably."
  },
  {
    "objectID": "02_intromlm.html#footnotes",
    "href": "02_intromlm.html#footnotes",
    "title": "2. Multilevel Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“(g)lmer” here stands for “(generalised) linear mixed effects regression”.↩︎"
  },
  {
    "objectID": "03_assumptranef.html",
    "href": "03_assumptranef.html",
    "title": "3. Assumptions and Diagnostics | Random Effect Structures",
    "section": "",
    "text": "Nested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results."
  },
  {
    "objectID": "03_assumptranef.html#footnotes",
    "href": "03_assumptranef.html#footnotes",
    "title": "3. Assumptions and Diagnostics | Random Effect Structures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s always going to be debateable about what is ‘too high’ because in certain situations you might expect correlations close to 1. It’s best to think through whether it is a feasible value given the study itself↩︎"
  },
  {
    "objectID": "04_centerglmer.html",
    "href": "04_centerglmer.html",
    "title": "4. Centering in MLM | Logistic MLM",
    "section": "",
    "text": "Centering & Scaling in LM\n\n\n\n\n\nWe have some data from a study investigating how perceived persuasiveness of a speaker is influenced by the rate at which they speak.\n\ndap2 &lt;- read_csv(\"https://uoepsy.github.io/data/dapr2_2122_report1.csv\")\n\nWe can fit a simple linear regression (one predictor) to evaluate how speech rate (variable sp_rate in the dataset) influences perceived persuasiveness (variable persuasive in the dataset). There are various ways in which we can transform the predictor variable sp_rate, which in turn can alter the interpretation of some of our estimates:\n\n\nRaw X\n\nm1 &lt;- lm(persuasive ~ sp_rate, data = dap2)\nsummary(m1)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 55.532060  6.4016670  8.674625 6.848945e-15\nsp_rate     -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\nThe intercept and the coefficient for neuroticism are interpreted as:\n\n(Intercept): A audio clip of someone speaking at zero phones per second is estimated as having an average persuasive rating of 55.53.\n\nsp_rate: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nMean-Centered X\nWe can mean center our predictor and fit the model again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_mc = sp_rate - mean(sp_rate))\nm2 &lt;- lm(persuasive ~ sp_rate_mc, data = dap2)\nsummary(m2)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667  1.3519418 39.110165 6.429541e-80\nsp_rate_mc  -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_mc: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nStandardised X\nWe can standardise our predictor and fit the model yet again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_z = scale(sp_rate))\nm3 &lt;- lm(persuasive ~ sp_rate_z, data = dap2)\nsummary(m3)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667   1.351942 39.110165 6.429541e-80\nsp_rate_z   -0.576077   1.356471 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_z: For every increase of one standard deviation in phones per second, perceived persuasiveness is estimated to decrease by -0.58.\n\nRemember that the scale(sp_rate) is subtracting the mean from each value, then dividing those by the standard deviation. The standard deviation of dap2$sp_rate is:\n\nsd(dap2$sp_rate)\n\n[1] 3.016315\n\n\nso in our variable dap2$sp_rate_z, a change of 3.02 gets scaled to be a change of 1 (because we are dividing by sd(dap2$sp_rate)).\n\ncoef(m1)[2] * sd(dap2$sp_rate)\n\n  sp_rate \n-0.576077 \n\ncoef(m3)[2]\n\nsp_rate_z \n-0.576077 \n\n\n\n\nNote that these models are identical. When we conduct a model comparison between the 3 models, the residual sums of squares is identical for all models:\n\nanova(m1,m2,m3)\n\nAnalysis of Variance Table\n\nModel 1: persuasive ~ sp_rate\nModel 2: persuasive ~ sp_rate_mc\nModel 3: persuasive ~ sp_rate_z\n  Res.Df   RSS Df Sum of Sq F Pr(&gt;F)\n1    148 40576                      \n2    148 40576  0         0         \n3    148 40576  0         0         \n\n\nWhat changes when you center or scale a predictor in a standard regression model (one fitted with lm())?\n\nThe variance explained by the predictor remains exactly the same\nThe intercept will change to be the estimated mean outcome where that predictor is “0”. Scaling and centering changes what “0” represents, thereby changing this estimate (the significance test will therefore also change because the intercept now has a different meaning)\nThe slope of the predictor will change according to any scaling (e.g. if you divide your predictor by 10, the slope will multiply by 10).\nThe test of the slope of the predictor remains exactly the same.\n\n\n\n\n\n\nExercises: Centering in the MLM\n\nData: Hangry\nThe study is interested in evaluating whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nThe data are available at: https://uoepsy.github.io/data/hangry.csv.\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant\n\n\nfivetwo\nWhether the participant follows the five-two diet\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead carefully the description of the study above, and try to write out (in lmer syntax) an appropriate model to test the research aims.\ne.g.:\noutcome ~ explanatory variables + (???? | grouping)\nTry to think about the maximal random effect structure (i.e. everything that can vary by-grouping is estimated as doing so).\nTo help you think through the steps to get from a description of a research study to a model specification, think about your answers to the following questions.\nQ: What is our outcome variable?\nQ: What are our explanatory variables?\nQ: Is there any grouping (or “clustering”) of our data that we consider to be a random sample? If so, what are the groups?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet.\nWe can split our data in to groups of each participant. We can also split it into groups of each diet. Which of these groups have we randomly sampled? Do we have a random sample of participants? Do we have a random sample of diets? Another way to think of this is “if i repeated the experiment, what these groups be different?”\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nOur outcome is irritability here, because it is the thing that we are trying to explain through peoples’ hunger levels and diets.\n\nlmer(irritability ~  explanatory variables + (???? | grouping))\n\nWe are interested in the effect of hunger on irritability, and whether this effect is different for the five-two diet. So we are interested in the interaction:\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | grouping))\n\n(remember that hunger + diet + hunger:diet is just a more explicit way of writing hunger*diet).\nIf we did this experiment again, would we have different participants?\nYes. If we did this experiment again, would we have different diets? No, because we’re interested in the specific differences between the five-two diet and no dieting. This means we will likely want to by-participant random deviations (e.g. the ( ... | participant) bit in lmer). But we won’t have by-diet random effects (1 | diet) because the diet differences are the specific differences that we wish to test.\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | participant))\n\nThinking about what can be modelled as randomly varying between participants, we have some options:\n\nparticipants vary in how irritable they are on average\n(the intercept, 1 | participant)\nparticipants vary in how much hunger influences their irritability\n(the effect of hunger, hunger | participant)\nparticipants vary in how much diet influences irritability\n(the effect of diet, diet | participant)\nparticipants vary in how much diet effects hunger’s influence on irritability\n(the interaction between diet and hunger, diet:hunger | participant)\n\nWe can vary 1 and 2, but not 3 and 4. This is because each participant is either following the five-two diet or they are not. So for a single participant, we can’t assess “the effect diet has” on anything, because we haven’t seen that participant under different diets. if we try to plot a single participants’ data, we can see that it is impossible for us to assess “the effect of diet”:\n\n\n\n\n\n\n\n\n\nBy contrast, we can vary the intercept and the effect of hunger, because each participant has multiple values of irritability, and multiple different observations of hunger. We can think about a single participant’s “effect of hunger on irritability” and how we might fit a line to their data:\n\n\n\n\n\n\n\n\n\n\nlmer(irritability ~  hunger + diet + hunger:diet + (1 + hunger | participant))\n\n\n\n\n\n\n\n\n\n\nTotal, Within, Between\n\n\n\n\n\nRecall our research aim:\n\n… whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting.\n\nForgetting about any differences due to diet, let’s just think about the relationship between irritability and hunger. How should we interpret this research aim?\nWas it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a composite of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that the ‘total effect’ isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it about how person i changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are.).\n\n\n\n\nQuestion 2\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll find group_by() %&gt;% mutate() very useful here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nhangry &lt;- \n    hangry %&gt;% group_by(ppt) %&gt;%\n        mutate(\n            avg_hunger = mean(q_hunger),\n            hunger_gc = q_hunger - avg_hunger\n        )\nhead(hangry)\n\n# A tibble: 6 × 6\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   fivetwo avg_hunger hunger_gc\n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1             17       30 N1p1  1             26.6     3.4  \n2             19       27 N1p1  1             26.6     0.400\n3             19       29 N1p1  1             26.6     2.4  \n4             20       33 N1p1  1             26.6     6.4  \n5             24       14 N1p1  1             26.6   -12.6  \n6             30       28 N1p2  1             32.6    -4.6  \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people?\n\nDoes it look like when people are more hungry than normal, they are more irritable?\n\n\n\n\n\n\nSolution\n\n\n\nWe might find it easier to look at a plot where each participant is represented as their mean plus an indication of their range of irritability scores:\n\nggplot(hangry,aes(x=avg_hunger,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nThere appears to be a slight positive relationship between a persons’ average hunger and their irritability scores.\nIt is harder to tell what the relationship is between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data:\n\nggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt))+\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2)\n\n\n\n\n\n\n\n\nI think there might be a positive trend in here, in that participants tend to be higher irritability when they are higher (for them) on the hunger score.\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation level deviations from those averages), that represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables as predictors, instead of the raw hunger scores.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nhunger * diet could be replaced by (hunger1 + hunger2) * diet, thereby allowing each aspect of hunger to interact with diet.\nWe can only put one of these variables in the random effects (1 + hunger | participant). Recall that above we discussed how we cannot have (diet | participant), because “an effect of diet” makes no sense for a single participant (they are either on the diet or they are not, so there is no ‘effect’). Similarly, each participant has only one value for their average hungriness.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(lme4)\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                control = lmerControl(optimizer=\"bobyqa\"))\n\n\n\n\n\nQuestion 5\n\n\nHopefully, you have fitted a model similar to the below:\n\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc) * fivetwo + \n                (1 + hunger_gc | ppt), data = hangry,\n            control = lmerControl(optimizer=\"bobyqa\"))\n\nBelow, we have obtained p-values using the Kenward Rogers Approximation of \\(df\\) for the test of whether the fixed effects are zero, so we can see the significance of each estimate.\nProvide an answer for each of these questions:\n\nFor those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\nDo people following no diet tend to be more irritable when they are more hungry than they usually are?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\n(Trickier:) What does the fivetwo coefficient represent?\n\n\n\n\n\n\n\n  \n    \n      Model Summary\n    \n    \n    \n      Parameter\n      Coefficient\n      SE\n      95% CI\n      t\n      df\n      p\n    \n  \n  \n    \n      Fixed Effects \n    \n    (Intercept)\n17.13\n5.21\n(6.75, 27.51)\n3.29\n77.00\n0.002 \n    avg hunger\n3.86e-03\n0.11\n(-0.21, 0.22)\n0.04\n77.00\n0.971 \n    hunger gc\n0.19\n0.08\n(0.03, 0.34)\n2.45\n70.40\n0.017 \n    fivetwo (1)\n-10.85\n6.62\n(-24.03, 2.32)\n-1.64\n77.00\n0.105 \n    avg hunger × fivetwo (1)\n0.47\n0.14\n(0.20, 0.74)\n3.44\n77.00\n&lt; .001\n    hunger gc × fivetwo (1)\n0.38\n0.10\n(0.18, 0.58)\n3.75\n73.64\n&lt; .001\n    \n      Random Effects \n    \n    SD (Intercept: ppt)\n6.93\n\n\n\n\n\n    SD (hunger_gc: ppt)\n0.38\n\n\n\n\n\n    Cor (Intercept~hunger_gc: ppt)\n-0.01\n\n\n\n\n\n    SD (Residual)\n4.83\n\n\n\n\n\n  \n  \n    \n      \n    \n  \n  \n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n1: For those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nA: ‘No diet’ is the reference level of the five-two variable, and because we have an interaction, that means the avg_hunger coefficient will provide the relevant estimate. There is no evidence (\\(p&gt;.05\\)) to suggest that when not dieting, hungrier people are more irritable than less hungry people.\n2: Is there evidence to suggest that this is different for those following the five-two diet? In what way?\nA: This is the interaction between avg_hunger:fivetwo1. We can see that, for every increase of 1 in average hunger, irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet.\nThese units are still in terms of the original scale (i.e. 0 to 100).\n3: Do people following no diet tend to be more irritable when they are more hungry than they usually are? A: This is the estimate for the coefficient of hunger_gc. For people following no diet, there is an estimated 0.19 increase in irritability for every 1 unit more hungry they become.\n4: Is there evidence to suggest that this is different for those following the five-two diet? In what way? A: This effect of a 1 unit change on within-person hunger increasing irritability is increased for those who are following the five-two diet by an additional 0.38\n5: What does the fivetwo1 coefficient represent? A: This represents the group difference of irritability between those on the five-two diet vs those not dieting, for someone who has an average hunger score of 0.\n\n\n\n\nQuestion 6\n\n\nConstruct two plots showing the two model estimated interactions. Think about your answers to the previous question, and check that they match with what you are seeing in the plots (do not underestimate the utility of this activity for helping understanding!).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis isn’t as difficult as it sounds. the sjPlot package can do it in one line of code!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(sjPlot)\nplot_model(hangrywb, type = \"int\")[[1]]\n\n\n\n\n\n\n\n\nWe saw in the model coefficients that for the reference level of fivetwo, the “No Diet” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. We also saw the interaction avg_hunger:fivetwo1 indicates that irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\nplot_model(hangrywb, type = \"int\")[[2]]\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_gc we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re on “No Diet”). This is the slope of the red line. The interaction hunger_gc:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive and significant, which matches with the fact that the blue line is clearly steeper in this plot.\n\n\n\n\nQuestion 7\n\n\nProvide tests or confidence intervals for the parameters of interest, and write-up the results.\n\n\n\n\n\n\nRemember: some options for inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\n\n\n\n\ntests or CIs for model parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\n\n\nmodel comparison(different fixed effects, same random effects)\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(parameters)\nmodel_parameters(hangrywb, ci_method = \"kr\", ci_random = FALSE)\n\n# Fixed Effects\n\nParameter                | Coefficient |   SE |          95% CI |     t |    df |      p\n----------------------------------------------------------------------------------------\n(Intercept)              |       17.13 | 5.21 | [  6.75, 27.51] |  3.29 | 77.00 | 0.002 \navg hunger               |    3.86e-03 | 0.11 | [ -0.21,  0.22] |  0.04 | 77.00 | 0.971 \nhunger gc                |        0.19 | 0.08 | [  0.03,  0.34] |  2.45 | 70.40 | 0.017 \nfivetwo [1]              |      -10.85 | 6.62 | [-24.03,  2.32] | -1.64 | 77.00 | 0.105 \navg hunger × fivetwo [1] |        0.47 | 0.14 | [  0.20,  0.74] |  3.44 | 77.00 | &lt; .001\nhunger gc × fivetwo [1]  |        0.38 | 0.10 | [  0.18,  0.58] |  3.75 | 73.64 | &lt; .001\n\n# Random Effects\n\nParameter                      | Coefficient\n--------------------------------------------\nSD (Intercept: ppt)            |        6.93\nSD (hunger_gc: ppt)            |        0.38\nCor (Intercept~hunger_gc: ppt) |       -0.01\nSD (Residual)                  |        4.83\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on a restricted diet such as the five-two, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes of relative-hunger level were included for participants. The model was fitting with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser from the lme4. \\(P\\)-values were obtained using Wald tests with Kenward-Roger approximation of denominator degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(\\beta = 0.19,\\ SE = 0.08,\\ t(2.45) = 70.4,\\ p=0.017\\)), and that this was increased for those following the five-two diet (\\(\\beta = 0.38,\\ SE = 0.1,\\ t(3.75) = 73.64,\\ p&lt;0.001\\)). Although for those not on a specific diet there was no evidence for an association between irritability and being generally a more hungry person (\\(p=0.971\\)), there a significant interaction was found between average hunger and being on the five-two diet (\\(\\beta = 0.47,\\ SE = 0.14,\\ t(3.44) = 77,\\ p&lt;0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association of both between-person hunger and within-person hunger with irritability.\n\n\n\n\nOther within-group transformations\nAs well as within-group mean centering a predictor (like we have done above), we can within-group standardise a predictor. This would disagregate within and between effects, but interpretation would of the within effect would be the estimated change in \\(y\\) associated with being 1 standard deviation higher in \\(x\\) for that group.\n\n\n\n\nPractice Datasets Weeks 4 and 5\nBelow are various datasets on which you can try out your new-found modelling skills. Read the descriptions carefully, keeping in mind the explanation of how the data is collected and the research question that motivates the study design.\n\n\n\n\n\n\nPractice 1: Music and Driving\n\n\n\n\n\nThese data are simulated to represent data from a fake experiment, in which participants were asked to drive around a route in a 30mph zone. Each participant completed the route 3 times (i.e. “repeated measures”), but each time they were listening to different audio (either speech, classical music or rap music). Their average speed across the route was recorded. This is a fairly simple design, that we might use to ask “how is the type of audio being listened to associated with driving speeds?”\nThe data are available at https://uoepsy.github.io/data/drivingmusicwithin.csv.\n\n\n\n\n\n\n\n\n\nPractice 2: CBT and Stress\n\n\n\n\n\nThese data are simulated to represent data from 50 participants, each measured at 3 different time-points (pre, during, and post) on a measure of stress. Participants were randomly allocated such that half received some cognitive behavioural therapy (CBT) treatment, and half did not. This study is interested in assessing whether the two groups (control vs treatment) differ in how stress changes across the 3 time points.\nThe data are available at https://uoepsy.github.io/data/stressint.csv.\n\n\n\n\n\n\n\n\n\nPractice 3: Erm.. I don’t believe you\n\n\n\n\n\nThese data are simulated to represent data from 30 participants who took part in an experiment designed to investigate whether fluency of speech influences how believable an utterance is perceived to be.\nEach participant listened to the same 20 statements, with 10 being presented in fluent speech, and 10 being presented with a disfluency (an “erm, …”). Fluency of the statements was counterbalanced such that 15 participants heard statements 1 to 10 as fluent and 11 to 20 as disfluent, and the remaining 15 participants heard statements 1 to 10 as disfluent, and 11 to 20 as fluent. The order of the statements presented to each participant was random.\nThe data are available at https://uoepsy.github.io/data/erm_belief.csv.\n\n\n\n\n\n\n\n\n\nPractice 4: Cognitive Aging\n\n\n\n\n\nThese data are simulated to represent a large scale international study of cognitive aging, for which data from 17 research centers has been combined. The study team are interested in whether different cognitive domains have different trajectories as people age. Do all cognitive domains decline at the same rate? Do some decline more steeply, and some less? The literature suggests that scores on cognitive ability are predicted by educational attainment, so they would like to control for this.\nEach of the 17 research centers recruited a minimum of 14 participants (Median = 21, Range 14-29) at age 45, and recorded their level of education (in years). Participants were then tested on 5 cognitive domains: processing speed, spatial visualisation, memory, reasoning, and vocabulary. Participants were contacted for follow-up on a further 9 occasions (resulting in 10 datapoints for each participant), and at every follow-up they were tested on the same 5 cognitive domains. Follow-ups were on average 3 years apart (Mean = 3, SD = 0.8).\nThe data are available at https://uoepsy.github.io/data/cogdecline.csv.\n\n\n\n\n\n\nOptional Exercises: Logistic MLM\n\nDon’t forget to look back at other materials!\nBack in DAPR2, we introduced logistic regression in semester 2, week 8. The lab contained some simulated data based on a hypothetical study about inattentional blindness. That content will provide a lot of the groundwork for this week, so we recommend revisiting it if you feel like it might be useful.\n\n\n\n\n\n\n\nFrom lmer() to glmer()\n\n\n\n\n\nRemember how we simply used glm() and could specify the family = \"binomial\" in order to fit a logistic regression? Well it’s much the same thing for multi-level models!\n\nGaussian model: lmer(y ~ x1 + x2 + (1 | g), data = data)\n\nBinomial model: glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial(link='logit'))\n\nor just glmer(y ~ x1 + x2 + (1 | g), data = data, family = \"binomial\")\nor glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData: Memory Recall & Finger Tapping\n\nResearch Question: After accounting for effects of sentence length, does the rhythmic tapping of fingers aid memory recall?\n\nResearchers recruited 40 participants. Each participant was tasked with studying and then recalling 10 randomly generated sentences between 1 and 14 words long. For 5 of these sentences, participants were asked to tap their fingers along with speaking the sentence in both the study period and in the recall period. For the remaining 5 sentences, participants were asked to sit still.\nThe data are available at https://uoepsy.github.io/data/memorytap.csv, and contains information on the length (in words) of each sentence, the condition (static vs tapping) under which it was studied and recalled, and whether the participant was correct in recalling it.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier (n=40)\n    slength\nNumber of words in sentence\n    condition\nCondition under which sentence is studied and recalled ('static' = sitting still, 'tap' = tapping fingers along to sentence)\n    correct\nWhether or not the sentence was correctly recalled\n  \n  \n  \n\n\n\n\n\n\nQuestion Optional\n\n\n\nResearch Question: After accounting for effects of sentence length, does the rhythmic tapping of fingers aid memory recall?\n\nFit an appropriate model to answer the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nour outcome is conceptually ‘memory recall’, and it’s been measured by “Whether or not a sentence was correctly recalled”. This is a binary variable.\n\nwe have multiple observations for each ?????\nThis will define our (  | ??? ) bit\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmemtap &lt;- read_csv(\"https://uoepsy.github.io/data/memorytap.csv\")\n\nWhen we fit the maximal model, note that we obtain a singular fit. The variance of the slength effect between participants is quite small relative to the others, and there is a correlation between it and the random intercepts.\n\ntapmod &lt;- glmer(correct ~ 1 + slength + condition + \n                  (1 + slength + condition | ppt),\n      data = memtap,\n      family = binomial)\nisSingular(tapmod)\n\n[1] TRUE\n\nVarCorr(tapmod)\n\n Groups Name         Std.Dev. Corr         \n ppt    (Intercept)  1.032849              \n        slength      0.070307 -1.000       \n        conditiontap 0.665626  0.590 -0.590\n\n\nlet’s remove the random effect of slength | ppt.\n\ntapmod2 &lt;- glmer(correct ~ 1 + slength + condition + \n                  (1 + condition | ppt),\n      data = memtap,\n      family = binomial)\n\nthe model now looks a bit better (not a singular fit):\n\nsummary(tapmod2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: correct ~ 1 + slength + condition + (1 + condition | ppt)\n   Data: memtap\n\n     AIC      BIC   logLik deviance df.resid \n   537.6    561.5   -262.8    525.6      394 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8949 -0.8955  0.4483  0.7962  1.6862 \n\nRandom effects:\n Groups Name         Variance Std.Dev. Corr\n ppt    (Intercept)  0.2755   0.5249       \n        conditiontap 0.4207   0.6486   0.66\nNumber of obs: 400, groups:  ppt, 40\n\nFixed effects:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   0.76140    0.37077   2.054   0.0400 *\nslength      -0.12086    0.04721  -2.560   0.0105 *\nconditiontap  0.50945    0.24317   2.095   0.0362 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) slngth\nslength     -0.890       \nconditiontp -0.154 -0.029\n\n\n\n\n\n\n\n\n\n\n\nFrom Log odds to odds ratios\n\n\n\n\n\nTake some time to remind yourself from DAPR2 of the interpretation of logistic regression coefficients.\nIn family = binomial(link='logit'), we are modelling the log-odds. We can obtain estimates on this scale using:\n\nfixef(model)\nsummary(model)$coefficients\ntidy(model) from broom.mixed\n\n(there are probably more ways, but I can’t think of them right now!)\n\nWe can use exp(), to get these back into odds and odds ratios.\n\n\n\n\nQuestion Optional\n\n\nInterpret each of the fixed effect estimates from your model.\n\n\n\n\n\nSolution\n\n\n\n\nfixef(tapmod2)\n\n (Intercept)      slength conditiontap \n   0.7613976   -0.1208591    0.5094460 \n\nexp(fixef(tapmod2))\n\n (Intercept)      slength conditiontap \n   2.1412669    0.8861589    1.6643689 \n\n\n\n(Intercept): For an sentence with zero words, when sitting statically, the odds of correctly recalling the sentence are 2.14. This is equivalent to a \\(\\frac{2.14}{1 + 2.14} = 0.6815287\\) probability of getting it correct.\n\nslength: After accounting for differences due to tapping/not-tapping during study & recall, for every 1 word longer a sentence is, the odds of correctly recalling the sentence is decreased by 0.89.\nconditiontap: After accounting for differences in recall due to sentence length, finger tapping during the study and recall of sentences was associated with 1.66 increased odds correct recall in comparison to sitting still.\n\n\n\n\n\nQuestion Optional\n\n\nChecking the assumptions in non-gaussian models in general (i.e. those where we set the family to some other error distribution) can be a bit tricky, and this is especially true for multilevel models.\nFor the logistic MLM, the standard assumptions of normality etc for our Level 1 residuals residuals(model) do not hold. However, it is still useful to quickly plot the residuals and check that \\(|residuals|\\leq 2\\) (or \\(|residuals|\\leq 3\\) if you’re more relaxed). We don’t need to worry too much about the pattern though.\nWhile we’re more relaxed about Level 1 residuals, we do still want our random effects ranef(model) to look fairly normally distributed.\n\nPlot the level 1 residuals and check whether any are greater than 3 in magnitude\nPlot the random effects (the level 2 residuals) and assess the normality.\n\n\n\n\n\n\nSolution\n\n\n\n\nplot(tapmod2)\n\n\n\n\n\n\n\nsum(abs(resid(tapmod2))&gt;3)\n\n[1] 0\n\n\nAll residuals are between -3 and 3.\nThe random effects look okay here. Not perfect, but bear in mind we have only 40 participants.\n\nqqnorm(ranef(tapmod2)$ppt[, 1], main = \"Random intercept\")\nqqline(ranef(tapmod2)$ppt[, 1])\nqqnorm(ranef(tapmod2)$ppt[, 2], main = \"Random slope of condition\")\nqqline(ranef(tapmod2)$ppt[, 2])\nhist(ranef(tapmod2)$ppt[, 1])\nhist(ranef(tapmod2)$ppt[, 2])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor beyond DAPR3\n\n\n\n\n\n\nThe HLMdiag package doesn’t support diagnosing influential points/clusters for glmer, but there is a package called influence.me which might help: https://journal.r-project.org/archive/2012/RJ-2012-011/RJ-2012-011.pdf\nThere are packages which aim to create more interpretable residual plots for these models via simulation, such as the DHARMa package: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html"
  },
  {
    "objectID": "05_recap.html",
    "href": "05_recap.html",
    "title": "5. Recap",
    "section": "",
    "text": "Flashcards: lm to lmer\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\n\nFormula:\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustered (multi-level) data\n\n\n\n\n\nWhen our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nIf we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nBy including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nWe can now see that the intercept estimate \\(\\beta_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has an adjustment for each subject.\nEach subject’s line has been moved up or down accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage\n\n\n\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different:\n\n\n\n\n\n\n\n\n\nWhy? One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters.\n\n\n\n\n\n\n\n\n\nRandom slopes\n\n\n\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(\\beta_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nAs with the intercept \\(\\beta_{0i}\\), the slope of the predictor \\(\\beta_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed effects\n\n\n\n\n\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the groups all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\nThese are the overall intercept and slope.\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n405.7897675  -0.6722654 \n\n\n\n\n\n\n\n\n\n\n\nRandom effects\n\n\n\n\n\nThe plots below show the fitted values for each subject from each model that we have gone through in these expandable boxes (simple linear regression, random intercept, and random intercept & slope):\n\n\n\n\n\n\n\n\n\nIn the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_0\\). The standard deviation (and variance, which is \\(\\sigma_0^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function).\n\n\n\n\n\n\n\n\n\nIn the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. We can extract the deviations for each group from the fixed effect estimates using the ranef() function.\nThese are the deviations from the overall intercept (\\(\\widehat \\gamma_{00} = 405.79\\)) and slope (\\(\\widehat \\gamma_{10} = -0.672\\)) for each subject \\(i\\).\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)          x1\nsub_308   31.327291 -1.43995253\nsub_309  -28.832219  0.41839420\nsub_310    2.711822  0.05993766\nsub_330   59.398971  0.38526670\nsub_331   74.958481  0.17391602\nsub_332   91.086535 -0.23461836\nsub_333   97.852988 -0.19057838\nsub_334  -54.185688 -0.55846794\nsub_335  -16.902018  0.92071637\nsub_337   52.217859 -1.16602280\nsub_349  -67.760246 -0.68438960\nsub_350   -5.821271 -1.23788002\nsub_351   61.198823  0.05499816\nsub_352   -7.905596 -0.66495059\nsub_369  -47.636645 -0.46810258\nsub_370  -33.121093 -1.11001234\nsub_371   77.576205 -0.20402571\nsub_372  -36.389281 -0.45829505\nsub_373 -197.579562  1.79897904\nsub_374  -52.195357  4.60508775\n\nwith conditional variances for \"subject\" \n\n\n\n\n\n\n\n\n\n\n\nGroup-level coefficients\n\n\n\n\n\nWe can see the estimated intercept and slope for each subject \\(i\\) specifically, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)         x1\nsub_308    437.1171 -2.1122179\nsub_309    376.9575 -0.2538712\nsub_310    408.5016 -0.6123277\nsub_330    465.1887 -0.2869987\nsub_331    480.7482 -0.4983494\nsub_332    496.8763 -0.9068837\nsub_333    503.6428 -0.8628438\nsub_334    351.6041 -1.2307333\nsub_335    388.8877  0.2484510\nsub_337    458.0076 -1.8382882\nsub_349    338.0295 -1.3566550\nsub_350    399.9685 -1.9101454\nsub_351    466.9886 -0.6172672\nsub_352    397.8842 -1.3372160\nsub_369    358.1531 -1.1403680\nsub_370    372.6687 -1.7822777\nsub_371    483.3660 -0.8762911\nsub_372    369.4005 -1.1305604\nsub_373    208.2102  1.1267137\nsub_374    353.5944  3.9328224\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nNotice that the above are the fixed effects + random effects estimates, i.e. the overall intercept and slope + deviations for each subject.\n\ncbind(\n  int = fixef(random_slopes_model)[1] + \n    ranef(random_slopes_model)$subject[,1],\n  slope = fixef(random_slopes_model)[2] + \n    ranef(random_slopes_model)$subject[,2]\n)\n\n           int      slope\n [1,] 437.1171 -2.1122179\n [2,] 376.9575 -0.2538712\n [3,] 408.5016 -0.6123277\n [4,] 465.1887 -0.2869987\n [5,] 480.7482 -0.4983494\n [6,] 496.8763 -0.9068837\n [7,] 503.6428 -0.8628438\n [8,] 351.6041 -1.2307333\n [9,] 388.8877  0.2484510\n[10,] 458.0076 -1.8382882\n[11,] 338.0295 -1.3566550\n[12,] 399.9685 -1.9101454\n[13,] 466.9886 -0.6172672\n[14,] 397.8842 -1.3372160\n[15,] 358.1531 -1.1403680\n[16,] 372.6687 -1.7822777\n[17,] 483.3660 -0.8762911\n[18,] 369.4005 -1.1305604\n[19,] 208.2102  1.1267137\n[20,] 353.5944  3.9328224\n\n\n\n\n\n\n\n\n\n\n\nAssumptions, Influence\n\n\n\n\n\nIn the simple linear model \\(\\color{red}{y} = \\color{blue}{\\beta_0 + \\beta_1(x)} + \\varepsilon\\), we distinguished between the systematic model part \\(\\beta_0 + \\beta_1(x)\\), around which observations randomly vary (the \\(\\varepsilon\\) part) - i.e. \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}} + \\text{error}\\).\nIn the multi-level model, our random effects are another source of random variation - \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}} + \\text{group_error} + \\text{individual_error}\\). As such, random effects are another form of residual, and our assumptions of zero mean constant variance apply at both levels of residuals (see Figure 1).\n\n\n\n\n\nFigure 1: The black dashed lines show our model assumptions.\n\n\n\n\n\nWe can assess these normality of both resid(model) and ranef(model) by constructing plots using functions such as hist(), qqnorm() and qqline().\n\nWe can also use plot(model, type=c(\"p\",\"smooth\")) to give us our residuals vs fitted plot (smooth line should be horizontal at approx zero, showing zero mean).\n\nplot(model, form = sqrt(abs(resid(.))) ~ fitted(.), type = c(\"p\",\"smooth\")) will give us our scale-location plot (smooth line should be horizontal, showing constant variance).\n\nWe can also use the check_model() function from the performance package to get lots of info at once:\n\nlibrary(performance)\ncheck_model(random_slopes_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\ncase-based bootstrap\n\n\n\n\ntests or CIs for model parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\nlibrary(lmeresampler)bootstrap(model, .f=fixef, type=\"case\", B = 2000, resample = c(??,??))\n\n\nmodel comparison(different fixed effects, same random effects)\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\ntakes time, needs careful thought about which levels to resample, but means we can relax distributional assumptions (e.g. about normality of residuals)\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Model Fitted values\n\n\n\n\n\nThe model fitted (or “model predicted”) values can be obtained using predict() (returning just the values) or broom.mixed::augment() (returning the values attached to the data that is inputted to the model).\nTo plot, them, we would typically like to plot the fitted values for each group (e.g. subject)\n\nlibrary(broom.mixed)\naugment(random_slopes_model) %&gt;%\n  ggplot(.,aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Fixed Effects\n\n\n\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. Packages like sjPlot make it incredibly easy (but sometimes too easy), so a nice option is to use the effects package to construct a dataframe of the linear prediction accross the values of a predictor, plus standard errors and confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\n# a quick option:  \nlibrary(sjPlot)\nplot_model(random_slopes_model, type = \"eff\")\n\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting random effects\n\n\n\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms &lt;- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\n\n\n\n\nMLM allows us to model effects in the linear model as varying between groups. Our coefficients we remember from simple linear models (the \\(\\beta\\)’s) are modelled as a distribution that has an overall mean around which our groups vary. We can see this in Figure 2, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 2 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\nFigure 2: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(\\beta\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is \\(\\rho = \\text{Cor}(\\zeta_{0i}, \\zeta_{1i}) = \\frac{\\sigma_{01}}{\\sigma_0 \\sigma_1}\\):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\nWe fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\nlibrary(lme4)\nm1 &lt;- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 637.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.49449 -0.57223 -0.01353  0.62544  2.39122 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.2616   1.5038       \n          x           0.7958   0.8921   0.55\n Residual             4.3672   2.0898       \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   1.7261     0.9673   1.785\nx             1.1506     0.2968   3.877\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)\n\n\n\n\n\n\n\nPractice Datasets Weeks 4 and 5\nBelow are various datasets on which you can try out your newfound modelling skills. Read the descriptions carefully, keeping in mind the explanation of how the data is collected and the research question that motivates the study design.\n\n\n\n\n\n\nPractice 1: Music and Driving\n\n\n\n\n\nThese data are simulated to represent data from a fake experiment, in which participants were asked to drive around a route in a 30mph zone. Each participant completed the route 3 times (i.e. “repeated measures”), but each time they were listening to different audio (either speech, classical music or rap music). Their average speed across the route was recorded. This is a fairly simple design, that we might use to ask “how is the type of audio being listened to associated with driving speeds?”\nThe data are available at https://uoepsy.github.io/data/drivingmusicwithin.csv.\n\n\n\n\n\n\n\n\n\nPractice 2: CBT and Stress\n\n\n\n\n\nThese data are simulated to represent data from 50 participants, each measured at 3 different time-points (pre, during, and post) on a measure of stress. Participants were randomly allocated such that half received some cognitive behavioural therapy (CBT) treatment, and half did not. This study is interested in assessing whether the two groups (control vs treatment) differ in how stress changes across the 3 time points.\nThe data are available at https://uoepsy.github.io/data/stressint.csv.\n\n\n\n\n\n\n\n\n\nPractice 3: Erm.. I don’t believe you\n\n\n\n\n\nThese data are simulated to represent data from 30 participants who took part in an experiment designed to investigate whether fluency of speech influences how believable an utterance is perceived to be.\nEach participant listened to the same 20 statements, with 10 being presented in fluent speech, and 10 being presented with a disfluency (an “erm, …”). Fluency of the statements was counterbalanced such that 15 participants heard statements 1 to 10 as fluent and 11 to 20 as disfluent, and the remaining 15 participants heard statements 1 to 10 as disfluent, and 11 to 20 as fluent. The order of the statements presented to each participant was random.\nThe data are available at https://uoepsy.github.io/data/erm_belief.csv.\n\n\n\n\n\n\n\n\n\nPractice 4: Cognitive Aging\n\n\n\n\n\nThese data are simulated to represent a large scale international study of cognitive aging, for which data from 17 research centers has been combined. The study team are interested in whether different cognitive domains have different trajectories as people age. Do all cognitive domains decline at the same rate? Do some decline more steeply, and some less? The literature suggests that scores on cognitive ability are predicted by educational attainment, so they would like to control for this.\nEach of the 17 research centers recruited a minimum of 14 participants (Median = 21, Range 14-29) at age 45, and recorded their level of education (in years). Participants were then tested on 5 cognitive domains: processing speed, spatial visualisation, memory, reasoning, and vocabulary. Participants were contacted for follow-up on a further 9 occasions (resulting in 10 datapoints for each participant), and at every follow-up they were tested on the same 5 cognitive domains. Follow-ups were on average 3 years apart (Mean = 3, SD = 0.8).\nThe data are available at https://uoepsy.github.io/data/cogdecline.csv."
  },
  {
    "objectID": "07_path1.html",
    "href": "07_path1.html",
    "title": "Path Analysis",
    "section": "",
    "text": "Relevant packages\n\nlavaan\n\nsemPlot or tidySEM\n\n\nBy now, we are getting more comfortable with the regression world, and we can see how it is extended to lots of different types of outcome and data structures. So far in DAPR3 it’s been all about the multiple levels. This has brought so many more potential study designs that we can now consider modelling - pretty much any study where we are interested in explaining some outcome variable, and where we have sampled clusters of observations (or clusters of clusters of clusters of … etc.).\nBut we are still restricted to thinking, similar to how we thought in DAPR2, about one single outcome variable. In fact, if we think about the structure of the fixed effects part of a model (i.e., the bit we’re specifically interested in), then we’re still limited to thinking of the world in terms of “this is my outcome variable, everything else predicts it”.\n\n\n\n\n\n\nRegression as a path diagram\n\n\n\n\n\n\nImagine writing the names of all your variables on a whiteboard\nSpecify which one is your dependent (or “outcome” or “response”) variable.\nSit back and relax, you’re done!\n\nIn terms of a theoretical model of the world, there’s not really much to it. We have few choices in the model we construct beyond specifying which is our outcome variable.\nWe can visualise our multiple regression model like this:\n\n\n\n\n\nFigure 1: In multiple regression, we decide which variable is our outcome variable, and then everything else is done for us\n\n\n\n\nOf course, there are a few other things that are included (an intercept term, the residual error, and the fact that our predictors can be correlated with one another), but the idea remains pretty much the same:\n\n\n\n\n\nFigure 2: Multiple regression with intercept, error, predictor covariances\n\n\n\n\n\n\n\n\n\n\n\n\n\nA model reflects a theory\n\n\n\n\n\nWhat if I my theoretical model of the world doesn’t fit the structure of “one outcome, multiple precictors”?\nLet’s suppose I have 5 variables: Age, Parental Income, Income, Autonomy, and Job Satisfaction. I draw them up on my whiteboard:\n\n\n\n\n\nFigure 3: My variables\n\n\n\n\nMy theoretical understanding of how these things fit together leads me to link my variables to end up with something like that in Figure 4.\n\n\n\n\n\nFigure 4: My theory about my system of variables\n\n\n\n\nIn this diagram, a persons income is influenced by their age, their parental income, and their level of autonomy, and in turn their income predicts their job satisfaction. Job satisfaction is also predicted by a persons age directly, and by their level of autonomy, which is also predicted by age. It’s complicated to look at, but in isolation each bit of this makes theoretical sense.\nTake each arrow in turn and think about what it represents:\n\n\n\n\n\nFigure 5: ?(caption)\n\n\n\n\nIf we think about trying to fit this “model” with the tools that we have, then we might end up wanting to fit three separate regression models, which between them specify all the different arrows in the diagram:\n\\[\n\\begin{align}\n\\textrm{Job Satisfaction} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_3(\\textrm{Income}) + \\varepsilon \\\\\n\\textrm{Income} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_2(\\textrm{Parental Income}) + \\varepsilon \\\\\n\\textrm{Autonomy} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\varepsilon \\\\\n\\end{align}\n\\]\nThis is all well and good, but what if I want to talk about how well my entire model (Figure 4) fits the data we observed?\n\n\n\n\n\n\n\n\n\nIntroducing Path Analysis\n\n\n\n\n\nThe starting point for Path Analysis is to think about our theories in terms of the connections between variables drawn on a whiteboard. By representing a theory as paths to and from different variables, we open up a whole new way of ‘modelling’ the world around us.\nThere are a few conventions to help us understand this sort of diagrammatical way of thinking. By using combinations of rectangles, ovals, single- and double-headed arrows, we can draw all sorts of model structures. In Path Diagrams, we use specific shapes and arrows to represent different things in our model:\nShapes and Arrows in Path Diagrams\n\nObserved variables are represented by squares or rectangles. These are the named variables of interest which exist in our dataset - i.e. the ones which we have measured directly.\nVariances/Covariances are represented by double-headed arrows. In many diagrams these are curved.\nRegressions are shown by single headed arrows (e.g., an arrow from \\(x\\) to \\(y\\) for the path \\(y~x\\)).\n\n\nLatent variables are represented by ovals, and we will return to these in a few weeks time!\n\n\n\n\n\n\n\n\n\n\n\nTerminology refresher\n\nExogenous variables are a bit like what we have been describing with words like “independent variable” or “predictor”. In a path diagram, they have no paths coming from other variables in the system, but have paths going to other variables.\n\nEndogenous variables are more like the “outcome”/“dependent”/“response” variables we are used to. They have some path coming from another variable in the system (and may also - but not necessarily - have paths going out from them).\n\n\n\n\n\n\n\n\n\n\nHow does it work (in brief)?\n\n\n\n\n\nThe logic behind path analysis is to estimate a system of equations that can reproduce the covariance structure that we see in the data.\n\nWe specify our theoretical model of the world as a system of paths between variables\nWe collect data on the relevant variables and we observe a correlation matrix (i.e. how each variable correlates with all others)\nWe fit our model to the data, and evaluate how well our theoretical model (a system of paths) can reproduce the correlation matrix we observed.\n\n\n\n\n\n\n\n\n\n\nOPTIONAL How does it work (less brief)?\n\n\n\n\n\nPath Diagram Tracing\nFor Path Diagrams that meet a certain set of pre-requisites, we can use a cool technique called Path Tracing to estimate the different paths (i.e., the coefficients) from just the covariance matrix of the dataset. For us to be able to do this, a Path Diagram must meet these criteria:\n\nAll our exogenous variables are correlated (unless we specifically assume that their correlation is zero)\nAll models are ‘recursive’ (no two-way causal relations, no feedback loops)\nResiduals are uncorrelated with exogenous variables\nEndogenous variables are not connected by correlations (we would use correlations between residuals here, because the residuals are not endogenous)\nAll ‘causal’ relations are linear and additive\n‘causes’ are unitary (if A -&gt; B and A -&gt; C, then it is presumed that this is the same aspect of A resulting in a change in both B and C, and not two distinct aspects of A, which would be better represented by two correlated variables A1 and A2).\n\n\n\n\n\n\n\nCausal?\n\n\n\n\n\nIt is a slippery slope to start using the word ‘cause’, and personally I am not that comfortable using it here. However, you will likely hear it a lot in resources about path analysis, so it is best to be warned.\nPlease keep in mind that we are using a very broad definition of ‘causal’, simply to reflect the one way nature of the relationship we are modeling. In Figure 6, a change in the variable X1 is associated with a change in Y, but not vice versa.\n\n\n\n\n\nFigure 6: Paths are still just regressions.\n\n\n\n\n\n\n\nTracing Rules\nThanks to Sewal Wright, we can express the correlation between any two variables in the system as the sum of all compound paths between the two variables.\ncompound paths are any paths you can trace between A and B for which there are:\n\nno loops\nno going forward then backward\nmaximum of one curved arrow per path\n\nEXAMPLE\nLet’s consider the example below, for which the paths are all labelled with lower case letters \\(a, b, c, \\text{and } d\\).\n\n\n\n\n\nFigure 7: A multiple regression model as a path diagram\n\n\n\n\nAccording to Wright’s tracing rules above, write out the equations corresponding to the 3 correlations between our observed variables (remember that \\(r_{a,b} = r_{b,a}\\), so it doesn’t matter at which variable we start the paths).\n\n\\(r_{x1,x2} = c\\)\n\n\\(r_{x1,y} = a + bc\\)\n\n\\(r_{x2,y} = b + ac\\)\n\nNow let’s suppose we observed the following correlation matrix:\n\negdat &lt;- read_csv(\"https://uoepsy.github.io/data/patheg.csv\")\negdat &lt;- read_csv(\"../../data/patheg.csv\")\nround(cor(egdat),2)\n\n     x1   x2    y\nx1 1.00 0.36 0.75\nx2 0.36 1.00 0.60\ny  0.75 0.60 1.00\n\n\nWe can plug these into our system of equations:\n\n\\(r_{x1,x2} = c = 0.36\\)\n\n\\(r_{x1,y} = a + bc = 0.75\\)\n\n\\(r_{x2,y} = b + ac = 0.60\\)\n\nAnd with some substituting and rearranging, we can work out the values of \\(a\\), \\(b\\) and \\(c\\).\n\n\n\n\n\n\nClick for answers\n\n\n\n\n\nThis is what I get:\na = 0.61\nb = 0.38\nc = 0.36\n\n\n\nWe can even work out what the path labeled \\(d\\) (the residual variance) is.\nFirst we sum up all the equations for the paths from Y to Y itself.\nThese are:\n\n\\(a^2\\) (from Y to X1 and back)\n\n\\(b^2\\) (from Y to X2 and back)\n\n\\(acb\\) (from Y to X1 to X2 to Y)\n\\(bca\\) (from Y to X2 to X1 to Y)\n\nSumming them all up and solving gives us:\n\\[\n\\begin{align}\nr_{y \\cdot x1, x2} & = a^2 + b^2 + acb + bca\\\\\n& = 0.61^2 + 0.38^2 + 2 \\times(0.61 \\times 0.38 \\times 0.36)\\\\\n& = 0.68 \\\\\n\\end{align}\n\\] We can think of this as the portion of the correlation of Y with itself that occurs via the predictors. Put another way, this is the amount of variance in Y explained jointly by X1 and X2, which sounds an awful lot like an \\(R^2\\)!\nThe path labelled \\(d\\) is simply all that is left in Y after taking out the variance explained by X1 and X2, meaning that the path \\(d = \\sqrt{1-R^2}\\) (i.e., the residual variance!).\nHooray! We’ve just worked out regression coefficients when all we had was the correlation matrix of the variables! It’s important to note that we have been using the correlation matrix, so, somewhat unsurprisingly, our estimates are standardised coefficients.\nBecause we have the data itself, let’s quickly find them with lm()\n\n# model:\nmodel1 &lt;- lm( scale(y) ~ scale(x1) + scale(x2), egdat)\n# extract the coefs\ncoef(model1) %&gt;% round(2)\n\n(Intercept)   scale(x1)   scale(x2) \n       0.00        0.61        0.38 \n\n# extract the r^2\nsummary(model1)$r.squared\n\n[1] 0.688\n\n\n\n\n\n\n\n\n\n\n\nIntroducing lavaan\n\n\n\n\n\nFor the remaining weeks of the course, we’re going to rely heavily on the lavaan (Latent Variable Analysis) package. This is the main package in R for fitting path diagrams (as well as more cool models like factor analysis sructures and structural equation mdoels). There is a huge scope of what this package can do.\nThe first thing to get to grips with is the various new operators which it allows us to use.\nOur old multiple regression formula in R was specified as y ~ x1 + x2 + x3 + ....\nIn lavaan, we continue to fit regressions using the ~ symbol, but we can also specify the construction of latent variables using =~ and residual variances & covariances using ~~.\n\n\n\nformula type\noperator\nmnemonic\n\n\n\n\nlatent variable definition\n=~\n“is measured by”\n\n\nregression\n~\n“is regressed on”\n\n\n(residual) (co)variance\n~~\n“is correlated with”\n\n\nintercept\n~1\n“intercept”\n\n\nnew parameter\n:=\n“is defined as”\n\n\n\n(from https://lavaan.ugent.be/tutorial/syntax1.html)\nIn practice, fitting models in lavaan tends to be a little different from things like lm() and (g)lmer().\nInstead of including the model formula inside the fit function (e.g., lm(y ~ x1 + x2, data = df)), we tend to do it in a step-by-step process. This is because as our models become more complex, our formulas can pretty long!\nWe write the model as a character string (e.g. model &lt;- \"y ~ x1 + x2\") and then we pass that formula along with the data to the relevant lavaan function, which for our purposes will be the sem() function, sem(model, data = mydata).\n\n\n\n\n\n\n\n\n\nFitting a multiple regression model with lavaan\n\n\n\n\n\nYou can see a multiple regression fitted with lavaan below.\n\nlibrary(lavaan)\nscsdat &lt;- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\n\n# the lm() way\nmreg_lm &lt;- lm(dass ~ zo + zc + ze + za + zn + scs, scsdat)\n\n# setting up the model\nmreg_model &lt;- \"\n    #regression\n    dass ~ zo + zc + ze + za + zn + scs\n\"\nmreg_sem &lt;- sem(mreg_model, data=scsdat)\n\nThese are the coefficients from our lm() model:\n\ncoefficients(mreg_lm)\n\n(Intercept)          zo          zc          ze          za          zn \n    62.1243     -0.0307     -0.0378      0.7449      0.2029      1.5209 \n        scs \n    -0.4865 \n\n\nAnd you can see the estimated parameters are the same for our sem() model!\n\nsummary(mreg_sem)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                           656\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  dass ~                                              \n    zo               -0.031    0.242   -0.127    0.899\n    zc               -0.038    0.248   -0.153    0.879\n    ze                0.745    0.377    1.976    0.048\n    za                0.203    0.378    0.537    0.591\n    zn                1.521    0.249    6.097    0.000\n    scs              -0.486    0.071   -6.893    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .dass             40.021    2.210   18.111    0.000\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 1: Model Specification\n\n\n\n\n\nThe first part of estimating a path model involves specifying the model. This means basically writing down the paths that are included in your theoretical model.\nLet’s start by looking at the example about job satisfaction, income, autonomy and age.\nRecall we had this theoretical model:\n\n\n\n\n\n\n\n\n\nAnd now let’s suppose that we collected data on these variables:\n\njobsatpath &lt;- read_csv(\"https://uoepsy.github.io/data/jobsatpath.csv\")\nhead(jobsatpath)\n\n\n\n\n\n\njobsat\nincome\nautonomy\nage\nparentincome\n\n\n\n\n22\n39\n17\n55\n47\n\n\n29\n35\n58\n51\n43\n\n\n69\n38\n45\n52\n49\n\n\n67\n27\n52\n43\n44\n\n\n54\n14\n36\n35\n40\n\n\n25\n25\n39\n48\n44\n\n\n...\n...\n...\n...\n...\n\n\n\n\n\n\n\nRemember we said that we could specify all these paths using three regression models? Well, to specify our path model, we simply write these out like we would do in lm(), but this time we do so all in one character string. We still have to make sure that we use the correct variable names, as when we make R estimate the model, it will look in the data for things like “jobsat”.\n\nmymodel &lt;- \"\njobsat ~ age + autonomy + income\nincome ~ autonomy + age + parentincome\nautonomy ~ age\n\"\n\nThere are some other things which we will automatically be estimated here: all our exogenous variables (the ones with arrows only going from them) will be free to correlate with one another. We can write this explicitly in the model if we like, using the two tildes ~~ between our two exogenous variables age and parentincome. We will also get the variances of all our variables.\nWe can see all the paths here:\n\nlavaanify(mymodel)\n\n\n\n            lhs op          rhs\n1        jobsat  ~          age\n2        jobsat  ~     autonomy\n3        jobsat  ~       income\n4        income  ~     autonomy\n5        income  ~          age\n6        income  ~ parentincome\n7      autonomy  ~          age\n8        jobsat ~~       jobsat\n9        income ~~       income\n10     autonomy ~~     autonomy\n11          age ~~          age\n12          age ~~ parentincome\n13 parentincome ~~ parentincome\n\n\nand even make a nice diagram:\n\nlibrary(semPlot)\nsemPaths(lavaanify(mymodel))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 2: Model Identification\n\n\n\n\n\nYou’ll have heard the term “model fit” many times since the start of DAPR2, when we began model-based thinking. However, there is a crucial difference in what it means when it is used in for path analysis.\nIn things like multiple regression, we have been using “model fit” to be the measure of “how much variance can we explain in y with our set of predictors?”. For a path model, examining “model fit” is more like asking “how well does our model reproduce the characteristics of the data that we observed?”.\nWe can represent the “characteristics of our data” in a covariance matrix, so one way of thinking of “model fit” is as “how well can our model reproduce our observed covariance matrix?”.\n\ncov(jobsatpath)\n\n             jobsat income autonomy    age parentincome\njobsat       341.62   4.47    107.3 -28.81        -6.47\nincome         4.47  50.17     46.7  29.56        14.83\nautonomy     107.30  46.68    365.6  32.94       -10.00\nage          -28.81  29.56     32.9  38.69         1.81\nparentincome  -6.47  14.83    -10.0   1.81        14.18\n\n\nBecause we are working with a covariance matrix here, we are really working with fewer “bits” of information than the 50 people in our dataset. We actually are concerned with the 15 unique variances and covariances in our covariance matrix between our 5 variables above.\nDegrees of freedom\nWhen we think of “degrees of freedom” for a multiple regression model, in DAPR2 we learned that \\(df = n-k-1\\) (\\(n\\) is the number of observations, \\(k\\) is the number of predictors). These degrees of freedom related to the corresponding \\(F\\) and \\(t\\)-distributions with which we performed our hypothesis tests (e.g. \\(t\\)-tests for a null hypothesis that a coefficient is zero, or \\(F\\)-tests for a null that the reduction in residual sums of squares is zero).\nBut in relation to our model’s ability to represent a \\(k \\times k\\) covariance matrix (i.e. the covariance matrix of our \\(k\\) variables), we are instead interested in the number of covariances (and not the number of observations). Our “degrees of freedom” in the this framework corresponds to the number of knowns (observed covariances/variances) minus the number of unknowns (parameters to be estimated by the model). A model is only able to be estimated if it has at least 0 degrees of freedom (if there are at least as many knowns as unknowns). A model with 0 degrees of freedom is termed just-identified. Under- and Over- identified models correspond to those with \\(&lt;0\\) and \\(&gt;0\\) degrees of freedom respectively.\n\nHow many knowns are there?\nThe number of known covariances in a set of \\(k\\) observed variables is equal to \\(\\frac{k \\cdot (k+1)}{2}\\).\n\nWhen learning about path models, the visualisations can play a key part. It often helps to draw all our variables (both observed and latent) on the whiteboard, and connect them up according to your theoretical model. You can then count the number of paths (arrows) and determine whether the number of knowns &gt; number of unknowns. We can reduce the number of unknowns by fixing parameters to be specific values.\n\nBy constraining some estimated parameter to be some specific value, we free-up a degree of freedom! For instance “the correlation between x1 and x2 is equal to 0.7 (\\(r_{x_1x_2} = .07\\))”. This would turn a previously estimated parameter into a fixed parameter, and this gains us the prize of a lovely degree of freedom!\nBy removing a path altogether, we are constraining it to be zero.\n\n\n\n\n\n\n\n\n\n\nOPTIONAL: multiple regression model is just-identified\n\n\n\n\n\nThe multiple regression model is an example of a just-identified model! In multiple regression, everything is allowed to covary with everything else, which means that there is a unique solution for all of the model’s parameters because there are as many paths as there are observed covariances. This means that in this path analysis world, a multiple regression model is “just-identified”.\nIf I have two predictors and one outcome variable, then there are 6 variances and covariances available. For instance:\n\ncov(somedata)\n\n         y     x1     x2\ny  45.6481 0.0149 1.3525\nx1  0.0149 1.0455 0.0196\nx2  1.3525 0.0196 1.0000\n\n\nThe multiple regression model will estimate the two variances of the exogenous variables (the predictors), their covariance, the two paths from each exogenous to the endogenous (each predictor to the outcome), and the error variance. This makes up 6 estimated parameters - which is exactly how many known covariances there are.\nCount the number of arrows (both single and double headed) in the diagram:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 3: Model Estimation\n\n\n\n\n\nEstimating the model is relatively straightforward. We pass the formula we have written to the sem() function, along with the data set in which we want it to look for the variables. It will be estimated using maximum likelihood estimation.\n\nmymodel.fit &lt;- sem(mymodel, data = jobsatpath)\n\nWe can then examine the parameter estimates:\n\nsummary(mymodel.fit)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                            50\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.050\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.080\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  jobsat ~                                            \n    age              -1.566    0.482   -3.250    0.001\n    autonomy          0.347    0.131    2.653    0.008\n    income            0.689    0.439    1.572    0.116\n  income ~                                            \n    autonomy          0.099    0.026    3.796    0.000\n    age               0.631    0.081    7.834    0.000\n    parentincome      1.036    0.128    8.099    0.000\n  autonomy ~                                          \n    age               0.851    0.418    2.038    0.042\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .jobsat          251.098   50.220    5.000    0.000\n   .income           11.293    2.259    5.000    0.000\n   .autonomy        330.771   66.154    5.000    0.000\n\n\nWe can now, to “visualise” our model, add the estimates to the diagram:\n\n\n\n\n\nModel estimates\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 4: Model Fit\n\n\n\n\n\nOnce we have estimated a model, we can evaluate how well it fits our sample data. As mentioned above in relation to model identification, when we talk about model fit here, we are asking “how well does our model reproduce the characteristics of the data that we observed?”, or more specifically “how well can our model reproduce our observed covariance matrix?”.\n\ncov(jobsatpath)\n\n             jobsat income autonomy    age parentincome\njobsat       341.62   4.47    107.3 -28.81        -6.47\nincome         4.47  50.17     46.7  29.56        14.83\nautonomy     107.30  46.68    365.6  32.94       -10.00\nage          -28.81  29.56     32.9  38.69         1.81\nparentincome  -6.47  14.83    -10.0   1.81        14.18\n\n\nThere are too many different indices of model fit for these types of models, and there’s lots of controversy over the various merits and disadvantages and proposed cutoffs of each method. We will return to this more in coming weeks, and the lecture this week contains information on some of them. The important thing to remember: “model fit” and “degrees of freedom” have quite different meanings to those you are likely used to.\nThe simplest metric of fit is a chi-square value that we can compute that reflects reflects the discrepancy between the model-implied covariance matrix and the observed covariance matrix. We can then calculate a p-value for this chi-square statistic by using the chi-square distribution with the degrees of freedom equivalent to that of the model.\nIf we denote the sample covariance matrix as \\(S\\) and the model-implied covariance matrix as \\(\\Sigma(\\theta)\\), then we can think of the null hypothesis here as \\(H_0: S - \\Sigma(\\hat\\theta) = 0\\). In this way our null hypothesis is sort of like saying that our theoretical model is correct (and can therefore perfectly reproduce the covariance matrix).\n\n\n\n\nExercises: Burnout\n\nData: Passion & Burnout\nResearchers are interested in the role of obsessive and harmonious passion in psychological wellbeing. The researchers collect data from 100 participants. Participants respond on sliding scales (0-100) for five measures:\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nworksat\nWork Satisfaction: On a scale of 0-100, how satisfied are you with your work?\n\n\nhp\nHarmonious Passion: On a scale of 0-100, how much do you feel that you freely choose to engage in work outside of working hours?\n\n\nop\nObsessive Passion: On a scale of 0-100, how much do you have uncontrollable urges to work outside of working hours?\n\n\nconflict\nWork Conflict: On a scale of 0-100, how much conflict do you experience in your work?\n\n\nburnout\nWork Burnout: On a scale of 0-100, how close to burnout at work are you?\n\n\n\n\n\n\n\nThe data is available at https://uoepsy.github.io/data/passionpath.csv\n\n\nQuestion 1\n\n\nLoad in the libraries we will use in these exercises:\n\ntidyverse\n\nlavaan\n\nsemPlot\n\nRead in the data.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nburnout &lt;- read_csv(\"https://uoepsy.github.io/data/passionpath.csv\")\n\n\n\n\n\nQuestion 2\n\n\nThe researchers have this theoretical model:\n\n\n\n\n\nFigure 8: Burnout Theory\n\n\n\n\nSpecify this model and store the formula as an object in R\n\n\n\n\n\nSolution\n\n\n\n\nburnoutmod &lt;- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\n\"\n\n\n\n\n\nQuestion 3\n\n\nFit the model to the data using the sem() function.\n\n\n\n\n\nSolution\n\n\n\n\nburnoutfit &lt;- sem(burnoutmod, data=burnout)\n\n\n\n\n\nQuestion 4\n\n\nExamine the parameter estimates\n\n\n\n\n\nSolution\n\n\n\n\nsummary(burnoutfit)\n\nlavaan 0.6.16 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.458\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.243\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  worksat ~                                           \n    hp                0.420    0.088    4.769    0.000\n  burnout ~                                           \n    worksat          -0.207    0.063   -3.304    0.001\n    conflict         -0.147    0.191   -0.772    0.440\n  conflict ~                                          \n    op                0.095    0.038    2.476    0.013\n    hp               -0.010    0.033   -0.308    0.758\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  hp ~~                                               \n    op               61.628   18.529    3.326    0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .worksat         155.762   22.028    7.071    0.000\n   .burnout          75.347   10.656    7.071    0.000\n   .conflict         19.465    2.753    7.071    0.000\n    hp              200.614   28.371    7.071    0.000\n    op              152.210   21.526    7.071    0.000\n\n\n\n\n\n\nQuestion 5\n\n\nProduce a diagram with the estimates on the paths. Can you also produce one which has the standardised estimates?\nTake a look at the help function for semPaths(). What do the arguments what and whatLabels do?\n\n\n\n\n\nSolution\n\n\n\nwhat will weight and colour the paths according something like the estimates. whatLabels will provide labels for the paths:\n\nsemPaths(burnoutfit, whatLabels = \"est\")\n\n\n\n\n\n\n\n\nThis will change them to the standardised estimates:\n\nsemPaths(burnoutfit, what = \"std\", whatLabels = \"std\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nHow many variables do you have in your model?\nHow many knowns are there in the covariance matrix?\n\nHow many unknowns are there in your model?\nHow many degrees of freedom do you therefore have?\n\n\n\n\n\n\nSolution\n\n\n\n\nHow many variables do you have in your model?\n5\nHow many knowns are there in the covariance matrix? \\(\\frac{5 \\times (5 + 1)}{2} = 15\\)\nHow many unknowns are there in your model?\nThere are 6 paths in Figure 8, but we also need to consider the variances of the 5 variables, so we have 11 things being estimated\nHow many degrees of freedom do you therefore have? 15 - 11 = 4\n\n\n\n\n\nQuestion 7\n\n\nTake a look at the summary of the model you fitted. Specifically, examine the bit near the top where it mentions the \\(\\chi^2\\) statistic.\nIs it significant? What do we conclude?\n\n\n\n\n\nSolution\n\n\n\nThe \\(\\chi^2\\) statistic is not significant:\n\npchisq(5.458, df=4, lower.tail=F)\n\n[1] 0.243\n\n\nWe therefore have no evidence to support rejecting our null hypothesis that our model provides a reasonable fit to the data.\n\n\n\n\nQuestion 8\n\n\nTry examing what the other fit measures (RMSEA, SRMR, CLI, TLI: How do they compare with the cutoffs provided in the lecture?\nhint: summary(modelfit, fit.measures = TRUE)\n\n\n\n\n\nSolution\n\n\n\nThe fit statistics for our model:\nComparative Fit Index (CFI) = 0.968 Tucker-Lewis Index (TLI) = 0.921 RMSEA = 0.060 SRMR = 0.063\nOnly CFI meets the criteria given in the lecture slides for “considered good”.\n\nsummary(burnoutfit, fit.measures = TRUE)\n\nlavaan 0.6.16 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.458\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.243\n\nModel Test Baseline Model:\n\n  Test statistic                                56.068\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.968\n  Tucker-Lewis Index (TLI)                       0.921\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1836.113\n  Loglikelihood unrestricted model (H1)      -1833.384\n                                                      \n  Akaike (AIC)                                3694.226\n  Bayesian (BIC)                              3722.883\n  Sample-size adjusted Bayesian (SABIC)       3688.142\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.060\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.172\n  P-value H_0: RMSEA &lt;= 0.050                    0.361\n  P-value H_0: RMSEA &gt;= 0.080                    0.475\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.063\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  worksat ~                                           \n    hp                0.420    0.088    4.769    0.000\n  burnout ~                                           \n    worksat          -0.207    0.063   -3.304    0.001\n    conflict         -0.147    0.191   -0.772    0.440\n  conflict ~                                          \n    op                0.095    0.038    2.476    0.013\n    hp               -0.010    0.033   -0.308    0.758\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  hp ~~                                               \n    op               61.628   18.529    3.326    0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .worksat         155.762   22.028    7.071    0.000\n   .burnout          75.347   10.656    7.071    0.000\n   .conflict         19.465    2.753    7.071    0.000\n    hp              200.614   28.371    7.071    0.000\n    op              152.210   21.526    7.071    0.000\n\n\n\n\n\n\nQuestion Extra: modification indices\n\n\nExamine the modification indices of the model (use the modindices() function).\nPay close attention to the mi column (this is the “modification index”, which is the change in the \\(\\chi^2\\) statistic). The other interesting column is going to be the sepc.all column, which is the estimated parameter value of the proposed path, in a model where all the variables are standardised. This means we can evaluate whether the estimated parameter is relatively small/moderate/large, because these are all standardised correlations between -1 and 1!\nAre there any paths which the modification indices suggest might improve the model? Do they make theoretical sense to include them?\n\n\n\n\n\nSolution\n\n\n\n\nmodindices(burnoutfit)\n\n        lhs op      rhs    mi     epc sepc.lv sepc.all sepc.nox\n12  worksat ~~  burnout 0.268 -13.039 -13.039   -0.120   -0.120\n13  worksat ~~ conflict 0.842  -5.053  -5.053   -0.092   -0.092\n15  worksat ~~       op 1.102 -15.122 -15.122   -0.098   -0.098\n16  burnout ~~ conflict 3.461 -28.991 -28.991   -0.757   -0.757\n17  burnout ~~       hp 0.036  -2.403  -2.403   -0.020   -0.020\n18  burnout ~~       op 3.185  18.424  18.424    0.172    0.172\n21  worksat  ~  burnout 0.095  -0.101  -0.101   -0.067   -0.067\n22  worksat  ~ conflict 1.306  -0.314  -0.314   -0.103   -0.103\n23  worksat  ~       op 1.102  -0.113  -0.113   -0.101   -0.101\n24  burnout  ~       hp 0.268   0.035   0.035    0.054    0.054\n25  burnout  ~       op 3.471   0.137   0.137    0.184    0.184\n26 conflict  ~  worksat 0.842  -0.032  -0.032   -0.099   -0.099\n27 conflict  ~  burnout 0.225  -0.062  -0.062   -0.126   -0.126\n28       hp  ~  worksat 1.102   0.316   0.316    0.309    0.309\n29       hp  ~  burnout 0.096  -0.051  -0.051   -0.033   -0.033\n32       op  ~  worksat 1.102  -0.097  -0.097   -0.109   -0.109\n33       op  ~  burnout 4.058   0.264   0.264    0.196    0.196\n\n\nThere seems to be a suggested reasonably large correlation between burnout and conflict. If we were to fit this model, our fit indices may well improve and meet our cut-offs. But this may well just be overfitting.\n\nburnoutmod2 &lt;- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\nburnout ~~ conflict\n\"\nsem(burnoutmod2, data=burnout)\n\nWe will not start adjusting models based on modification indices today (or indeed in this course at all). As a general rule for dapR3 course, we want you to specify and test a specific model, and not seek to use exploratory modifications."
  },
  {
    "objectID": "08_path2.html",
    "href": "08_path2.html",
    "title": "Mediation",
    "section": "",
    "text": "Relevant packages\n\nlavaan\n\nsemPlot or tidySEM\nmediation\n\n\n\n\n\n\n\n\nWhat is ‘mediation’?\n\n\n\n\n\nLet’s imagine we are interested in peoples’ intention to get vaccinated, and we observe the following variables:\n\nIntention to vaccinate (scored on a range of 0-100)\n\nHealth Locus of Control (HLC) score (average score on a set of items relating to perceived control over ones own health)\n\nReligiosity (average score on a set of items relating to an individual’s religiosity).\n\n\nIf we draw out our variables, and think about this in the form of a standard regression model with “Intention to vaccinate” as our outcome variable, then all the lines are filled in for us (see Figure 1).\n\n\n\n\n\nFigure 1: Multiple regression as a path model\n\n\n\n\nBut what if our theory suggests that some other model might be of more relevance? For instance, what if we believe that participants’ religiosity has an effect on their Health Locus of Control score, which in turn affects the intention to vaccinate (see Figure 2)?\n\n\n\n\n\nFigure 2: Mediation as a path model (If you’re interested, you can find the inspiration for this example data from the paper here. I haven’t properly read it though!)\n\n\n\n\nIn this case, the HLC variable is thought of as a mediator, because it mediates the effect of religiosity on intention to vaccinate. In this theoretical model, we distinguishing between two possible types of effect: direct and indirect.\n\nDirect vs Indirect\nIn path diagrams:\n\nDirect effect = one single-headed arrow between the two variables concerned\n\nIndirect effect = An effect transmitted via some other variables\n\n\n\n\n\nWe’ve seen how path analysis works in last week’s lab, and we can use that same logic to investigate models which have quite different structures such as those including mediating variables.\nBecause we have multiple endogenous variables here, then we’re immediately drawn to path analysis, because we’re in essence thinking of conducting several regression models. As we can’t fit our theoretical model into a nice straightforward regression model (we would need several), then we can use path analysis instead, and just smush lots of regressions together, allowing us to estimate things all at once.\n\n\n\n\n\n\nDoing Path Mediation 1: Data\n\n\n\n\n\nWe’ll continue with the example in Figure 2, for which we have simulated some data.\n\nData: VaxDat.csv\n100 parents were recruited and completed a survey that included measures of Health Locus of Control, Religiosity, and a sliding scale of 0 to 100 on how definite they were that their child would receive the vaccination for measles, mumps & rubella.\n\nIntention to vaccinate (scored on a range of 0-100)\n\nHealth Locus of Control (HLC) score (average on a set of 5 items - each scoring 0 to 6 - relating to perceived control over ones own health)\n\nReligiosity (average on a set of 5 items - each scoring 0 to 6 - relating to an individual’s religiosity).\n\nThe data is available at https://uoepsy.github.io/data/vaxdat.csv\n\nFirst we read in our data:\n\nvax &lt;- read_csv(\"https://uoepsy.github.io/data/vaxdat.csv\")\nsummary(vax)\n\n  religiosity        hlc         intention   \n Min.   :-1.0   Min.   :0.40   Min.   :39.0  \n 1st Qu.: 1.8   1st Qu.:2.00   1st Qu.:59.0  \n Median : 2.4   Median :3.00   Median :64.0  \n Mean   : 2.4   Mean   :2.99   Mean   :65.1  \n 3rd Qu.: 3.0   3rd Qu.:3.60   3rd Qu.:74.0  \n Max.   : 4.6   Max.   :5.80   Max.   :88.0  \n\n\nIt looks like we have a value we shouldn’t have there. We have a value of religiosity of -1. But the average of 5 items which each score 0 to 6 will have to fall within those bounds.\nLet’s replace any values &lt;0 with NA.\n\nvax &lt;- \n    vax %&gt;% \n    mutate(religiosity = ifelse(religiosity &lt; 0, NA, religiosity))\nsummary(vax)\n\n  religiosity        hlc         intention   \n Min.   :0.20   Min.   :0.40   Min.   :39.0  \n 1st Qu.:1.80   1st Qu.:2.00   1st Qu.:59.0  \n Median :2.40   Median :3.00   Median :64.0  \n Mean   :2.43   Mean   :2.99   Mean   :65.1  \n 3rd Qu.:3.00   3rd Qu.:3.60   3rd Qu.:74.0  \n Max.   :4.60   Max.   :5.80   Max.   :88.0  \n NA's   :1                                   \n\n\nNow let’s just check the marginal distributions of each variable.\nThere are lots of functions that we might use, but for a quick eyeball, I quite like the pairs.panels() and multi.hist() functions from the psych package.\n\nlibrary(psych)\npairs.panels(vax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Mediation 1: Model Specification\n\n\n\n\n\nOkay, we’re ready to start thinking about our model.\nFirst we specify the relevant paths:\n\nmed_model &lt;- \" \n    intention ~ religiosity\n    intention ~ hlc\n    hlc ~ religiosity\n\"\n\nIf we fit this model as it is, we won’t actually be testing the indirect effect, we will simply be fitting a couple of regressions.\nTo specifically test the indirect effect, we need to explicitly define the indirect effect in our model, by first creating a label for each of its sub-component paths, and then defining the indirect effect itself as the product of these two paths (why the product? Click here for a lovely explanation from Aja Murray).\nTo do this, we use a new operator, :=.\n\nmed_model &lt;- \" \n    intention ~ religiosity\n    intention ~ a*hlc\n    hlc ~ b*religiosity\n    \n    indirect:=a*b\n\"\n\n\n:=\nThis operator ‘defines’ new parameters which take on values that are an arbitrary function of the original model parameters. The function, however, must be specified in terms of the parameter labels that are explicitly mentioned in the model syntax.\n(the lavaan project)\nNote. The labels we use are completely up to us. This would be equivalent:\n\nmed_model &lt;- \" \n    intention ~ religiosity\n    intention ~ peppapig * hlc\n    hlc ~ kermit * religiosity\n    \n    indirect:= kermit * peppapig\n\"\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Mediation 1: Model Estimation\n\n\n\n\n\nIt is common to estimate the indirect effect using a bootstrapping approach (remember, bootstrapping involves resampling the data with replacement, thousands of times, in order to empirically generate a sampling distribution).\n\nWhy do we bootstrap mediation analysis?\nWe compute our indirect effect as the product of the sub-component paths. However, this results in the estimated indirect effect rarely following a normal distribution, and makes our usual analytically derived standard errors & p-values inappropriate.\nInstead, bootstrapping has become the norm for assessing sampling distributions of indirect effects in mediation models.\n\nWe can do this easily in lavaan:\n\nmm1.est &lt;- sem(med_model, data=vax, se = \"bootstrap\") \nsummary(mm1.est, ci = TRUE)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n                                                  Used       Total\n  Number of observations                            99         100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  intention ~                                                           \n    religiosty        0.616    1.117    0.551    0.581   -1.516    2.847\n    hlc        (a)    5.823    0.977    5.959    0.000    3.776    7.561\n  hlc ~                                                                 \n    religiosty (b)    0.557    0.085    6.536    0.000    0.394    0.726\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .intention        62.112    8.315    7.470    0.000   45.208   76.231\n   .hlc               0.741    0.102    7.247    0.000    0.540    0.939\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    indirect          3.243    0.713    4.546    0.000    1.996    4.780\n\n\n\nWe can see that the 95% bootstrapped confidence interval for the indirect effect of religiosity on intention to vaccinate does not include zero. We can conclude that the indirect effect is significant at \\(p &lt;.05\\). The direct effect is not significantly different from zero, suggesting that we have complete mediation (religiosity has no effect on intention to vaccinate after controlling for health locus of control).\n\nFinally, we can visualise the estimates for our model using the semPaths() function from the semPlot package, and also with the graph_sem() function from the tidySEM package.\nOften, if we want to include a path diagram in a report then the output of these functions would not usually meet publication standards, and instead we tend to draw them in programs like powerpoint!)\n\nlibrary(tidySEM)\ngraph_sem(mm1.est, layout = get_layout(mm1.est))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartial/Complete Mediation\n\n\n\n\n\nIf we have a variable \\(X\\) that we take to cause variable \\(Y\\), then our path diagram will look like so:\n\n\n\n\n\n\n\n\n\nIn this diagram, path \\(c\\) is the total effect. This is the unmediated effect of \\(X\\) on \\(Y\\).\nHowever, while the effect of \\(X\\) on \\(Y\\) could in part be explained by the process of being mediated by some variable \\(M\\), the variable \\(X\\) could still affect \\(Y\\) directly.\nOur mediating model is shown below:\n\n\n\n\n\n\n\n\n\nIn this case, path \\(c'\\) is the direct effect.\n\nComplete mediation is when \\(X\\) no longer affects \\(Y\\) after \\(M\\) has been controlled (so path \\(c'\\) is not significantly different from zero).\nPartial mediation is when the path from \\(X\\) to \\(Y\\) is reduced in magnitude when the mediator \\(M\\) is introduced, but still different from zero.\n\nThe Proportion Mediated is the amount of the total effect of \\(X\\) to \\(Y\\) that goes via \\(M\\). i.e. \\(\\frac{a \\times b}{c}\\) in the images above.\n\n\n\n\n\nExercises: Conduct Problems\nThis week’s exercises focus on the technique of path analysis using data on conduct problems in adolescence.\n\nData: conduct problems\nA researcher has collected data on n=557 adolescents and would like to know whether there are associations between conduct problems (both aggressive and non-aggressive) and academic performance and whether the relations are mediated by the quality of relationships with teachers.\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nID\nParticipant ID\n\n\nAcad\nAcademic performance (standardised)\n\n\nTeach_r\nTeacher Relationship Quality (standardised)\n\n\nNon_agg\nNon-aggressive conduct problems (standardised)\n\n\nAgg\nAggressive conduct problems (standardised)\n\n\n\n\n\n\n\nThe data is available at https://uoepsy.github.io/data/cp_teachacad.csv\n\n\nQuestion 1\n\n\nFirst, read in the dataset from https://uoepsy.github.io/data/cp_teachacad.csv\n\n\n\n\n\nSolution\n\n\n\n\ncp_teach&lt;-read_csv(\"https://uoepsy.github.io/data/cp_teachacad.csv\")\nsummary(cp_teach)\n\n       ID           Acad           Teach_r         Non_agg           Agg       \n Min.   :  1   Min.   :-3.042   Min.   :-3.68   Min.   :-3.28   Min.   :-3.27  \n 1st Qu.:140   1st Qu.:-0.736   1st Qu.:-0.89   1st Qu.:-0.69   1st Qu.:-0.73  \n Median :279   Median :-0.026   Median : 0.01   Median :-0.09   Median :-0.02  \n Mean   :279   Mean   :-0.061   Mean   :-0.09   Mean   :-0.06   Mean   :-0.02  \n 3rd Qu.:418   3rd Qu.: 0.606   3rd Qu.: 0.62   3rd Qu.: 0.57   3rd Qu.: 0.68  \n Max.   :557   Max.   : 3.057   Max.   : 3.52   Max.   : 3.44   Max.   : 3.31  \n\n\n\n\n\n\nQuestion 2\n\n\nUse the sem() function in lavaan to specify and estimate a straightforward linear regression model to test whether aggressive and non-aggressive conduct problems significantly predict academic performance.\nHow do your results compare to those you obtain using the lm() function?\n\n\n\n\n\nSolution\n\n\n\n\n# we can fit the model in lavaan as follows:\n# first we specify the model using lavaan syntax\nsr_lavaan&lt;-'Acad~Non_agg+Agg'\n# next we can estimate the model using the sem() function\nsr_lavaan.est&lt;-sem(sr_lavaan, data=cp_teach)\n# we can inspect the results using the summary() function\nsummary(sr_lavaan.est)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         3\n\n  Number of observations                           557\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Acad ~                                              \n    Non_agg           0.182    0.057    3.178    0.001\n    Agg               0.318    0.057    5.599    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .Acad              0.943    0.057   16.688    0.000\n\n# the same model can be fit using lm():\n\nsr_lm&lt;-lm(Acad~Non_agg+Agg, data=cp_teach)\nsummary(sr_lm)\n\n\nCall:\nlm(formula = Acad ~ Non_agg + Agg, data = cp_teach)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8962 -0.5957  0.0073  0.6219  3.1325 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.0420     0.0414   -1.02   0.3097    \nNon_agg       0.1824     0.0576    3.17   0.0016 ** \nAgg           0.3181     0.0570    5.58  3.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.974 on 554 degrees of freedom\nMultiple R-squared:  0.194, Adjusted R-squared:  0.191 \nF-statistic: 66.5 on 2 and 554 DF,  p-value: &lt;2e-16\n\n\nWe can see that both non-aggressive and aggressive conduct problems significantly predict academic perfofmance. We can also see that we get the same results when we use the sem() function as we do when we use the lm() function. Lavaan will give essentially the same results as lm() for simple and multiple regression problems. However, if we have multiple outcome variables in our model it is advantageous to do this using path mediation model with lavaan. This allows us to include all the regressions in a single model.\n\n\n\n\nQuestion 3\n\n\nNow specify a model in which non-aggressive conduct problems have both a direct and indirect effect (via teacher relationships) on academic performance\n\n\n\n\n\nSolution\n\n\n\n\nmodel1&lt;-'\n    #we regress academic performance on non-aggressive conduct problems (the direct effect)\n    Acad~Non_agg\n    \n    #we regress academic peformance on teacher relationship quality\n    Acad~Teach_r\n    \n    #we regress teacher relationship quality on non-aggressive conduct problems\n    Teach_r~Non_agg \n'\n\n\n\n\n\nQuestion 4\n\n\nMake sure in your model you define (using the := operator) the indirect effect in order to test the hypothesis that non-aggressive conduct problems have both a direct and an indirect effect (via teacher relationships) on academic performance.\nFit the model and examine the 95% CI.\n\n\n\n\n\nSolution\n\n\n\n\n#model specification\nmodel1&lt;-'\n    Acad~Non_agg\n    #we label the two parameters that comprise the indirect effect b and c\n    Acad~b*Teach_r    \n    Teach_r~c*Non_agg  \n    \n    # the indirect effect is the product of b and c. We create a new parameter (ind) to estimate the indirect effect\n    ind:=b*c   \n'\n\n#model estimation\nmodel1.est&lt;-sem(model1, data=cp_teach, se='bootstrap') \n\n# we request bootstrapped standard errors to assess the signifance of the indirect effect\nsummary(model1.est, ci=T)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           557\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  Acad ~                                                                \n    Non_agg           0.158    0.058    2.723    0.006    0.048    0.280\n    Teach_r    (b)    0.328    0.049    6.701    0.000    0.233    0.426\n  Teach_r ~                                                             \n    Non_agg    (c)    0.769    0.034   22.945    0.000    0.697    0.835\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .Acad              0.919    0.058   15.923    0.000    0.812    1.032\n   .Teach_r           0.713    0.042   17.108    0.000    0.632    0.793\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ind               0.252    0.038    6.611    0.000    0.177    0.331\n\n\nWe can see that the 95% bootstrapped confidence interval for the indirect effect of non-aggressive conduct problems on academic performance (‘ind’) does not include zero. We can conclude that the indirect effect is significant at \\(p &lt;.05\\). The direct effect is also statistically significant at \\(p &lt; .05\\).\n\n\n\n\nQuestion 5\n\n\nSpecify a new parameter which is the total (direct+indirect) effect of non-aggressive conduct problems on academic performance.\nHint: we should have already got labels in our model for the constituent effects, so we can just use := to create a sum of them.\n\n\n\n\n\nSolution\n\n\n\nWe can create a new parameter that is the sum of the direct and indirect effect to evaluate the total effect of non-aggressive conduct problems on academic performance.\n\n#model specification\n\nmodel1&lt;-'\n    # we now also label the indirect effect of non-aggressive conduct problems on academic performance\n    Acad~a*Non_agg    \n    Acad~b*Teach_r    \n    Teach_r~c*Non_agg  \n    \n    ind:=b*c   \n    #the total effect is the indirect effect plus the direct effect\n    total:=b*c+a\n'\n\n#model estimation\nmodel1.est&lt;-sem(model1, data=cp_teach,se='bootstrap') \n\n# we request bootstrapped standard errors to assess the signifance of the indirect effect\nsummary(model1.est, ci=T)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           557\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  Acad ~                                                                \n    Non_agg    (a)    0.158    0.055    2.861    0.004    0.046    0.266\n    Teach_r    (b)    0.328    0.047    7.023    0.000    0.238    0.420\n  Teach_r ~                                                             \n    Non_agg    (c)    0.769    0.034   22.804    0.000    0.702    0.835\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .Acad              0.919    0.056   16.282    0.000    0.805    1.029\n   .Teach_r           0.713    0.042   17.122    0.000    0.633    0.802\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ind               0.252    0.037    6.808    0.000    0.178    0.323\n    total             0.410    0.041    9.892    0.000    0.326    0.491\n\n\n\n\n\n\n\nExercises: More Conduct Problems\n\nQuestion 6\n\n\nNow specify a model in which both aggressive and non-aggressive conduct problems have both direct and indirect effects (via teacher relationships) on academic performance. Include the parameters for the indirect effects.\n\n\n\n\n\nSolution\n\n\n\nWe now have two predictors, one mediator and one outcome (and two indirect effects, one for each predictor). We can represent this in two lines: one where we specify academic performance as the outcome variable and one where we specify teacher relationships (the mediator) as the outcome variable.\n\nmodel2&lt;-\n   'Acad~Agg+Non_agg+b*Teach_r\n    Teach_r~c1*Agg+c2*Non_agg\n   \n    ind1:=b*c1 #indirect effect for aggressive conduct problems\n    ind2:=b*c2 #indirect effect for non-aggressive conduct problems\n'\n\n\n\n\n\nQuestion 7\n\n\nNow estimate the model and test the significance of the indirect effects\n\n\n\n\n\nSolution\n\n\n\n\nmodel2.est&lt;-sem(model2,  data=cp_teach,se='bootstrap') \nsummary(model2.est, ci=T)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                           557\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  Acad ~                                                                \n    Agg               0.171    0.061    2.788    0.005    0.045    0.289\n    Non_agg           0.091    0.061    1.493    0.135   -0.026    0.210\n    Teach_r    (b)    0.256    0.055    4.665    0.000    0.154    0.375\n  Teach_r ~                                                             \n    Agg       (c1)    0.574    0.048   11.967    0.000    0.482    0.666\n    Non_agg   (c2)    0.358    0.043    8.389    0.000    0.276    0.441\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .Acad              0.908    0.057   15.903    0.000    0.787    1.013\n   .Teach_r           0.540    0.030   17.991    0.000    0.480    0.598\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ind1              0.147    0.034    4.358    0.000    0.088    0.218\n    ind2              0.092    0.023    4.048    0.000    0.052    0.141\n\n\nWe can see that the 95% confidence intervals for both indirect effects do not include zero, therefore, we can conclude that they are significant at \\(p &lt; .05\\).\n\n\n\n\nQuestion 8\n\n\nOpen powerpoint, or google drawings, or microsoft paint (!), and create a diagram of your estimated model.\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nFigure 3: Effect of conduct problems on academic performance mediated by quality of teacher relationship.\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nWrite a brief paragraph reporting on the results of the model estimates in Question B2. Include a figure or table to display the parameter estimates.\n\n\n\n\n\nSolution\n\n\n\n\nA path mediation model was used to test the direct and indirect effects (via teacher relationship quality) of aggressive and non-aggressive conduct problems on academic performance. In the model, academic performance was regressed on teacher relationship quality, non-aggressive conduct problems and aggressive conduct problems while teacher relationship quality (the mediator) was regressed on aggressive and non-aggressive conduct problems. The indirect effects were tested using the product of the coefficient for the regression of outcome on mediator and the coefficient for the regression of mediator on predictor. The statistical significance of the indirect effects were evaluated using boostrapped 95% confidence intervals with 1000 draws.\nUnstandardised parameter estimates are provided in Figure 4. Solid lines indicate that a parameter is significant at \\(p &lt;. 05\\), while dashed lines represent non-significant paths.The indirect effects of both non-aggressive (\\(b = 0.09\\), 95% CI=0.05-0.14) and aggressive (\\(b = 0.15\\), 95% CI=0.08-0.22) conduct problems on academic performance were statistically significant.\n\n\n\n\n\nFigure 4: Effect of conduct problems on academic performance mediated by quality of teacher relationship.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional: Mediation using separate models\n\n\n\n\n\nFollowing Baron & Kenny 1986, we can conduct mediation analysis by using three separate regression models.\n\n\\(m \\sim x\\)\n\\(y \\sim x + m\\)\n\nThe mediation package is a very nice approach that allows us to conduct mediation this way, and it also extends to allow us to have categorical variables (we can use a logistic regression as one of our models), or to have multilevel data (our models can be fitted using lme4).\nStep 1. Estimate the effect of the predictor on the mediator m ~ x:\n\nmod2 &lt;- lm(hlc ~ religiosity, data = vax)\nsummary(mod2)$coefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    1.643     0.2354    6.98 3.70e-10\nreligiosity    0.557     0.0899    6.19 1.43e-08\n\n\nStep 2. Estimate the effects of the predictor and mediator on the outcome y ~ x + m:\n\nmod3 &lt;- lm(intention ~ religiosity + hlc, data = vax)\nsummary(mod3)$coefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   46.109      2.655   17.36 2.23e-31\nreligiosity    0.616      0.978    0.63 5.30e-01\nhlc            5.823      0.935    6.23 1.23e-08\n\n\nStep 3. Estimate the mediation effects\n\nlibrary(mediation)\nmymediation &lt;- mediate(model.m = mod2, \n                       model.y = mod3, \n                       treat='religiosity', \n                       mediator='hlc',\n                       boot=TRUE, sims=500)\n\nsummary(mymediation)\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME              3.243        2.018         4.91  &lt;2e-16 ***\nADE               0.616       -1.568         2.75    0.66    \nTotal Effect      3.858        1.704         5.78  &lt;2e-16 ***\nProp. Mediated    0.840        0.454         1.80  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 99 \n\n\nSimulations: 500 \n\n\n\nACME: Average Causal Mediation Effects (the indirect path \\(\\mathbf{a \\times b}\\) in Figure 5).\n\nADE: Average Direct Effects (the direct path \\(\\mathbf{c'}\\) in Figure 5).\n\nTotal Effect: sum of the mediation (indirect) effect and the direct effect (this is \\(\\mathbf{(a \\times b) + c)}\\) in Figure 5).\n\n\n\n\n\n\nFigure 5: Mediation Model. The indirect effect is the product of paths a and b. c’ is the direct effect of x on y."
  },
  {
    "objectID": "09_pca.html",
    "href": "09_pca.html",
    "title": "Principal Component Analysis (PCA)",
    "section": "",
    "text": "Relevant packages\n\npsych"
  },
  {
    "objectID": "09_pca.html#explore",
    "href": "09_pca.html#explore",
    "title": "Principal Component Analysis (PCA)",
    "section": "1. Explore",
    "text": "1. Explore\n\nFirst things first, we should always plot and describe our data. This is always a sensible thing to do - while many of the datasets we give you are nice and clean and organised, the data you get out of questionnaire tools, experiment software etc, are almost always quite a bit messier. It is also very useful to just eyeball the patterns of relationships between variables.\n\n\nQuestion A1\n\n\nLoad the job performance data into R and call it job. Check whether or not the data were read correctly into R - do the dimensions correspond to the description of the data above?\n\n\n\n\n\nSolution\n\n\n\nLet’s load the data:\n\nlibrary(tidyverse)\n\njob &lt;- read_csv('https://uoepsy.github.io/data/police_performance.csv')\ndim(job)\n\n[1] 50  7\n\n\nThere are 50 observations on 6 variables.\nThe top 6 rows in the data are:\n\nhead(job)\n\n# A tibble: 6 × 7\n  commun probl_solv logical learn physical appearance arrest_rate\n   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1     12         52      20    44       48         16      0.613 \n2     12         57      25    45       50         16      0.419 \n3     12         54      21    45       50         16      0     \n4     13         52      21    46       51         17      0.645 \n5     14         54      24    46       51         17      0.0645\n6     14         48      20    47       51         18      0.645 \n\n\n\n\n\n\nQuestion A2\n\n\nProvide descriptive statistics for each variable in the dataset.\n\n\n\n\n\nSolution\n\n\n\nWe now inspect some descriptive statistics for each variable in the dataset:\n\n# Quick summary\nsummary(job)\n\n     commun       probl_solv      logical       learn         physical   \n Min.   :12.0   Min.   :48.0   Min.   :20   Min.   :44.0   Min.   :48.0  \n 1st Qu.:16.0   1st Qu.:52.2   1st Qu.:22   1st Qu.:48.0   1st Qu.:52.2  \n Median :18.0   Median :54.0   Median :24   Median :50.0   Median :54.0  \n Mean   :17.7   Mean   :54.2   Mean   :24   Mean   :50.3   Mean   :54.2  \n 3rd Qu.:19.8   3rd Qu.:56.0   3rd Qu.:26   3rd Qu.:52.0   3rd Qu.:56.0  \n Max.   :24.0   Max.   :59.0   Max.   :31   Max.   :56.0   Max.   :59.0  \n   appearance    arrest_rate   \n Min.   :16.0   Min.   :0.000  \n 1st Qu.:19.0   1st Qu.:0.371  \n Median :21.0   Median :0.565  \n Mean   :21.1   Mean   :0.512  \n 3rd Qu.:23.0   3rd Qu.:0.669  \n Max.   :28.0   Max.   :0.935  \n\n\nOPTIONAL\nIf you wish to create a nice looking table for a report, you could try the following code. However, I should warn you: this code is quite difficult to understand so, if you are interested, attend a lab!\n\nlibrary(gt)\n\n# Mean and SD of each variable\njob %&gt;% \n  summarise(across(everything(), list(M = mean, SD = sd))) %&gt;%\n  pivot_longer(everything()) %&gt;% \n  mutate(\n    value = round(value, 2),\n    name = str_replace(name, '_M', '.M'),\n    name = str_replace(name, '_SD', '.SD')\n  ) %&gt;%\n  separate(name, into = c('variable', 'summary'), sep = '\\\\.') %&gt;%\n  pivot_wider(names_from = summary, values_from = value) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n    \n      variable\n      M\n      SD\n    \n  \n  \n    commun\n17.68\n2.74\n    probl_solv\n54.16\n2.41\n    logical\n24.02\n2.49\n    learn\n50.28\n2.84\n    physical\n54.16\n2.41\n    appearance\n21.06\n2.99\n    arrest_rate\n0.51\n0.23"
  },
  {
    "objectID": "09_pca.html#is-pca-needed",
    "href": "09_pca.html#is-pca-needed",
    "title": "Principal Component Analysis (PCA)",
    "section": "2. Is PCA needed?",
    "text": "2. Is PCA needed?\n\nThere are many reasons we might want to reduce the dimensionality of data:\n\nTheory testing\n\nWhat are the number and nature of dimensions that best describe a theoretical construct?\n\nTest construction\n\nHow should I group my items into sub-scales?\nWhich items are the best measures of my constructs?\n\nPragmatic\n\nI have multicollinearity issues/too many variables, how can I defensibly combine my variables?\n\n\nPCA is most often used for the latter - we are less interested in the theory behind our items, we just want a useful way of simplifying lots of variables down to a smaller number.\nRecall that we are wanting to see how well the skills ratings predict arrest rate.\nWe might fit this model:\n\nmod &lt;- lm(arrest_rate ~ commun + probl_solv + logical + learn + physical + appearance, data = job)\n\nHowever, we might have reason to think that many of these predictors might be quite highly correlated with one another, and so we may be unable to draw accurate inferences. This is borne out in our variance inflation factor (VIF):\n\nlibrary(car)\nvif(mod)\n\n    commun probl_solv    logical      learn   physical appearance \n     34.67       1.17       1.23      43.56      34.98      21.78 \n\n\nAs the original variables are highly correlated, it is possible to reduce the dimensionality of the problem under investigation without losing too much information.\nOn the other side, if the correlation between the variables under study are weak, a larger number of components is needed in order to explain sufficient variability.\n\n\nQuestion A3\n\n\nWorking with only the skills ratings (not the arrest rate - we’ll come back to that right at the end), investigate whether or not the variables are highly correlated and explain whether or not you PCA might be useful in this case.\nHint: We only have 6 variables here, but if we had many, how might you visualise cor(job)?\n\n\n\n\n\nSolution\n\n\n\nLet’s start by looking at the correlation matrix of the data:\n\nlibrary(pheatmap)\n\njob_skills &lt;- job %&gt;% select(-arrest_rate)\n\nR &lt;- cor(job_skills)\n\npheatmap(R, breaks = seq(-1, 1, length.out = 100))\n\n\n\n\nFigure 1: Correlation between the variables in the ``Job’’ dataset\n\n\n\n\nThe correlation between the variables seems to be quite large (it doesn’t matter about direction here, only magnitude; if negative correlations were present, we would think in absolute value).\nThere appears to be a group of highly correlated variables comprising physical ability, appearance, communication skills, and learning ability which are correlated among themselves but uncorrelated with another group of variables. The second group comprises problem solving and logical ability.\nThis suggests that PCA might be useful in this problem to reduce the dimensionality without a significant loss of information."
  },
  {
    "objectID": "09_pca.html#cov-vs-cor",
    "href": "09_pca.html#cov-vs-cor",
    "title": "Principal Component Analysis (PCA)",
    "section": "3. Cov vs Cor?",
    "text": "3. Cov vs Cor?\n\nShould we perform PCA on the covariance or the correlation matrix?\nThis depends on the variances of the variables in the dataset. If the variables have large differences in their variances, then the variables with the largest variances will tend to dominate the first few principal components.\nA solution to this is to standardise the variables prior to computing the covariance matrix - i.e., compute the correlation matrix!\n\n# show that the correlation matrix and the covariance matrix of the standardized variables are identical\nall.equal(cor(job_skills), cov(scale(job_skills)))\n\n[1] TRUE\n\n\n\n\nQuestion A4\n\n\nLook at the variance of the skills ratings in the data set. Do you think that PCA should be carried on the covariance matrix or the correlation matrix?\n\n\n\n\n\nSolution\n\n\n\nLet’s have a look at the standard deviation of each variable:\n\njob_skills %&gt;% \n  summarise(across(everything(), sd))\n\n# A tibble: 1 × 6\n  commun probl_solv logical learn physical appearance\n   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1   2.74       2.41    2.49  2.84     2.41       2.99\n\n\nAs the standard deviations appear to be fairly similar (and so will the variances) we can perform PCA using the covariance matrix."
  },
  {
    "objectID": "09_pca.html#perform-pca",
    "href": "09_pca.html#perform-pca",
    "title": "Principal Component Analysis (PCA)",
    "section": "4. Perform PCA",
    "text": "4. Perform PCA\n\nQuestion A5\n\n\nUsing the principal() function from the psych package, we can perform a PCA of the job performance data, Call the output job_pca.\njob_pca &lt;- principal(job_skills, nfactors = ncol(job_skills), covar = ..., rotate = 'none')\njob_pca$loadings\nDepending on your answer to the previous question, either set covar = TRUE or covar = FALSE within the principal() function.\nWarning: the output of the function will be in terms of standardized variables nevertheless. So you will see output with standard deviation of 1.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(psych)\n\njob_pca &lt;- principal(job_skills, nfactors = ncol(job_skills), covar = TRUE, rotate = 'none')\n\n\n\n\n\nPCA OUTPUT\nWe can print the output by just typing the name of our PCA:\n\njob_pca\n\nPrincipal Components Analysis\nCall: principal(r = job_skills, nfactors = ncol(job_skills), rotate = \"none\", \n    covar = TRUE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n            PC1   PC2   PC3   PC4   PC5   PC6 h2       u2 com\ncommun     0.98 -0.12  0.02 -0.06  0.10 -0.05  1  6.7e-16 1.1\nprobl_solv 0.22  0.81  0.54  0.00  0.00  0.00  1  7.8e-16 1.9\nlogical    0.33  0.75 -0.58  0.00  0.00  0.00  1  8.9e-16 2.3\nlearn      0.99 -0.11  0.02 -0.05  0.00  0.11  1  0.0e+00 1.1\nphysical   0.99 -0.08  0.01 -0.05 -0.11 -0.05  1 -4.4e-16 1.0\nappearance 0.98 -0.13  0.02  0.16  0.01  0.00  1  3.3e-16 1.1\n\n                       PC1  PC2  PC3  PC4  PC5  PC6\nSS loadings           4.04 1.26 0.63 0.03 0.02 0.02\nProportion Var        0.67 0.21 0.11 0.01 0.00 0.00\nCumulative Var        0.67 0.88 0.99 0.99 1.00 1.00\nProportion Explained  0.67 0.21 0.11 0.01 0.00 0.00\nCumulative Proportion 0.67 0.88 0.99 0.99 1.00 1.00\n\nMean item complexity =  1.4\nTest of the hypothesis that 6 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0 \n with the empirical chi square  0  with prob &lt;  NA \n\nFit based upon off diagonal values = 1\n\n\nThe output is made up of two parts.\nFirst, it shows the loading matrix. In each column of the loading matrix we find how much each of the measured variables contributes to the computed new axis/direction (that is, the principal component). Notice that there are as many principal components as variables.\nThe second part of the output displays the contribution of each component to the total variance:\n\nSS loadings: The sum of the squared loadings. The eigenvalues (see Lecture).\n\nProportion Var: how much of the overall variance the component accounts for out of all the variables.\nCumulative Var: cumulative sum of Proportion Var.\nProportion Explained: relative amount of variance explained (\\(\\frac{\\text{Proportion Var}}{\\text{sum(Proportion Var)}}\\).\nCumulative Proportion: cumulative sum of Proportion Explained.\n\nLet’s focus on the row of that output called “Cumulative Var”. This displays the cumulative sum of the variances of each principal component. Taken all together, the six principal components taken explain all of the total variance in the original data. In other words, the total variance of the principal components (the sum of their variances) is equal to the total variance in the original data (the sum of the variances of the variables).\nHowever, our goal is to reduce the dimensionality of our data, so it comes natural to wonder which of the six principal components explain most of the variability, and which components instead do not contribute substantially to the total variance.\nTo that end, the second row “Proportion Var” displays the proportion of the total variance explained by each component, i.e. the variance of the principal component divided by the total variance.\nThe last row, as we saw, is the cumulative proportion of explained variance: 0.67, 0.67 + 0.21, 0.67 + 0.21 + 0.11, and so on.\nWe also notice that the first PC alone explains 67.3% of the total variability, while the first two components together explain almost 90% of the total variability. From the third component onwards, we do not see such a sharp increase in the proportion of explained variance, and the cumulative proportion slowly reaches the total ratio of 1 (or 100%).\n\n\n\nOptional: (some of) the math behind it\n\n\n\nDoing data reduction can feel a bit like magic, and in part that’s just because it’s quite complicated.\nThe intuition\nConsider one way we might construct a correlation matrix - as the product of vector \\(\\mathbf{f}\\) with \\(\\mathbf{f'}\\) (f transposed): \\[\n\\begin{equation*}\n\\mathbf{f} =\n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix}\n\\qquad\n\\mathbf{f} \\mathbf{f'} =\n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0.9, 0.8, 0.7, 0.6, 0.5, 0.4 \\\\\n\\end{bmatrix}\n\\qquad = \\qquad\n\\begin{bmatrix}\n0.81, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 0.64, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 0.49, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 0.36, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 0.25, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 0.16 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nBut we constrain this such that the diagonal has values of 1 (the correlation of a variable with itself is 1), and lets call it R. \\[\n\\begin{equation*}\n\\mathbf{R} =\n\\begin{bmatrix}\n1.00, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 1.00, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 1.00, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 1.00, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 1.00, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 1.00 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nPCA is about trying to determine a vector f which generates the correlation matrix R. a bit like unscrambling eggs!\nin PCA, we express \\(\\mathbf{R = CC'}\\), where \\(\\mathbf{C}\\) are our principal components.\nIf \\(n\\) is number of variables in \\(R\\), then \\(i^{th}\\) component \\(C_i\\) is the linear sum of each variable multiplied by some weighting:\n\\[\nC_i = \\sum_{j=1}^{n}w_{ij}x_{j}\n\\]\nHow do we find \\(C\\)?\nThis is where “eigen decomposition” comes in.\nFor the \\(n \\times n\\) correlation matrix \\(\\mathbf{R}\\), there is an eigenvector \\(x_i\\) that solves the equation \\[\n\\mathbf{x_i R} = \\lambda_i \\mathbf{x_i}\n\\] Where the vector multiplied by the correlation matrix is equal to some eigenvalue \\(\\lambda_i\\) multiplied by that vector.\nWe can write this without subscript \\(i\\) as: \\[\n\\begin{align}\n& \\mathbf{R X} = \\mathbf{X \\lambda} \\\\\n& \\text{where:} \\\\\n& \\mathbf{R} = \\text{correlation matrix} \\\\\n& \\mathbf{X} = \\text{matrix of eigenvectors} \\\\\n& \\mathbf{\\lambda} = \\text{vector of eigenvalues}\n\\end{align}\n\\] the vectors which make up \\(\\mathbf{X}\\) must be orthogonal (\\(\\mathbf{XX' = I}\\)), which means that \\(\\mathbf{R = X \\lambda X'}\\)\nWe can actually do this in R manually. Creating a correlation matrix\n\n# lets create a correlation matrix, as the produce of ff'\nf &lt;- seq(.9,.4,-.1)\nR &lt;- f %*% t(f)\n#give rownames and colnames\nrownames(R)&lt;-colnames(R)&lt;-paste0(\"V\",seq(1:6))\n#constrain diagonals to equal 1\ndiag(R)&lt;-1\nR\n\n     V1   V2   V3   V4   V5   V6\nV1 1.00 0.72 0.63 0.54 0.45 0.36\nV2 0.72 1.00 0.56 0.48 0.40 0.32\nV3 0.63 0.56 1.00 0.42 0.35 0.28\nV4 0.54 0.48 0.42 1.00 0.30 0.24\nV5 0.45 0.40 0.35 0.30 1.00 0.20\nV6 0.36 0.32 0.28 0.24 0.20 1.00\n\n\nEigen Decomposition\n\n# do eigen decomposition\ne &lt;- eigen(R)\nprint(e, digits=2)\n\neigen() decomposition\n$values\n[1] 3.16 0.82 0.72 0.59 0.44 0.26\n\n$vectors\n      [,1]   [,2]   [,3]  [,4]   [,5]   [,6]\n[1,] -0.50 -0.061  0.092  0.14  0.238  0.816\n[2,] -0.47 -0.074  0.121  0.21  0.657 -0.533\n[3,] -0.43 -0.096  0.182  0.53 -0.675 -0.184\n[4,] -0.39 -0.142  0.414 -0.78 -0.201 -0.104\n[5,] -0.34 -0.299 -0.860 -0.20 -0.108 -0.067\n[6,] -0.28  0.934 -0.178 -0.10 -0.067 -0.045\n\n\nThe eigenvectors are orthogonal (\\(\\mathbf{XX' = I}\\)):\n\nround(e$vectors %*% t(e$vectors),2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0    0\n[2,]    0    1    0    0    0    0\n[3,]    0    0    1    0    0    0\n[4,]    0    0    0    1    0    0\n[5,]    0    0    0    0    1    0\n[6,]    0    0    0    0    0    1\n\n\nThe Principal Components \\(\\mathbf{C}\\) are the eigenvectors scaled by the square root of the eigenvalues:\n\n#eigenvectors\ne$vectors\n\n       [,1]    [,2]    [,3]   [,4]    [,5]    [,6]\n[1,] -0.496 -0.0611  0.0923  0.139  0.2385  0.8155\n[2,] -0.468 -0.0743  0.1210  0.214  0.6566 -0.5327\n[3,] -0.433 -0.0963  0.1820  0.530 -0.6751 -0.1842\n[4,] -0.390 -0.1416  0.4143 -0.778 -0.2006 -0.1036\n[5,] -0.340 -0.2992 -0.8604 -0.197 -0.1076 -0.0669\n[6,] -0.282  0.9338 -0.1784 -0.100 -0.0667 -0.0452\n\n#scaled by sqrt of eigenvalues\ndiag(sqrt(e$values))\n\n     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] 1.78 0.000 0.000 0.000 0.000 0.000\n[2,] 0.00 0.906 0.000 0.000 0.000 0.000\n[3,] 0.00 0.000 0.848 0.000 0.000 0.000\n[4,] 0.00 0.000 0.000 0.769 0.000 0.000\n[5,] 0.00 0.000 0.000 0.000 0.664 0.000\n[6,] 0.00 0.000 0.000 0.000 0.000 0.512\n\nC &lt;- e$vectors %*% diag(sqrt(e$values))\nC\n\n       [,1]    [,2]    [,3]    [,4]    [,5]    [,6]\n[1,] -0.883 -0.0554  0.0782  0.1070  0.1584  0.4174\n[2,] -0.833 -0.0673  0.1025  0.1648  0.4361 -0.2727\n[3,] -0.770 -0.0873  0.1542  0.4077 -0.4483 -0.0943\n[4,] -0.694 -0.1284  0.3512 -0.5987 -0.1332 -0.0530\n[5,] -0.604 -0.2712 -0.7293 -0.1514 -0.0715 -0.0342\n[6,] -0.502  0.8464 -0.1513 -0.0771 -0.0443 -0.0231\n\n\nAnd we can reproduce our correlation matrix, because \\(\\mathbf{R = CC'}\\).\n\nC %*% t(C)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.72 0.63 0.54 0.45 0.36\n[2,] 0.72 1.00 0.56 0.48 0.40 0.32\n[3,] 0.63 0.56 1.00 0.42 0.35 0.28\n[4,] 0.54 0.48 0.42 1.00 0.30 0.24\n[5,] 0.45 0.40 0.35 0.30 1.00 0.20\n[6,] 0.36 0.32 0.28 0.24 0.20 1.00\n\n\nNow lets imagine we only consider 1 principal component.\nWe can do this with the principal() function:\n\nlibrary(psych)\npc1&lt;-principal(R, nfactors = 1, covar = FALSE, rotate = 'none')\npc1\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 1, rotate = \"none\", covar = FALSE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n    PC1   h2   u2 com\nV1 0.88 0.78 0.22   1\nV2 0.83 0.69 0.31   1\nV3 0.77 0.59 0.41   1\nV4 0.69 0.48 0.52   1\nV5 0.60 0.37 0.63   1\nV6 0.50 0.25 0.75   1\n\n                PC1\nSS loadings    3.16\nProportion Var 0.53\n\nMean item complexity =  1\nTest of the hypothesis that 1 component is sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n\nFit based upon off diagonal values = 0.95\n\n\nLook familiar? It looks like the first component we computed manually. The first column of \\(\\mathbf{C}\\):\n\ncbind(pc1$loadings, C=C[,1])\n\n     PC1      C\nV1 0.883 -0.883\nV2 0.833 -0.833\nV3 0.770 -0.770\nV4 0.694 -0.694\nV5 0.604 -0.604\nV6 0.502 -0.502\n\n\nWe can now ask “how well does this component (on its own) recreate our correlation matrix?”\n\nC[,1] %*% t(C[,1])\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] 0.780 0.735 0.680 0.613 0.534 0.444\n[2,] 0.735 0.693 0.641 0.578 0.503 0.418\n[3,] 0.680 0.641 0.592 0.534 0.465 0.387\n[4,] 0.613 0.578 0.534 0.481 0.419 0.348\n[5,] 0.534 0.503 0.465 0.419 0.365 0.304\n[6,] 0.444 0.418 0.387 0.348 0.304 0.252\n\n\nIt looks close, but not quite. How much not quite? Measurably so!\n\nR - (C[,1] %*% t(C[,1]))\n\n        V1      V2      V3      V4      V5      V6\nV1  0.2200 -0.0154 -0.0498 -0.0727 -0.0838 -0.0836\nV2 -0.0154  0.3067 -0.0809 -0.0976 -0.1033 -0.0982\nV3 -0.0498 -0.0809  0.4075 -0.1140 -0.1153 -0.1066\nV4 -0.0727 -0.0976 -0.1140  0.5187 -0.1193 -0.1085\nV5 -0.0838 -0.1033 -0.1153 -0.1193  0.6346 -0.1036\nV6 -0.0836 -0.0982 -0.1066 -0.1085 -0.1036  0.7477\n\n\nNotice the values on the diagonals of \\(\\mathbf{c_1}\\mathbf{c_1}'\\).\n\ndiag(C[,1] %*% t(C[,1]))\n\n[1] 0.780 0.693 0.592 0.481 0.365 0.252\n\n\nThese aren’t 1, like they are in \\(R\\). But they are proportional: this is the amount of variance in each observed variable that is explained by this first component. Sound familiar?\n\npc1$communality\n\n   V1    V2    V3    V4    V5    V6 \n0.780 0.693 0.592 0.481 0.365 0.252 \n\n\nAnd likewise the 1 minus these is the unexplained variance:\n\n1 - diag(C[,1] %*% t(C[,1]))\n\n[1] 0.220 0.307 0.408 0.519 0.635 0.748\n\npc1$uniquenesses\n\n   V1    V2    V3    V4    V5    V6 \n0.220 0.307 0.408 0.519 0.635 0.748"
  },
  {
    "objectID": "09_pca.html#how-many-components-to-keep",
    "href": "09_pca.html#how-many-components-to-keep",
    "title": "Principal Component Analysis (PCA)",
    "section": "5. How many components to keep?",
    "text": "5. How many components to keep?\n\nThere is no single best method to select the optimal number of components to keep, while discarding the remaining ones (which are then considered as noise components).\nThe following three heuristic rules are commonly used in the literature:\n\nThe cumulative proportion of explained variance criterion\nKaiser’s rule\nThe scree plot\nVelicer’s Minimum Average Partial method\nParallel analysis\n\nIn the next sections we will analyse each of them in turn.\n\n\nThe cumulative proportion of explained variance criterion\nThe rule suggests to keep as many principal components as needed in order to explain approximately 80-90% of the total variance.\n\n\nQuestion A6\n\n\nLooking again at the PCA output, how many principal components would you keep if you were following the cumulative proportion of explained variance criterion?\n\n\n\n\n\nSolution\n\n\n\nLet’s look again at the PCA summary:\n\njob_pca$loadings\n\n\nLoadings:\n           PC1    PC2    PC3    PC4    PC5    PC6   \ncommun      0.984 -0.120                0.101       \nprobl_solv  0.223  0.810  0.543                     \nlogical     0.329  0.747 -0.578                     \nlearn       0.987 -0.110                       0.105\nphysical    0.988                      -0.110       \nappearance  0.979 -0.125         0.161              \n\n                 PC1   PC2   PC3   PC4   PC5   PC6\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nProportion Var 0.673 0.210 0.105 0.006 0.004 0.003\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n\n\nThe following part of the output tells us that the first two components explain 88.3% of the total variance.\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\nAccording to this criterion, we should keep 2 principal components.\n\n\n\n\nKaiser’s rule\nAccording to Kaiser’s rule, we should keep the principal components having variance larger than 1. Standardized variables have a variance equal 1. Because we have 6 variables in the data set, and the total variance is 6, the value 1 represents the average variance in the data: \\[\n\\frac{1 + 1 + 1 + 1 + 1 + 1}{6} = 1\n\\]\nHint:\nThe variances of each PC are shown in the row of the output named SS loadings and also in job_pca$values. The average variance is:\n\nmean(job_pca$values)\n\n[1] 1\n\n\n\n\nQuestion A7\n\n\nLooking again at the PCA output, how many principal components would you keep if you were following Kaiser’s criterion?\n\n\n\n\n\nSolution\n\n\n\n\njob_pca$loadings\n\n\nLoadings:\n           PC1    PC2    PC3    PC4    PC5    PC6   \ncommun      0.984 -0.120                0.101       \nprobl_solv  0.223  0.810  0.543                     \nlogical     0.329  0.747 -0.578                     \nlearn       0.987 -0.110                       0.105\nphysical    0.988                      -0.110       \nappearance  0.979 -0.125         0.161              \n\n                 PC1   PC2   PC3   PC4   PC5   PC6\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nProportion Var 0.673 0.210 0.105 0.006 0.004 0.003\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n\n\nThe variances are shown in the row\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nFrom the result we see that only the first two principal components have variance greater than 1, so this rule suggests to keep 2 PCs only.\n\n\n\n\nThe scree plot\nThe scree plot is a graphical criterion which involves plotting the variance for each principal component. This can be easily done by calling plot on the variances, which are stored in job_pca$values\n\nplot(x = 1:length(job_pca$values), y = job_pca$values, \n     type = 'b', xlab = '', ylab = 'Variance', \n     main = 'Police officers: scree plot', frame.plot = FALSE)\n\n\n\n\n\n\n\n\nwhere the argument type = 'b' tells R that the plot should have both points and lines.\nA typical scree plot features higher variances for the initial components and quickly drops to small variances where the curve is almost flat. The flat part of the curve represents the noise components, which are not able to capture the main sources of variability in the system.\nAccording to the scree plot criterion, we should keep as many principal components as where the “elbow” in the plot occurs. By elbow we mean the variance before the curve looks almost flat.\nAlternatively, some people prefer to use the function scree() from the psych package:\n\nscree(job_skills, factors = FALSE)\n\n\n\n\n\n\n\n\nThis also draws a horizontal line at y = 1. So, if you are making a decision about how many PCs to keep by looking at where the plot falls below the y = 1 line, you are basically following Kaiser’s rule. In fact, Kaiser’s criterion tells you to keep as many PCs as are those with a variance (= eigenvalue) greater than 1.\n\n\nQuestion A8\n\n\nAccording to the scree plot, how many principal components would you retain?\n\n\n\n\n\nSolution\n\n\n\nThis criterion then suggests to keep three principal components.\n\n\n\n\nVelicer’s Minimum Average Partial method\nThe Minimum Average Partial (MAP) test computes the partial correlation matrix (removing and adjusting for a component from the correlation matrix), sequentially partialling out each component. At each step, the partial correlations are squared and their average is computed.\nAt first, the components which are removed will be those that are most representative of the shared variance between 2+ variables, meaning that the “average squared partial correlation” will decrease. At some point in the process, the components being removed will begin represent variance that is specific to individual variables, meaning that the average squared partial correlation will increase.\nThe MAP method is to keep the number of components for which the average squared partial correlation is at the minimum.\nWe can conduct MAP in R using:\n\nVSS(data, plot = FALSE, method=\"pc\", n = ncol(data))\n\n(be aware there is a lot of other information in this output too! For now just focus on the map column)\n\n\nQuestion A9\n\n\nHow many components should we keep according to the MAP method?\n\n\n\n\n\nSolution\n\n\n\n\njob_map &lt;- VSS(job_skills, plot=FALSE, method=\"pc\", n = ncol(job_skills))$map\npaste(\"MAP is lowest for\", which.min(job_map), \"components\")\n\n[1] \"MAP is lowest for 2 components\"\n\n\nAccording to the MAP criterion we should keep 2 principal components.\n\n\n\n\nParallel analysis\nParallel analysis involves simulating lots of datasets of the same dimension but in which the variables are uncorrelated. For each of these simulations, a PCA is conducted on its correlation matrix, and the eigenvalues are extracted. We can then compare our eigenvalues from the PCA on our actual data to the average eigenvalues across these simulations. In theory, for uncorrelated variables, no components should explain more variance than any others, and eigenvalues should be equal to 1. In reality, variables are rarely truly uncorrelated, and so there will be slight variation in the magnitude of eigenvalues simply due to chance. The parallel analysis method suggests keeping those components for which the eigenvalues are greater than those from the simulations.\nIt can be conducted in R using:\n\nfa.parallel(job_skills, fa=\"pc\", quant=.95)\n\n\n\nQuestion A10\n\n\nHow many components should we keep according to parallel analysis?\n\n\n\n\n\nSolution\n\n\n\n\nfa.parallel(job_skills, fa=\"pc\", quant=.95)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1 \n\n\nParallel analysis suggests to keep 1 principal component only as there is only one PC with an eigenvalue higher than the simulated random ones in red.\n\n\n\n\nInterpretation\nBecause three out of the five selection criteria introduced above suggest to keep 2 principal components, in the following we will work with the first two PCs only.\nLet’s have a look at the selected principal components:\n\njob_pca$loadings[, 1:2]\n\n             PC1     PC2\ncommun     0.984 -0.1197\nprobl_solv 0.223  0.8095\nlogical    0.329  0.7466\nlearn      0.987 -0.1097\nphysical   0.988 -0.0784\nappearance 0.979 -0.1253\n\n\nand at their corresponding proportion of total variance explained:\n\njob_pca$values / sum(job_pca$values)\n\n[1] 0.67253 0.21016 0.10510 0.00577 0.00372 0.00273\n\n\nWe see that the first PC accounts for 67.3% of the total variability. All loadings seem to have the same magnitude apart from probl_solv and logical which are closer to zero. The first component looks like a sort of average of the officers performance scores excluding problem solving and logical ability.\nThe second principal component, which explains only 21% of the total variance, has two loadings clearly distant from zero: the ones associated to problem solving and logical ability. It distinguishes police officers with strong logical and problem solving skills and low scores on other skills (note the negative magnitudes).\nWe have just seen how to interpret the first components by looking at the magnitude and sign of the coefficients for each measured variable.\n\nFor interpretation purposes, it might help hiding very small loadings. This can be done by specifying the cutoff value in the print() function. However, this only works when you pass the loadings for all the PCs:\n\nprint(job_pca$loadings, cutoff = 0.3)\n\n\nLoadings:\n           PC1    PC2    PC3    PC4    PC5    PC6   \ncommun      0.984                                   \nprobl_solv         0.810  0.543                     \nlogical     0.329  0.747 -0.578                     \nlearn       0.987                                   \nphysical    0.988                                   \nappearance  0.979                                   \n\n                 PC1   PC2   PC3   PC4   PC5   PC6\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nProportion Var 0.673 0.210 0.105 0.006 0.004 0.003\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n\n\n\n\n\n\nOptional: How well are the units represented in the reduced space?\n\n\n\nWe now focus our attention on the following question: Are all the statistical units (police officers) well represented in the 2D plot?\nThe 2D representation of the original data, which comprise 6 measured variables, is an approximation and henceforth it may happen that not all units are well represented in this new space.\nTypically, it is good to assess the approximation for each statistical unit by inspecting the scores on the discarded principal components. If a unit has a high score on those components, then this is a sign that the unit might be highly misplaced in the new space and misrepresented.\nConsider the 3D example below. There are three cases (= units or individuals). In the original space they are all very different from each other. For example, cases 1 and 2 are very different in their x and y values, but very similar in their z value. Cases 2 and 3 are very similar in their x and y values but very different in their z value. Cases 1 and 3 have very different values for all three variables x, y, and z.\nHowever, when represented in the 2D space given by the two principal components, units 2 and 3 seems like they are very similar when, in fact, they were very different in the original space which also accounted for the z variable.\n\n\n\n\n\n\n\n\n\nWe typically measure how badly a unit is represented in the new coordinate system by considering the sum of squared scores on the discarded principal components:\n\nscores_discarded &lt;- job_pca$scores[, -(1:2)]\nsum_sq &lt;- rowSums(scores_discarded^2)\nsum_sq\n\n [1]  28.51  46.89  63.69  64.24  36.58  17.39  49.24  35.10  18.56  19.27\n[11]  18.56  24.44  12.39  59.10  24.43  33.18  13.40  12.69  11.22  78.87\n[21]  14.16  34.18  95.57  18.40  16.45  14.41  31.97  33.52  40.12  32.48\n[31]  16.85  24.85  30.84  16.00  29.59  11.01   8.07  18.18  14.60  23.73\n[41]  29.82  41.37   9.30  65.42  21.98  63.97  36.09  84.98 129.65  88.00\n\n\nUnits with a high score should be considered for further inspection as it may happen that they are represented as close to another unit when, in fact, they might be very different.\n\nboxplot(sum_sq)\n\n\n\n\n\n\n\n\nThere seem to be only five outliers, and they are not too high compared to the rest of the scores. For this reason, we will consider the 2D representation of the data to be satisfactory."
  },
  {
    "objectID": "09_pca.html#using-pca-scores",
    "href": "09_pca.html#using-pca-scores",
    "title": "Principal Component Analysis (PCA)",
    "section": "6. Using PCA scores",
    "text": "6. Using PCA scores\nSupposing that we decide to reduce our six variables down to two principal components:\n\njob_pca2 &lt;- principal(job_skills, nfactors = 2, covar = TRUE, rotate = 'none')\n\n\nWe can, for each of our observations, get their scores on each of our components.\n\nhead(job_pca2$scores)\n\n       PC1    PC2\n[1,] -6.10 -1.796\n[2,] -4.69  4.164\n[3,] -5.18 -0.131\n[4,] -4.31 -1.758\n[5,] -3.71  1.207\n[6,] -3.88 -5.200\n\n\n\nIn the literature, some authors also suggest to look at the correlation between each principal component and the measured variables:\n\n# First PC\ncor(job_pca2$scores[,1], job_skills)\n\n     commun probl_solv logical learn physical appearance\n[1,]  0.985      0.214   0.319 0.988    0.989      0.981\n\n\nThe first PC is strongly correlated with all the measured variables except probl_solv and logical. As we mentioned above, all variables seem to contributed to the first PC.\n\n# Second PC\ncor(job_pca2$scores[,2], job_skills)\n\n     commun probl_solv logical  learn physical appearance\n[1,] -0.163      0.792   0.738 -0.154   -0.122     -0.169\n\n\nThe second PC is strongly correlated with probl_solv and logical, and slightly negatively correlated with the remaining variables. This separates police officers with clear logical and problem solving skills and a low rating on other skills.\n\nQuestion A11\n\n\nWe have reduced our six variables down to two principal components, and we are now able to use the scores on each component in a subsequent analysis!\nJoin the two PC scores to the original dataset with the arrest rates in. Then fit a linear model to look at how the arrest rate of police officers is predicted by the two components representing different composites of the skills ratings by HR.\n\n\n\n\n\nSolution\n\n\n\n\n# add the PCA scores to the dataset\njob &lt;- \n  job %&gt;% mutate(\n    pc1 = job_pca2$scores[,1],\n    pc2 = job_pca2$scores[,2]\n  )\n# use the scores in an analysis\nmod &lt;- lm(arrest_rate ~ pc1 + pc2, data = job)\n\n# multicollinearity isn't a problem, because the components are orthogonal!! \nlibrary(car)\nvif(mod)\n\npc1 pc2 \n  1   1 \n\nsummary(mod)\n\n\nCall:\nlm(formula = arrest_rate ~ pc1 + pc2, data = job)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.385 -0.119 -0.016  0.160  0.384 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.51161    0.02943   17.38   &lt;2e-16 ***\npc1          0.03583    0.01089    3.29   0.0019 ** \npc2         -0.00923    0.01210   -0.76   0.4490    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.208 on 47 degrees of freedom\nMultiple R-squared:   0.2,  Adjusted R-squared:  0.166 \nF-statistic: 5.86 on 2 and 47 DF,  p-value: 0.00532\n\n\n\n\n\n\nPlotting PCA scores\nWe can also visualise the statistical units (police officers) in the reduced space given by the retained principal component scores.\n\ntibble(pc1 = job_pca2$scores[, 1],\n       pc2 = job_pca2$scores[, 2]) %&gt;%\n  ggplot(.,aes(x=pc1,y=pc2))+\n  geom_point()"
  },
  {
    "objectID": "09_pca.html#footnotes",
    "href": "09_pca.html#footnotes",
    "title": "Principal Component Analysis (PCA)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEven if we cut open someone’s brain, it’s unclear what we would be looking for in order to ‘measure’ it. It is unclear whether anxiety even exists as a physical thing, or rather if it is simply the overarching concept we apply to a set of behaviours and feelings↩︎"
  },
  {
    "objectID": "10_efa.html",
    "href": "10_efa.html",
    "title": "Exploratory Factor Analysis (EFA): Part 1",
    "section": "",
    "text": "Relevant packages\n\npsych\nGPArotation"
  },
  {
    "objectID": "10_efa.html#check-suitability",
    "href": "10_efa.html#check-suitability",
    "title": "Exploratory Factor Analysis (EFA): Part 1",
    "section": "1. Check Suitability",
    "text": "1. Check Suitability\n\nQuestion A1\n\n\nRead in the dataset from https://uoepsy.github.io/data/conduct_probs.csv.\nThe first column is clearly an ID column, and it is easiest just to discard this when we are doing factor analysis.\nCreate a correlation matrix for the items.\nInspect the items to check their suitability for exploratory factor analysis.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(psych)\ndf &lt;- read.csv(\"https://uoepsy.github.io/data/conduct_probs.csv\")\n# discard the first column\ndf &lt;- df[,-1]\n\ncorr.test(df)  \n\nCall:corr.test(x = df)\nCorrelation matrix \n       item1 item2 item3 item4 item5 item6 item7 item8 item9 item10\nitem1   1.00  0.59  0.49  0.48  0.60  0.17  0.30  0.32  0.26   0.20\nitem2   0.59  1.00  0.53  0.51  0.66  0.20  0.33  0.30  0.29   0.19\nitem3   0.49  0.53  1.00  0.49  0.55  0.15  0.25  0.24  0.25   0.15\nitem4   0.48  0.51  0.49  1.00  0.65  0.23  0.29  0.32  0.28   0.25\nitem5   0.60  0.66  0.55  0.65  1.00  0.21  0.30  0.29  0.27   0.21\nitem6   0.17  0.20  0.15  0.23  0.21  1.00  0.54  0.57  0.41   0.44\nitem7   0.30  0.33  0.25  0.29  0.30  0.54  1.00  0.83  0.61   0.58\nitem8   0.32  0.30  0.24  0.32  0.29  0.57  0.83  1.00  0.61   0.59\nitem9   0.26  0.29  0.25  0.28  0.27  0.41  0.61  0.61  1.00   0.44\nitem10  0.20  0.19  0.15  0.25  0.21  0.44  0.58  0.59  0.44   1.00\nSample Size \n[1] 450\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n       item1 item2 item3 item4 item5 item6 item7 item8 item9 item10\nitem1      0     0     0     0     0     0     0     0     0      0\nitem2      0     0     0     0     0     0     0     0     0      0\nitem3      0     0     0     0     0     0     0     0     0      0\nitem4      0     0     0     0     0     0     0     0     0      0\nitem5      0     0     0     0     0     0     0     0     0      0\nitem6      0     0     0     0     0     0     0     0     0      0\nitem7      0     0     0     0     0     0     0     0     0      0\nitem8      0     0     0     0     0     0     0     0     0      0\nitem9      0     0     0     0     0     0     0     0     0      0\nitem10     0     0     0     0     0     0     0     0     0      0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\ncortest.bartlett(cor(df), n=450)\n\n$chisq\n[1] 2238\n\n$p.value\n[1] 0\n\n$df\n[1] 45\n\nKMO(df)  \n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = df)\nOverall MSA =  0.87\nMSA for each item = \n item1  item2  item3  item4  item5  item6  item7  item8  item9 item10 \n  0.90   0.88   0.92   0.88   0.84   0.94   0.82   0.81   0.95   0.94 \n\npairs.panels(df)\n\n\n\n\n\n\n\n\nor alternatively, if you want a ggplot based approach:\n\nlibrary(GGally)\nggpairs(data=df, diag=list(continuous=\"density\"), axisLabels=\"show\")"
  },
  {
    "objectID": "10_efa.html#how-many-factors",
    "href": "10_efa.html#how-many-factors",
    "title": "Exploratory Factor Analysis (EFA): Part 1",
    "section": "2. How many factors?",
    "text": "2. How many factors?\n\nQuestion A2\n\n\nHow many dimensions should be retained? This question can be answered in the same way as we did above for PCA.\nUse a scree plot, parallel analysis, and MAP test to guide you.\nYou can use fa.parallel(data, fm = \"fa\") to conduct both parallel analysis and view the scree plot!\n\n\n\n\n\nSolution\n\n\n\n\nfa.parallel(df, fa = \"fa\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n\n\nIn this case the scree plot has a kink at the third factor, so we probably want to retain 2 factors.\nWe can conduct the MAP test using VSS(data).\n\nVSS(df, plot = FALSE, n = ncol(df))$map\n\n [1] 0.1058 0.0338 0.0576 0.1035 0.1494 0.2520 0.3974 0.4552 1.0000     NA\n\n\nThe MAP test suggests retaining 2 factors."
  },
  {
    "objectID": "10_efa.html#perform-efa",
    "href": "10_efa.html#perform-efa",
    "title": "Exploratory Factor Analysis (EFA): Part 1",
    "section": "3. Perform EFA",
    "text": "3. Perform EFA\nNow we need to perform the factor analysis. But there are two further things we need to consider, and they are:\n\nwhether we want to apply a rotation to our factor loadings, in order to make them easier to interpret, and\n\nhow do we want to extract our factors (it turns out there are loads of different approaches!).\n\n\nRotations?\nRotations are so called because they transform our loadings matrix in such a way that it can make it more easy to interpret. You can think of it as a transformation applied to our loadings in order to optimise interpretability, by maximising the loading of each item onto one factor, while minimising its loadings to others. We can do this by simple rotations, but maintaining our axes (the factors) as perpendicular (i.e., uncorrelated) as in Figure 3, or we can allow them to be transformed beyond a rotation to allow the factors to correlate (Figure 4).\n\n\n\n\n\nFigure 2: No rotation\n\n\n\n\n\n\n\n\n\nFigure 3: Orthogonal rotation\n\n\n\n\n\n\n\n\n\nFigure 4: Oblique rotation\n\n\n\n\nIn our path diagram of the model (Figure 5), all the factor loadings remain present, but some of them become negligible. We can also introduce the possible correlation between our factors, as indicated by the curved arrow between \\(F_1\\) and \\(F_2\\).\n\n\n\n\n\nFigure 5: Path diagrams for EFA with rotation\n\n\n\n\n\n\nFactor Extraction\nPCA (using eigendecomposition) is itself a method of extracting the different dimensions from our data. However, there are lots more available for factor analysis.\nYou can find a lot of discussion about different methods both in the help documentation for the fa() function from the psych package:\n\nFactoring method fm=“minres” will do a minimum residual as will fm=“uls”. Both of these use a first derivative. fm=“ols” differs very slightly from “minres” in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative. This will be slower. fm=“wls” will do a weighted least squares (WLS) solution, fm=“gls” does a generalized weighted least squares (GLS), fm=“pa” will do the principal factor solution, fm=“ml” will do a maximum likelihood factor analysis. fm=“minchi” will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm =“minrank” will do a minimum rank factor analysis. “old.min” will do minimal residual the way it was done prior to April, 2017 (see discussion below). fm=“alpha” will do alpha factor analysis as described in Kaiser and Coffey (1965)\n\nAnd there are lots of discussions both in papers and on forums.\nAs you can see, this is a complicated issue, but when you have a large sample size, a large number of variables, for which you have similar communalities, then the extraction methods tend to agree. For now, don’t fret too much about the factor extraction method.2\n\n\nQuestion A3\n\n\nUse the function fa() from the psych package to conduct and EFA to extract 2 factors (this is what we suggest based on the various tests above, but you might feel differently - the ideal number of factors is subjective!). Use a suitable rotation (rotate = ?) and extraction method (fm = ?).\n\nconduct_efa &lt;- fa(data, nfactors = ?, rotate = ?, fm = ?)\n\n\n\n\n\n\nSolution\n\n\n\nFor example, you could choose an oblimin rotation to allow factors to correlate and use minres as the extraction method.\n\nconduct_efa &lt;- fa(df, nfactors=2, rotate='oblimin', fm=\"minres\")"
  },
  {
    "objectID": "10_efa.html#inspect",
    "href": "10_efa.html#inspect",
    "title": "Exploratory Factor Analysis (EFA): Part 1",
    "section": "4. Inspect",
    "text": "4. Inspect\nWe can simply print the name of our model in order to see a lot of information. Let’s go through it in pieces.\n\nLoadings\n\n\nFactor Analysis using method =  minres\nCall: fa(r = df, nfactors = 2, rotate = \"oblimin\", fm = \"minres\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n         MR1   MR2   h2   u2 com\nitem1   0.03  0.71 0.52 0.48   1\nitem2   0.01  0.77 0.60 0.40   1\nitem3  -0.02  0.68 0.45 0.55   1\nitem4   0.06  0.68 0.50 0.50   1\nitem5  -0.04  0.87 0.73 0.27   1\nitem6   0.63 -0.02 0.39 0.61   1\nitem7   0.89  0.00 0.80 0.20   1\nitem8   0.92 -0.01 0.84 0.16   1\nitem9   0.63  0.09 0.45 0.55   1\nitem10  0.67 -0.03 0.43 0.57   1\n\n\nFactor loading’s, like PCA loading’s, show the relationship of each measured variable to each factor. They range between -1.00 and 1.00 Larger absolute values represent stronger relationship between measured variable and factor.\n\nThe columns that (depending upon estimation method) might be called MR/ML/PC are the factors. The number assigned to is arbitrary, and they might not always be in a numeric order (this has to do with a rotated solution). Typically, the numbering maps to how much variance each factor account for.\nh2: This is the “communality”, which is how much variance in the item is explained by the factors. It is calculated as the sum of the squared loadings.\n\nu2: This is \\(1 - h2\\). It is the residual variance, or the “uniqueness” for that item (i.e. the amount left unexplained).\n\ncom: This is the “Item complexity”. It tells us how much a given item reflects a single factor (vs being “more complex” in that it represents multiple factors). It equals one if an item loads only on one factor, 2 if evenly loads on two factors, etc.\n\nYou can get these on their own using\n\nconduct_efa$loadings\n\n\n\nVariance Accounted For\n\n\n                       MR1  MR2\nSS loadings           2.92 2.80\nProportion Var        0.29 0.28\nCumulative Var        0.29 0.57\nProportion Explained  0.51 0.49\nCumulative Proportion 0.51 1.00\n\n\nBelow the factor loadings, we have a familiar set of measures of the variance in the data accounted for by each factor. This is very similar to what we saw with PCA.\n\nSS loadings: The sum of the squared loadings. The eigenvalues.\n\nProportion Var: how much of the overall variance the factor accounts for out of all the variables.\nCumulative Var: cumulative sum of Proportion Var.\nProportion Explained: relative amount of variance explained (\\(\\frac{\\text{Proportion Var}}{\\text{sum(Proportion Var)}}\\).\nCumulative Proportion: cumulative sum of Proportion Explained.\n\nYou can get these on their own using\n\nconduct_efa$Vaccounted\n\n\n\nFactor Correlations\n\n\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.43\nMR2 0.43 1.00\n\nMean item complexity =  1\n\n\nWhether we see this section will depend if we have run a factor analysis with \\(\\geq 2\\) factors and a rotation.\n\nfactor correlations: shows the correlation matrix between the factors.\nmean item complexity: shows the mean of the com column from the loadings above.\n\nYou can get these on their own using\n\nconduct_efa$Phi\n\n\n\nTests, Fit Indices etc\nWe also get a whole load of other stuff that can sometimes be useful. These include: a test of an hypothesis that the 2 factors are sufficient; information on the number of observations; fit indices such as RMSEA, TLI RMSR etc; and measures of factor score adequacy (we’ll get to talking about factor scores next week).\n\n\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  45  with the objective function =  5.03 with Chi Square =  2238\ndf of  the model are 26  and the objective function was  0.09 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  450 with the empirical chi square  13.7  with prob &lt;  0.98 \nThe total n.obs was  450  with Likelihood Chi Square =  40  with prob &lt;  0.039 \n\nTucker Lewis Index of factoring reliability =  0.989\nRMSEA index =  0.035  and the 90 % confidence intervals are  0.008 0.055\nBIC =  -119\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.96 0.94\nMultiple R square of scores with factors          0.92 0.88\nMinimum correlation of possible factor scores     0.84 0.76\n\n\n\n\nQuestion A4\n\n\nInspect the loadings (conduct_efa$loadings) and give the factors you extracted labels based on the patterns of loadings.\nLook back to the description of the items, and suggest a name for your factors\n\n\n\n\n\nSolution\n\n\n\nYou can inspect the loadings using:\n\nprint(conduct_efa$loadings, sort=TRUE)\n\n\nLoadings:\n       MR1    MR2   \nitem6   0.634       \nitem7   0.890       \nitem8   0.924       \nitem9   0.629       \nitem10  0.669       \nitem1          0.706\nitem2          0.772\nitem3          0.681\nitem4          0.676\nitem5          0.872\n\n                MR1   MR2\nSS loadings    2.90 2.784\nProportion Var 0.29 0.278\nCumulative Var 0.29 0.568\n\n\nWe can see that the first five items have high loadings for one factor and the second five items have high loadings for the other.\nThe first five items all have in common that they are non-aggressive forms of conduct problems, while the last five items are all aggressive behaviours. We could, therefore, label our factors: ‘non-aggressive’ and ‘aggressive’ conduct problems.\n\n\n\n\nQuestion A5\n\n\nHow correlated are your factors?\nWe can inspect the factor correlations (if we used an oblique rotation) using:\n\nconduct_efa$Phi\n\n\n\n\n\n\nSolution\n\n\n\n\nconduct_efa$Phi\n\n     MR1  MR2\nMR1 1.00 0.43\nMR2 0.43 1.00\n\n\nWe can see here that there is a moderate correlation between the two factors. An oblique rotation would be appropriate here."
  },
  {
    "objectID": "10_efa.html#write-up",
    "href": "10_efa.html#write-up",
    "title": "Exploratory Factor Analysis (EFA): Part 1",
    "section": "5. Write-up",
    "text": "5. Write-up\n\nQuestion A6\n\n\nDrawing on your previous answers and conducting any additional analyses you believe would be necessary to identify an optimal factor structure for the 10 conduct problems, write a brief text that summarises your method and the results from your chosen optimal model.\n\n\n\n\n\nSolution\n\n\n\nThe main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description).\nAn example summary would be:\n\nFirst, the data were checked for their suitability for factor analysis. Normality was checked using visual inspection of histograms, linearity was checked through the inspection of the linear and lowess lines for the pairwise relations of the variables, and factorability was confirmed using a KMO test, which yielded an overall KMO of \\(.87\\) with no variable KMOs \\(&lt;.50\\). An exploratory factor analysis was conducted to inform the structure of a new conduct problems test. Inspection of a scree plot alongside parallel analysis (using principal components analysis; PA-PCA) and the MAP test were used to guide the number of factors to retain. All three methods suggested retaining two factors; however, a one-factor and three-factor solution were inspected to confirm that the two-factor solution was optimal from a substantive and practical perspective, e.g., that it neither blurred important factor distinctions nor included a minor factor that would be better combined with the other in a one-factor solution. These factor analyses were conducted using minres extraction and (for the two- and three-factor solutions) an oblimin rotation, because it was expected that the factors would correlate. Inspection of the factor loadings and correlations reinforced that the two-factor solution was optimal: both factors were well-determined, including 5 loadings \\(&gt;|0.3|\\) and the one-factor model blurred the distinction between different forms of conduct problems. The factor loadings are provided in Table 13. Based on the pattern of factor loadings, the two factors were labelled ‘aggressive conduct problems’ and ‘non-aggressive conduct problems’. These factors had a correlation of \\(r=.43\\). Overall, they accounted for 57% of the variance in the items, suggesting that a two-factor solution effectively summarised the variation in the items.\n\n\n\n\nTable 1: Factor Loadings\n\n\n\nMR1\nMR2\n\n\n\n\nitem1\n\n0.71\n\n\nitem2\n\n0.77\n\n\nitem3\n\n0.68\n\n\nitem4\n\n0.68\n\n\nitem5\n\n0.87\n\n\nitem6\n0.63\n\n\n\nitem7\n0.89\n\n\n\nitem8\n0.92\n\n\n\nitem9\n0.63\n\n\n\nitem10\n0.67"
  },
  {
    "objectID": "10_efa.html#footnotes",
    "href": "10_efa.html#footnotes",
    "title": "Exploratory Factor Analysis (EFA): Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen we have some clear hypothesis about relationships between measured variables and latent factors, we might want to impose a specific factor structure on the data (e.g., items 1 to 10 all measure social anxiety, items 11 to 15 measure health anxiety, and so on). When we impose a specific factor structure, we are doing Confirmatory Factor Analysis (CFA). This is not covered in this course, but it’s important to note that in practice EFA is not wholly “exploratory” (your theory will influence the decisions you make) nor is CFA wholly “confirmatory” (in which you will inevitably get tempted to explore how changing your factor structure might improve fit).↩︎\n(It’s a bit like the optimiser issue in the multi-level model block)↩︎\nYou should provide the table of factor loadings. It is conventional to omit factor loadings \\(&lt;|0.3|\\); however, be sure to ensure that you mention this in a table note.↩︎"
  },
  {
    "objectID": "11_efa2.html",
    "href": "11_efa2.html",
    "title": "Exploratory Factor Analysis (EFA): Part 2",
    "section": "",
    "text": "Relevant packages\n\ntidyverse\npsych\nGPArotation"
  },
  {
    "objectID": "11_efa2.html#footnotes",
    "href": "11_efa2.html#footnotes",
    "title": "Exploratory Factor Analysis (EFA): Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOf course this all assuming that the scales aren’t completely miscalibrated↩︎"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n\nSolution\n\n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "example_00_anova.html",
    "href": "example_00_anova.html",
    "title": "Analysis Example: Rpt & Mixed ANOVA",
    "section": "",
    "text": "This is optional for the DAPR3 course, but may be useful for your dissertations should your field/supervisor prefer the ANOVA framework to that of the linear model.\nThis walks briefly through these models with the ez package. There are many other packages available, and many good tutorials online should you desire extra resources in the future:\n\nhttps://www.datanovia.com/en/lessons/repeated-measures-anova-in-r\nhttps://www.r-bloggers.com/2021/04/repeated-measures-of-anova-in-r-complete-tutorial/\nhttps://stats.idre.ucla.edu/r/seminars/repeated-measures-analysis-with-r/\nhttps://www.datanovia.com/en/lessons/mixed-anova-in-r/\n\n\n\nData: Audio interference in executive functioning\nThis data is from a simulated study that aims to investigate the following research questions:\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n\n24 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room.\nThe data is in stored in two separate files - the research administering the tests recorded the SDMT score in one spreadsheet, while details of the audio used in the experiment are held in a separate sheet\n\nInformation about the audio condition for each trial of each participant is stored in .csv format at https://uoepsy.github.io/data/ef_music.csv. The data is in long format (1 row per participant-trial).\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\ntrial_n\nTrial Number (1-15)\n\n\naudio\nAudio heard during the test (‘no_audio’, ‘white_noise’,‘music’)\n\n\nheadphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n\n\n\n\n\n\nInformation on participants’ Symbol Digit Modalities Test (SDMT) for each trial is stored in .xlsx format at https://uoepsy.github.io/data/ef_sdmt.xlsx. The data is in wide format (1 row per participant, 1 column per trial).\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\nTrial_01\nSDMT score in trial 1\n\n\nTrial_02\nSDMT score in trial 2\n\n\nTrial_03\nSDMT score in trial 3\n\n\n…\nSDMT score in trial …\n\n\n…\nSDMT score in trial …\n\n\nTrial_15\nSDMT score in trial 15\n\n\n\n\n\n\nThe code below will read in both datasets and join them for you:\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\ndownload.file(url = \"https://uoepsy.github.io/data/ef_sdmt.xlsx\",\n              destfile = \"ef_sdmt.xlsx\",\n              mode = \"wb\")\nefdata &lt;- \n  left_join(\n    read_csv(\"https://uoepsy.github.io/data/ef_music.csv\"),\n    read_xlsx(\"ef_sdmt.xlsx\") %&gt;%\n      pivot_longer(Trial_01:Trial_15, names_to = \"trial_n\", values_to = \"SDMT\")\n  )\n\n\n\nOne-Way Repeated Measures ANOVA\nFor a repeated measures ANOVA, we have one independent variable that is within group.\nThis would be appropriate if our research question were the following:\n\nHow do different types of audio interfere with executive functioning?\n\nMapping this to the variables in our dataset, our model is going to be SDMT ~ audio, and we want to account for PID differences. So for now we will ignore the headphones variable.\n\n\nCode\nhead(efdata)\n\n\n# A tibble: 6 × 5\n  PID    trial_n  audio       headphones  SDMT\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 PPT_01 Trial_02 no_audio    speakers      31\n2 PPT_01 Trial_08 no_audio    speakers      23\n3 PPT_01 Trial_11 no_audio    speakers      23\n4 PPT_01 Trial_13 no_audio    speakers      24\n5 PPT_01 Trial_15 no_audio    speakers      34\n6 PPT_01 Trial_01 white_noise speakers      38\n\n\nThe easiest way to conduct a repeated measures ANOVA in R is to use the ez package, which comes with some handy functions to visualise the experimental design.\nWe can see from below that every participant completed 5 trials for each type of audio interference:\n\n\nCode\nlibrary(ez)\nezDesign(data = efdata, x = audio, y = PID)\n\n\n\n\n\n\n\n\n\nThe ezANOVA() function takes a few arguments.\nThe ones you will need for this are:\n\ndata the name of the dataframe\ndv the column name for the dependent variable\nwid the column name for the participant id variable\nwithin the column name(s) for the predictor variable(s) that vary within participants\nbetween the column name(s) for any predictor variable(s) that vary between participants\n\nFit a repeated measures ANOVA to examine the effect of the audio type on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio)\n\n\n$ANOVA\n  Effect DFn DFd        F            p p&lt;.05       ges\n2  audio   2  46 44.69618 1.647271e-11     * 0.2534633\n\n$`Mauchly's Test for Sphericity`\n  Effect         W          p p&lt;.05\n2  audio 0.8105961 0.09927715      \n\n$`Sphericity Corrections`\n  Effect       GGe      p[GG] p[GG]&lt;.05      HFe        p[HF] p[HF]&lt;.05\n2  audio 0.8407573 5.0677e-10         * 0.899603 1.427119e-10         *\n\n\n\n\nMixed ANOVA\nMixed ANOVA can be used to investigate effects of independent variables that are at two different levels, i.e. some are within clusters and some are between.\n\nDoes the effect of audio interference on executive functioning differ depending upon whether or not noise-cancelling headphones are used?\n\nLook at the two lines below. Can you work out what the plots will look like before you run them?\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\nezDesign(data = efdata, x = headphones, y = audio)\n\n\nParticipants 1-20 are in one condition, and 21-40 are in another.\nThis should look like a two big blocks on the diagonal.\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\n\n\n\n\n\n\n\n\n\nIn each condition, all different types of audio were observed in the same number of trials. This should be a full grid:\n\n\nCode\nezDesign(data = efdata, x = headphones, y = audio)\n\n\n\n\n\n\n\n\n\nFit a mixed ANOVA to examine the interaction between audio and headphone use on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio, between = headphones)\n\n\n$ANOVA\n            Effect DFn DFd         F            p p&lt;.05        ges\n2       headphones   1  22  9.815545 4.836945e-03     * 0.26784992\n3            audio   2  44 59.615596 2.980503e-13     * 0.32788320\n4 headphones:audio   2  44  8.677316 6.657590e-04     * 0.06629911\n\n$`Mauchly's Test for Sphericity`\n            Effect         W         p p&lt;.05\n3            audio 0.9422531 0.5355001      \n4 headphones:audio 0.9422531 0.5355001      \n\n$`Sphericity Corrections`\n            Effect       GGe        p[GG] p[GG]&lt;.05      HFe        p[HF]\n3            audio 0.9454057 1.196469e-12         * 1.031585 2.980503e-13\n4 headphones:audio 0.9454057 8.648057e-04         * 1.031585 6.657590e-04\n  p[HF]&lt;.05\n3         *\n4         *\n\n\nThe ez package also contains some easy plotting functions for factorial experiments, such as ezPlot(). It takes similar arguments to the ezANOVA() function.\n\nlook up the help documentation for ezPlot().\nlet’s use ezPlot() to make a nice plot\n\n\n\nCode\nezPlot(data = efdata, dv = SDMT, \n       wid = PID, within = audio, between = headphones,\n       x = audio, split = headphones)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same thing in lmer\n\n\n\n\n\n\n\nCode\nlibrary(lme4)\nlibrary(lmerTest)\nmod &lt;- lmer(SDMT ~ 1 + headphones * audio + (1 + audio | PID), \n            data = efdata)\nanova(mod, type=\"III\")\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                 Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nheadphones        325.0  325.04     1    22  9.8155  0.004837 ** \naudio            3212.0 1606.01     2    22 48.4976 8.626e-09 ***\nheadphones:audio  490.1  245.06     2    22  7.4001  0.003486 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nlibrary(sjPlot)\nplot_model(mod, type=\"eff\", terms=c(\"audio\",\"headphones\"))"
  },
  {
    "objectID": "example_01_repeated_measures.html",
    "href": "example_01_repeated_measures.html",
    "title": "Analysis Walkthrough 1",
    "section": "",
    "text": "Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:\n\nFirst the data and research context is introduced. For the purpose of these tutorials, we will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated.\nSecond, we go through any tidying of the data that is required, before creating some brief descriptives and visualizations of the raw data.\nThen, we conduct an analysis. Where possible, we translate the research questions into formal equations prior to fitting the models in lme4. Model comparisons are conducted, along with checks of distributional assumptions on our model residuals.\nFinally, we visualize and interpret our analysis.\n\nPlease note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ppls.psych.stats@ed.ac.uk."
  },
  {
    "objectID": "example_01_repeated_measures.html#equations",
    "href": "example_01_repeated_measures.html#equations",
    "title": "Analysis Walkthrough 1",
    "section": "Equations",
    "text": "Equations\nWe’re going to fit the model below, and examine the change in speed associated with moving from speech (our reference level) to both classical, and rap conditions.\nRecall that because music is categorical with 3 levels, we’re going to be estimating 2 (\\(3-1\\)) coefficients.\n\\[\\begin{aligned}\n&\\text{for trial }j \\text{ from participant } i \\\\\n  \\operatorname{speed}_{i[j]} =& \\beta_{0i} + \\beta_1(\\operatorname{music}_{\\operatorname{classical}_j}) + \\beta_2(\\operatorname{music}_{\\operatorname{rap}_j}) + \\varepsilon_{i[j]} \\\\\n    \\beta_{0i} =& \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "example_01_repeated_measures.html#fitting-the-models",
    "href": "example_01_repeated_measures.html#fitting-the-models",
    "title": "Analysis Walkthrough 1",
    "section": "Fitting the models",
    "text": "Fitting the models\n\n\nCode\nlibrary(lme4)\n\n\nHere we run an empty model so that we have something to compare our model which includes our independent variable. Other than to give us a reference model, we do not have a huge amount of interest in this. It includes no predictors, but a random intercept by participant (pid) to take account of the fact we have three measurements per person.\n\n\nCode\nm0 &lt;- lmer(speed ~ 1 + (1 | pid), data = simRPT)\n\n\nNext, add a fixed effect of our predictor (music condition, music). First though, we’ll want to re-level it so that “speech” is the reference level (because that’s what we said we wanted).\n\n\nCode\nsimRPT &lt;-\n  simRPT %&gt;%\n  mutate(\n    music = fct_relevel(factor(music), \"speech\")\n  )\n\nm1 &lt;- lmer(speed ~ 1 + music + (1 | pid), data = simRPT)\nsummary(m1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: speed ~ 1 + music + (1 | pid)\n   Data: simRPT\n\nREML criterion at convergence: 895.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.47567 -0.56300 -0.01549  0.54691  2.42387 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n pid      (Intercept) 20.32    4.508   \n Residual             13.54    3.679   \nNumber of obs: 150, groups:  pid, 50\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)     29.0408     0.8229  35.290\nmusicclassical   5.2048     0.7358   7.073\nmusicrap        -0.5858     0.7358  -0.796\n\nCorrelation of Fixed Effects:\n            (Intr) msccls\nmusicclsscl -0.447       \nmusicrap    -0.447  0.500\n\n\nAnd we can compare our models. A Kenward-Rogers F ratio suggests that we appear to have a significant differences in speeds between conditions.\n\n\nCode\nlibrary(pbkrtest)\nKRmodcomp(m1, m0)\n\n\nlarge : speed ~ 1 + music + (1 | pid)\nsmall : speed ~ 1 + (1 | pid)\n       stat   ndf   ddf F.scaling   p.value    \nFtest 37.53  2.00 98.00         1 7.913e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "example_01_repeated_measures.html#check-model",
    "href": "example_01_repeated_measures.html#check-model",
    "title": "Analysis Walkthrough 1",
    "section": "Check model",
    "text": "Check model\nThe residuals look reasonably normally distributed, and there seems to be fairly constant variance across the linear predictor. We might be a little concerned about the potential tails of the plot below, at which residuals don’t appear to have a mean of zero\n\n\nCode\nplot(m1, type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\nCode\nlibrary(lattice)\nqqmath(m1)\n\n\n\n\n\n\n\n\n\nRandom effects are (roughly) normally distributed:\n\n\nCode\nrans &lt;- as.data.frame(ranef(m1)$pid)\nggplot(rans, aes(sample = `(Intercept)`)) + \n  stat_qq() + stat_qq_line() +\n  labs(title=\"random intercept\")"
  },
  {
    "objectID": "example_02_intervention.html",
    "href": "example_02_intervention.html",
    "title": "Analysis Walkthrough 2",
    "section": "",
    "text": "Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:\n\nFirst the data and research context is introduced. For the purpose of these tutorials, we will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated.\nSecond, we go through any tidying of the data that is required, before creating some brief descriptives and visualizations of the raw data.\nThen, we conduct an analysis. Where possible, we translate the research questions into formal equations prior to fitting the models in lme4. Model comparisons are conducted, along with checks of distributional assumptions on our model residuals.\nFinally, we visualize and interpret our analysis.\n\nPlease note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ppls.psych.stats@ed.ac.uk."
  },
  {
    "objectID": "example_02_intervention.html#equations",
    "href": "example_02_intervention.html#equations",
    "title": "Analysis Walkthrough 2",
    "section": "Equations",
    "text": "Equations\nAs we want to assess how the change in stress differs between two groups, we’re looking at an interaction of time * group. But this interaction is across levels (i.e. group is a participant level variable, and time is observation level).\nAt the observation level, we model stress as a function of time. Time is a 3-level categorical, so we have two coefficients.\nWe are allowing the intercept to vary across participants (i.e. participants differ in how stressed they are), and we also model this intercept as differing depending on whether someone is in the treatment or control group.\nSimilarly, the two coefficients for time are also going to be modelled as a function of the group that the participant is in.\n\\[\n\\begin{aligned}\n&\\text{for timepoint }j \\text{ from participant } i \\\\\n  \\operatorname{stress}_{i[j]}  &= \\beta_{0i} + \\beta_{1i}(\\operatorname{timeDuring}_j) + \\beta_{2i}(\\operatorname{timePost}_j) + \\varepsilon_{i[j]} \\\\\n  \\beta_{0i} &= \\gamma_{00} + \\gamma_{01}(\\operatorname{groupTreatment}_i) + \\zeta_{0i} \\\\\n  \\beta_{1i} &= \\gamma_{10} + \\gamma_{11}(\\operatorname{groupTreatment}_i) \\\\\n  \\beta_{2i} &= \\gamma_{20} + \\gamma_{21}(\\operatorname{groupTreatment}_i) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "example_02_intervention.html#fitting-the-models",
    "href": "example_02_intervention.html#fitting-the-models",
    "title": "Analysis Walkthrough 2",
    "section": "Fitting the models",
    "text": "Fitting the models\n\n\nCode\nlibrary(lme4)\n\n\nBase model:\n\n\nCode\nm0 &lt;- lmer(stress ~ 1 +\n             (1 | ppt), data = simMIX)\nsummary(m0)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1286.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.02099 -0.49670  0.01511  0.49445  1.96548 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 179.7    13.40   \n Residual             209.7    14.48   \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   52.220      2.234   23.37\n\n\nMain effects:\n\n\nCode\nm1 &lt;- lmer(stress ~ 1 + time + group +\n             (1 | ppt), data = simMIX)\nsummary(m1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + time + group + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1185.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.69666 -0.62649  0.01574  0.59245  1.92387 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 166.57   12.91   \n Residual              98.01    9.90   \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)      71.100      3.046  23.344\ntimeDuring      -13.760      1.980  -6.950\ntimePost        -20.980      1.980 -10.596\ngroupTreatment  -14.600      3.992  -3.657\n\nCorrelation of Fixed Effects:\n            (Intr) tmDrng timPst\ntimeDuring  -0.325              \ntimePost    -0.325  0.500       \ngroupTrtmnt -0.655  0.000  0.000\n\n\nInteraction:\n\n\nCode\nm2 &lt;- lmer(stress ~ 1 + time + group + time*group +\n             (1 | ppt), data = simMIX)\nsummary(m2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + time + group + time * group + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1096.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.83946 -0.53326 -0.05639  0.53766  2.31546 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 184.82   13.595  \n Residual              43.26    6.577  \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)                 62.840      3.020  20.805\ntimeDuring                  -3.200      1.860  -1.720\ntimePost                    -6.760      1.860  -3.634\ngroupTreatment               1.920      4.272   0.449\ntimeDuring:groupTreatment  -21.120      2.631  -8.028\ntimePost:groupTreatment    -28.440      2.631 -10.810\n\nCorrelation of Fixed Effects:\n            (Intr) tmDrng timPst grpTrt tmDr:T\ntimeDuring  -0.308                            \ntimePost    -0.308  0.500                     \ngroupTrtmnt -0.707  0.218  0.218              \ntmDrng:grpT  0.218 -0.707 -0.354 -0.308       \ntmPst:grpTr  0.218 -0.354 -0.707 -0.308  0.500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compare all models with a parametric bootstrap likelihood ratio test:\n\n\nCode\nlibrary(pbkrtest)\nPBmodcomp(m1, m0)\nPBmodcomp(m2, m1)\n\n\n\n\nBootstrap test; time: 32.27 sec; samples: 1000; extremes: 0;\nlarge : stress ~ 1 + time + group + (1 | ppt)\nstress ~ 1 + (1 | ppt)\n         stat df   p.value    \nLRT    90.348  3 &lt; 2.2e-16 ***\nPBtest 90.348     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBootstrap test; time: 28.19 sec; samples: 1000; extremes: 0;\nlarge : stress ~ 1 + time + group + time * group + (1 | ppt)\nstress ~ 1 + time + group + (1 | ppt)\n         stat df   p.value    \nLRT    83.846  2 &lt; 2.2e-16 ***\nPBtest 83.846     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd extract some bootstrap 95% CIs\n\n\nCode\nconfint(m2,method=\"boot\")\n\n\n\n\n                                  2.5 % 97.5 %\n                            0.00  10.85  16.59\n                            0.00   5.62   7.39\n(Intercept)                62.84  56.96  68.90\ntimeDuring                 -3.20  -6.60  -0.12\ntimePost                   -6.76 -10.47  -3.13\ngroupTreatment              1.92  -7.00  10.05\ntimeDuring:groupTreatment -21.12 -26.40 -16.11\ntimePost:groupTreatment   -28.44 -33.67 -23.26"
  },
  {
    "objectID": "example_02_intervention.html#check-model",
    "href": "example_02_intervention.html#check-model",
    "title": "Analysis Walkthrough 2",
    "section": "Check Model",
    "text": "Check Model\nThe residuals look reasonably normally distributed, and there seems to be fairly constant variance across the linear predictor. We might be a little concerned about the potential tails of the plot below, at which residuals don’t appear to have a mean of zero\n\n\nCode\nplot(m2, type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\nCode\nplot(m2, sqrt(abs(resid(.)))~fitted(.))\n\n\n\n\n\n\n\n\n\nCode\nlibrary(lattice)\nqqmath(m2)\n\n\n\n\n\n\n\n\n\nRandom effects are (roughly) normally distributed:\n\n\nCode\nrans &lt;- as.data.frame(ranef(m2)$ppt)\nggplot(rans, aes(sample = `(Intercept)`)) + \n  stat_qq() + stat_qq_line() +\n  labs(title=\"random intercept\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DAPR3",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 3 (DAPR3) lab workbook. Using the menu above, you can find lab materials for each week. These include sets of exercises along with walkthrough readings in which we introduce some of the more important R code. It is strongly recommended that students have taken Data Analysis for Psychology in R 1 and 2 (DAPR1 & DAPR2)."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Welcome to DAPR3",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Welcome to DAPR3",
    "section": "Tips on googling statistics and R",
    "text": "Tips on googling statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Welcome to DAPR3",
    "section": "Feedback on labs",
    "text": "Feedback on labs\nIf you wish to make suggestions for improvements to these workbooks, please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  }
]