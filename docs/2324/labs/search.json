[
  {
    "objectID": "01_regressionrefresh.html",
    "href": "01_regressionrefresh.html",
    "title": "Regression Refresh and Clustered Data",
    "section": "",
    "text": "Preliminaries\n\nOpen Rstudio, and create a new project for this course!!\nCreate a new RMarkdown document or R script (whichever you like) for this week.\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already.\n\n\ntidyverse : for organising data\npatchwork: for organising plots\nICC : for quickly calculating intraclass correlation coefficient\nlme4 : for fitting generalised linear mixed effects models\nparameters : inference!\npbkrTest : more inference!\nHLMdiag : for examining case diagnostics at multiple levels\nlmeresampler : for bootstrapping!\neffects : for tables/plots\nsjPlot : for tables/plots\nbroom.mixed : tidying methods for mixed models\n\nYou can install all of these at once using:\n\ninstall.packages(c(\"tidyverse\",\"ICC\",\"lme4\",\"parameters\",\"pbkrTest\",\"effects\",\"broom.mixed\",\"sjPlot\",\"HLMdiag\"))\n# the lmeresampler package has had some recent updates. better to install the most recent version:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"aloy/lmeresampler\")\n\n\n\nThis Week\n\n\nExercises: Linear Models & Pooling\n\n\n\n\n\n\nRegression Refresh\n\n\n\n\n\nRecall that in the DAPR2 course last year we learned all about the linear regression model, which took the form:\n\\[\n\\begin{align}\\\\\n& \\text{for observation }i \\\\\n& \\color{red}{Y_i}\\color{black} = \\color{blue}{\\beta_0 \\cdot{} 1 + \\beta_1 \\cdot{} X_{1i} \\ + \\ ... \\ + \\ \\beta_p \\cdot{} X_{pi}}\\color{black} + \\varepsilon_i \\\\\n\\end{align}\n\\]\nAnd if we wanted to write this more simply, we can express \\(X_1\\) to \\(X_p\\) as an \\(n \\times p\\) matrix (samplesize \\(\\times\\) parameters), and \\(\\beta_0\\) to \\(\\beta_p\\) as a vector of coefficients:\n\\[\n\\begin{align}\n& \\color{red}{\\mathbf{y}}\\color{black} = \\color{blue}{\\boldsymbol{X\\beta}}\\color{black} + \\boldsymbol{\\varepsilon} \\\\\n& \\quad \\\\\n& \\text{where} \\\\\n& \\varepsilon \\sim N(0, \\sigma) \\text{ independently} \\\\\n\\end{align}\n\\] In R, we fitted these models using:\n\nlm(y ~ x1 + x2 + .... xp, data = mydata)  \n\n\n\n\n\nData: Wellbeing Across Scotland\nIn DAPR2, one of the examples we used in learning about linear regression was in examining the relationship between time spent outdoors and mental wellbeing. In that example researchers had collected data from 32 residents of Edinburgh & Lothians.\nResearchers want to study this relationship across all of Scotland. They contact all the Local Authority Areas (LAAs) and ask them to collect data for them for them, with participants completing the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being, and being asked to estimate the average number of hours they spend outdoors each week.\nTwenty of the Local Authority Areas provided data. It is available at https://uoepsy.github.io/data/LAAwellbeing.csv, and you can read it into your R environment using the code below:\n\nscotmw &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n\nThe dataset contains information on 132 participants. You can see the variables in the table below\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant ID\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nSelf report estimated number of hours per week spent outdoors\n    wellbeing\nWarwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n    density\nLAA Population Density (people per square km)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nRead in the Local Authority data from https://uoepsy.github.io/data/LAAwellbeing.csv and plot the bivariate relationship between wellbeing and time spent outdoors.\nThen, using lm(), fit the simple linear model:\n\\[\n\\text{Wellbeing}_i = \\beta_0 + \\beta_1 \\cdot \\text{Hours per week spent outdoors}_i + \\varepsilon_i\n\\]\nThink about the assumptions we make about the model:\n\\[\n\\text{where} \\quad \\varepsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\] Have we satisfied this assumption (specifically, the assumption of independence of errors)?\n\n\n\n\n Solution \n\n\n\nscotmw &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\") \n\nggplot(data = scotmw, aes(x = outdoor_time, y = wellbeing))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nsimplemod &lt;- lm(wellbeing ~ outdoor_time, data = scotmw)\nsummary(simplemod)\n\n\nCall:\nlm(formula = wellbeing ~ outdoor_time, data = scotmw)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-27.4395  -7.5658  -0.3175   6.0831  26.7208 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   39.2723     2.6674  14.723   &lt;2e-16 ***\noutdoor_time   0.1603     0.1462   1.096    0.275    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.43 on 130 degrees of freedom\nMultiple R-squared:  0.009155,  Adjusted R-squared:  0.001534 \nF-statistic: 1.201 on 1 and 130 DF,  p-value: 0.2751\n\n\nOur model from the previous question will assume that the residuals for all participants are independent of one another. But is this a reasonable assumption that we can make? Might we not think that the residents of the highlands might have generally higher levels of wellbeing than those living in Glasgow? Additionally, the association between outdoor time and wellbeing might be different depending on where you live?\nThe natural grouping of the people into their respective geographic area introduces a level of dependence which we would be best to account for.\n\n\n\n\nQuestion 2\n\n\nTry running the code below.\n\nggplot(data = scotmw, aes(x = outdoor_time, y = wellbeing))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\nThen try editing the code to include an aesthetic mapping from the LAA to the color in the plot.\nHow do your thoughts about the relationship between outdoor time and wellbeing change?\n\n\n\n\n Solution \n\n\nFrom the second plot, we see a lot of the LAA appear to have a positive relationship (outdoor time is associated with higher wellbeing). There seem to be differences between LAAs in both the general wellbeing level (residents of Na h-Eileanan Siar - the outer hebrides - have high wellbeing), and in how outdoor time is associated with wellbeing (for instance, outdoor time doesn’t seem to help in Glasgow City).\n\np1 &lt;- ggplot(data = scotmw, aes(x = outdoor_time, y = wellbeing))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\np2 &lt;- ggplot(data = scotmw, aes(x = outdoor_time, y = wellbeing, col = laa))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\nlibrary(patchwork)\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplete Pooling\n\n\n\n\n\nWe can consider the simple regression model (lm(wellbeing ~ outdoor_time, data = scotmw)) to “pool” the information from all observations together. In this ‘Complete Pooling’ approach, we simply ignore the natural clustering of the people into their local authority areas, as if we were unaware of it. The problem is that this assumes the same regression line for all local authority areas, which might not be that appropriate. Additionally, we violate the assumption that our residuals are independent, because all of the residuals from certain groups will be more like one another than they are to the others.\n\n\n\n\n\nComplete pooling can lead to bad fit for certain groups\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo Pooling\n\n\n\n\n\nThere are various ways we could attempt to deal with the problem that our data are in groups (or “clusters”). With the tools you have learned in DAPR2, you may be tempted to try including LAA in the model as another predictor, to allow for some local authority areas being generally better than others:\n\nlm(wellbeing ~ outdoor_time + laa, data = scotmw)\n\nOr even to include an interaction to allow for local authority areas to show different patterns of association between outdoor time and wellbeing:\n\nlm(wellbeing ~ outdoor_time * laa, data = scotmw)\n\nThis approach gets termed the “No Pooling” method, because the information from each cluster contributes only to an estimated parameter for that cluster, and there is no pooling of information across clusters. This is a good start, but it means that a) we are estimating a lot of parameters, and b) we are not necessarily estimating the parameter of interest (the overall effect of practice on reading age). Furthermore, we’ll probably end up having high variance in the estimates at each group.\n\n\n\n\nQuestion 3\n\n\nFit a linear model which accounts for the grouping of participants into their different local authorities, but holds the association between outdoor time and wellbeing as constant across LAAs:\n\nmod1 &lt;- lm(wellbeing ~ outdoor_time + laa, data = scotmw)\n\nCan you construct a plot of the fitted values from this model, coloured by LAA?\n\n\n\n\n\n\nHint\n\n\n\n\n\nyou might want to use the augment() function from the broom package\n\n\n\n\n\n\n\n Solution \n\n\n\nlibrary(broom)\naugment(mod1) %&gt;%\n  ggplot(.,aes(x=outdoor_time, y=.fitted, col=laa))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nWhat happens (to the plot, and to your parameter estimates) when you include the interaction between laa and outdoor_time?\n\n\n\n\n Solution \n\n\n\nmod2 &lt;- lm(wellbeing ~ outdoor_time * laa, data = scotmw)\n\nbroom::augment(mod2) %&gt;%\n  ggplot(.,aes(x=outdoor_time, y=.fitted, col=laa))+\n  geom_line()\n\n\n\n\n\n\n\n\nWe can see now that our model is fitting a different relationship between wellbeing and outdoor time for each LAA. This is good - we’re going to get better estimates for different LAAs (e.g. wellbeing of residents of the Highlands increases with more outdoor time, and wellbeing of residents of Glasgow does not).\nWe can see that this model provides a better fit - it results in a significant reduction in the residual sums of squares:\n\nanova(mod1, mod2)\n\nAnalysis of Variance Table\n\nModel 1: wellbeing ~ outdoor_time + laa\nModel 2: wellbeing ~ outdoor_time * laa\n  Res.Df    RSS Df Sum of Sq   F   Pr(&gt;F)   \n1    111 2826.9                             \n2     92 1864.4 19    962.57 2.5 0.001975 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBut accounting for this heterogeneity over clusters in the effect of interest comes at the expense of not pooling information across groups to get one estimate for “the association between outdoor time and wellbeing”. Additionally, these models will tend to have low statistical power because they are using fewer observations (only those within each cluster) to estimate parameters which only represent within-cluster effects.\n\n\n\n\n\nExercises: Advanced Data Wrangling\nWith more complex data structures comes more in-depth data wrangling in order to get it ready for fitting and estimating our model. Typically, the data we get will not be neat and tidy, and will come in different formats. Often we simply get whatever our experiment/questionnaire software spits out, and we have to work from there. When you are designing a study, you can do work on the front end to minimise the data-wrangling. Try to design an experiment/questionnaire while keeping in mind what the data comes out looking like.\nBelow we have some data from a fake experiment. We’ve tried to make it a bit more difficult to work with - a bit more similar to what we would actually get when doing real-life research.\n\nData: Audio interference in executive functioning\nThis data is from a simulated study that aims to investigate the following research question:\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n\n24 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room.\nThe data is in stored in two separate files - the researcher administering the tests recorded the SDMT score in one spreadsheet, while details of the audio used in the experiment are held in a separate sheet.\n\n\n\n\n\n\nef_music.csv\n\n\n\n\n\n\nInformation about the audio condition for each trial of each participant is stored in .csv format at https://uoepsy.github.io/data/ef_music.csv. The data is in long format (1 row per participant-trial).\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\ntrial_n\nTrial Number (1-15)\n\n\naudio\nAudio heard during the test (‘no_audio’, ‘white_noise’,‘music’)\n\n\nheadphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nef_sdmt.xlsx\n\n\n\n\n\nInformation on participants’ Symbol Digit Modalities Test (SDMT) for each trial is stored in .xlsx format at https://uoepsy.github.io/data/ef_sdmt.xlsx. The data is in wide format (1 row per participant, 1 column per trial).\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\nTrial_01\nSDMT score in trial 1\n\n\nTrial_02\nSDMT score in trial 2\n\n\nTrial_03\nSDMT score in trial 3\n\n\n…\nSDMT score in trial …\n\n\n…\nSDMT score in trial …\n\n\nTrial_15\nSDMT score in trial 15\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGet the data into your R session.\nNote: For one of the files, this is a bit different to how we have given you data in previous exercises. You may remember that for a .csv file, you can read directly into R from the link using, read_csv(\"https://uoepsy.......).\nHowever, in reality you are likely to be confronted with data in all sorts of weird formats, such as .xlsx files from MS Excel. Have a look around the internet to try and find any packages/functions/techniques for getting both the datasets in to R.\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor the .xlsx data:\n\nStep 1: download the data to your computer\n\nStep 2: load the readxl package.\n\nStep 3: use the read_xlsx() function to read in the data, pointing it to the relevant place on your computer.\n\n\n\n\n\n\n\n\n Solution \n\n\nReading in the data for each condition is easy, as it’s just the same as we have been doing in DAPR previously:\n\nef_music &lt;- read_csv(\"https://uoepsy.github.io/data/ef_music.csv\")\nhead(ef_music)\n\n\n\n# A tibble: 6 × 4\n  PID    trial_n  audio       headphones\n  &lt;chr&gt;  &lt;chr&gt;    &lt;fct&gt;       &lt;fct&gt;     \n1 PPT_01 Trial_02 no_audio    speakers  \n2 PPT_01 Trial_08 no_audio    speakers  \n3 PPT_01 Trial_11 no_audio    speakers  \n4 PPT_01 Trial_13 no_audio    speakers  \n5 PPT_01 Trial_15 no_audio    speakers  \n6 PPT_01 Trial_01 white_noise speakers  \n\n\nThe other data is a bit more tricky, but we can actually do all these steps from within R.\n\n# Step 1 - Download the data:  \ndownload.file(url = \"https://uoepsy.github.io/data/ef_sdmt.xlsx\", \n              destfile = \"ef_sdmt.xlsx\", mode = \"wb\")\n# Step 2\nlibrary(readxl)\n# Step 3\nef_sdmt &lt;- read_xlsx(\"ef_sdmt.xlsx\")\nhead(ef_sdmt)\n\n\n\n# A tibble: 6 × 16\n  PID    Trial_01 Trial_02 Trial_03 Trial_04 Trial_05 Trial_06 Trial_07 Trial_08\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 PPT_01       38       31       33       13       19       22       24       23\n2 PPT_02       47       43       30       38       56       61       58       55\n3 PPT_03       29       36       29       34       44       36       35       41\n4 PPT_04       23       14        8       36       27       16       26       15\n5 PPT_05       22       24       18       19       23       15       32       18\n6 PPT_06       29       27       37       39       36       40       28       26\n# ℹ 7 more variables: Trial_09 &lt;dbl&gt;, Trial_10 &lt;dbl&gt;, Trial_11 &lt;dbl&gt;,\n#   Trial_12 &lt;dbl&gt;, Trial_13 &lt;dbl&gt;, Trial_14 &lt;dbl&gt;, Trial_15 &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nPivoting dataframes\n\n\n\n\n\nOne of the more confusing things to get to grips with is the idea of reshaping a dataframe.\nFor different reasons, you might sometimes want to have data in wide, or in long format.\n\n\n\n\n\nSource: https://fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/\n\n\n\n\nWhen the data is wide, we can make it long using pivot_longer(). When we make data longer, we’re essentially making lots of columns into 2 longer columns. Above, in the animation, the wide variable x, y and z go into a new longer column called name that specifies which (x/y/z) it came from, and the values get put into the val column.\nThe animation takes a shortcut in the code it displays above, but you could also use pivot_longer(c(x,y,z), names_to = \"name\", values_to = \"val\"). To reverse this, and put it back to being wide, we tell R which columns to take the names and values from: pivot_wider(names_from = name, values_from = val).\n\n\n\n\nQuestion 6\n\n\nIs each dataset in wide or long format? We want them both in long format, so try to reshape either/both if necessary.\n\n\n\n\n\n\nHint\n\n\n\n\n\nHint: in the tidyverse functions, you can specify all columns between column x and column z by using the colon, x:z.\n\n\n\n\n\n\n\n Solution \n\n\nOnly the SDMT data is in wide format:\n\nhead(ef_sdmt)\n\n# A tibble: 6 × 16\n  PID    Trial_01 Trial_02 Trial_03 Trial_04 Trial_05 Trial_06 Trial_07 Trial_08\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 PPT_01       38       31       33       13       19       22       24       23\n2 PPT_02       47       43       30       38       56       61       58       55\n3 PPT_03       29       36       29       34       44       36       35       41\n4 PPT_04       23       14        8       36       27       16       26       15\n5 PPT_05       22       24       18       19       23       15       32       18\n6 PPT_06       29       27       37       39       36       40       28       26\n# ℹ 7 more variables: Trial_09 &lt;dbl&gt;, Trial_10 &lt;dbl&gt;, Trial_11 &lt;dbl&gt;,\n#   Trial_12 &lt;dbl&gt;, Trial_13 &lt;dbl&gt;, Trial_14 &lt;dbl&gt;, Trial_15 &lt;dbl&gt;\n\n\n\nef_sdmt_long &lt;-\n  ef_sdmt %&gt;%\n  pivot_longer(Trial_01:Trial_15, names_to = \"trial_n\", values_to = \"SDMT\")\n\nhead(ef_sdmt_long)\n\n# A tibble: 6 × 3\n  PID    trial_n   SDMT\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 PPT_01 Trial_01    38\n2 PPT_01 Trial_02    31\n3 PPT_01 Trial_03    33\n4 PPT_01 Trial_04    13\n5 PPT_01 Trial_05    19\n6 PPT_01 Trial_06    22\n\n\n\n\n\n\n\n\n\n\n\nJoining dataframes\n\n\n\n\n\nThere are lots of different ways to join data-sets, depending on whether we want to keep rows from one data-set or the other, or keep only those in both data-sets etc.\n\n\n\n\n\nCheck out the help documentation for them all using ?full_join.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nNow comes a fun bit.\nWe have two datasets for this study. We’re interested in how the type of audio (information on this is contained in ef_music.csv) interferes with scores on an executive functioning task (scores are held in the ef_sdmt.xlsx).\nWe’re going to need to join these together!\nWe can just stick them side by side, because they’re in different orders:\n\n\n\nhead(ef_music)\n\n# A tibble: 6 × 4\n  PID    trial_n  audio       headphones\n  &lt;chr&gt;  &lt;chr&gt;    &lt;fct&gt;       &lt;fct&gt;     \n1 PPT_01 Trial_02 no_audio    speakers  \n2 PPT_01 Trial_08 no_audio    speakers  \n3 PPT_01 Trial_11 no_audio    speakers  \n4 PPT_01 Trial_13 no_audio    speakers  \n5 PPT_01 Trial_15 no_audio    speakers  \n6 PPT_01 Trial_01 white_noise speakers  \n\n\n\n\n\n\nhead(ef_sdmt_long)\n\n# A tibble: 6 × 3\n  PID    trial_n   SDMT\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 PPT_01 Trial_01    38\n2 PPT_01 Trial_02    31\n3 PPT_01 Trial_03    33\n4 PPT_01 Trial_04    13\n5 PPT_01 Trial_05    19\n6 PPT_01 Trial_06    22\n\n\n\n\nProvided that both data-sets contain information on participant number and trial number, which uniquely identify each observation, we can join them together by matching on those variables!\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe’re going to want to use one of left/right/semi/anti/full_join(), and give the function both the long formatted datasets.\nWe should end up with 600 rows (40 participants * 15 trials each).\n\n\n\n\n\n\n\n Solution \n\n\n\nefdata &lt;- full_join(ef_music, ef_sdmt_long)\nhead(efdata)\n\n# A tibble: 6 × 5\n  PID    trial_n  audio       headphones  SDMT\n  &lt;chr&gt;  &lt;chr&gt;    &lt;fct&gt;       &lt;fct&gt;      &lt;dbl&gt;\n1 PPT_01 Trial_02 no_audio    speakers      31\n2 PPT_01 Trial_08 no_audio    speakers      23\n3 PPT_01 Trial_11 no_audio    speakers      23\n4 PPT_01 Trial_13 no_audio    speakers      24\n5 PPT_01 Trial_15 no_audio    speakers      34\n6 PPT_01 Trial_01 white_noise speakers      38\n\n\n\n\n\n\n\nExercises: Clustering & ICC\n\nQuestion 8\n\n\nContinuing with our audio/executive functioning study, consider the following questions:\nWhat are the units of observations?\nWhat are the groups/clusters?\nWhat varies within these clusters?\nWhat varies between these clusters?\n\n\n\n\n Solution \n\n\nWhat are the units of observations? trials\nWhat are the groups/clusters? participants\nWhat varies within these clusters? the type of audio\nWhat varies between these clusters? whether they listen via headphones or speakers\n\n\n\n\nQuestion 9\n\n\nCalculate the ICC, using the ICCbare() function from the ICC package.\nWhat does it show?\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember, you can look up the help for a function by typing a ? followed by the function name in the console.\n\n\n\n\n\n\n\n Solution \n\n\n47% of the variance in SDMT scores is explained by the participant groupings.\n\nlibrary(ICC)\nICCbare(x = PID, y = SDMT, data = efdata)\n\n[1] 0.4782452\n\n\n\n\n\n\n\n\n\n\n\nICC\n\n\n\n\n\nThink back to the lectures, and about what the ICC represents - the ratio of the variance between the groups to the total variance.\nYou can think of the “variance between the groups” as the group means varying around the overall mean (the black dots around the black line), and the total variance as that plus the addition of the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\nggplot(efdata, aes(x=PID, y=SDMT))+\n  geom_point(aes(col=PID),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(efdata$SDMT,na.rm=T))+\n  theme(axis.text.x = element_text(angle=60,hjust=1))+\n  guides(col='none')\n\n\n\n\n\n\n\n\nYou can also think of the ICC as the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\n\n\n\n\n\n\nOptional: ICC as the expected correlation between two observations from same group\n\n\n\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n  cluster observation   y\n1 group_1           1   4\n2 group_1           2   2\n3 group_2           1   4\n4 group_2           2   2\n5 group_3           1   7\n6 group_3           2   5\n7     ...         ... ...\n\n\nThe ICC for this data is 0.18:\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, but grouped.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen these:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 7     2    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nor these:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional - Extra difficult. Calculate ICC manually\n\n\n\n\n\nWe have equal group sizes here (there are 2 \\(\\times\\) 12 participants, each with 15 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n= & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\] So we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a new column which is the overall mean of y. This bit is fairly straightforward.\n\nefdata %&gt;% mutate(\n  grand_mean = mean(SDMT)\n)\n\n\nWe have seen a lot of the combination of group_by() %&gt;% summarise(), but we can also combine group_by() with mutate()!\n\nTry the following:\n\nefdata %&gt;% mutate(\n    grand_mean = mean(SDMT)\n  ) %&gt;% \n  group_by(PID) %&gt;%\n  mutate(\n    group_mean = mean(SDMT)\n  )\n\n\nThe grouping gets carried forward.\nUsing group_by() can quite easily land you in trouble if you forget that you have grouped the dataframe.\nLook at the output of class() when we have grouped the data. It still mentions something about the grouping.\n\nefdata &lt;- efdata %&gt;% mutate(\n    grand_mean = mean(SDMT)\n  ) %&gt;% \n  group_by(PID) %&gt;%\n  mutate(\n    group_mean = mean(SDMT)\n  )\n\nclass(efdata)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nTo remove the grouping, we can use ungroup() (we could also just add this to the end of our code sequence above and re-run it):\n\nefdata &lt;- ungroup(efdata)\nclass(efdata)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nefdata &lt;- efdata %&gt;% \n  mutate(\n    within = (SDMT-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(efdata$between)\nsswithin = sum(efdata$within)\n\nFinally, we divide them by the degrees of freedom.\n\n# Mean Squares between\nmsb = ssbetween / (24-1)\n# Mean Squares within \nmse = sswithin / (360-24)\n\nAnd calculate the ICC!!!\n\n# ICC\n(msb-mse) /(msb + (14*mse))\n\n[1] 0.4782452\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nWe have two variables of interest here:\n\naudio (type of audio listened to in a trial)\nheadphones (whether or not the participant had headphones on)\n\nWe’re going to look at them separately for now (we’ll get on to addressing the research question next week).\nCompare how the estimates and the uncertainty (the standard errors) for the audio coefficients compare between these two models:\n\nmod1 &lt;- lm(SDMT ~ audio, data = efdata)\nmod2 &lt;- lm(SDMT ~ audio + PID, data = efdata)\n\nand look how the headphones coefficients compare between these two:\n\nmod3 &lt;- lm(SDMT ~ headphones, data = efdata)\nmod4 &lt;- lm(SDMT ~ headphones + PID, data = efdata)\n\nWhat do you notice?\n\n\n\n\n\n\nHints\n\n\n\n\n\nRecall our answers to question 8:\nWhat are the units of observations? trials\nWhat are the groups/clusters? participants\nWhat varies within these clusters? the type of audio\nWhat varies between these clusters? whether they listen via headphones or speakers\n\n\n\n\n\n\n\n Solution \n\n\nOur standard errors for the audio coefficients become narrower when we account for participant-level differences (mod2).\nThis makes sense, because mod2 explains some variation in the audio groups as being due to participants - the highest “white noise” point is actually still a decrease in SDMT compared to “no_audio” for that participant. Whereas mod1 doesn’t know that that highest “white noise” point is high because it comes from a specific participant. Put another way, having a separate line for each participant (RH plot below), gives us more confidence in the differences between audio-types.\nBecause we have perfectly balanced data (every participant has the same number of trials in each audio-type) then our estimates here do not actually change at all.\n\n\n\n\n\n\n\n\n\nIn contrast, our estimates for headphones coefficients do change when we include PID in the model. And the standard errors actually get larger. This is because the inclusion of the participant in mod4 accounts for some of the variance in each group, which means that our comparison between “speakers” and “headphones” is actually a comparison between two groups of 12 participants (RH plot below), rather than 2 groups of 180 trials (LH plot below).\n\n\n\n\n\n\n\n\n\nThis sort of perfectly balanced design has traditionally been approached with extensions of ANOVA (“repeated measures ANOVA”, “mixed ANOVA”). These methods can partition out variance due to one level of clustering (e.g. subjects), and can examine factorial designs when one factor is within cluster, and the other is between. You can see an example here if you are interested. However, ANOVA has a lot of constraints - it can’t handle multiple levels of clustering (e.g. children in classes in schools), it will likely require treating variables such as time as a factor, and it’s not great with missing data.\nThe multi-level model (MLM) provides a more flexible framework, and this is what we will begin to look at next week."
  },
  {
    "objectID": "02_intromlm.html",
    "href": "02_intromlm.html",
    "title": "Multilevel Models",
    "section": "",
    "text": "This Week\n\n\n\n\n\n\n\nA Note on terminology\n\n\n\n\n\nThe methods we’re going to learn about in the first five weeks of this course are known by lots of different names: “multilevel models”; “hierarchical linear models”; “mixed-effect models”; “mixed models”; “nested data models”; “random coefficient models”; “random-effects models”; “random parameter models”… and so on).\nWhat the idea boils down to is that model parameters vary at more than one level. This week, we’re going to explore what that means.\nThroughout this course, we will tend to use the terms “mixed effect model”, “linear mixed model (LMM)” and “multilevel model (MLM)” interchangeably.\n\n\n\n\n\n\n\n\n\nMultilevel Model Notation\n\n\n\n\n\nMultilevel Models (MLMs) (or “Linear Mixed Models” (LMMs)) take the approach of allowing the groups/clusters to vary around our \\(\\beta\\) estimates.\nIn the lectures, we saw this as:\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nWe are now assuming \\(\\color{orange}{\\zeta_0}\\), \\(\\color{orange}{\\zeta_1}\\), and \\(\\varepsilon\\) to be normally distributed with a mean of 0, and we denote their variances as \\(\\sigma_{\\color{orange}{\\zeta_0}}^2\\), \\(\\sigma_{\\color{orange}{\\zeta_1}}^2\\), \\(\\sigma_\\varepsilon^2\\) respectively.\nThe \\(\\color{orange}{\\zeta}\\) components also get termed the “random effects” part of the model, Hence names like “random effects model”, etc.\n\n\n\n\n\n\n\n\n\nAlternative (“mixed effect”) notation\n\n\n\n\n\nMany people use the symbol \\(u\\) in place of \\(\\zeta\\).\nIn various resources, you are likely to see \\(\\alpha\\) used to denote the intercept instead of \\(\\beta_0\\).\nSometimes, you will see the levels collapsed into one equation, as it might make for more intuitive reading. This often fits with the name “mixed effects” for these models:\n\\[\n\\color{red}{y_{ij}}\\color{black} = (\\color{blue}{\\beta_0}\\color{black} + \\color{orange}{\\zeta_{0i}}\\color{black}) \\cdot 1 + ( \\color{blue}{\\beta_{1}}\\color{black} + \\color{orange}{\\zeta_{1i}} \\color{black}) \\cdot x_{ij}  +  \\varepsilon_{ij} \\\\\n\\]\nAnd then we also have the condensed matrix form of the model, in which the Z matrix represents the grouping structure, and the \\(u\\) (or \\(\\zeta\\)) are the estimated random deviations. \\[\n\\mathbf{y} = \\boldsymbol{X\\beta} + \\boldsymbol{Zu} + \\boldsymbol{\\varepsilon}\n\\]\n\n\n\n\n\n\n\n\n\nFitting Multilevel Models: Model Formula\n\n\n\n\n\nTo fit multilevel models, qe’re going to use the lme4 package, and specifically the functions lmer() and glmer().\n“(g)lmer” here stands for “(generalised) linear mixed effects regression”.\nYou will have seen some use of these functions in the lectures. The broad syntax is:\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nWe write the first bit of our formula just the same as our old friend the normal linear model y ~ 1 + x + x2 + ..., where y is the name of our outcome variable, 1 is the intercept (which we don’t have to explicitly state as it will be included anyway) and x, x2 etc are the names of our explanatory variables.\nWith lme4, we now have the addition of random effect terms, specified in parenthesis with the | operator (the vertical line | is often found to the left of the z key on QWERTY keyboards). We use the | operator to separate the parameters (intercept, slope etc.) on the LHS, from the grouping variable(s) on the RHS, by which we would like to model these parameters as varying.\n\nRandom Intercepts\nLet us suppose that we wish to model our intercept not as a fixed constant, but as varying randomly according to some grouping around a fixed center. We can such a model by allowing the intercept to vary by our grouping variable (g below):\n\nlmer(y ~ 1 + x + (1|g), data = df)\n\n\\[\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{Y_{ij}} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot X_{ij}} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\end{align}\n\\]\n\n\nRandom Intercepts and Slopes\nBy extension we can also allow the effect y~x to vary between groups, by including the x on the left hand side of | in the random effects part of the call to lmer().\n\nlmer(y ~ 1 + x + (1 + x |g), data = df)\n\n\\[\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\nFitting Multilevel Models: Model Estimation\n\n\n\n\n\nWe can choose whether to estimate our model parameters with ML (maximum likelihood) or REML (restricted maximum likelihood) with the REML argument of lmer():\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nlmer() models are by default fitted with REML, which tends to be better for small samples.\n\nMaximum Likelihood (ML)\nRemember back to DAPR2 when we introduced logistic regression, and we briefly discussed Maximum likelihood estimation in an explanation of how models are fitted.\nThe key idea of maximum likelihood estimation (MLE) is that we (well, the computer) iteratively finds the set of estimates for our model which it considers to best reproduce our observed data. Recall our simple linear regression model of how time spent outdoors (hrs per week) is associated with mental wellbeing: \\[\n\\color{red}{Wellbeing_i} = \\color{blue}{\\beta_0 \\cdot{} 1 + \\beta_1 \\cdot{} OutdoorTime_{i}} + \\varepsilon_i\n\\] There are values of \\(\\beta_0\\) and \\(\\beta_1\\) and \\(\\sigma_\\varepsilon\\) which maximise the probability of observing the data that we have. For linear regression, these we obtained these same values a different way, via minimising the sums of squares. This approach is not possible for more complex models (e.g., logistic) which is why we turn to MLE.\n\nTo read about the subtle difference between “likelihood” and “probability”, you can find a short explanation here\n\nIf we are estimating just one single parameter (e.g. a mean), then we can imagine the process of maximum likelihood estimation in a one-dimensional world - simply finding the top of the curve:\n\n\n\n\n\nFigure 1: MLE\n\n\n\n\nHowever, our typical models estimate a whole bunch of parameters. The simple regression model above is already having to estimate \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma_\\varepsilon\\), and our multi-level models have far more! With lots of parameters being estimated and all interacting to influence the likelihood, our nice curved line becomes a complex surface (see Left panel of Figure 2). So what we (our computers) need to do is find the maximum, but avoid local maxima and singularities (see Figure 3).\n\n\n\n\n\nFigure 2: MLE for a more complex model\n\n\n\n\n\n\nRestricted Maximum Likelihood (REML)\nWhen it comes to estimating multilevel models, maximum likelihood will consider the fixed effects as fixed, known values when it estimates the variance components (the random effect variances). This leads to biased estimates of the variance components, specifically biasing them toward being too small, especially if \\(n_\\textrm{clusters} - n_\\textrm{level 2 predictors} - 1 &lt; 50\\). This leads to the standard errors of the fixed effects being too small, thereby inflating our type 1 error rate (i.e. greater chance of incorrectly rejecting our null hypothesis).\nRestricted Maximum Likelihood (REML) is a method that separates the estimation of fixed and random parts of the model, leading to unbiased estimates of the variance components.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting Multilevel Models: Model Convergence\n\n\n\n\n\nAlongside the ML/REML choice for model estimation, we have some control over the underlying algorithm that is used to move around/search the likelihood surface for our estimates. We’ll learn more about this next week.\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning. There are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them).\nFor the time being, if lmer() gives you convergence errors, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model.\n\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))\n\n\nWhat is a convergence warning??\nThere are different techniques for maximum likelihood estimation, which we apply by using different ‘optimisers’. Technical problems to do with model convergence and ‘singular fit’ come into play when the optimiser we are using either can’t find a suitable maximum, or gets stuck in a singularity (think of it like a black hole of likelihood, which signifies that there is not enough variation in our data to construct such a complex model).\n\n\n\n\n\nFigure 3: local/global maxima and singularities\n\n\n\n\n\n\n\n\n\nExercises: Cross-Sectional\n\nData: Wellbeing Across Scotland\nRecall our dataset from last week, in which we used linear regression to determine how outdoor time (hours per week) is associated with wellbeing in different local authority areas (LAAs) of Scotland. We have data from various LAAs, from Glasgow City, to the Highlands.\n\nscotmw &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant ID\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nSelf report estimated number of hours per week spent outdoors\n    wellbeing\nWarwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n    density\nLAA Population Density (people per square km)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nUsing lmer() from the lme4 package, fit a model predict wellbeing from outdoor_time, with by-LAA random intercepts.\nPass the model to summary() to see the output.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(lme4)\nri_model &lt;- lmer(wellbeing ~ outdoor_time + (1 | laa), data = scotmw)\nsummary(ri_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: wellbeing ~ outdoor_time + (1 | laa)\n   Data: scotmw\n\nREML criterion at convergence: 866.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2218 -0.7192  0.1217  0.6395  1.8287 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n laa      (Intercept) 106.23   10.307  \n Residual              25.46    5.046  \nNumber of obs: 132, groups:  laa, 20\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  38.18979    2.64778   14.42\noutdoor_time  0.21349    0.07236    2.95\n\nCorrelation of Fixed Effects:\n            (Intr)\noutdoor_tim -0.463\n\n\n\n\n\n\nQuestion 2\n\n\nSometimes the easiest way to start understanding your model is to visualise it.\nLoad the package broom.mixed. Along with some handy functions tidy() and glance() which give us the information we see in summary(), there is a handy function called augment() which returns us the data in the model plus the fitted values, residuals, hat values, Cook’s D etc..\n\nlibrary(broom.mixed)\naugment(model)\n\n\n\n# A tibble: 132 × 14\n   wellbeing outdoor_time laa          .fitted .resid  .hat .cooksd .fixed   .mu\n       &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1        37           20 West Lothian    32.4  4.64  0.139 7.91e-2   42.5  32.4\n 2        34           23 Falkirk         31.7  2.28  0.192 3.00e-2   43.1  31.7\n 3        39           29 Falkirk         33.0  6.00  0.195 2.13e-1   44.4  33.0\n 4        42           21 Scottish Bo…    40.1  1.95  0.163 1.74e-2   42.7  40.1\n 5        37           10 Dumfries an…    37.4 -0.407 0.167 7.84e-4   40.3  37.4\n 6        42           19 Argyll and …    43.9 -1.91  0.122 1.12e-2   42.2  43.9\n 7        38           13 Perth and K…    46.1 -8.06  0.139 2.39e-1   41.0  46.1\n 8        44           21 East Renfre…    44.4 -0.414 0.168 8.17e-4   42.7  44.4\n 9        47           16 Inverclyde      42.7  4.30  0.193 1.07e-1   41.6  42.7\n10        35           12 Midlothian      33.1  1.94  0.161 1.69e-2   40.8  33.1\n# ℹ 122 more rows\n# ℹ 5 more variables: .offset &lt;dbl&gt;, .sqrtXwt &lt;dbl&gt;, .sqrtrwt &lt;dbl&gt;,\n#   .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;\n\n\nAdd to the code below to plot the model fitted values, and color them according to LAA. (you will need to edit ri_model to be whatever name you assigned to your model).\n\naugment(ri_model) %&gt;%\n  ggplot(aes(x = outdoor_time, y = ...... \n\n\n\n\n\n\nSolution\n\n\n\n\naugment(ri_model) %&gt;%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWe have just fitted the model: \\[\n\\begin{align}\n& \\text{For person } j \\text{ in LAA } i \\\\\n& \\color{red}{\\textrm{Wellbeing}_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot \\textrm{Outdoor Time}_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\end{align}\n\\]\nFor our estimates of \\(\\gamma_{00}\\) (the fixed value around which LAA intercepts vary) and \\(\\beta_1\\) (the fixed estimate of the relationship between wellbeing and outdoor time), we can use fixef().\n\nfixef(ri_model)\n\n (Intercept) outdoor_time \n   38.189795     0.213492 \n\n\nCan you add to the plot in the previous question, a thick black line with the intercept and slope given by fixef()?\n\n\n\n\n\n\nHints\n\n\n\n\n\nHint: geom_abline()\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\naugment(ri_model) %&gt;%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_abline(intercept = fixef(ri_model)[1], slope = fixef(ri_model)[2], lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nBy now, you should have a plot which looks more or less like the left-hand figure below (we have added on the raw data - the points).\n\n\n\n\n\n\n\nFigure 4: Model fitted values\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Summary model outputlmer(wellbeing~1 + outdoor_time + (1|laa),data = scotmw)\n\n\n\n\n\n  Match the coloured sections Red, Orange, Yellow and Blue in Figure 5 to the descriptions below of Figure 4 A through D.\n\nwhere the black line cuts the y axis\nthe standard deviation of the distances from all the individual LAA lines to the black line\nthe slope of the black line\nthe standard deviation of the distances from all the individual observations to the line for the LAA to which it belongs.\n\n\n\n\n\n\nSolution\n\n\n\n\nYellow = B\nRed = D\n\nBlue = A\n\nOrange = C\n\n\n\n\n\nQuestion\n\n\nMatch the colours sections and descriptions from the previous question, to the mathematical terms in the model equation:\n\\[\n\\begin{align}\n& \\text{Level 1:} \\\\\n& \\color{red}{Wellbeing_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1} \\cdot OutdoorTime_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n\\quad \\\\\n& \\text{where} \\\\\n& \\color{orange}{\\zeta_0}\\color{black} \\sim N(0, \\sigma_{\\color{orange}{\\zeta_{0}}}\\color{black})  \\text{ independently} \\\\\n& \\varepsilon \\sim N(0, \\sigma_{\\varepsilon}) \\text{ independently} \\\\\n\\end{align}\n\\]\n\n\n\n\n\nSolution\n\n\n\n\nYellow = B = \\(\\sigma_{\\color{orange}{\\zeta_{0}}}\\)\nRed = D = \\(\\sigma_{\\varepsilon}\\)\n\nBlue = A = \\(\\gamma_{00}\\)\n\nOrange = C = \\(\\beta_{1}\\)\n\n\n\n\n\nQuestion 5\n\n\nFit a model which allows also (along with the intercept) the effect of outdoor_time to vary by-LAA.\nThen, using augment() again, plot the model fitted values. What do you think you will see?\nDoes it look like this model better represents the individual LAAs? Take a look at, for instance, Glasgow City.\n\n\n\n\n\nSolution\n\n\n\n\nrs_model &lt;- lmer(wellbeing ~ 1 + outdoor_time + (1 + outdoor_time | laa), data = scotmw)\n\naugment(rs_model) %&gt;%\n  ggplot(aes(x = outdoor_time, y = .fitted, col = laa)) + \n  geom_line() + \n  geom_point(aes(y=wellbeing), alpha=.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises: Repeated Measures\nWhile the wellbeing example considers the groupings or ‘clusters’ of different LAAs, a more common grouping in psychological research is that of several observations belonging to the same individual. One obvious benefit of this is that we can collect many more observations with fewer participants but control for the resulting dependency of observations.\n\nData: Audio interference in executive functioning\nRecall the data from the previous week, from an experiment in which executive functioning was measured (via the SDMT) for people when listening to different types of audio, either via normal speakers or via noise-cancelling headphones.\nThis week, we have data from a replication of that study, in which the researchers managed to recruit 30 participants. Unfortunately, some participants did not complete all the trials, so we have an unbalanced design. The data is available at https://uoepsy.github.io/data/ef_replication.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    PID\nParticipant ID\n    trial_n\nTrial Number (1-15)\n    audio\nAudio heard during the test ('no_audio', 'white_noise','music')\n    headphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n    SDMT\nSymbol Digit Modalities Test (SDMT) score\n  \n  \n  \n\n\n\n\n\n\nQuestion 6\n\n\nHow many participants are there in the data?\nHow many have complete data (15 trials)?\nWhat is the average number of trials that participants completed? What is the minimum?\nDoes every participant have some data for each type of audio?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe count() function will likely be useful here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nefrep &lt;- read_csv(\"https://uoepsy.github.io/data/ef_replication.csv\")\n\nHere are the counts of trials for each participant. We can see that no participant completed all 15 trials. Everyone completed at least 10, and the median was 12.\n\nefrep %&gt;% \n  count(PID) %&gt;%\n  summary()\n\n     PID                  n     \n Length:30          Min.   :10  \n Class :character   1st Qu.:11  \n Mode  :character   Median :12  \n                    Mean   :12  \n                    3rd Qu.:13  \n                    Max.   :14  \n\n\nWe can add in audio to our counting to see that everyone has data from \\(\\geq 2\\) trials for a given audio type.\n\nefrep %&gt;% \n  count(PID,audio) %&gt;%\n  summary()\n\n     PID                    audio          n    \n Length:90          no_audio   :30   Min.   :2  \n Class :character   white_noise:30   1st Qu.:4  \n Mode  :character   music      :30   Median :4  \n                                     Mean   :4  \n                                     3rd Qu.:5  \n                                     Max.   :5  \n\n\n\n\n\n\nQuestion\n\n\nThe model below is sometimes referred to as the “null model” (or “intercept only model”). The grouping structure of the data is specified in the model, but nothing more.\n\nnullmod &lt;- lmer(SDMT ~ 1 + (1 | PID), data = efrep)\n\nFit the model and examine the summary.\nHow much of the variation in SDMT scores is down to participant grouping?\n\n\n\n\n\nSolution\n\n\n\n\nnullmod &lt;- lmer(SDMT ~ 1 + (1 | PID), data = efrep)\nsummary(nullmod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: SDMT ~ 1 + (1 | PID)\n   Data: efrep\n\nREML criterion at convergence: 2664.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.58680 -0.68262  0.03241  0.65701  2.26736 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n PID      (Intercept) 62.02    7.875   \n Residual             79.82    8.934   \nNumber of obs: 360, groups:  PID, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   34.789      1.514   22.98\n\n\n\\(\\frac{62.02}{62.02+79.82} = 0.44\\), or 44% of the variance in SDMT scores is explained by participant differences.\nWe can check this matches (closely enough) with the ICCbare() function from last week:\n\nlibrary(ICC)\nICCbare(x = PID, y = SDMT, data = efrep)\n\n[1] 0.4363587\n\n\n\n\n\n\nQuestion 7\n\n\nSet the reference levels of the audio and headphones variables to “no audio” and “speakers” respectively.\nFit a multilevel model to address the research question below.\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n\n\n\n\n\n\n\nthings to think about:\n\n\n\n\n\n\nwhat is our outcome variable of interest?\nwhat are our predictor variables that we are interested in?\n\nthese should be in the fixed effects part.\n\n\nwhat is the clustering?\n\nthis should be the random effects (1 | cluster) part\n\ndoes audio type (audio) vary within clusters, or between?\n\nif so, we might be able to fit a random slope of audio | cluster. if not, then it doesn’t make sense to do so.\n\n\ndoes delivery mode (headphones) vary within clusters, or between? - if so, we might be able to fit a random slope of headphones | cluster. if not, then it doesn’t make sense to do so.\n\nIf you get an error about model convergence, consider changing the optimiser (see the “estimation” box above)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nefrep &lt;- efrep %&gt;%\n  mutate(\n    audio = fct_relevel(factor(audio), \"no_audio\"),\n    headphones = fct_relevel(factor(headphones), \"speakers\")\n  )\n\n\nsdmt_mod &lt;- lmer(SDMT ~ audio * headphones + \n              (1 + audio | PID), data = efrep,\n              REML = TRUE, control = lmerControl(optimizer=\"bobyqa\"))\nsummary(sdmt_mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: SDMT ~ audio * headphones + (1 + audio | PID)\n   Data: efrep\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 2376.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.34520 -0.62861  0.05762  0.60807  2.21250 \n\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr       \n PID      (Intercept)      51.17    7.153               \n          audiowhite_noise 15.22    3.901    -0.25      \n          audiomusic       13.69    3.700     0.03 -0.16\n Residual                  31.49    5.612               \nNumber of obs: 360, groups:  PID, 30\n\nFixed effects:\n                                          Estimate Std. Error t value\n(Intercept)                               33.25800    1.98899  16.721\naudiowhite_noise                          -0.03044    1.44501  -0.021\naudiomusic                                -8.01731    1.41148  -5.680\nheadphonesanc_headphones                   6.85313    2.81173   2.437\naudiowhite_noise:headphonesanc_headphones  8.02458    2.04497   3.924\naudiomusic:headphonesanc_headphones       -3.58747    2.00128  -1.793\n\nCorrelation of Fixed Effects:\n            (Intr) adwht_ audmsc hdphn_ adw_:_\naudiowht_ns -0.352                            \naudiomusic  -0.174  0.192                     \nhdphnsnc_hd -0.707  0.249  0.123              \nadwht_ns:h_  0.249 -0.707 -0.136 -0.351       \nadmsc:hdph_  0.123 -0.135 -0.705 -0.173  0.191\n\n\n\n\n\n\nQuestion 8\n\n\nWe now have a model, but we don’t have any p-values, confidence intervals, or inferential criteria on which to draw conclusions.\nPick a method of your choosing and perform a test of/provide an interval for the relevant effect of interest.\nProvide a brief write-up of the results along with a visualisation.\n\n\n\n\n\n\nOptions\n\n\n\n\n\nAs with normal regression, we have two main ways in which we can conduct inference. We can focus on our coefficients, or we can compare models.\nThere are a whole load of different methods available for drawing inferences from multilevel models, which means it can be a bit of a never-ending rabbit hole. For the purposes of this course, we’ll limit ourselves to these two:\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\n\n\n\n\nmodel parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\n\n\nmodel comparison\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\ntest\n\n\n\n\n\n\nExercises: Longitudinal\nAnother very crucial advantage of these methods is that we can use them to study how people change over time.\n\nData: Wellbeing in Work\nThe “Wellbeing in Work” dataset contains information on employees who were randomly assigned to one of three employment conditions:\n\ncontrol: No change to employment. Employees continue at 5 days a week, with standard allocated annual leave quota.\n\nunlimited_leave : Employees were given no limit to their annual leave, but were still expected to meet required targets as specified in their job description.\nfourday_week: Employees worked a 4 day week for no decrease in pay, and were still expected to meet required targets as specified in their job description.\n\nWellbeing was was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post.\nThe researchers had two main questions:\n\nQ1): Overall, did the participants’ wellbeing stay the same or did it change?\nQ2): Did the employment condition groups differ in the how wellbeing changed over the assessment period?\n\nThe data is available, in .rda format, at https://uoepsy.github.io/data/wellbeingwork3.rda. You can read it directly into your R environment using:\n\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\n\nAfter running the code above you will find the data in an object called wellbeingwork3 in your environment.\n\n\nQuestion 9\n\n\n\nQ1): Overall, did the participants’ wellbeing stay the same or did it change?\n\nEach of our participants have measurements at 4 assessments. We need to think about what this means for the random effects that we will include in our model (our random effect structure). Would we like our models to accommodate individuals to vary in their overall wellbeing, to vary in how they change in wellbeing over the course of the assessment period, or both?\nTo investigate whether wellbeing changed over the course of the assessments, or whether it stayed the same, we can fit and compare 2 models:\n\nThe “null” or “intercept-only” model.\nA model with wellbeing predicted by time point.\n\nAnd we can then compare them in terms of model fit (as mentioned above, there are lots of different ways we might do this).\nOur sample size here (180 participants, each with 4 observations) is reasonably large given the relative simplicity of our model. We might consider running a straightforward Likelihood Ratio Test using anova(restricted_model, full_model) to compare our two models (in which case we should fit them with REML=FALSE)\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nRemember, we shouldn’t use likelihood ratio tests to compare models with different random effect structures.\n\n(For now, don’t worry too much about “singular fits”. We’ll talk more about how we might deal with them next week!)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThis is our null model:\n\nm.null &lt;- lmer(Wellbeing ~ 1 + (1 | ID), data=wellbeingwork3, REML=FALSE)\nsummary(m.null)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: Wellbeing ~ 1 + (1 | ID)\n   Data: wellbeingwork3\n\n     AIC      BIC   logLik deviance df.resid \n  4400.8   4414.6  -2197.4   4394.8      717 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5224 -0.6067 -0.0426  0.5918  3.6020 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept)  4.762   2.182   \n Residual             22.479   4.741   \nNumber of obs: 720, groups:  ID, 180\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  40.0431     0.2402   166.7\n\n\nWe can see the 4.76 / (4.76 + 22.48), or 0.17 of the total variance is attributable to participant-level variation.\nNow lets suppose we want to compare this null model with a model with an effect of TimePoint (to assess whether there is overall change over time). Which model should we compare m.null to?\n\nmodA &lt;- lmer(Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\nmodB &lt;- lmer(Wellbeing ~ 1 + TimePoint + (1 | ID), data=wellbeingwork3, REML=FALSE)\n\nA comparison between these m.null and modA will not be assessing the influence of only the fixed effect of TimePoint (remember, we shouldn’t compare models that differ in both fixed and random effects)\nHowever, modB doesn’t include our by-participant random effects of timepoint, so comparing this to m.null is potentially going to mis-attribute random deviations in participants’ change to being an overall effect of timepoint.\nIf we want to conduct a model comparison to isolate the effect of overall change over time (a fixed effect of TimePoint), we might want to compare these two models:\n\nm.base0 &lt;- lmer(Wellbeing ~ 1 + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\nm.base &lt;- lmer(Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID), data=wellbeingwork3, REML=FALSE)\n\nThe first of these models is a bit weird to think about - how can we have by-participant random deviations of TimePoint if we don’t have a fixed effect of TimePoint? That makes very little sense. What it is actually fitting is a model where there is assumed to be no overall effect of TimePoint. So the fixed effect is 0.\n\n# Straightforward LRT\nanova(m.base0, m.base)\n\nData: wellbeingwork3\nModels:\nm.base0: Wellbeing ~ 1 + (1 + TimePoint | ID)\nm.base: Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID)\n        npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nm.base0    5 4202.4 4225.2 -2096.2   4192.4                         \nm.base     6 4171.7 4199.2 -2079.8   4159.7 32.649  1  1.104e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nQuestion 10\n\n\n\nQ: Did the employment condition groups differ in the how wellbeing changed over the assessment period?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nIt helps to break it down. There are two questions here:\n\ndo groups differ overall?\n\ndo groups differ over time?\n\nWe can begin to see that we’re asking two questions about the Condition variable here: “is there an effect of Condition?” and “Is there an interaction between TimePoint and Condition?”.\nTry fitting two more models which incrementally build these levels of complexity, and compare them (perhaps to one another, perhaps to models from the previous question - think about what each comparison is testing!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nm.int &lt;- lmer(Wellbeing ~ 1 + TimePoint + Condition + (1 + TimePoint | ID), \n              data=wellbeingwork3, REML=FALSE)\nm.full &lt;- lmer(Wellbeing ~ 1+ TimePoint*Condition + (1 + TimePoint | ID), \n               data=wellbeingwork3, REML=FALSE)\n\nWe’re going to compare each model to the previous one to examine the improvement in fit due to inclusion of each parameter. We could do this quickly with\n\nanova(m.base0, m.base, m.int, m.full)\n\nData: wellbeingwork3\nModels:\nm.base0: Wellbeing ~ 1 + (1 + TimePoint | ID)\nm.base: Wellbeing ~ 1 + TimePoint + (1 + TimePoint | ID)\nm.int: Wellbeing ~ 1 + TimePoint + Condition + (1 + TimePoint | ID)\nm.full: Wellbeing ~ 1 + TimePoint * Condition + (1 + TimePoint | ID)\n        npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nm.base0    5 4202.4 4225.2 -2096.2   4192.4                         \nm.base     6 4171.7 4199.2 -2079.8   4159.7 32.649  1  1.104e-08 ***\nm.int      8 4164.3 4200.9 -2074.2   4148.3 11.393  2   0.003358 ** \nm.full    10 4144.6 4190.4 -2062.3   4124.6 23.711  2  7.098e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nConditions differed overall in wellbeing change \\(\\chi^2(2)=11.39, p = .003\\)\nConditions differed in change over assessment period \\(\\chi^2(2)=23.71, p &lt; .001\\)\n\n\n\n\n\nQuestion 11\n\n\nVisualise the model estimated change in wellbeing over time for each Condition.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThere are lots of ways you can visualise the model, try a couple:\n\nUsing the effects package, this might help: as.data.frame(effect(\"TimePoint:Condition\", model))\n\nWe can also use sjPlot, as we have seen in DAPR2\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nUsing the effect() function (and then adding the means and SEs from the original data):\n\nef &lt;- as.data.frame(effect(\"TimePoint:Condition\", m.full))\n\nggplot(ef, aes(TimePoint, fit, color=Condition)) + \n  geom_line() +\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=Condition), alpha=.2)+\n  stat_summary(data=wellbeingwork3, aes(y=Wellbeing), \n               fun.data=mean_se, geom=\"pointrange\", size=1) +\n  theme_bw()\n\n\n\n\n\n\n\n\nAdditionally, sjPlot can give us the model fitted values, but it’s trickier to add on the observed means. We can add the raw data using show.data=TRUE, but that will make it a bit messier\n\nlibrary(sjPlot)\nplot_model(m.full, type=\"int\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 12\n\n\nExamine the parameter estimates and interpret them (i.e., what does each parameter represent?\nCan you match them with parts of the plot obtained from plot_model(m.full, type=\"int\")?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe can get the fixed effects using fixef(model), and we can also use tidy(model) from the broom.mixed package, and similar to lm models in DAPR2, we can pull out the bit of the summary() using summary(model)$coefficients.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n                                   Estimate Std. Error t value\n(Intercept)                          38.352      0.394  97.270\nTimePoint                            -0.023      0.322  -0.072\nConditionunlimited_leave             -0.018      0.558  -0.033\nConditionfourday_week                -0.260      0.558  -0.466\nTimePoint:Conditionunlimited_leave    1.357      0.456   2.976\nTimePoint:Conditionfourday_week       2.282      0.456   5.005\n\n\n\n(Intercept) ==&gt; Wellbeing at baseline in ‘control’ group.\n\nTimePoint ==&gt; Slope of welleing change in ‘control’ group.\n\nConditionunlimited_leave ==&gt; baseline wellbeing difference from ‘unlimited_leave’ group relative to ‘control’ group.\n\nConditionfourday_week ==&gt; baseline wellbeing difference from ‘fourday_week’ group relative to ‘control’ group.\n\nTimePoint:Conditionunlimited_leave ==&gt; slope of wellbeing change in ‘unlimited_leave’ group relative to ‘control’ group.\n\nTimePoint:Conditionfourday_week ==&gt; slope of wellbeing change in ‘fourday_week’ group relative to ‘control’ group.\n\n\nplot_model(m.full, type=\"int\")\n\n\n\n\nFigure 6: Well-being over time, for employees working in different conditions\n\n\n\n\n\n(Intercept) ==&gt; the height of the red line at timepoint 0.\n\nTimePoint ==&gt; slope of the red line (basically flat).\n\nConditionunlimited_leave ==&gt; the difference at timepoint 0 in height of the blue line from height of the red line (basically 0, but blue line starts ever so slightly below the red).\n\nConditionfourday_week ==&gt; the difference at timepoint 0 in height of the green line from height of the red line (also very small, but it is a bit further below the red than the blue is).\n\nTimePoint:Conditionunlimited_leave ==&gt; the difference from slope of red line to slope of blue line. i.e. while the red line goes -0.023 for every 1 across, the blue line goes \\(-0.023+1.357=1.334\\) up for every 1 across.\n\nTimePoint:Conditionfourday_week ==&gt; difference from slope of red line to slope of green line. i.e. while the red line goes -0.023 for every 1 across, the green line goes \\(-0.023+2.282=2.259\\) up for every 1 across.\n\n\nCompared to the control group, wellbeing increased by 1.36 points/year more for employees with unlimited leave, and by 2.28 points/year for employees on the 4 day week."
  },
  {
    "objectID": "03_assumptranef.html",
    "href": "03_assumptranef.html",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "",
    "text": "This Week"
  },
  {
    "objectID": "03_assumptranef.html#footnotes",
    "href": "03_assumptranef.html#footnotes",
    "title": "Assumptions, Diagnostics, and Random Effect Structures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s always going to be debateable about what is ‘too high’ because in certain situations you might expect correlations close to 1. It’s best to think through whether it is a feasible value given the study itself↩︎"
  },
  {
    "objectID": "04_centerglmer.html",
    "href": "04_centerglmer.html",
    "title": "Centering in MLM | Logistic MLM",
    "section": "",
    "text": "This Week\n\n\n\n\n\n\n\nCentering & Scaling in LM\n\n\n\n\n\nWe have some data from a study investigating how perceived persuasiveness of a speaker is influenced by the rate at which they speak (you may remember this from the first report in DAPR2 last year!).\n\ndap2 &lt;- read_csv(\"https://uoepsy.github.io/data/dapr2_2122_report1.csv\")\n\nWe can fit a simple linear regression (one predictor) to evaluate how speech rate (variable sp_rate in the dataset) influences perceived persuasiveness (variable persuasive in the dataset). There are various ways in which we can transform the predictor variable sp_rate, which in turn can alter the interpretation of some of our estimates:\n\n\nRaw X\n\nm1 &lt;- lm(persuasive ~ sp_rate, data = dap2)\nsummary(m1)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 55.532060  6.4016670  8.674625 6.848945e-15\nsp_rate     -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\nThe intercept and the coefficient for neuroticism are interpreted as:\n\n(Intercept): A audio clip of someone speaking at zero phones per second is estimated as having an average persuasive rating of 55.53.\n\nsp_rate: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nMean-Centered X\nWe can mean center our predictor and fit the model again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_mc = sp_rate - mean(sp_rate))\nm2 &lt;- lm(persuasive ~ sp_rate_mc, data = dap2)\nsummary(m2)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667  1.3519418 39.110165 6.429541e-80\nsp_rate_mc  -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_mc: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nStandardised X\nWe can standardise our predictor and fit the model yet again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_z = scale(sp_rate))\nm3 &lt;- lm(persuasive ~ sp_rate_z, data = dap2)\nsummary(m3)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667   1.351942 39.110165 6.429541e-80\nsp_rate_z   -0.576077   1.356471 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_z: For every increase of one standard deviation in phones per second, perceived persuasiveness is estimated to decrease by -0.58.\n\nRemember that the scale(sp_rate) is subtracting the mean from each value, then dividing those by the standard deviation. The standard deviation of dap2$sp_rate is:\n\nsd(dap2$sp_rate)\n\n[1] 3.016315\n\n\nso in our variable dap2$sp_rate_z, a change of 3.02 gets scaled to be a change of 1 (because we are dividing by sd(dap2$sp_rate)).\n\ncoef(m1)[2] * sd(dap2$sp_rate)\n\n  sp_rate \n-0.576077 \n\ncoef(m3)[2]\n\nsp_rate_z \n-0.576077 \n\n\n\n\nNote that these models are identical. When we conduct a model comparison between the 3 models, the residual sums of squares is identical for all models:\n\nanova(m1,m2,m3)\n\nAnalysis of Variance Table\n\nModel 1: persuasive ~ sp_rate\nModel 2: persuasive ~ sp_rate_mc\nModel 3: persuasive ~ sp_rate_z\n  Res.Df   RSS Df Sum of Sq F Pr(&gt;F)\n1    148 40576                      \n2    148 40576  0         0         \n3    148 40576  0         0         \n\n\nWhat changes when you center or scale a predictor in a standard regression model (one fitted with lm())?\n\nThe variance explained by the predictor remains exactly the same\nThe intercept will change to be the estimated mean outcome where that predictor is “0”. Scaling and centering changes what “0” represents, thereby changing this estimate (the significance test will therefore also change because the intercept now has a different meaning)\nThe slope of the predictor will change according to any scaling (e.g. if you divide your predictor by 10, the slope will multiply by 10).\nThe test of the slope of the predictor remains exactly the same.\n\n\n\n\n\nExercises: Centering in the MLM\n\nData: Hangry\nThe study is interested in evaluating whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nThe data are available at: https://uoepsy.github.io/data/hangry.csv.\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant\n\n\nfivetwo\nWhether the participant follows the five-two diet\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead carefully the description of the study above, and try to write out (in lmer syntax) an appropriate model to test the research aims.\ne.g.:\noutcome ~ explanatory variables + (???? | grouping)\nTry to think about the maximal random effect structure (i.e. everything that can vary by-grouping is estimated as doing so).\nTo help you think through the steps to get from a description of a research study to a model specification, think about your answers to the following questions.\nQ: What is our outcome variable?\nQ: What are our explanatory variables?\nQ: Is there any grouping (or “clustering”) of our data that we consider to be a random sample? If so, what are the groups?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet.\nWe can split our data in to groups of each participant. We can also split it into groups of each diet. Which of these groups have we randomly sampled? Do we have a random sample of participants? Do we have a random sample of diets? Another way to think of this is “if i repeated the experiment, what these groups be different?”\n\n\n\n\n\n\n\n\n Solution \n\n\nOur outcome is irritability here, because it is the thing that we are trying to explain through peoples’ hunger levels and diets.\n\nlmer(irritability ~  explanatory variables + (???? | grouping))\n\nWe are interested in the effect of hunger on irritability, and whether this effect is different for the five-two diet. So we are interested in the interaction:\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | grouping))\n\n(remember that hunger + diet + hunger:diet is just a more explicit way of writing hunger*diet).\nIf we did this experiment again, would we have different participants?\nYes. If we did this experiment again, would we have different diets? No, because we’re interested in the specific differences between the five-two diet and no dieting. This means we will likely want to by-participant random deviations (e.g. the ( ... | participant) bit in lmer). But we won’t have by-diet random effects (1 | diet) because the diet differences are the specific differences that we wish to test.\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | participant))\n\nThinking about what can be modelled as randomly varying between participants, we have some options:\n\nparticipants vary in how irritable they are on average\n(the intercept, 1 | participant)\nparticipants vary in how much hunger influences their irritability\n(the effect of hunger, hunger | participant)\nparticipants vary in how much diet influences irritability\n(the effect of diet, diet | participant)\nparticipants vary in how much diet effects hunger’s influence on irritability\n(the interaction between diet and hunger, diet:hunger | participant)\n\nWe can vary 1 and 2, but not 3 and 4. This is because each participant is either following the five-two diet or they are not. So for a single participant, we can’t assess “the effect diet has” on anything, because we haven’t seen that participant under different diets. if we try to plot a single participants’ data, we can see that it is impossible for us to assess “the effect of diet”:\n\n\n\n\n\n\n\n\n\nBy contrast, we can vary the intercept and the effect of hunger, because each participant has multiple values of irritability, and multiple different observations of hunger. We can think about a single participant’s “effect of hunger on irritability” and how we might fit a line to their data:\n\n\n\n\n\n\n\n\n\n\nlmer(irritability ~  hunger + diet + hunger:diet + (1 + hunger | participant))\n\n\n\n\n\nTotal, Within, Between\nRecall our research aim:\n\n… whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting.\n\nForgetting about any differences due to diet, let’s just think about the relationship between irritability and hunger. How should we interpret this research aim?\nWas it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a composite of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that the ‘total effect’ isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it about how person i changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are.).\n\n\nQuestion 2\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll find group_by() %&gt;% mutate() very useful here.\n\n\n\n\n\n\n\n Solution \n\n\n\nhangry &lt;- \n    hangry %&gt;% group_by(ppt) %&gt;%\n        mutate(\n            avg_hunger = mean(q_hunger),\n            hunger_gc = q_hunger - avg_hunger\n        )\nhead(hangry)\n\n# A tibble: 6 × 6\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   fivetwo avg_hunger hunger_gc\n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1             17       30 N1p1  1             26.6     3.4  \n2             19       27 N1p1  1             26.6     0.400\n3             19       29 N1p1  1             26.6     2.4  \n4             20       33 N1p1  1             26.6     6.4  \n5             24       14 N1p1  1             26.6   -12.6  \n6             30       28 N1p2  1             32.6    -4.6  \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people?\n\nDoes it look like when people are more hungry than normal, they are more irritable?\n\n\n\n\n\n Solution \n\n\nWe might find it easier to look at a plot where each participant is represented as their mean plus an indication of their range of irritability scores:\n\nggplot(hangry,aes(x=avg_hunger,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nThere appears to be a slight positive relationship between a persons’ average hunger and their irritability scores.\nIt is harder to tell what the relationship is between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data:\n\nggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt))+\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2)\n\n\n\n\n\n\n\n\nI think there might be a positive trend in here, in that participants tend to be higher irritability when they are higher (for them) on the hunger score.\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation level deviations from those averages), that represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables as predictors, instead of the raw hunger scores.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nhunger * diet could be replaced by (hunger1 + hunger2) * diet, thereby allowing each aspect of hunger to interact with diet.\nWe can only put one of these variables in the random effects (1 + hunger | participant). Recall that above we discussed how we cannot have (diet | participant), because “an effect of diet” makes no sense for a single participant (they are either on the diet or they are not, so there is no ‘effect’). Similarly, each participant has only one value for their average hungriness.\n\n\n\n\n\n\n\n\n Solution \n\n\n\nlibrary(lme4)\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                control = lmerControl(optimizer=\"bobyqa\"))\n\n\n\n\n\nQuestion 5\n\n\nHopefully, you have fitted a model similar to the below:\n\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc) * fivetwo + \n                (1 + hunger_gc | ppt), data = hangry,\n            control = lmerControl(optimizer=\"bobyqa\"))\n\nBelow, we have obtained p-values using the Satterthwaite Approximation of \\(df\\) for the test of whether the fixed effects are zero, so we can see the significance of each estimate.\nProvide an answer for each of these questions:\n\nFor those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\nDo people following no diet tend to be more irritable when they are more hungry than they usually are?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\n(Trickier:) What does the fivetwo coefficient represent?\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      Estimate\n      Std. Error\n      df\n      t value\n      Pr(&gt;|t|)\n    \n  \n  \n    (Intercept)\n17.131\n5.15\n77\n3.33\n0.0013\n    avg_hunger\n0.004\n0.11\n77\n0.04\n0.9708\n    hunger_gc\n0.186\n0.08\n65.4\n2.46\n0.0167\n    fivetwo1\n-10.855\n6.54\n77\n-1.66\n0.1008\n    avg_hunger:fivetwo1\n0.466\n0.13\n77\n3.49\n&lt;0.001\n    hunger_gc:fivetwo1\n0.381\n0.1\n68.5\n3.76\n&lt;0.001\n  \n  \n  \n\n\n\n\n\n\n\n\n Solution \n\n\n1: For those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nA: ‘No diet’ is the reference level of the five-two variable, and because we have an interaction, that means the avg_hunger coefficient will provide the relevant estimate. There is no evidence (\\(p&gt;.05\\)) to suggest that when not dieting, hungrier people are more irritable than less hungry people.\n2: Is there evidence to suggest that this is different for those following the five-two diet? In what way?\nA: This is the interaction between avg_hunger:fivetwo1. We can see that, for every increase of 1 in average hunger, irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet.\nThese units are still in terms of the original scale (i.e. 0 to 100).\n3: Do people following no diet tend to be more irritable when they are more hungry than they usually are? A: This is the estimate for the coefficient of hunger_gc. For people following no diet, there is an estimated 0.19 increase in irritability for every 1 unit more hungry they become.\n4: Is there evidence to suggest that this is different for those following the five-two diet? In what way? A: This effect of a 1 unit change on within-person hunger increasing irritability is increased for those who are following the five-two diet by an additional 0.38\n5: What does the fivetwo1 coefficient represent? A: This represents the group difference of irritability between those on the five-two diet vs those not dieting, for someone who has an average hunger score of 0.\n\n\n\n\nQuestion 6\n\n\nConstruct two plots showing the two model estimated interactions. Think about your answers to the previous question, and check that they match with what you are seeing in the plots (do not underestimate the utility of this activity for helping understanding!).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis isn’t as difficult as it sounds. the sjPlot package can do it in one line of code!\n\n\n\n\n\n\n\n Solution \n\n\n\nlibrary(sjPlot)\nplot_model(hangrywb, type = \"int\")[[1]]\n\n\n\n\n\n\n\n\nWe saw in the model coefficients that for the reference level of fivetwo, the “No Diet” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. We also saw the interaction avg_hunger:fivetwo1 indicates that irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\nplot_model(hangrywb, type = \"int\")[[2]]\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_gc we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re on “No Diet”). This is the slope of the red line. The interaction hunger_gc:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive and significant, which matches with the fact that the blue line is clearly steeper in this plot.\n\n\n\n\nQuestion 7\n\n\nLoad the lmerTest package and fit the model again. Take a look at the summary - you should now have the \\(df\\), \\(t\\)-value, and \\(p\\)-value for each estimate.\nWrite-up the results.\n\n\n\n\n Solution \n\n\n\nlibrary(lmerTest)\nhangrywb2 &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                REML = TRUE,\n                control = lmerControl(optimizer=\"bobyqa\"))\nsummary(hangrywb2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nq_irritability ~ (avg_hunger + hunger_gc) * fivetwo + (1 + hunger_gc |  \n    ppt)\n   Data: hangry\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 2734.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.41378 -0.59064 -0.04539  0.54264  2.39536 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ppt      (Intercept) 48.0827  6.9342        \n          hunger_gc    0.1451  0.3809   -0.01\n Residual             23.3050  4.8275        \nNumber of obs: 405, groups:  ppt, 81\n\nFixed effects:\n                      Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)          17.130959   5.146478  76.999477   3.329 0.001341 ** \navg_hunger            0.003863   0.105327  76.998524   0.037 0.970835    \nhunger_gc             0.185773   0.075601  65.409225   2.457 0.016659 *  \nfivetwo1            -10.854713   6.535684  76.999696  -1.661 0.100813    \navg_hunger:fivetwo1   0.465897   0.133539  76.998690   3.489 0.000806 ***\nhunger_gc:fivetwo1    0.381412   0.101393  68.486392   3.762 0.000352 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) avg_hn hngr_g fivtw1 avg_:1\navg_hunger  -0.971                            \nhunger_gc   -0.002  0.000                     \nfivetwo1    -0.787  0.765  0.002              \navg_hngr:f1  0.766 -0.789  0.000 -0.968       \nhngr_gc:fv1  0.002  0.000 -0.746 -0.002  0.000\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on a restricted diet such as the five-two, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes of relative-hunger level were included for participants. The model was fitting with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser from the lme4. \\(P\\)-values were obtained using the Satterthwaite approximation for degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(\\beta = 0.186,\\ SE = 0.08,\\ t(65.4) = 2.46,\\ p = 0.0167\\)), and that this was increased for those following the five-two diet (\\(\\beta = 0.381,\\ SE = 0.1,\\ t(68.5) = 3.76,\\ p &lt;0.001\\)). Although for those not on a specific diet there was no evidence for an association between irritability and being generally a more hungry person (\\(p = 0.9708\\)), there a significant interaction was found between average hunger and being on the five-two diet (\\(\\beta = 0.466,\\ SE = 0.13,\\ t(77) = 3.49,\\ p &lt;0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association for both between-hunger and within-hunger with irritability.\n\n\n\n\nOther within-group transformations\nAs well as within-group mean centering a predictor (like we have done above). There are quite a few similar things we can do, for which the logic is the same.\nFor instance, we can within-group standardise a predictor. This would disagregate within and between effects, but interpretation would of the within effect would be the estimated change in \\(y\\) associated with being 1 standard deviation higher in \\(x\\) for that group.\nWe can also do within-group transformations on our outcome variable. This allows to address questions such as:\n“Are people more irritable than they usually are (\\(y\\) is group-mean centered) if they are, for them, more hungry than they usually are (\\(x\\) is group-mean centered)?”\n\n\n\nOptional: Logistic MLM\n\nDon’t forget to look back at other materials!\nBack in DAPR2, we introduced logistic regression in semester 2, week 8. The lab contained some simulated data based on a hypothetical study about inattentional blindness. That content will provide a lot of the groundwork for this week, so we recommend revisiting it if you feel like it might be useful.\n\n\nlmer() &gt;&gt; glmer()\nRemember how we simply used glm() and could specify the family = \"binomial\" in order to fit a logistic regression? Well it’s much the same thing for multi-level models!\n\nGaussian model: lmer(y ~ x1 + x2 + (1 | g), data = data)\n\nBinomial model: glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial(link='logit'))\n\nor just glmer(y ~ x1 + x2 + (1 | g), data = data, family = \"binomial\")\nor glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial)\n\n\n\n\n Binary? Binomial?\n\n\nFor binary regression, all the data in our outcome variable has to be a 0 or a 1.\nFor example, the correct variable below:\n\n\n\n\n\n\n  \n    \n    \n      participant\n      question\n      correct\n    \n  \n  \n    1\n1\n1\n    1\n2\n0\n    1\n3\n1\n    ...\n...\n...\n  \n  \n  \n\n\n\n\nBut we can re-express this information in a different way, when we know the total number of questions asked.\n\n\n\n\n\n\n  \n    \n    \n      participant\n      questions_correct\n      questions_incorrect\n    \n  \n  \n    1\n2\n1\n    2\n1\n2\n    3\n3\n0\n    ...\n...\n...\n  \n  \n  \n\n\n\n\nTo model data when it is in this form, we can express our outcome as cbind(questions_correct, questions_incorrect)\n\n\n\n\nMemory Recall & Finger Tapping\n\nResearch Question: After accounting for effects of sentence length, does the rhythmic tapping of fingers aid memory recall?\n\nResearchers recruited 40 participants. Each participant was tasked with studying and then recalling 10 randomly generated sentences between 1 and 14 words long. For 5 of these sentences, participants were asked to tap their fingers along with speaking the sentence in both the study period and in the recall period. For the remaining 5 sentences, participants were asked to sit still.\nThe data are available at https://uoepsy.github.io/data/memorytap.csv, and contains information on the length (in words) of each sentence, the condition (static vs tapping) under which it was studied and recalled, and whether the participant was correct in recalling it.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier (n=40)\n    slength\nNumber of words in sentence\n    condition\nCondition under which sentence is studied and recalled ('static' = sitting still, 'tap' = tapping fingers along to sentence)\n    correct\nWhether or not the sentence was correctly recalled\n  \n  \n  \n\n\n\n\n\n\nQuestion 8 (Optional)\n\n\n\nResearch Question: After accounting for effects of sentence length, does the rhythmic tapping of fingers aid memory recall?\n\nFit an appropriate model to answer the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nour outcome is conceptually ‘memory recall’, and it’s been measured by “Whether or not a sentence was correctly recalled”. This is a binary variable.\n\nwe have multiple observations for each ?????\nThis will define our (  | ??? ) bit\n\n\n\n\n\n\n\n\n Solution \n\n\n\nmemtap &lt;- read_csv(\"https://uoepsy.github.io/data/memorytap.csv\")\n\nWhen we fit the maximal model, note that we obtain a singular fit. The variance of the slength effect between participants is quite small relative to the others, and there is a correlation between it and the random intercepts.\n\ntapmod &lt;- glmer(correct ~ 1 + slength + condition + \n                  (1 + slength + condition | ppt),\n      data = memtap,\n      family = binomial)\nisSingular(tapmod)\n\n[1] TRUE\n\nVarCorr(tapmod)\n\n Groups Name         Std.Dev. Corr         \n ppt    (Intercept)  1.032849              \n        slength      0.070307 -1.000       \n        conditiontap 0.665626  0.590 -0.590\n\n\nlet’s remove the random effect of slength | ppt.\n\ntapmod2 &lt;- glmer(correct ~ 1 + slength + condition + \n                  (1 + condition | ppt),\n      data = memtap,\n      family = binomial)\n\nthe model now looks a bit better (not a singular fit):\n\nsummary(tapmod2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: correct ~ 1 + slength + condition + (1 + condition | ppt)\n   Data: memtap\n\n     AIC      BIC   logLik deviance df.resid \n   537.6    561.5   -262.8    525.6      394 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8949 -0.8955  0.4483  0.7962  1.6862 \n\nRandom effects:\n Groups Name         Variance Std.Dev. Corr\n ppt    (Intercept)  0.2755   0.5249       \n        conditiontap 0.4207   0.6486   0.66\nNumber of obs: 400, groups:  ppt, 40\n\nFixed effects:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   0.76140    0.37077   2.054   0.0400 *\nslength      -0.12086    0.04721  -2.560   0.0105 *\nconditiontap  0.50945    0.24317   2.095   0.0362 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) slngth\nslength     -0.890       \nconditiontp -0.154 -0.029\n\n\n\n\n\n\nTake some time to remind yourself from DAPR2 of the interpretation of logistic regression coefficients.\nIn family = binomial(link='logit'), we are modelling the log-odds. We can obtain estimates on this scale using:\n\nfixef(model)\nsummary(model)$coefficients\ntidy(model) from broom.mixed\n\n(there are probably more ways, but I can’t think of them right now!)\n\nWe can use exp(), to get these back into odds and odds ratios.\n\n\nQuestion 9 (Optional)\n\n\nInterpret each of the fixed effect estimates from your model.\n\n\n\n\n Solution \n\n\n\nfixef(tapmod2)\n\n (Intercept)      slength conditiontap \n   0.7613976   -0.1208591    0.5094460 \n\nexp(fixef(tapmod2))\n\n (Intercept)      slength conditiontap \n   2.1412669    0.8861589    1.6643689 \n\n\n\n(Intercept): For an sentence with zero words, when sitting statically, the odds of correctly recalling the sentence are 2.14. This is equivalent to a \\(\\frac{2.14}{1 + 2.14} = 0.6815287\\) probability of getting it correct.\n\nslength: After accounting for differences due to tapping/not-tapping during study & recall, for every 1 word longer a sentence is, the odds of correctly recalling the sentence is decreased by 0.89.\nconditiontap: After accounting for differences in recall due to sentence length, finger tapping during the study and recall of sentences was associated with 1.66 increased odds correct recall in comparison to sitting still.\n\n\n\n\n\nQuestion 10 (Optional)\n\n\nChecking the assumptions in non-gaussian models in general (i.e. those where we set the family to some other error distribution) can be a bit tricky, and this is especially true for multilevel models.\nFor the logistic MLM, the standard assumptions of normality etc for our Level 1 residuals residuals(model) do not hold. However, it is still useful to quickly plot the residuals and check that \\(|residuals|\\leq 2\\) (or \\(|residuals|\\leq 3\\) if you’re more relaxed). We don’t need to worry too much about the pattern though.\nWhile we’re more relaxed about Level 1 residuals, we do still want our random effects ranef(model) to look fairly normally distributed.\n\nPlot the level 1 residuals and check whether any are greater than 3 in magnitude\nPlot the random effects (the level 2 residuals) and assess the normality.\n\n\n\n\n\n\n\nfor beyond DAPR3\n\n\n\n\n\n\nThe HLMdiag package doesn’t support diagnosing influential points/clusters for glmer, but there is a package called influence.me which might help: https://journal.r-project.org/archive/2012/RJ-2012-011/RJ-2012-011.pdf\nThere are packages which aim to create more interpretable residual plots for these models via simulation, such as the DHARMa package: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\n\n\n\n\n\n\n\n Solution \n\n\n\nplot(tapmod2)\n\n\n\n\n\n\n\nsum(abs(resid(tapmod2))&gt;3)\n\n[1] 0\n\n\nAll residuals are between -3 and 3.\nThe random effects look okay here. Not perfect, but bear in mind we have only 40 participants.\n\nqqnorm(ranef(tapmod2)$ppt[, 1], main = \"Random intercept\")\nqqline(ranef(tapmod2)$ppt[, 1])\nqqnorm(ranef(tapmod2)$ppt[, 2], main = \"Random slope of condition\")\nqqline(ranef(tapmod2)$ppt[, 2])\nhist(ranef(tapmod2)$ppt[, 1])\nhist(ranef(tapmod2)$ppt[, 2])"
  },
  {
    "objectID": "05_recap.html",
    "href": "05_recap.html",
    "title": "Recap of multilevel models",
    "section": "",
    "text": "This Week\nThis week, there aren’t any exercises, but there are some ‘flashcard’ dropdowns to help you test your understanding of some of the key concepts.\nPlease use the lab sessions to go over exercises from previous weeks, as well as asking any questions about the content below.\n\n\nFlashcards: lm to lmer\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\n\n\n\n\n\n\n\nSimple Linear Regression”\n\n\n\n\n\n\nFormula:\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustered (multi-level) data\n\n\n\n\n\nWhen our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nIf we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nBy including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nWe can now see that the intercept estimate \\(\\beta_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has an adjustment for each subject.\nEach subject’s line has been moved up or down accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage\n\n\n\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different:\n\n\n\n\n\n\n\n\n\nWhy? One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters.\n\n\n\n\n\n\n\n\n\nRandom slopes\n\n\n\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(\\beta_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nAs with the intercept \\(\\beta_{0i}\\), the slope of the predictor \\(\\beta_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed effects\n\n\n\n\n\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the groups all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\nThese are the overall intercept and slope.\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n405.7897675  -0.6722654 \n\n\n\n\n\n\n\n\n\n\n\nRandom effects\n\n\n\n\n\nThe plots below show the fitted values for each subject from each model that we have gone through in these expandable boxes (simple linear regression, random intercept, and random intercept & slope):\n\n\n\n\n\n\n\n\n\nIn the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_0\\). The standard deviation (and variance, which is \\(\\sigma_0^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function).\n\n\n\n\n\n\n\n\n\nIn the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. We can extract the deviations for each group from the fixed effect estimates using the ranef() function.\nThese are the deviations from the overall intercept (\\(\\widehat \\gamma_{00} = 405.79\\)) and slope (\\(\\widehat \\gamma_{10} = -0.672\\)) for each subject \\(i\\).\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)          x1\nsub_308   31.327291 -1.43995253\nsub_309  -28.832219  0.41839420\nsub_310    2.711822  0.05993766\nsub_330   59.398971  0.38526670\nsub_331   74.958481  0.17391602\nsub_332   91.086535 -0.23461836\nsub_333   97.852988 -0.19057838\nsub_334  -54.185688 -0.55846794\nsub_335  -16.902018  0.92071637\nsub_337   52.217859 -1.16602280\nsub_349  -67.760246 -0.68438960\nsub_350   -5.821271 -1.23788002\nsub_351   61.198823  0.05499816\nsub_352   -7.905596 -0.66495059\nsub_369  -47.636645 -0.46810258\nsub_370  -33.121093 -1.11001234\nsub_371   77.576205 -0.20402571\nsub_372  -36.389281 -0.45829505\nsub_373 -197.579562  1.79897904\nsub_374  -52.195357  4.60508775\n\nwith conditional variances for \"subject\" \n\n\n\n\n\n\n\n\n\n\n\nGroup-level coefficients\n\n\n\n\n\nWe can see the estimated intercept and slope for each subject \\(i\\) specifically, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)         x1\nsub_308    437.1171 -2.1122179\nsub_309    376.9575 -0.2538712\nsub_310    408.5016 -0.6123277\nsub_330    465.1887 -0.2869987\nsub_331    480.7482 -0.4983494\nsub_332    496.8763 -0.9068837\nsub_333    503.6428 -0.8628438\nsub_334    351.6041 -1.2307333\nsub_335    388.8877  0.2484510\nsub_337    458.0076 -1.8382882\nsub_349    338.0295 -1.3566550\nsub_350    399.9685 -1.9101454\nsub_351    466.9886 -0.6172672\nsub_352    397.8842 -1.3372160\nsub_369    358.1531 -1.1403680\nsub_370    372.6687 -1.7822777\nsub_371    483.3660 -0.8762911\nsub_372    369.4005 -1.1305604\nsub_373    208.2102  1.1267137\nsub_374    353.5944  3.9328224\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nNotice that the above are the fixed effects + random effects estimates, i.e. the overall intercept and slope + deviations for each subject.\n\ncbind(\n  int = fixef(random_slopes_model)[1] + \n    ranef(random_slopes_model)$subject[,1],\n  slope = fixef(random_slopes_model)[2] + \n    ranef(random_slopes_model)$subject[,2]\n)\n\n           int      slope\n [1,] 437.1171 -2.1122179\n [2,] 376.9575 -0.2538712\n [3,] 408.5016 -0.6123277\n [4,] 465.1887 -0.2869987\n [5,] 480.7482 -0.4983494\n [6,] 496.8763 -0.9068837\n [7,] 503.6428 -0.8628438\n [8,] 351.6041 -1.2307333\n [9,] 388.8877  0.2484510\n[10,] 458.0076 -1.8382882\n[11,] 338.0295 -1.3566550\n[12,] 399.9685 -1.9101454\n[13,] 466.9886 -0.6172672\n[14,] 397.8842 -1.3372160\n[15,] 358.1531 -1.1403680\n[16,] 372.6687 -1.7822777\n[17,] 483.3660 -0.8762911\n[18,] 369.4005 -1.1305604\n[19,] 208.2102  1.1267137\n[20,] 353.5944  3.9328224\n\n\n\n\n\n\n\n\n\n\n\nAssumptions, Influence\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\ncase-based bootstrap\n\n\n\n\nmodel parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\nlibrary(lmeresampler)bootstrao(model, .f=fixef, type=\"case\", B = 2000, resample = c(??,??))\n\n\nmodel comparison\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\ntakes time, needs careful thought about which levels to resample, but means we can relax distributional assumptions (e.g. about normality of residuals)\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Model Fitted values\n\n\n\n\n\nThe model fitted (or “model predicted”) values can be obtained using predict() (returning just the values) or broom.mixed::augment() (returning the values attached to the data that is inputted to the model).\nTo plot, them, we would typically like to plot the fitted values for each group (e.g. subject)\n\nlibrary(broom.mixed)\naugment(random_slopes_model) %&gt;%\n  ggplot(.,aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Fixed Effects\n\n\n\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. Packages like sjPlot make it incredibly easy (but sometimes too easy), so a nice option is to use the effects package to construct a dataframe of the linear prediction accross the values of a predictor, plus standard errors and confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\n# a quick option:  \nlibrary(sjPlot)\nplot_model(random_slopes_model, type = \"eff\")\n\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting random effects\n\n\n\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms &lt;- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\n\n\n\n\nMLM allows us to model effects in the linear model as varying between groups. Our coefficients we remember from simple linear models (the \\(\\beta\\)’s) are modelled as a distribution that has an overall mean around which our groups vary. We can see this in Figure 1, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 1 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\nFigure 1: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(\\beta\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is \\(\\rho = \\text{Cor}(\\zeta_{0i}, \\zeta_{1i}) = \\frac{\\sigma_{01}}{\\sigma_0 \\sigma_1}\\):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\nWe fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\nlibrary(lme4)\nm1 &lt;- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 637.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.49449 -0.57223 -0.01353  0.62544  2.39122 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.2616   1.5038       \n          x           0.7958   0.8921   0.55\n Residual             4.3672   2.0898       \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   1.7261     0.9673   1.785\nx             1.1506     0.2968   3.877\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n Solution \n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n Optional hello my optional friend\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "example_00_anova.html",
    "href": "example_00_anova.html",
    "title": "Analysis Example: Rpt & Mixed ANOVA",
    "section": "",
    "text": "This is optional for the DAPR3 course, but may be useful for your dissertations should your field/supervisor prefer the ANOVA framework to that of the linear model.\nThis walks briefly through these models with the ez package. There are many other packages available, and many good tutorials online should you desire extra resources in the future:\n\nhttps://www.datanovia.com/en/lessons/repeated-measures-anova-in-r\nhttps://www.r-bloggers.com/2021/04/repeated-measures-of-anova-in-r-complete-tutorial/\nhttps://stats.idre.ucla.edu/r/seminars/repeated-measures-analysis-with-r/\nhttps://www.datanovia.com/en/lessons/mixed-anova-in-r/\n\n\n\nData: Audio interference in executive functioning\nThis data is from a simulated study that aims to investigate the following research questions:\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n\n24 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room.\nThe data is in stored in two separate files - the research administering the tests recorded the SDMT score in one spreadsheet, while details of the audio used in the experiment are held in a separate sheet\n\nInformation about the audio condition for each trial of each participant is stored in .csv format at https://uoepsy.github.io/data/ef_music.csv. The data is in long format (1 row per participant-trial).\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\ntrial_n\nTrial Number (1-15)\n\n\naudio\nAudio heard during the test (‘no_audio’, ‘white_noise’,‘music’)\n\n\nheadphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n\n\n\n\n\n\nInformation on participants’ Symbol Digit Modalities Test (SDMT) for each trial is stored in .xlsx format at https://uoepsy.github.io/data/ef_sdmt.xlsx. The data is in wide format (1 row per participant, 1 column per trial).\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\nTrial_01\nSDMT score in trial 1\n\n\nTrial_02\nSDMT score in trial 2\n\n\nTrial_03\nSDMT score in trial 3\n\n\n…\nSDMT score in trial …\n\n\n…\nSDMT score in trial …\n\n\nTrial_15\nSDMT score in trial 15\n\n\n\n\n\n\nThe code below will read in both datasets and join them for you:\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\ndownload.file(url = \"https://uoepsy.github.io/data/ef_sdmt.xlsx\",\n              destfile = \"ef_sdmt.xlsx\",\n              mode = \"wb\")\nefdata &lt;- \n  left_join(\n    read_csv(\"https://uoepsy.github.io/data/ef_music.csv\"),\n    read_xlsx(\"ef_sdmt.xlsx\") %&gt;%\n      pivot_longer(Trial_01:Trial_15, names_to = \"trial_n\", values_to = \"SDMT\")\n  )\n\n\n\nOne-Way Repeated Measures ANOVA\nFor a repeated measures ANOVA, we have one independent variable that is within group.\nThis would be appropriate if our research question were the following:\n\nHow do different types of audio interfere with executive functioning?\n\nMapping this to the variables in our dataset, our model is going to be SDMT ~ audio, and we want to account for PID differences. So for now we will ignore the headphones variable.\n\n\nCode\nhead(efdata)\n\n\n# A tibble: 6 × 5\n  PID    trial_n  audio       headphones  SDMT\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 PPT_01 Trial_02 no_audio    speakers      31\n2 PPT_01 Trial_08 no_audio    speakers      23\n3 PPT_01 Trial_11 no_audio    speakers      23\n4 PPT_01 Trial_13 no_audio    speakers      24\n5 PPT_01 Trial_15 no_audio    speakers      34\n6 PPT_01 Trial_01 white_noise speakers      38\n\n\nThe easiest way to conduct a repeated measures ANOVA in R is to use the ez package, which comes with some handy functions to visualise the experimental design.\nWe can see from below that every participant completed 5 trials for each type of audio interference:\n\n\nCode\nlibrary(ez)\nezDesign(data = efdata, x = audio, y = PID)\n\n\n\n\n\n\n\n\n\nThe ezANOVA() function takes a few arguments.\nThe ones you will need for this are:\n\ndata the name of the dataframe\ndv the column name for the dependent variable\nwid the column name for the participant id variable\nwithin the column name(s) for the predictor variable(s) that vary within participants\nbetween the column name(s) for any predictor variable(s) that vary between participants\n\nFit a repeated measures ANOVA to examine the effect of the audio type on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio)\n\n\n$ANOVA\n  Effect DFn DFd        F            p p&lt;.05       ges\n2  audio   2  46 44.69618 1.647271e-11     * 0.2534633\n\n$`Mauchly's Test for Sphericity`\n  Effect         W          p p&lt;.05\n2  audio 0.8105961 0.09927715      \n\n$`Sphericity Corrections`\n  Effect       GGe      p[GG] p[GG]&lt;.05      HFe        p[HF] p[HF]&lt;.05\n2  audio 0.8407573 5.0677e-10         * 0.899603 1.427119e-10         *\n\n\n\n\nMixed ANOVA\nMixed ANOVA can be used to investigate effects of independent variables that are at two different levels, i.e. some are within clusters and some are between.\n\nDoes the effect of audio interference on executive functioning differ depending upon whether or not noise-cancelling headphones are used?\n\nLook at the two lines below. Can you work out what the plots will look like before you run them?\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\nezDesign(data = efdata, x = headphones, y = audio)\n\n\nParticipants 1-20 are in one condition, and 21-40 are in another.\nThis should look like a two big blocks on the diagonal.\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\n\n\n\n\n\n\n\n\n\nIn each condition, all different types of audio were observed in the same number of trials. This should be a full grid:\n\n\nCode\nezDesign(data = efdata, x = headphones, y = audio)\n\n\n\n\n\n\n\n\n\nFit a mixed ANOVA to examine the interaction between audio and headphone use on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio, between = headphones)\n\n\n$ANOVA\n            Effect DFn DFd         F            p p&lt;.05        ges\n2       headphones   1  22  9.815545 4.836945e-03     * 0.26784992\n3            audio   2  44 59.615596 2.980503e-13     * 0.32788320\n4 headphones:audio   2  44  8.677316 6.657590e-04     * 0.06629911\n\n$`Mauchly's Test for Sphericity`\n            Effect         W         p p&lt;.05\n3            audio 0.9422531 0.5355001      \n4 headphones:audio 0.9422531 0.5355001      \n\n$`Sphericity Corrections`\n            Effect       GGe        p[GG] p[GG]&lt;.05      HFe        p[HF]\n3            audio 0.9454057 1.196469e-12         * 1.031585 2.980503e-13\n4 headphones:audio 0.9454057 8.648057e-04         * 1.031585 6.657590e-04\n  p[HF]&lt;.05\n3         *\n4         *\n\n\nThe ez package also contains some easy plotting functions for factorial experiments, such as ezPlot(). It takes similar arguments to the ezANOVA() function.\n\nlook up the help documentation for ezPlot().\nlet’s use ezPlot() to make a nice plot\n\n\n\nCode\nezPlot(data = efdata, dv = SDMT, \n       wid = PID, within = audio, between = headphones,\n       x = audio, split = headphones)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same thing in lmer\n\n\n\n\n\n\n\nCode\nlibrary(lme4)\nlibrary(lmerTest)\nmod &lt;- lmer(SDMT ~ 1 + headphones * audio + (1 + audio | PID), \n            data = efdata)\nanova(mod, type=\"III\")\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                 Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nheadphones        325.0  325.04     1    22  9.8155  0.004837 ** \naudio            3212.0 1606.01     2    22 48.4976 8.626e-09 ***\nheadphones:audio  490.1  245.06     2    22  7.4001  0.003486 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nlibrary(sjPlot)\nplot_model(mod, type=\"eff\", terms=c(\"audio\",\"headphones\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DAPR3",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 3 (DAPR3) lab workbook. Using the menu above, you can find lab materials for each week. These include sets of exercises along with walkthrough readings in which we introduce some of the more important R code. It is strongly recommended that students have taken Data Analysis for Psychology in R 1 and 2 (DAPR1 & DAPR2)."
  },
  {
    "objectID": "index.html#solutions",
    "href": "index.html#solutions",
    "title": "Welcome to DAPR3",
    "section": "Solutions",
    "text": "Solutions\nSolutions will be made available immediately below each exercise, in the week after they are set, so make sure to re-visit the labs to check your answers."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Welcome to DAPR3",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Welcome to DAPR3",
    "section": "Tips on googling statistics and R",
    "text": "Tips on googling statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Welcome to DAPR3",
    "section": "Feedback on labs",
    "text": "Feedback on labs\nIf you wish to make suggestions for improvements to these workbooks, please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  }
]