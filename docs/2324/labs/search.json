[
  {
    "objectID": "01_regressionrefresh.html",
    "href": "01_regressionrefresh.html",
    "title": "1. Regression Refresh | Clustered Data",
    "section": "",
    "text": "Preliminaries\n\nOpen Rstudio!\n\nCreate a new RMarkdown document or R script (whichever you like) for this week.\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already.\n\n\ntidyverse : for organising data\npatchwork: for organising plots\nICC : for quickly calculating intraclass correlation coefficient\nlme4 : for fitting generalised linear mixed effects models\nparameters : inference!\npbkrtest : more inference!\nHLMdiag : for examining case diagnostics at multiple levels\nlmeresampler : for bootstrapping!\neffects : for tables/plots\nsjPlot : for tables/plots\nbroom.mixed : tidying methods for mixed models\n\nYou can install all of these at once using:\ninstall.packages(c(\"tidyverse\",\"ICC\",\"lme4\",\"parameters\",\"pbkrtest\",\n                   \"effects\",\"broom.mixed\",\"sjPlot\",\"HLMdiag\"))\n# the lmeresampler package has had some recent updates. \n# better to install the most recent version:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"aloy/lmeresampler\")"
  },
  {
    "objectID": "01_regressionrefresh.html#footnotes",
    "href": "01_regressionrefresh.html#footnotes",
    "title": "1. Regression Refresh | Clustered Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nANOVA is just a special case of the linear model↩︎"
  },
  {
    "objectID": "02_intromlm.html",
    "href": "02_intromlm.html",
    "title": "2. Intro to Multilevel Models",
    "section": "",
    "text": "A Note on terminology\n\n\n\n\n\nThe methods we’re going to learn about in the first five weeks of this course are known by lots of different names: “multilevel models”; “hierarchical linear models”; “mixed-effect models”; “mixed models”; “nested data models”; “random coefficient models”; “random-effects models”; “random parameter models”… and so on).\nWhat the idea boils down to is that model parameters vary at more than one level. This week, we’re going to explore what that means.\nThroughout this course, we will tend to use the terms “mixed effect model”, “linear mixed model (LMM)” and “multilevel model (MLM)” interchangeably."
  },
  {
    "objectID": "02_intromlm.html#footnotes",
    "href": "02_intromlm.html#footnotes",
    "title": "2. Intro to Multilevel Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“(g)lmer” here stands for “(generalised) linear mixed effects regression”.↩︎\nWhat is the difference between testing coefficients and comparing models? This is most easily seen when the model includes a categorical variable as a predictor. The summary() function would return, for each level of the predictor (excluding the reference level), a coefficient, its standard error, the t-statistic, and the p-value for a test of whether the coefficient is significantly different from 0. A model comparison between that model and a null model without the categorical predictor would collapse all levels into a single test across all levels of the predictor.↩︎"
  },
  {
    "objectID": "03_assumptcent.html",
    "href": "03_assumptcent.html",
    "title": "3. Assumptions and Diagnostics | Centering",
    "section": "",
    "text": "Exercises: Assumptions & Diagnostics\n\nData: Wellbeing Across Scotland\nFor these next set of exercises we continue with our recurring study in which researchers want to look at the relationship between time spent outdoors and mental wellbeing, across all of Scotland. Data is collected from 20 of the Local Authority Areas and is accessible at https://uoepsy.github.io/data/LAAwellbeing.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant ID\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nSelf report estimated number of hours per week spent outdoors\n    wellbeing\nWarwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n    density\nLAA Population Density (people per square km)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nThe code below will read in the data and fit the model with by-LAA random intercepts and slopes of outdoor time.\n\nlibrary(tidyverse)\nlibrary(lme4)\nscotmw &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\nrs_model &lt;- lmer(wellbeing ~ 1 + outdoor_time + (1 + outdoor_time | laa), data = scotmw)\n\n\nPlot the residuals vs fitted values, and assess the extend to which the assumption holds that the residuals are zero mean.\nConstruct a scale-location plot. This is where the square-root of the absolute value of the standardised residuals is plotted against the fitted values, and allows you to more easily assess the assumption of constant variance.\n\n\nOptional: can you create the same plot using ggplot, starting with the augment() function from the broom.mixed package?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nplot(model) will give you this plot, but you might want to play with the type = c(......) argument to get the smoothing line\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nplot(rs_model, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nAs we can see, the mean value of the residuals is quite close to zero, right the way across the fitted values. This is good.\n\n\n\n\n\n\nNote\n\n\n\nTo change labels for the x and y axes, add the following arguments to the plot() function:\n\nxlab = \"Fitted values\"\nylab = \"Residuals\"\n\n\n\n\nplot(rs_model,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nIn this plot we can see that the variance of the residuals is fairly constant across the fitted values. There is a slight dip at the lower end. We can see this in the previous plot too - all the points at the LHS of the plot are slightly more tightly grouped around the line. This is not enough to worry me, personally.\n\nlibrary(broom.mixed)\naugment(rs_model) %&gt;%\n  mutate(\n    sqrtr = sqrt(abs(.resid))\n  ) %&gt;%\n  ggplot(aes(x=.fitted, y=sqrtr)) + \n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo remove the confidence bands (in grey) from the smoothed line, you can change the last command to:\ngeom_smooth(se = FALSE)\n\n\n\n\n\n\nQuestion 2\n\n\nExamine the normality of both the level 1 and level 2 residuals.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nUse hist() if you like, or qqnorm(residuals) followed by qqline(residuals)\nExtracting the level 2 residuals (the random effects) can be difficult. ranef(model) will get you some of the way.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nLevel 1\n\nhist(resid(rs_model))\n\n\n\n\n\n\n\nqqnorm(resid(rs_model))\nqqline(resid(rs_model))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIndexing random effects\nThe output of ranef() is a type of data container called “list”. The list includes different named slots and you can find the names as follows:\n\nnames(ranef(rs_model))\n\n[1] \"laa\"\n\n\nYou have a named slot for each group in your lmer specification. In this case, we only specified laa as grouping. To access the random effects for the specific group, type ranef(model) followed by $groupname\n\nranef(rs_model)$laa\n\n                      (Intercept) outdoor_time\nAngus                  -4.8568066  0.095850303\nArgyll and Bute         1.6488121 -0.005181049\nCity of Edinburgh      10.8163125  0.199034174\nDumfries and Galloway  -2.6893688 -0.012005965\nEast Ayrshire          -5.6749200 -0.232750990\nEast Renfrewshire       0.5024800  0.084907037\nFalkirk                -7.4525578 -0.156694328\nGlasgow City          -10.9101439 -0.464232183\nHighland                6.7315989  0.280992008\nInverclyde             -1.2966048  0.197142062\nMidlothian             -1.7585791 -0.485961786\nMoray                  -2.2165380  0.133392034\nNa h-Eileanan Siar     14.0595006  0.399493656\nOrkney Islands          0.1789928  0.154590585\nPerth and Kinross       1.4894924  0.256689754\nScottish Borders       -0.2638474 -0.148174460\nShetland Islands        4.7262680  0.388873631\nStirling                6.5060959 -0.075781592\nWest Dunbartonshire    -6.4127140 -0.266515096\nWest Lothian           -3.1274727 -0.343667797\n\n\nYou will have multiple columns, one for each random effect.\nYou can get the random intercepts by laa, by selecting the first column as follows:\n\nranef(rs_model)$laa[, 1]\n\n [1]  -4.8568066   1.6488121  10.8163125  -2.6893688  -5.6749200   0.5024800\n [7]  -7.4525578 -10.9101439   6.7315989  -1.2966048  -1.7585791  -2.2165380\n[13]  14.0595006   0.1789928   1.4894924  -0.2638474   4.7262680   6.5060959\n[19]  -6.4127140  -3.1274727\n\n\nRespectively, for the random slopes by laa, you select the second column as follows:\n\nranef(rs_model)$laa[, 2]\n\n [1]  0.095850303 -0.005181049  0.199034174 -0.012005965 -0.232750990\n [6]  0.084907037 -0.156694328 -0.464232183  0.280992008  0.197142062\n[11] -0.485961786  0.133392034  0.399493656  0.154590585  0.256689754\n[16] -0.148174460  0.388873631 -0.075781592 -0.266515096 -0.343667797\n\n\n\n\n\nLevel 2\n\nqqnorm(ranef(rs_model)$laa[, 1], main = \"Random intercept\")\nqqline(ranef(rs_model)$laa[, 1])\n\n\n\n\n\n\n\nqqnorm(ranef(rs_model)$laa[, 2], main = \"Random slope\")\nqqline(ranef(rs_model)$laa[, 2])\n\n\n\n\n\n\n\n\nThe normality of the residuals at both levels looks pretty decent here. This is especially good given that we only actually have 20 clusters (the LAAs). We have quite a small sample at this level.\n\n\n\n\nQuestion 3\n\n\n\nWhich person in the dataset has the greatest influence on our model?\n\nFor which person is the model fit the worst (i.e., who has the highest residual?)\nWhich LAA has the greatest influence on our model?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nas well as hlm_influence() in the HLMdiag package there is another nice function, hlm_augment()\nwe can often end up in confusion because the \\(i^{th}\\) observation inputted to our model (and therefore the \\(i^{th}\\) observation of hlm_influence() output) might not be the \\(i^{th}\\) observation in our original dataset - there may be missing data! (Luckily, we have no missing data in this dataset).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(HLMdiag)\nl1_inf &lt;- hlm_influence(rs_model,level=1)\ndotplot_diag(l1_inf$cooksd, cutoff=\"internal\")+\n  ylim(0,.15)\n\n\n\n\n\n\n\n\nGreatest influence:\n\nhlm_augment(rs_model, level=1) %&gt;% arrange(desc(cooksd))\n\n# A tibble: 132 × 15\n      id wellbeing outdoor_time laa          .resid .fitted .ls.resid .ls.fitted\n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1    74        35           33 Scottish Bo…  -5.15    40.2    -3.73        38.7\n 2   129        60            5 City of Edi…   8.89    51.1     6.43        53.6\n 3   109        31            8 Inverclyde    -9.22    40.2    -3.03        34.0\n 4    59        32            7 Scottish Bo…  -6.43    38.4    -7.45        39.5\n 5    31        35            7 Moray         -3.44    38.4     0.198       34.8\n 6    90        70           34 Na h-Eilean…  -3.16    73.2    -1.19        71.2\n 7    87        54           29 Highland      -5.33    59.3    -5.66        59.7\n 8    62        26           21 Midlothian    -4.77    30.8     0.214       25.8\n 9    67        37            7 East Ayrshi…   4.58    32.4     4.62        32.4\n10    64        46           18 City of Edi… -10.5     56.5   -10.1         56.1\n# ℹ 122 more rows\n# ℹ 7 more variables: .mar.resid &lt;dbl&gt;, .mar.fitted &lt;dbl&gt;, cooksd &lt;dbl&gt;,\n#   mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;, covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\nscotmw[74, ]\n\n# A tibble: 1 × 6\n  ppt   name                 laa              outdoor_time wellbeing density\n  &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ID46  Groundskeeper Willie Scottish Borders           33        35      29\n\n\n\n\n\n\n\n\n\n\n\nHighest residual:\n\nhlm_augment(rs_model, level=1) %&gt;% arrange(desc(abs(.resid)))\n\n# A tibble: 132 × 15\n      id wellbeing outdoor_time laa          .resid .fitted .ls.resid .ls.fitted\n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1    64        46           18 City of Edi… -10.5     56.5    -10.1        56.1\n 2   107        22           12 East Ayrshi… -10.3     32.3    -10.1        32.1\n 3    72        22           24 West Lothian -10.0     32.0     -9.24       31.2\n 4   109        31            8 Inverclyde    -9.22    40.2     -3.03       34.0\n 5   130        22           16 West Dunbar…  -8.98    31.0     -8.61       30.6\n 6   129        60            5 City of Edi…   8.89    51.1      6.43       53.6\n 7    93        65           18 City of Edi…   8.51    56.5      8.90       56.1\n 8    85        47           15 City of Edi…  -8.25    55.2     -8.52       55.5\n 9     7        38           13 Perth and K…  -7.84    45.8     -5.28       43.3\n10   121        31           16 Dumfries an…  -7.78    38.8     -7.70       38.7\n# ℹ 122 more rows\n# ℹ 7 more variables: .mar.resid &lt;dbl&gt;, .mar.fitted &lt;dbl&gt;, cooksd &lt;dbl&gt;,\n#   mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;, covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\nscotmw[64, ]\n\n# A tibble: 1 × 6\n  ppt   name            laa               outdoor_time wellbeing density\n  &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ID37  Nicola Sturgeon City of Edinburgh           18        46    1958\n\n\nMost influential LAA:\n\nhlm_augment(rs_model, level=\"laa\") %&gt;% arrange(desc(cooksd))\n\n# A tibble: 20 × 10\n   laa       .ranef.intercept .ranef.outdoor_time .ls.intercept .ls.outdoor_time\n   &lt;chr&gt;                &lt;dbl&gt;               &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n 1 Midlothi…           -1.76             -0.486           8.75           -1.26  \n 2 Na h-Eil…           14.1               0.399          21.4             0.0553\n 3 Glasgow …          -10.9              -0.464          -9.45           -0.593 \n 4 City of …           10.8               0.199          16.2            -0.144 \n 5 Stirling             6.51             -0.0758         13.3            -0.500 \n 6 Shetland…            4.73              0.389           3.69            0.424 \n 7 Angus               -4.86              0.0959         -6.82            0.265 \n 8 West Lot…           -3.13             -0.344           4.13           -0.725 \n 9 Falkirk             -7.45             -0.157         -12.3            -0.0360\n10 Invercly…           -1.30              0.197         -14.5             1.17  \n11 Highland             6.73              0.281           8.95            0.155 \n12 West Dun…           -6.41             -0.267          -4.87           -0.395 \n13 Moray               -2.22              0.133          -5.81            0.267 \n14 Perth an…            1.49              0.257         -20.4             1.76  \n15 East Ayr…           -5.67             -0.233          -3.63           -0.391 \n16 Orkney I…            0.179             0.155          -0.816           0.212 \n17 Scottish…           -0.264            -0.148           3.28           -0.367 \n18 Dumfries…           -2.69             -0.0120         -7.27            0.261 \n19 Argyll a…            1.65             -0.00518         4.67           -0.187 \n20 East Ren…            0.502             0.0849          1.40            0.0247\n# ℹ 5 more variables: cooksd &lt;dbl&gt;, mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;,\n#   covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\n\n\n\n\n\nQuestion 4\n\n\n\nLooking at the random effects, which LAA shows the least benefit to wellbeing as outdoor time increases, and which shows the greatest benefit?\n\nWhat is the estimated wellbeing for people from City of Edinburgh with zero hours of outdoor time per week, and what is their associated increases in wellbeing for every hour per week increase in outdoor time?\n\n\n\n\n\n\nSolution\n\n\n\nIt looks like the residents of Midlothian have the least improvement, and the Western Isles (Na h-Eileanan Siar) show the most increases of wellbeing with outdoor time. We can see this from the LAA-random slopes of outdoor time:\n\nranef(rs_model)\n\n$laa\n                      (Intercept) outdoor_time\nAngus                  -4.8568066  0.095850303\nArgyll and Bute         1.6488121 -0.005181049\nCity of Edinburgh      10.8163125  0.199034174\nDumfries and Galloway  -2.6893688 -0.012005965\nEast Ayrshire          -5.6749200 -0.232750990\nEast Renfrewshire       0.5024800  0.084907037\nFalkirk                -7.4525578 -0.156694328\nGlasgow City          -10.9101439 -0.464232183\nHighland                6.7315989  0.280992008\nInverclyde             -1.2966048  0.197142062\nMidlothian             -1.7585791 -0.485961786\nMoray                  -2.2165380  0.133392034\nNa h-Eileanan Siar     14.0595006  0.399493656\nOrkney Islands          0.1789928  0.154590585\nPerth and Kinross       1.4894924  0.256689754\nScottish Borders       -0.2638474 -0.148174460\nShetland Islands        4.7262680  0.388873631\nStirling                6.5060959 -0.075781592\nWest Dunbartonshire    -6.4127140 -0.266515096\nWest Lothian           -3.1274727 -0.343667797\n\nwith conditional variances for \"laa\" \n\n\nWe can get the cluster-specific coefficients either by adding the fixef() and ranef() together, or using coef():\n\ncoef(rs_model)\n\n$laa\n                      (Intercept) outdoor_time\nAngus                    33.36700   0.31050188\nArgyll and Bute          39.87261   0.20947052\nCity of Edinburgh        49.04011   0.41368575\nDumfries and Galloway    35.53443   0.20264561\nEast Ayrshire            32.54888  -0.01809942\nEast Renfrewshire        38.72628   0.29955861\nFalkirk                  30.77124   0.05795725\nGlasgow City             27.31366  -0.24958061\nHighland                 44.95540   0.49564358\nInverclyde               36.92720   0.41179364\nMidlothian               36.46522  -0.27131021\nMoray                    36.00726   0.34804361\nNa h-Eileanan Siar       52.28330   0.61414523\nOrkney Islands           38.40280   0.36924216\nPerth and Kinross        39.71329   0.47134133\nScottish Borders         37.95995   0.06647711\nShetland Islands         42.95007   0.60352520\nStirling                 44.72990   0.13886998\nWest Dunbartonshire      31.81109  -0.05186352\nWest Lothian             35.09633  -0.12901622\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\n\ncoef(rs_model)$laa[\"City of Edinburgh\",]\n\n                  (Intercept) outdoor_time\nCity of Edinburgh    49.04011    0.4136857\n\n\n\n\n\n\n\n\n\n\n\nExercises: Centering in the MLM\n\n\n\n\n\n\nCentering & Scaling in LM\n\n\n\n\n\nWe have some data from a study investigating how perceived persuasiveness of a speaker is influenced by the rate at which they speak.\n\ndap2 &lt;- read_csv(\"https://uoepsy.github.io/data/dapr2_2122_report1.csv\")\n\nWe can fit a simple linear regression (one predictor) to evaluate how speech rate (variable sp_rate in the dataset) influences perceived persuasiveness (variable persuasive in the dataset). There are various ways in which we can transform the predictor variable sp_rate, which in turn can alter the interpretation of some of our estimates:\n\n\nRaw X\n\nm1 &lt;- lm(persuasive ~ sp_rate, data = dap2)\nsummary(m1)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 55.532060  6.4016670  8.674625 6.848945e-15\nsp_rate     -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\nThe intercept and the coefficient for speech rate are interpreted as:\n\n(Intercept): A audio clip of someone speaking at zero phones per second is estimated as having an average persuasive rating of 55.53.\n\nsp_rate: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nMean-Centered X\nWe can mean center our predictor and fit the model again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_mc = sp_rate - mean(sp_rate))\nm2 &lt;- lm(persuasive ~ sp_rate_mc, data = dap2)\nsummary(m2)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667  1.3519418 39.110165 6.429541e-80\nsp_rate_mc  -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_mc: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nStandardised X\nWe can standardise our predictor and fit the model yet again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_z = scale(sp_rate))\nm3 &lt;- lm(persuasive ~ sp_rate_z, data = dap2)\nsummary(m3)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667   1.351942 39.110165 6.429541e-80\nsp_rate_z   -0.576077   1.356471 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_z: For every increase of one standard deviation in phones per second, perceived persuasiveness is estimated to decrease by -0.58.\n\nRemember that the scale(sp_rate) is subtracting the mean from each value, then dividing those by the standard deviation. The standard deviation of dap2$sp_rate is:\n\nsd(dap2$sp_rate)\n\n[1] 3.016315\n\n\nso in our variable dap2$sp_rate_z, a change of 3.02 gets scaled to be a change of 1 (because we are dividing by sd(dap2$sp_rate)).\n\ncoef(m1)[2] * sd(dap2$sp_rate)\n\n  sp_rate \n-0.576077 \n\ncoef(m3)[2]\n\nsp_rate_z \n-0.576077 \n\n\n\n\nNote that these models are identical. When we conduct a model comparison between the 3 models, the residual sums of squares is identical for all models:\n\nanova(m1,m2,m3)\n\nAnalysis of Variance Table\n\nModel 1: persuasive ~ sp_rate\nModel 2: persuasive ~ sp_rate_mc\nModel 3: persuasive ~ sp_rate_z\n  Res.Df   RSS Df Sum of Sq F Pr(&gt;F)\n1    148 40576                      \n2    148 40576  0         0         \n3    148 40576  0         0         \n\n\nWhat changes when you center or scale a predictor in a standard regression model (one fitted with lm())?\n\nThe variance explained by the predictor remains exactly the same\nThe intercept will change to be the estimated mean outcome where that predictor is “0”. Scaling and centering changes what “0” represents, thereby changing this estimate (the significance test will therefore also change because the intercept now has a different meaning)\nThe slope of the predictor will change according to any scaling (e.g. if you divide your predictor by 10, the slope will multiply by 10).\nThe test of the slope of the predictor remains exactly the same.\n\n\n\n\n\nData: Hangry\nThe study is interested in evaluating whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nThe data are available at: https://uoepsy.github.io/data/hangry.csv.\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant\n\n\nfivetwo\nWhether the participant follows the five-two diet\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead carefully the description of the study above, and try to write out (in lmer syntax) an appropriate model to test the research aims.\ne.g.:\noutcome ~ explanatory variables + (???? | grouping)\nTry to think about the maximal random effect structure (i.e. everything that can vary by-grouping is estimated as doing so).\nTo help you think through the steps to get from a description of a research study to a model specification, think about your answers to the following questions.\nQ: What is our outcome variable?\nQ: What are our explanatory variables?\nQ: Is there any grouping (or “clustering”) of our data that we consider to be a random sample? If so, what are the groups?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet.\nWe can split our data in to groups of each participant. We can also split it into groups of each diet. Which of these groups have we randomly sampled? Do we have a random sample of participants? Do we have a random sample of diets? Another way to think of this is “if i repeated the experiment, what these groups be different?”\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nOur outcome is irritability here, because it is the thing that we are trying to explain through peoples’ hunger levels and diets.\n\nlmer(irritability ~  explanatory variables + (???? | grouping))\n\nWe are interested in the effect of hunger on irritability, and whether this effect is different for the five-two diet. So we are interested in the interaction:\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | grouping))\n\n(remember that hunger + diet + hunger:diet is just a more explicit way of writing hunger*diet).\nIf we did this experiment again, would we have different participants?\nYes. If we did this experiment again, would we have different diets? No, because we’re interested in the specific differences between the five-two diet and no dieting. This means we will likely want to by-participant random deviations (e.g. the ( ... | participant) bit in lmer). But we won’t have by-diet random effects (1 | diet) because the diet differences are the specific differences that we wish to test.\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | participant))\n\nThinking about what can be modelled as randomly varying between participants, we have some options:\n\nparticipants vary in how irritable they are on average\n(the intercept, 1 | participant)\nparticipants vary in how much hunger influences their irritability\n(the effect of hunger, hunger | participant)\nparticipants vary in how much diet influences irritability\n(the effect of diet, diet | participant)\nparticipants vary in how much diet effects hunger’s influence on irritability\n(the interaction between diet and hunger, diet:hunger | participant)\n\nWe can vary 1 and 2, but not 3 and 4. This is because each participant is either following the five-two diet or they are not. So for a single participant, we can’t assess “the effect diet has” on anything, because we haven’t seen that participant under different diets. if we try to plot a single participants’ data, we can see that it is impossible for us to assess “the effect of diet”:\n\n\n\n\n\n\n\n\n\nBy contrast, we can vary the intercept and the effect of hunger, because each participant has multiple values of irritability, and multiple different observations of hunger. We can think about a single participant’s “effect of hunger on irritability” and how we might fit a line to their data:\n\n\n\n\n\n\n\n\n\n\nlmer(irritability ~  hunger + diet + hunger:diet + (1 + hunger | participant))\n\n\n\n\n\n\n\n\n\n\nTotal, Within, Between\n\n\n\n\n\nRecall our research aim:\n\n… whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting.\n\nForgetting about any differences due to diet, let’s just think about the relationship between irritability and hunger. How should we interpret this research aim?\nWas it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a composite of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that the ‘total effect’ isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it about how person i changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are.).\n\n\n\n\nQuestion 2\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll find group_by() %&gt;% mutate() very useful here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nhangry &lt;- \n    hangry %&gt;% group_by(ppt) %&gt;%\n        mutate(\n            avg_hunger = mean(q_hunger),\n            hunger_gc = q_hunger - avg_hunger\n        )\nhead(hangry)\n\n# A tibble: 6 × 6\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   fivetwo avg_hunger hunger_gc\n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1             17       30 N1p1  1             26.6     3.4  \n2             19       27 N1p1  1             26.6     0.400\n3             19       29 N1p1  1             26.6     2.4  \n4             20       33 N1p1  1             26.6     6.4  \n5             24       14 N1p1  1             26.6   -12.6  \n6             30       28 N1p2  1             32.6    -4.6  \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people?\n\nDoes it look like when people are more hungry than normal, they are more irritable?\n\n\n\n\n\n\nSolution\n\n\n\nWe might find it easier to look at a plot where each participant is represented as their mean plus an indication of their range of irritability scores:\n\nggplot(hangry,aes(x=avg_hunger,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nThere appears to be a slight positive relationship between a persons’ average hunger and their irritability scores.\nIt is harder to tell what the relationship is between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data:\n\nggplot(hangry,aes(x=hunger_gc,y=q_irritability, color=ppt)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nI think there might be a positive trend in here, in that participants tend to be higher irritability when they are higher (for them) on the hunger score.\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation level deviations from those averages), that represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables as predictors, instead of the raw hunger scores.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nhunger * diet could be replaced by (hunger1 + hunger2) * diet, thereby allowing each aspect of hunger to interact with diet.\nWe can only put one of these variables in the random effects (1 + hunger | participant). Recall that above we discussed how we cannot have (diet | participant), because “an effect of diet” makes no sense for a single participant (they are either on the diet or they are not, so there is no ‘effect’). Similarly, each participant has only one value for their average hungriness.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(lme4)\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                control = lmerControl(optimizer=\"bobyqa\"))\n\n\n\n\n\nQuestion 5\n\n\nHopefully, you have fitted a model similar to the below:\n\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc) * fivetwo + \n                (1 + hunger_gc | ppt), data = hangry,\n            control = lmerControl(optimizer=\"bobyqa\"))\n\nBelow, we have obtained p-values using the Kenward Rogers Approximation of \\(df\\) for the test of whether the fixed effects are zero, so we can see the significance of each estimate.\nProvide an answer for each of these questions:\n\nFor those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\nDo people following no diet tend to be more irritable when they are more hungry than they usually are?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\n(Trickier:) What does the fivetwo coefficient represent?\n\n\n\n\n\n\n\n  \n    \n      Model Summary\n    \n    \n    \n      Parameter\n      Coefficient\n      SE\n      95% CI\n      t\n      df\n      p\n    \n  \n  \n    \n      Fixed Effects \n    \n    (Intercept)\n17.13\n5.21\n(6.75, 27.51)\n3.29\n77.00\n0.002 \n    avg hunger\n3.86e-03\n0.11\n(-0.21, 0.22)\n0.04\n77.00\n0.971 \n    hunger gc\n0.19\n0.08\n(0.03, 0.34)\n2.45\n70.40\n0.017 \n    fivetwo (1)\n-10.85\n6.62\n(-24.03, 2.32)\n-1.64\n77.00\n0.105 \n    avg hunger × fivetwo (1)\n0.47\n0.14\n(0.20, 0.74)\n3.44\n77.00\n&lt; .001\n    hunger gc × fivetwo (1)\n0.38\n0.10\n(0.18, 0.58)\n3.75\n73.64\n&lt; .001\n    \n      Random Effects \n    \n    SD (Intercept: ppt)\n6.93\n\n\n\n\n\n    SD (hunger_gc: ppt)\n0.38\n\n\n\n\n\n    Cor (Intercept~hunger_gc: ppt)\n-0.01\n\n\n\n\n\n    SD (Residual)\n4.83\n\n\n\n\n\n  \n  \n    \n      \n    \n  \n  \n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n1: For those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nA: ‘No diet’ is the reference level of the five-two variable, and because we have an interaction, that means the avg_hunger coefficient will provide the relevant estimate. There is no evidence (\\(p&gt;.05\\)) to suggest that when not dieting, hungrier people are more irritable than less hungry people.\n2: Is there evidence to suggest that this is different for those following the five-two diet? In what way?\nA: This is the interaction between avg_hunger:fivetwo1. We can see that, for every increase of 1 in average hunger, irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet.\nThese units are still in terms of the original scale (i.e. 0 to 100).\n3: Do people following no diet tend to be more irritable when they are more hungry than they usually are? A: This is the estimate for the coefficient of hunger_gc. For people following no diet, there is an estimated 0.19 increase in irritability for every 1 unit more hungry they become.\n4: Is there evidence to suggest that this is different for those following the five-two diet? In what way? A: This effect of a 1 unit change on within-person hunger increasing irritability is increased for those who are following the five-two diet by an additional 0.38\n5: What does the fivetwo1 coefficient represent? A: This represents the group difference of irritability between those on the five-two diet vs those not dieting, for someone who has an average hunger score of 0.\n\n\n\n\nQuestion 6\n\n\nConstruct two plots showing the two model estimated interactions. Think about your answers to the previous question, and check that they match with what you are seeing in the plots (do not underestimate the utility of this activity for helping understanding!).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis isn’t as difficult as it sounds. the sjPlot package can do it in one line of code!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(sjPlot)\nplot_model(hangrywb, type = \"int\")[[1]]\n\n\n\n\n\n\n\n\nWe saw in the model coefficients that for the reference level of fivetwo, the “No Diet” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. We also saw the interaction avg_hunger:fivetwo1 indicates that irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\nplot_model(hangrywb, type = \"int\")[[2]]\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_gc we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re on “No Diet”). This is the slope of the red line. The interaction hunger_gc:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive and significant, which matches with the fact that the blue line is clearly steeper in this plot.\n\n\n\n\nQuestion 7\n\n\nProvide tests or confidence intervals for the parameters of interest, and write-up the results.\n\n\n\n\n\n\nRemember: some options for inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\n\n\n\n\ntests or CIs for model parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\n\n\nmodel comparison(different fixed effects, same random effects)\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(parameters)\nmodel_parameters(hangrywb, ci_method = \"kr\", ci_random = FALSE)\n\n# Fixed Effects\n\nParameter                | Coefficient |   SE |          95% CI |     t |    df |      p\n----------------------------------------------------------------------------------------\n(Intercept)              |       17.13 | 5.21 | [  6.75, 27.51] |  3.29 | 77.00 | 0.002 \navg hunger               |    3.86e-03 | 0.11 | [ -0.21,  0.22] |  0.04 | 77.00 | 0.971 \nhunger gc                |        0.19 | 0.08 | [  0.03,  0.34] |  2.45 | 70.40 | 0.017 \nfivetwo [1]              |      -10.85 | 6.62 | [-24.03,  2.32] | -1.64 | 77.00 | 0.105 \navg hunger × fivetwo [1] |        0.47 | 0.14 | [  0.20,  0.74] |  3.44 | 77.00 | &lt; .001\nhunger gc × fivetwo [1]  |        0.38 | 0.10 | [  0.18,  0.58] |  3.75 | 73.64 | &lt; .001\n\n# Random Effects\n\nParameter                      | Coefficient\n--------------------------------------------\nSD (Intercept: ppt)            |        6.93\nSD (hunger_gc: ppt)            |        0.38\nCor (Intercept~hunger_gc: ppt) |       -0.01\nSD (Residual)                  |        4.83\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on a restricted diet such as the five-two, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes of relative-hunger level were included for participants. The model was fitting with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser from the lme4. \\(P\\)-values were obtained using Wald tests with Kenward-Roger approximation of denominator degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(\\beta = 0.19,\\ SE = 0.08,\\ t(2.45) = 70.4,\\ p=0.017\\)), and that this was increased for those following the five-two diet (\\(\\beta = 0.38,\\ SE = 0.1,\\ t(3.75) = 73.64,\\ p&lt;0.001\\)). Although for those not on a specific diet there was no evidence for an association between irritability and being generally a more hungry person (\\(p=0.971\\)), there a significant interaction was found between average hunger and being on the five-two diet (\\(\\beta = 0.47,\\ SE = 0.14,\\ t(3.44) = 77,\\ p&lt;0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association of both between-person hunger and within-person hunger with irritability.\n\n\n\n\nOther within-group transformations\nAs well as within-group mean centering a predictor (like we have done above), we can within-group standardise a predictor. This would disagregate within and between effects, but interpretation would of the within effect would be the estimated change in \\(y\\) associated with being 1 standard deviation higher in \\(x\\) for that group."
  },
  {
    "objectID": "04_ranefglmer.html",
    "href": "04_ranefglmer.html",
    "title": "4. Random Effect Structures | Logistic MLM",
    "section": "",
    "text": "Nested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results."
  },
  {
    "objectID": "04_ranefglmer.html#footnotes",
    "href": "04_ranefglmer.html#footnotes",
    "title": "4. Random Effect Structures | Logistic MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s always going to be debateable about what is ‘too high’ because in certain situations you might expect correlations close to 1. It’s best to think through whether it is a feasible value given the study itself↩︎"
  },
  {
    "objectID": "05_recap.html",
    "href": "05_recap.html",
    "title": "5. Recap & Practice Datasets",
    "section": "",
    "text": "Flashcards: lm to lmer\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\n\nFormula:\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustered (multi-level) data\n\n\n\n\n\nWhen our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nIf we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nBy including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nWe can now see that the intercept estimate \\(\\beta_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has an adjustment for each subject.\nEach subject’s line has been moved up or down accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage\n\n\n\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different:\n\n\n\n\n\n\n\n\n\nWhy? One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters.\n\n\n\n\n\n\n\n\n\nRandom slopes\n\n\n\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(\\beta_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nAs with the intercept \\(\\beta_{0i}\\), the slope of the predictor \\(\\beta_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel parameters: Fixed effects\n\n\n\n\n\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the groups all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\nThese are the overall intercept and slope. Think of these as the estimated intercept and slope for the average group.\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n405.7897675  -0.6722654 \n\n\n\n\n\n\n\n\n\n\n\nModel parameters: Variance components\n\n\n\n\n\nAs well as estimating the fixed effects, multilevel models are also defined by the “variance components”. These are the variances and covariances of the random effects.\ni.e. how much do groups vary in around the fixed intercept? and around the fixed slope? Do groups with higher intercepts also have higher slopes (this is the correlation).\n\n\n\n\n\n\n\n\n\nWe can extract these using the VarCorr() function, and we can also see them in the “random effects” part of the summary() output from a model.\n\nVarCorr(random_slopes_model)\n\n Groups   Name        Std.Dev. Corr  \n subject  (Intercept) 72.7164        \n          x1           1.3642  -0.347\n Residual             25.7358        \n\n\n\nRemember, variance is just standard deviation squared!\n\n\n\n\n\n\n\n\n\n\nGroup-specific random effects\n\n\n\n\n\nThe plots below show the fitted values for each subject from each model that we have gone through in these expandable boxes (simple linear regression, random intercept, and random intercept & slope):\n\n\n\n\n\n\n\n\n\nIn the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_0\\). The standard deviation (and variance, which is \\(\\sigma_0^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function).\n\n\n\n\n\n\n\n\n\nIn the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. We can extract the deviations for each group from the fixed effect estimates using the ranef() function.\nThese are the deviations from the overall intercept (\\(\\widehat \\gamma_{00} = 405.79\\)) and slope (\\(\\widehat \\gamma_{10} = -0.672\\)) for each subject \\(i\\).\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)          x1\nsub_308   31.327291 -1.43995253\nsub_309  -28.832219  0.41839420\nsub_310    2.711822  0.05993766\nsub_330   59.398971  0.38526670\nsub_331   74.958481  0.17391602\nsub_332   91.086535 -0.23461836\nsub_333   97.852988 -0.19057838\nsub_334  -54.185688 -0.55846794\nsub_335  -16.902018  0.92071637\nsub_337   52.217859 -1.16602280\nsub_349  -67.760246 -0.68438960\nsub_350   -5.821271 -1.23788002\nsub_351   61.198823  0.05499816\nsub_352   -7.905596 -0.66495059\nsub_369  -47.636645 -0.46810258\nsub_370  -33.121093 -1.11001234\nsub_371   77.576205 -0.20402571\nsub_372  -36.389281 -0.45829505\nsub_373 -197.579562  1.79897904\nsub_374  -52.195357  4.60508775\n\nwith conditional variances for \"subject\" \n\n\n\n\n\n\n\n\n\n\n\nGroup-specific coefficients\n\n\n\n\n\nWe can see the estimated intercept and slope for each subject \\(i\\) specifically, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)         x1\nsub_308    437.1171 -2.1122179\nsub_309    376.9575 -0.2538712\nsub_310    408.5016 -0.6123277\nsub_330    465.1887 -0.2869987\nsub_331    480.7482 -0.4983494\nsub_332    496.8763 -0.9068837\nsub_333    503.6428 -0.8628438\nsub_334    351.6041 -1.2307333\nsub_335    388.8877  0.2484510\nsub_337    458.0076 -1.8382882\nsub_349    338.0295 -1.3566550\nsub_350    399.9685 -1.9101454\nsub_351    466.9886 -0.6172672\nsub_352    397.8842 -1.3372160\nsub_369    358.1531 -1.1403680\nsub_370    372.6687 -1.7822777\nsub_371    483.3660 -0.8762911\nsub_372    369.4005 -1.1305604\nsub_373    208.2102  1.1267137\nsub_374    353.5944  3.9328224\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nNotice that the above are the fixed effects + random effects estimates, i.e. the overall intercept and slope + deviations for each subject.\n\ncbind(\n  int = fixef(random_slopes_model)[1] + \n    ranef(random_slopes_model)$subject[,1],\n  slope = fixef(random_slopes_model)[2] + \n    ranef(random_slopes_model)$subject[,2]\n)\n\n           int      slope\n [1,] 437.1171 -2.1122179\n [2,] 376.9575 -0.2538712\n [3,] 408.5016 -0.6123277\n [4,] 465.1887 -0.2869987\n [5,] 480.7482 -0.4983494\n [6,] 496.8763 -0.9068837\n [7,] 503.6428 -0.8628438\n [8,] 351.6041 -1.2307333\n [9,] 388.8877  0.2484510\n[10,] 458.0076 -1.8382882\n[11,] 338.0295 -1.3566550\n[12,] 399.9685 -1.9101454\n[13,] 466.9886 -0.6172672\n[14,] 397.8842 -1.3372160\n[15,] 358.1531 -1.1403680\n[16,] 372.6687 -1.7822777\n[17,] 483.3660 -0.8762911\n[18,] 369.4005 -1.1305604\n[19,] 208.2102  1.1267137\n[20,] 353.5944  3.9328224\n\n\n\n\n\n\n\n\n\n\n\nAssumptions, Influence\n\n\n\n\n\nIn the simple linear model \\(\\color{red}{y} = \\color{blue}{\\beta_0 + \\beta_1(x)} + \\varepsilon\\), we distinguished between the systematic model part \\(\\beta_0 + \\beta_1(x)\\), around which observations randomly vary (the \\(\\varepsilon\\) part) - i.e. \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}} + \\text{error}\\).\nIn the multi-level model, our random effects are another source of random variation - \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}} + \\text{group_error} + \\text{individual_error}\\). As such, random effects are another form of residual, and our assumptions of zero mean constant variance apply at both levels of residuals (see Figure 1).\n\n\n\n\n\nFigure 1: The black dashed lines show our model assumptions.\n\n\n\n\n\nWe can assess these normality of both resid(model) and ranef(model) by constructing plots using functions such as hist(), qqnorm() and qqline().\n\nWe can also use plot(model, type=c(\"p\",\"smooth\")) to give us our residuals vs fitted plot (smooth line should be horizontal at approx zero, showing zero mean).\n\nplot(model, form = sqrt(abs(resid(.))) ~ fitted(.), type = c(\"p\",\"smooth\")) will give us our scale-location plot (smooth line should be horizontal, showing constant variance).\n\nWe can also use the check_model() function from the performance package to get lots of info at once:\n\nlibrary(performance)\ncheck_model(random_slopes_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\ncase-based bootstrap\n\n\n\n\ntests or CIs for model parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\nlibrary(lmeresampler)bootstrap(model, .f=fixef, type=\"case\", B = 2000, resample = c(??,??))\n\n\nmodel comparison(different fixed effects, same random effects)\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\ntakes time, needs careful thought about which levels to resample, but means we can relax distributional assumptions (e.g. about normality of residuals)\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Model Fitted values\n\n\n\n\n\nThe model fitted (or “model predicted”) values can be obtained using predict() (returning just the values) or broom.mixed::augment() (returning the values attached to the data that is inputted to the model).\nTo plot, them, we would typically like to plot the fitted values for each group (e.g. subject)\n\nlibrary(broom.mixed)\naugment(random_slopes_model) %&gt;%\n  ggplot(.,aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Fixed Effects\n\n\n\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. Packages like sjPlot make it incredibly easy (but sometimes too easy), so a nice option is to use the effects package to construct a dataframe of the linear prediction accross the values of a predictor, plus standard errors and confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\n# a quick option:  \nlibrary(sjPlot)\nplot_model(random_slopes_model, type = \"eff\")\n\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting random effects\n\n\n\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms &lt;- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\n\n\n\n\nMLM allows us to model effects in the linear model as varying between groups. Our coefficients we remember from simple linear models (the \\(\\beta\\)’s) are modelled as a distribution that has an overall mean around which our groups vary. We can see this in Figure 2, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 2 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\nFigure 2: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(\\beta\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is \\(\\rho = \\text{Cor}(\\zeta_{0i}, \\zeta_{1i}) = \\frac{\\sigma_{01}}{\\sigma_0 \\sigma_1}\\):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\nWe fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\nlibrary(lme4)\nm1 &lt;- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 637.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.49449 -0.57223 -0.01353  0.62544  2.39122 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.2616   1.5038       \n          x           0.7958   0.8921   0.55\n Residual             4.3672   2.0898       \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   1.7261     0.9673   1.785\nx             1.1506     0.2968   3.877\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)\n\n\n\n\n\n\n\n\n\nPractice Datasets Weeks 4 and 5\nBelow are various datasets on which you can try out your new-found modelling skills. Read the descriptions carefully, keeping in mind the explanation of how the data is collected and the research question that motivates the study design.\nAll datasets with hierarchical structures that we have seen in DAPR3 can be found here.\n\n\n\n\n\n\nPractice 1: Music and Driving\n\n\n\n\n\nThese data are simulated to represent data from a fake experiment, in which participants were asked to drive around a route in a 30mph zone. Each participant completed the route 3 times (i.e. “repeated measures”), but each time they were listening to different audio (either speech, classical music or rap music). Their average speed across the route was recorded. This is a fairly simple design, that we might use to ask “how is the type of audio being listened to associated with driving speeds?”\nThe data are available at https://uoepsy.github.io/data/drivingmusicwithin.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    pid\nParticipant Identifier\n    speed\nAvg Speed Driven on Route (mph)\n    music\nMusic listened to while driving (classical music / rap music / spoken word)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice 2: CBT and Stress\n\n\n\n\n\nThese data are simulated to represent data from 50 participants, each measured at 3 different time-points (pre, during, and post) on a measure of stress. Participants were randomly allocated such that half received some cognitive behavioural therapy (CBT) treatment, and half did not. This study is interested in assessing whether the two groups (control vs treatment) differ in how stress changes across the 3 time points.\nThe data are available at https://uoepsy.github.io/data/stressint.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    stress\nStress (range 0 to 100)\n    time\nTime (pre/post/during)\n    group\nWhether participant is in the CBT group or control group\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice 3: Erm.. I don’t believe you\n\n\n\n\n\nThese data are simulated to represent data from 30 participants who took part in an experiment designed to investigate whether fluency of speech influences how believable an utterance is perceived to be.\nEach participant listened to the same 20 statements, with 10 being presented in fluent speech, and 10 being presented with a disfluency (an “erm, …”). Fluency of the statements was counterbalanced such that 15 participants heard statements 1 to 10 as fluent and 11 to 20 as disfluent, and the remaining 15 participants heard statements 1 to 10 as disfluent, and 11 to 20 as fluent. The order of the statements presented to each participant was random. Participants rated each statement on how believable it is on a scale of 0 to 100.\nThe data are available at https://uoepsy.github.io/data/erm_belief.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    trial_n\nTrial number\n    sentence\nStatement identifier\n    condition\nCondition (fluent v disfluent)\n    belief\nbelief rating (0-100)\n    statement\nStatement\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice 4: Cognitive Aging\n\n\n\n\n\nThese data are simulated to represent a large scale international study of cognitive aging, for which data from 17 research centers has been combined. The study team are interested in whether different cognitive domains have different trajectories as people age. Do all cognitive domains decline at the same rate? Do some decline more steeply, and some less? The literature suggests that scores on cognitive ability are predicted by educational attainment, so they would like to control for this.\nEach of the 17 research centers recruited a minimum of 14 participants (Median = 21, Range 14-29) at age 45, and recorded their level of education (in years). Participants were then tested on 5 cognitive domains: processing speed, spatial visualisation, memory, reasoning, and vocabulary. Participants were contacted for follow-up on a further 9 occasions (resulting in 10 datapoints for each participant), and at every follow-up they were tested on the same 5 cognitive domains. Follow-ups were on average 3 years apart (Mean = 3, SD = 0.8).\nThe data are available at https://uoepsy.github.io/data/cogdecline.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    cID\nCenter ID\n    pptID\nParticipant Identifier\n    educ\nEducational attainment (years of education)\n    age\nAge at visit (years)\n    processing_speed\nScore on Processing Speed domain task\n    spatial_visualisation\nScore on Spatial Visualisation domain task\n    memory\nScore on Memory domain task\n    reasoning\nScore on Reasoning domain task\n    vocabulary\nScore on Vocabulary domain task"
  },
  {
    "objectID": "05b_lmmdatasets.html",
    "href": "05b_lmmdatasets.html",
    "title": "LMM Datasets",
    "section": "",
    "text": "Disclaimer\nNone of these data are real! Where possible, we have tried to make the values plausible (e.g. by presenting established measures). The patterns present in this data are simply a wild concoction straight from the whimsical brains of the stats teaching crew.\n\n\n\n\n\n\nCognitive Aging\n\n\n\n\n\nA study is interested in examining how cognition changes as people age. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78).\nDATASET: https://uoepsy.github.io/data/dapr3_decline.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    visit\nVisit number (1 - 10)\n    age\nAge (years) at visit\n    ACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n    imp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nRoutine and Emotions in Children\n\n\n\n\n\nAre children with more day-to-day routine better at regulating their emotions? A study of 200 children from 20 schools (9 private schools and 11 state schools) completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ).\nDATASET: https://uoepsy.github.io/data/crqeds.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    schoolid\nSchool Identifier\n    EDS\nEmotion Dysregulation Score (range 1-6, higher values indicate more *dys*regulation of emotions)\n    CRQ\nChildhood Routine Questionnaire Score (range 0-7, higher values indicate more day-to-day routine)\n    schooltype\nSchool type (private / state)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nMannequins and clothing purchases\n\n\n\n\n\nDoes clothing seem more attractive to shoppers when it is viewed on a model, and is this dependent on item price? 30 participants were presented with a set of pictures of items of clothing, and rated each item how likely they were to buy it. Each participant saw 20 items, ranging in price from £5 to £100. 15 participants saw these items worn by a model, while the other 15 saw the items against a white background.\nDATASET: https://uoepsy.github.io/data/dapr3_mannequin.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    purch_rating\nPurchase Rating (sliding scale 0 to 100, with higher ratings indicating greater perceived likelihood of purchase)\n    price\nPrice presented with item (range £5 to £100)\n    ppt\nParticipant Identifier\n    condition\nWhether items are seen on a model or on a white background\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nBig Fish Little Fish\n\n\n\n\n\nDo bigger fish (relative to those fish around them) have more self-esteem?\nDATASET: https://uoepsy.github.io/data/bflp.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    pond\nPond identifier (which pond was each fish surveyed from?)\n    self_esteem\nSelf Esteem of Little Fish (SELF) scale (range 1-5, with higher values indicating more self-esteem)\n    fish_weight\nWeight (kg)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnxiety and Alcohol Use\n\n\n\n\n\nA research study is investigating how greater levels of anxiety might lead to changes in drinking habits, and whether this is different depending on whether people identify as ‘social drinkers’. Data was collected from 50 participants across 5 different centers, on c10 different occasions. On each occasion, researchers administered the generalised anxiety disorder (GAD-7) questionnaire to measure levels of anxiety over the past week, and collected information on the units of alcohol participants had consumed within the week.\nDATASET: https://uoepsy.github.io/data/alcgad.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    alcunits\nAlcohol units consumed within the past week\n    gad\nGeneralised Anxiety Disorder scale (range 0-21, with higher values indicating greater levels of generalised anxiety)\n    center\nResearch center from which participant was recruited\n    ppt\nParticipant identifier\n    group\nWhether the participant identifies as a 'social drinker' or not (0 = not a social drinker, 1 = social drinker)\n    urb_rural\nWhether participant lives in an urban or rural location (0 = urban, 1 = rural)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPhysiotherapy and physical functioning\n\n\n\n\n\nA researcher is interested in the efficacy of physiotherapy in helping people to regain normal physical functioning. They are curious whether doing more physiotherapy leads to better outcomes, or if it is possibly that the patients who tend to do more of their exercises tend to have better outcomes. 20 in-patients from 2 different hospitals (1 private, 1 govt funded) were monitored over the course of their recovery following knee-surgery. Every day, the time each patient spent doing their physiotherapy exercises was recorded. At the end of each day, participants completed the “Time get up and go” task, a measure of physical functioning.\nDATASET: https://uoepsy.github.io/data/dapr3_tgu.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    tgu\nTime Get up and Go Task - measure of physical functioning. Scored in minutes, with lower scores indicating better physical functioning\n    phys\nMinutes of physiotherapy exercises completed that day\n    hospital\nHospital ID\n    patient\nPatient ID\n    prioritylevel\nPriority level of patients' surgery (rank 1-4, with 1 being most urgent surgey, and 4 being least urgent)\n    private\n0 = government funded hospital, 1 = private hospital\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nMindful Cognitive Aging\n\n\n\n\n\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants undertake to complete daily mindfulness sessions, while the remaining participants did not.\nDATASET: https://uoepsy.github.io/data/dapr3_mindfuldecline.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    sitename\nSite Identifier\n    ppt\nParticipant Identifier\n    condition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n    visit\nVisit number (1 - 10)\n    age\nAge (years) at visit\n    ACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n    imp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-center Mindful Cognitive Aging\n\n\n\n\n\nA large study involving 14 different research centers is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. Each site recruits between 15 and 30 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). For each center, roughly half of the participants engaged with daily mindfulness sessions, while the remaining participants did not.\nDATASET: https://uoepsy.github.io/data/dapr3_mindfuldeclineFULL.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    sitename\nSite Identifier\n    ppt\nParticipant Identifier\n    condition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n    visit\nVisit number (1 - 10)\n    age\nAge (years) at visit\n    ACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n    imp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nWellbeing Across Scotland\n\n\n\n\n\nResearchers want to study the relationship between time spent outdoors and mental wellbeing, across all of Scotland. They contact all the Local Authority Areas (LAAs) and ask them to collect data for them, with participants completing the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being, and being asked to estimate the average number of hours they spend outdoors each week. Twenty of the Local Authority Areas provided data.\nDATASET: https://uoepsy.github.io/data/LAAwellbeing.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nNumber of hours spent outdoors per week\n    wellbeing\nWellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\n    density\nPopulation density of local authority area (number of people per square km)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAudio interference in executive functioning\n\n\n\n\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used? 30 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a maximum of 15 times. During the tests, participants listened to either no audio (max 5 tests), white noise (max 5 tests) or classical music (max 5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room.\nDATASET: https://uoepsy.github.io/data/ef_replication.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    PID\nParticipant ID\n    trial_n\nTrial Number (1-15)\n    audio\nAudio heard during the test ('no_audio', 'white_noise','music')\n    headphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n    SDMT\nSymbol Digit Modalities Test (SDMT) score\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nWellbeing in work\n\n\n\n\n\nThe “Wellbeing in Work” dataset contains information on employee wellbeing, assessed at baseline (start of study), 12 months post, 24 months post, and 36 months post. over the course of 36 months. Participants were randomly assigned to one of three employment conditions:\n\ncontrol: No change to employment. Employees continue at 5 days a week, with standard allocated annual leave quota.\n\nunlimited_leave : Employees were given no limit to their annual leave, but were still expected to meet required targets as specified in their job description.\nfourday_week: Employees worked a 4 day week for no decrease in pay, and were still expected to meet required targets as specified in their job description.\n\nThe researchers have two main questions: Overall, did the participants’ wellbeing stay the same or did it change? Did the employment condition groups differ in the how wellbeing changed over the assessment period?\nDATASET: https://uoepsy.github.io/data/wellbeingwork3.rda\n\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\nd3 &lt;- wellbeingwork3\ntibble(variable=names(d3),\n       description = c(\n         \"Participant ID\",\n         \"Timepoint (0 = baseline, 1 = 12 months, 2 = 24 months, 3 = 36 months)\",\n         \"Employment Condition ('control' = 5 day week, 28 days of leave. 'unlimited_leave' = 5 days a week, unlimited leave. 'fourday_week' = 4 day week, 28 days of leave)\",\n         \"Wellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\")\n) %&gt;% gt::gt()\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ID\nParticipant ID\n    TimePoint\nTimepoint (0 = baseline, 1 = 12 months, 2 = 24 months, 3 = 36 months)\n    Condition\nEmployment Condition ('control' = 5 day week, 28 days of leave. 'unlimited_leave' = 5 days a week, unlimited leave. 'fourday_week' = 4 day week, 28 days of leave)\n    Wellbeing\nWellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nHangry\n\n\n\n\n\nThe study is interested in evaluating whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nDATASET: https://uoepsy.github.io/data/hangry.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    q_irritability\nScore on irritability questionnaire (0:100)\n    q_hunger\nScore on hunger questionnaire (0:100)\n    ppt\nParticipant Identifier\n    fivetwo\nWhether the participant follows the five-two diet\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTest-enhanced learning\n\n\n\n\n\nAn experiment was run to conceptually replicate “test-enhanced learning” (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (StudyStudy), the other group studied the material once then did a test (StudyTest). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (Test_word).\nThe researchers are interested in how test-enhanced learning influences time-to-recall. The critical (replication) prediction is that the StudyTest group will retain the material better (lower reaction times) on the 1-week follow-up test compared to the StudyStudy group.\nDATASET: https://uoepsy.github.io/data/RTtestlearning.RData\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    Subject_ID\nUnique Participant Identifier\n    Group\nGroup denoting whether the participant studied the material twice (StudyStudy), or studied it once then did a test (StudyTest)\n    Delay\nTime of recall test ('min' = Immediate, 'week' = One week later)\n    Test_word\nWord being recalled (175 different test words)\n    Rtime\nTime to respond (milliseconds)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nMono/bilingual differences in vocabulary development\n\n\n\n\n\n74 children from 10 schools were administered the full Boston Naming Test (BNT-60) on a yearly basis for 5 years to examine development of word retrieval. Five of the schools taught lessons in a bilingual setting with English as one of the languages, and the remaining five schools taught in monolingual English.\nDATASET: https://uoepsy.github.io/data/bntmono.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    child_id\nunique child identifier\n    school_id\nunique school identifier\n    BNT60\nscore on the Boston Naming Test-60. Scores range from 0 to 60\n    schoolyear\nYear of school\n    mlhome\nMono/Bi-lingual School. 0 = Bilingual, 1 = Monolingual\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAggressive Behaviours Intervention\n\n\n\n\n\nIn 2010 A US state’s commissioner for education was faced with growing community concern about rising levels of adolescent antisocial behaviours.\nAfter a series of focus groups, the commissioner approved the trialing of an intervention in which yearly Parent Management Training (PMT) group sessions were offered to the parents of a cohort of students entering 10 different high schools. Every year, the parents were asked to fill out an informant-based version of the Aggressive Behaviour Scale (ABS), measuring verbal and physical abuse, socially inappropriate behavior, and resisting care. Where possible, the same parents were followed up throughout the child’s progression through high school. Alongside this, parents from a cohort of students entering 10 further high schools in the state were recruited to also complete the same informant-based ABS, but were not offered the PMT group sessions.\nThe commissioner has two main questions: Does the presentation of aggressive behaviours increase as children enter the secondary school system? If so, is there any evidence for the effectiveness of Parent Management Training (PMT) group sessions in curbing the rise of aggressive behaviors during a child’s transition into the secondary school system?\nDATASET: https://uoepsy.github.io/data/abs_intervention.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    schoolid\nSchool Name\n    ppt\nParticipant Identifier\n    age\nAge (years)\n    interv\nWhether or not parents attended Parent Management Training (PMT) group sessions (0 = No, 1 = Yes)\n    ABS\nAggressive Behaviours Scale. Measures verbal and physical abuse, socially inappropriate behavior, and resisting care. Scores range from 0 to 100, with higher scores indicating more aggressive behaviours.\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice 1: Music and Driving\n\n\n\n\n\nThese data are simulated to represent data from a fake experiment, in which participants were asked to drive around a route in a 30mph zone. Each participant completed the route 3 times (i.e. “repeated measures”), but each time they were listening to different audio (either speech, classical music or rap music). Their average speed across the route was recorded. This is a fairly simple design, that we might use to ask “how is the type of audio being listened to associated with driving speeds?”\nThe data are available at https://uoepsy.github.io/data/drivingmusicwithin.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    pid\nParticipant Identifier\n    speed\nAvg Speed Driven on Route (mph)\n    music\nMusic listened to while driving (classical music / rap music / spoken word)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice 2: CBT and Stress\n\n\n\n\n\nThese data are simulated to represent data from 50 participants, each measured at 3 different time-points (pre, during, and post) on a measure of stress. Participants were randomly allocated such that half received some cognitive behavioural therapy (CBT) treatment, and half did not. This study is interested in assessing whether the two groups (control vs treatment) differ in how stress changes across the 3 time points.\nThe data are available at https://uoepsy.github.io/data/stressint.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    stress\nStress (range 0 to 100)\n    time\nTime (pre/post/during)\n    group\nWhether participant is in the CBT group or control group\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice 3: Erm.. I don’t believe you\n\n\n\n\n\nThese data are simulated to represent data from 30 participants who took part in an experiment designed to investigate whether fluency of speech influences how believable an utterance is perceived to be.\nEach participant listened to the same 20 statements, with 10 being presented in fluent speech, and 10 being presented with a disfluency (an “erm, …”). Fluency of the statements was counterbalanced such that 15 participants heard statements 1 to 10 as fluent and 11 to 20 as disfluent, and the remaining 15 participants heard statements 1 to 10 as disfluent, and 11 to 20 as fluent. The order of the statements presented to each participant was random. Participants rated each statement on how believable it is on a scale of 0 to 100.\nThe data are available at https://uoepsy.github.io/data/erm_belief.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    trial_n\nTrial number\n    sentence\nStatement identifier\n    condition\nCondition (fluent v disfluent)\n    belief\nbelief rating (0-100)\n    statement\nStatement\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice 4: Cognitive Aging\n\n\n\n\n\nThese data are simulated to represent a large scale international study of cognitive aging, for which data from 17 research centers has been combined. The study team are interested in whether different cognitive domains have different trajectories as people age. Do all cognitive domains decline at the same rate? Do some decline more steeply, and some less? The literature suggests that scores on cognitive ability are predicted by educational attainment, so they would like to control for this.\nEach of the 17 research centers recruited a minimum of 14 participants (Median = 21, Range 14-29) at age 45, and recorded their level of education (in years). Participants were then tested on 5 cognitive domains: processing speed, spatial visualisation, memory, reasoning, and vocabulary. Participants were contacted for follow-up on a further 9 occasions (resulting in 10 datapoints for each participant), and at every follow-up they were tested on the same 5 cognitive domains. Follow-ups were on average 3 years apart (Mean = 3, SD = 0.8).\nThe data are available at https://uoepsy.github.io/data/cogdecline.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    cID\nCenter ID\n    pptID\nParticipant Identifier\n    educ\nEducational attainment (years of education)\n    age\nAge at visit (years)\n    processing_speed\nScore on Processing Speed domain task\n    spatial_visualisation\nScore on Spatial Visualisation domain task\n    memory\nScore on Memory domain task\n    reasoning\nScore on Reasoning domain task\n    vocabulary\nScore on Vocabulary domain task"
  },
  {
    "objectID": "07_path1.html",
    "href": "07_path1.html",
    "title": "7. Path Analysis",
    "section": "",
    "text": "Relevant packages\n\nlavaan\n\nsemPlot or tidySEM\n\n\nBy now, we are getting more comfortable with the regression world, and we can see how it is extended to lots of different types of outcome and data structures. So far in DAPR3 it’s been all about the multiple levels. This has brought so many more potential study designs that we can now consider modelling - pretty much any study where we are interested in explaining some outcome variable, and where we have sampled clusters of observations (or clusters of clusters of clusters of … etc.).\nBut we are still restricted to thinking, similar to how we thought in DAPR2, about one single outcome variable. In fact, if we think about the structure of the fixed effects part of a model (i.e., the bit we’re specifically interested in), then we’re still limited to thinking of the world in terms of “this is my outcome variable, everything else predicts it”.\n\n\n\n\n\n\nRegression as a path diagram\n\n\n\n\n\n\nImagine writing the names of all your variables on a whiteboard\nSpecify which one is your dependent (or “outcome” or “response”) variable.\nSit back and relax, you’re done!\n\nIn terms of a theoretical model of the world, there’s not really much to it. We have few choices in the model we construct beyond specifying which is our outcome variable.\nWe can visualise our multiple regression model like this:\n\n\n\n\n\nFigure 1: In multiple regression, we decide which variable is our outcome variable, and then everything else is done for us\n\n\n\n\nOf course, there are a few other things that are included (an intercept term, the residual error, and the fact that our predictors can be correlated with one another), but the idea remains pretty much the same:\n\n\n\n\n\nFigure 2: Multiple regression with intercept, error, predictor covariances\n\n\n\n\n\n\n\n\n\n\n\n\n\nA model reflects a theory\n\n\n\n\n\nWhat if I my theoretical model of the world doesn’t fit the structure of “one outcome, multiple precictors”?\nLet’s suppose I have 5 variables: Age, Parental Income, Income, Autonomy, and Job Satisfaction. I draw them up on my whiteboard:\n\n\n\n\n\nFigure 3: My variables\n\n\n\n\nMy theoretical understanding of how these things fit together leads me to link my variables to end up with something like that in Figure 4.\n\n\n\n\n\nFigure 4: My theory about my system of variables\n\n\n\n\nIn this diagram, a persons income is influenced by their age, their parental income, and their level of autonomy, and in turn their income predicts their job satisfaction. Job satisfaction is also predicted by a persons age directly, and by their level of autonomy, which is also predicted by age. It’s complicated to look at, but in isolation each bit of this makes theoretical sense.\nTake each arrow in turn and think about what it represents:\n\n\n\n\n\nFigure 5: ?(caption)\n\n\n\n\nIf we think about trying to fit this “model” with the tools that we have, then we might end up wanting to fit three separate regression models, which between them specify all the different arrows in the diagram:\n\\[\n\\begin{align}\n\\textrm{Job Satisfaction} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_3(\\textrm{Income}) + \\varepsilon \\\\\n\\textrm{Income} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\beta_2(\\textrm{Autonomy}) + \\beta_2(\\textrm{Parental Income}) + \\varepsilon \\\\\n\\textrm{Autonomy} & = \\beta_0 + \\beta_1(\\textrm{Age}) + \\varepsilon \\\\\n\\end{align}\n\\]\nThis is all well and good, but what if I want to talk about how well my entire model (Figure 4) fits the data we observed?\n\n\n\n\n\n\n\n\n\nIntroducing Path Analysis\n\n\n\n\n\nThe starting point for Path Analysis is to think about our theories in terms of the connections between variables drawn on a whiteboard. By representing a theory as paths to and from different variables, we open up a whole new way of ‘modelling’ the world around us.\nThere are a few conventions to help us understand this sort of diagrammatical way of thinking. By using combinations of rectangles, ovals, single- and double-headed arrows, we can draw all sorts of model structures. In Path Diagrams, we use specific shapes and arrows to represent different things in our model:\nShapes and Arrows in Path Diagrams\n\nObserved variables are represented by squares or rectangles. These are the named variables of interest which exist in our dataset - i.e. the ones which we have measured directly.\nVariances/Covariances are represented by double-headed arrows. In many diagrams these are curved.\nRegressions are shown by single headed arrows (e.g., an arrow from \\(x\\) to \\(y\\) for the path \\(y~x\\)).\n\n\nLatent variables are represented by ovals, and we will return to these in a few weeks time!\n\n\n\n\n\n\n\n\n\n\n\nTerminology refresher\n\nExogenous variables are a bit like what we have been describing with words like “independent variable” or “predictor”. In a path diagram, they have no paths coming from other variables in the system, but have paths going to other variables.\n\nEndogenous variables are more like the “outcome”/“dependent”/“response” variables we are used to. They have some path coming from another variable in the system (and may also - but not necessarily - have paths going out from them).\n\n\n\n\n\n\n\n\n\n\nHow does it work (in brief)?\n\n\n\n\n\nThe logic behind path analysis is to estimate a system of equations that can reproduce the covariance structure that we see in the data.\n\nWe specify our theoretical model of the world as a system of paths between variables\nWe collect data on the relevant variables and we observe a correlation matrix (i.e. how each variable correlates with all others)\nWe fit our model to the data, and evaluate how well our theoretical model (a system of paths) can reproduce the correlation matrix we observed.\n\n\n\n\n\n\n\n\n\n\nOPTIONAL How does it work (less brief)?\n\n\n\n\n\nPath Diagram Tracing\nFor Path Diagrams that meet a certain set of pre-requisites, we can use a cool technique called Path Tracing to estimate the different paths (i.e., the coefficients) from just the covariance matrix of the dataset. For us to be able to do this, a Path Diagram must meet these criteria:\n\nAll our exogenous variables are correlated (unless we specifically assume that their correlation is zero)\nAll models are ‘recursive’ (no two-way causal relations, no feedback loops)\nResiduals are uncorrelated with exogenous variables\nEndogenous variables are not connected by correlations (we would use correlations between residuals here, because the residuals are not endogenous)\nAll ‘causal’ relations are linear and additive\n‘causes’ are unitary (if A -&gt; B and A -&gt; C, then it is presumed that this is the same aspect of A resulting in a change in both B and C, and not two distinct aspects of A, which would be better represented by two correlated variables A1 and A2).\n\n\n\n\n\n\n\nCausal?\n\n\n\n\n\nIt is a slippery slope to start using the word ‘cause’, and personally I am not that comfortable using it here. However, you will likely hear it a lot in resources about path analysis, so it is best to be warned.\nPlease keep in mind that we are using a very broad definition of ‘causal’, simply to reflect the one way nature of the relationship we are modeling. In Figure 6, a change in the variable X1 is associated with a change in Y, but not vice versa.\n\n\n\n\n\nFigure 6: Paths are still just regressions.\n\n\n\n\n\n\n\nTracing Rules\nThanks to Sewal Wright, we can express the correlation between any two variables in the system as the sum of all compound paths between the two variables.\ncompound paths are any paths you can trace between A and B for which there are:\n\nno loops\nno going forward then backward\nmaximum of one curved arrow per path\n\nEXAMPLE\nLet’s consider the example below, for which the paths are all labelled with lower case letters \\(a, b, c, \\text{and } d\\).\n\n\n\n\n\nFigure 7: A multiple regression model as a path diagram\n\n\n\n\nAccording to Wright’s tracing rules above, write out the equations corresponding to the 3 correlations between our observed variables (remember that \\(r_{a,b} = r_{b,a}\\), so it doesn’t matter at which variable we start the paths).\n\n\\(r_{x1,x2} = c\\)\n\n\\(r_{x1,y} = a + bc\\)\n\n\\(r_{x2,y} = b + ac\\)\n\nNow let’s suppose we observed the following correlation matrix:\n\negdat &lt;- read_csv(\"https://uoepsy.github.io/data/patheg.csv\")\negdat &lt;- read_csv(\"../../data/patheg.csv\")\nround(cor(egdat),2)\n\n     x1   x2    y\nx1 1.00 0.36 0.75\nx2 0.36 1.00 0.60\ny  0.75 0.60 1.00\n\n\nWe can plug these into our system of equations:\n\n\\(r_{x1,x2} = c = 0.36\\)\n\n\\(r_{x1,y} = a + bc = 0.75\\)\n\n\\(r_{x2,y} = b + ac = 0.60\\)\n\nAnd with some substituting and rearranging, we can work out the values of \\(a\\), \\(b\\) and \\(c\\).\n\n\n\n\n\n\nClick for answers\n\n\n\n\n\nThis is what I get:\na = 0.61\nb = 0.38\nc = 0.36\n\n\n\nWe can even work out what the path labeled \\(d\\) (the residual variance) is.\nFirst we sum up all the equations for the paths from Y to Y itself.\nThese are:\n\n\\(a^2\\) (from Y to X1 and back)\n\n\\(b^2\\) (from Y to X2 and back)\n\n\\(acb\\) (from Y to X1 to X2 to Y)\n\\(bca\\) (from Y to X2 to X1 to Y)\n\nSumming them all up and solving gives us:\n\\[\n\\begin{align}\nr_{y \\cdot x1, x2} & = a^2 + b^2 + acb + bca\\\\\n& = 0.61^2 + 0.38^2 + 2 \\times(0.61 \\times 0.38 \\times 0.36)\\\\\n& = 0.68 \\\\\n\\end{align}\n\\] We can think of this as the portion of the correlation of Y with itself that occurs via the predictors. Put another way, this is the amount of variance in Y explained jointly by X1 and X2, which sounds an awful lot like an \\(R^2\\)!\nThe path labelled \\(d\\) is simply all that is left in Y after taking out the variance explained by X1 and X2, meaning that the path \\(d = \\sqrt{1-R^2}\\) (i.e., the residual variance!).\nHooray! We’ve just worked out regression coefficients when all we had was the correlation matrix of the variables! It’s important to note that we have been using the correlation matrix, so, somewhat unsurprisingly, our estimates are standardised coefficients.\nBecause we have the data itself, let’s quickly find them with lm()\n\n# model:\nmodel1 &lt;- lm( scale(y) ~ scale(x1) + scale(x2), egdat)\n# extract the coefs\ncoef(model1) %&gt;% round(2)\n\n(Intercept)   scale(x1)   scale(x2) \n       0.00        0.61        0.38 \n\n# extract the r^2\nsummary(model1)$r.squared\n\n[1] 0.688\n\n\n\n\n\n\n\n\n\n\n\nIntroducing lavaan\n\n\n\n\n\nFor the remaining weeks of the course, we’re going to rely heavily on the lavaan (Latent Variable Analysis) package. This is the main package in R for fitting path diagrams (as well as more cool models like factor analysis sructures and structural equation mdoels). There is a huge scope of what this package can do.\nThe first thing to get to grips with is the various new operators which it allows us to use.\nOur old multiple regression formula in R was specified as y ~ x1 + x2 + x3 + ....\nIn lavaan, we continue to fit regressions using the ~ symbol, but we can also specify the construction of latent variables using =~ and residual variances & covariances using ~~.\n\n\n\nformula type\noperator\nmnemonic\n\n\n\n\nlatent variable definition\n=~\n“is measured by”\n\n\nregression\n~\n“is regressed on”\n\n\n(residual) (co)variance\n~~\n“is correlated with”\n\n\nintercept\n~1\n“intercept”\n\n\nnew parameter\n:=\n“is defined as”\n\n\n\n(from https://lavaan.ugent.be/tutorial/syntax1.html)\nIn practice, fitting models in lavaan tends to be a little different from things like lm() and (g)lmer().\nInstead of including the model formula inside the fit function (e.g., lm(y ~ x1 + x2, data = df)), we tend to do it in a step-by-step process. This is because as our models become more complex, our formulas can pretty long!\nWe write the model as a character string (e.g. model &lt;- \"y ~ x1 + x2\") and then we pass that formula along with the data to the relevant lavaan function, which for our purposes will be the sem() function, sem(model, data = mydata).\n\n\n\n\n\n\n\n\n\nFitting a multiple regression model with lavaan\n\n\n\n\n\nYou can see a multiple regression fitted with lavaan below.\n\nlibrary(lavaan)\nscsdat &lt;- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\n\n# the lm() way\nmreg_lm &lt;- lm(dass ~ zo + zc + ze + za + zn + scs, scsdat)\n\n# setting up the model\nmreg_model &lt;- \"\n    #regression\n    dass ~ zo + zc + ze + za + zn + scs\n\"\nmreg_sem &lt;- sem(mreg_model, data=scsdat)\n\nThese are the coefficients from our lm() model:\n\ncoefficients(mreg_lm)\n\n(Intercept)          zo          zc          ze          za          zn \n    62.1243     -0.0307     -0.0378      0.7449      0.2029      1.5209 \n        scs \n    -0.4865 \n\n\nAnd you can see the estimated parameters are the same for our sem() model!\n\nsummary(mreg_sem)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                           656\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  dass ~                                              \n    zo               -0.031    0.242   -0.127    0.899\n    zc               -0.038    0.248   -0.153    0.879\n    ze                0.745    0.377    1.976    0.048\n    za                0.203    0.378    0.537    0.591\n    zn                1.521    0.249    6.097    0.000\n    scs              -0.486    0.071   -6.893    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .dass             40.021    2.210   18.111    0.000\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 1: Model Specification\n\n\n\n\n\nThe first part of estimating a path model involves specifying the model. This means basically writing down the paths that are included in your theoretical model.\nLet’s start by looking at the example about job satisfaction, income, autonomy and age.\nRecall we had this theoretical model:\n\n\n\n\n\n\n\n\n\nAnd now let’s suppose that we collected data on these variables:\n\njobsatpath &lt;- read_csv(\"https://uoepsy.github.io/data/jobsatpath.csv\")\nhead(jobsatpath)\n\n\n\n\n\n\njobsat\nincome\nautonomy\nage\nparentincome\n\n\n\n\n22\n39\n17\n55\n47\n\n\n29\n35\n58\n51\n43\n\n\n69\n38\n45\n52\n49\n\n\n67\n27\n52\n43\n44\n\n\n54\n14\n36\n35\n40\n\n\n25\n25\n39\n48\n44\n\n\n...\n...\n...\n...\n...\n\n\n\n\n\n\n\nRemember we said that we could specify all these paths using three regression models? Well, to specify our path model, we simply write these out like we would do in lm(), but this time we do so all in one character string. We still have to make sure that we use the correct variable names, as when we make R estimate the model, it will look in the data for things like “jobsat”.\n\nmymodel &lt;- \"\njobsat ~ age + autonomy + income\nincome ~ autonomy + age + parentincome\nautonomy ~ age\n\"\n\nThere are some other things which we will automatically be estimated here: all our exogenous variables (the ones with arrows only going from them) will be free to correlate with one another. We can write this explicitly in the model if we like, using the two tildes ~~ between our two exogenous variables age and parentincome. We will also get the variances of all our variables.\nWe can see all the paths here:\n\nlavaanify(mymodel)\n\n\n\n            lhs op          rhs\n1        jobsat  ~          age\n2        jobsat  ~     autonomy\n3        jobsat  ~       income\n4        income  ~     autonomy\n5        income  ~          age\n6        income  ~ parentincome\n7      autonomy  ~          age\n8        jobsat ~~       jobsat\n9        income ~~       income\n10     autonomy ~~     autonomy\n11          age ~~          age\n12          age ~~ parentincome\n13 parentincome ~~ parentincome\n\n\nand even make a nice diagram:\n\nlibrary(semPlot)\nsemPaths(lavaanify(mymodel))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 2: Model Identification\n\n\n\n\n\nYou’ll have heard the term “model fit” many times since the start of DAPR2, when we began model-based thinking. However, there is a crucial difference in what it means when it is used in for path analysis.\nIn things like multiple regression, we have been using “model fit” to be the measure of “how much variance can we explain in y with our set of predictors?”. For a path model, examining “model fit” is more like asking “how well does our model reproduce the characteristics of the data that we observed?”.\nWe can represent the “characteristics of our data” in a covariance matrix, so one way of thinking of “model fit” is as “how well can our model reproduce our observed covariance matrix?”.\n\ncov(jobsatpath)\n\n             jobsat income autonomy    age parentincome\njobsat       341.62   4.47    107.3 -28.81        -6.47\nincome         4.47  50.17     46.7  29.56        14.83\nautonomy     107.30  46.68    365.6  32.94       -10.00\nage          -28.81  29.56     32.9  38.69         1.81\nparentincome  -6.47  14.83    -10.0   1.81        14.18\n\n\nBecause we are working with a covariance matrix here, we are really working with fewer “bits” of information than the 50 people in our dataset. We actually are concerned with the 15 unique variances and covariances in our covariance matrix between our 5 variables above.\nDegrees of freedom\nWhen we think of “degrees of freedom” for a multiple regression model, in DAPR2 we learned that \\(df = n-k-1\\) (\\(n\\) is the number of observations, \\(k\\) is the number of predictors). These degrees of freedom related to the corresponding \\(F\\) and \\(t\\)-distributions with which we performed our hypothesis tests (e.g. \\(t\\)-tests for a null hypothesis that a coefficient is zero, or \\(F\\)-tests for a null that the reduction in residual sums of squares is zero).\nBut in relation to our model’s ability to represent a \\(k \\times k\\) covariance matrix (i.e. the covariance matrix of our \\(k\\) variables), we are instead interested in the number of covariances (and not the number of observations). Our “degrees of freedom” in the this framework corresponds to the number of knowns (observed covariances/variances) minus the number of unknowns (parameters to be estimated by the model). A model is only able to be estimated if it has at least 0 degrees of freedom (if there are at least as many knowns as unknowns). A model with 0 degrees of freedom is termed just-identified. Under- and Over- identified models correspond to those with \\(&lt;0\\) and \\(&gt;0\\) degrees of freedom respectively.\n\nHow many knowns are there?\nThe number of known covariances in a set of \\(k\\) observed variables is equal to \\(\\frac{k \\cdot (k+1)}{2}\\).\n\nWhen learning about path models, the visualisations can play a key part. It often helps to draw all our variables (both observed and latent) on the whiteboard, and connect them up according to your theoretical model. You can then count the number of paths (arrows) and determine whether the number of knowns &gt; number of unknowns. We can reduce the number of unknowns by fixing parameters to be specific values.\n\nBy constraining some estimated parameter to be some specific value, we free-up a degree of freedom! For instance “the correlation between x1 and x2 is equal to 0.7 (\\(r_{x_1x_2} = .07\\))”. This would turn a previously estimated parameter into a fixed parameter, and this gains us the prize of a lovely degree of freedom!\nBy removing a path altogether, we are constraining it to be zero.\n\n\n\n\n\n\n\n\n\n\nOPTIONAL: multiple regression model is just-identified\n\n\n\n\n\nThe multiple regression model is an example of a just-identified model! In multiple regression, everything is allowed to covary with everything else, which means that there is a unique solution for all of the model’s parameters because there are as many paths as there are observed covariances. This means that in this path analysis world, a multiple regression model is “just-identified”.\nIf I have two predictors and one outcome variable, then there are 6 variances and covariances available. For instance:\n\ncov(somedata)\n\n         y     x1     x2\ny  45.6481 0.0149 1.3525\nx1  0.0149 1.0455 0.0196\nx2  1.3525 0.0196 1.0000\n\n\nThe multiple regression model will estimate the two variances of the exogenous variables (the predictors), their covariance, the two paths from each exogenous to the endogenous (each predictor to the outcome), and the error variance. This makes up 6 estimated parameters - which is exactly how many known covariances there are.\nCount the number of arrows (both single and double headed) in the diagram:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 3: Model Estimation\n\n\n\n\n\nEstimating the model is relatively straightforward. We pass the formula we have written to the sem() function, along with the data set in which we want it to look for the variables. It will be estimated using maximum likelihood estimation.\n\nmymodel.fit &lt;- sem(mymodel, data = jobsatpath)\n\nWe can then examine the parameter estimates:\n\nsummary(mymodel.fit)\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                            50\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.050\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.080\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  jobsat ~                                            \n    age              -1.566    0.482   -3.250    0.001\n    autonomy          0.347    0.131    2.653    0.008\n    income            0.689    0.439    1.572    0.116\n  income ~                                            \n    autonomy          0.099    0.026    3.796    0.000\n    age               0.631    0.081    7.834    0.000\n    parentincome      1.036    0.128    8.099    0.000\n  autonomy ~                                          \n    age               0.851    0.418    2.038    0.042\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .jobsat          251.098   50.220    5.000    0.000\n   .income           11.293    2.259    5.000    0.000\n   .autonomy        330.771   66.154    5.000    0.000\n\n\nWe can now, to “visualise” our model, add the estimates to the diagram:\n\n\n\n\n\nModel estimates\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Path Analysis 4: Model Fit\n\n\n\n\n\nOnce we have estimated a model, we can evaluate how well it fits our sample data. As mentioned above in relation to model identification, when we talk about model fit here, we are asking “how well does our model reproduce the characteristics of the data that we observed?”, or more specifically “how well can our model reproduce our observed covariance matrix?”.\n\ncov(jobsatpath)\n\n             jobsat income autonomy    age parentincome\njobsat       341.62   4.47    107.3 -28.81        -6.47\nincome         4.47  50.17     46.7  29.56        14.83\nautonomy     107.30  46.68    365.6  32.94       -10.00\nage          -28.81  29.56     32.9  38.69         1.81\nparentincome  -6.47  14.83    -10.0   1.81        14.18\n\n\nThere are too many different indices of model fit for these types of models, and there’s lots of controversy over the various merits and disadvantages and proposed cutoffs of each method. We will return to this more in coming weeks, and the lecture this week contains information on some of them. The important thing to remember: “model fit” and “degrees of freedom” have quite different meanings to those you are likely used to.\nThe simplest metric of fit is a chi-square value that we can compute that reflects reflects the discrepancy between the model-implied covariance matrix and the observed covariance matrix. We can then calculate a p-value for this chi-square statistic by using the chi-square distribution with the degrees of freedom equivalent to that of the model.\nIf we denote the sample covariance matrix as \\(S\\) and the model-implied covariance matrix as \\(\\Sigma(\\theta)\\), then we can think of the null hypothesis here as \\(H_0: S - \\Sigma(\\hat\\theta) = 0\\). In this way our null hypothesis is sort of like saying that our theoretical model is correct (and can therefore perfectly reproduce the covariance matrix).\n\n\n\n\n\n\n\nExercises: Burnout\n\nData: Passion & Burnout\nResearchers are interested in the role of obsessive and harmonious passion in psychological wellbeing. The researchers collect data from 100 participants. Participants respond on sliding scales (0-100) for five measures:\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nworksat\nWork Satisfaction: On a scale of 0-100, how satisfied are you with your work?\n\n\nhp\nHarmonious Passion: On a scale of 0-100, how much do you feel that you freely choose to engage in work outside of working hours?\n\n\nop\nObsessive Passion: On a scale of 0-100, how much do you have uncontrollable urges to work outside of working hours?\n\n\nconflict\nWork Conflict: On a scale of 0-100, how much conflict do you experience in your work?\n\n\nburnout\nWork Burnout: On a scale of 0-100, how close to burnout at work are you?\n\n\n\n\n\n\n\nThe data is available at https://uoepsy.github.io/data/passionpath.csv\n\n\nQuestion 1\n\n\nLoad in the libraries we will use in these exercises:\n\ntidyverse\n\nlavaan\n\nsemPlot\n\nRead in the data.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nburnout &lt;- read_csv(\"https://uoepsy.github.io/data/passionpath.csv\")\n\n\n\n\n\nQuestion 2\n\n\nThe researchers have this theoretical model:\n\n\n\n\n\nFigure 8: Burnout Theory\n\n\n\n\nSpecify this model and store the formula as an object in R\n\n\n\n\n\nSolution\n\n\n\n\nburnoutmod &lt;- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\n\"\n\n\n\n\n\nQuestion 3\n\n\nFit the model to the data using the sem() function.\n\n\n\n\n\nSolution\n\n\n\n\nburnoutfit &lt;- sem(burnoutmod, data=burnout)\n\n\n\n\n\nQuestion 4\n\n\nExamine the parameter estimates\n\n\n\n\n\nSolution\n\n\n\n\nsummary(burnoutfit)\n\nlavaan 0.6.16 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.458\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.243\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  worksat ~                                           \n    hp                0.420    0.088    4.769    0.000\n  burnout ~                                           \n    worksat          -0.207    0.063   -3.304    0.001\n    conflict         -0.147    0.191   -0.772    0.440\n  conflict ~                                          \n    op                0.095    0.038    2.476    0.013\n    hp               -0.010    0.033   -0.308    0.758\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  hp ~~                                               \n    op               61.628   18.529    3.326    0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .worksat         155.762   22.028    7.071    0.000\n   .burnout          75.347   10.656    7.071    0.000\n   .conflict         19.465    2.753    7.071    0.000\n    hp              200.614   28.371    7.071    0.000\n    op              152.210   21.526    7.071    0.000\n\n\n\n\n\n\nQuestion 5\n\n\nProduce a diagram with the estimates on the paths. Can you also produce one which has the standardised estimates?\nTake a look at the help function for semPaths(). What do the arguments what and whatLabels do?\n\n\n\n\n\nSolution\n\n\n\nwhat will weight and colour the paths according something like the estimates. whatLabels will provide labels for the paths:\n\nsemPaths(burnoutfit, whatLabels = \"est\")\n\n\n\n\n\n\n\n\nThis will change them to the standardised estimates:\n\nsemPaths(burnoutfit, what = \"std\", whatLabels = \"std\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nHow many variables do you have in your model?\nHow many knowns are there in the covariance matrix?\n\nHow many unknowns are there in your model?\nHow many degrees of freedom do you therefore have?\n\n\n\n\n\n\nSolution\n\n\n\n\nHow many variables do you have in your model?\n5\nHow many knowns are there in the covariance matrix? \\(\\frac{5 \\times (5 + 1)}{2} = 15\\)\nHow many unknowns are there in your model?\nThere are 6 paths in Figure 8, but we also need to consider the variances of the 5 variables, so we have 11 things being estimated\nHow many degrees of freedom do you therefore have? 15 - 11 = 4\n\n\n\n\n\nQuestion 7\n\n\nTake a look at the summary of the model you fitted. Specifically, examine the bit near the top where it mentions the \\(\\chi^2\\) statistic.\nIs it significant? What do we conclude?\n\n\n\n\n\nSolution\n\n\n\nThe \\(\\chi^2\\) statistic is not significant:\n\npchisq(5.458, df=4, lower.tail=F)\n\n[1] 0.243\n\n\nWe therefore have no evidence to support rejecting our null hypothesis that our model provides a reasonable fit to the data.\n\n\n\n\nQuestion 8\n\n\nTry examing what the other fit measures (RMSEA, SRMR, CLI, TLI: How do they compare with the cutoffs provided in the lecture?\nhint: summary(modelfit, fit.measures = TRUE)\n\n\n\n\n\nSolution\n\n\n\nThe fit statistics for our model:\nComparative Fit Index (CFI) = 0.968 Tucker-Lewis Index (TLI) = 0.921 RMSEA = 0.060 SRMR = 0.063\nOnly CFI meets the criteria given in the lecture slides for “considered good”.\n\nsummary(burnoutfit, fit.measures = TRUE)\n\nlavaan 0.6.16 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.458\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.243\n\nModel Test Baseline Model:\n\n  Test statistic                                56.068\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.968\n  Tucker-Lewis Index (TLI)                       0.921\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1836.113\n  Loglikelihood unrestricted model (H1)      -1833.384\n                                                      \n  Akaike (AIC)                                3694.226\n  Bayesian (BIC)                              3722.883\n  Sample-size adjusted Bayesian (SABIC)       3688.142\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.060\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.172\n  P-value H_0: RMSEA &lt;= 0.050                    0.361\n  P-value H_0: RMSEA &gt;= 0.080                    0.475\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.063\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  worksat ~                                           \n    hp                0.420    0.088    4.769    0.000\n  burnout ~                                           \n    worksat          -0.207    0.063   -3.304    0.001\n    conflict         -0.147    0.191   -0.772    0.440\n  conflict ~                                          \n    op                0.095    0.038    2.476    0.013\n    hp               -0.010    0.033   -0.308    0.758\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  hp ~~                                               \n    op               61.628   18.529    3.326    0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .worksat         155.762   22.028    7.071    0.000\n   .burnout          75.347   10.656    7.071    0.000\n   .conflict         19.465    2.753    7.071    0.000\n    hp              200.614   28.371    7.071    0.000\n    op              152.210   21.526    7.071    0.000\n\n\n\n\n\n\nQuestion Extra: modification indices\n\n\nExamine the modification indices of the model (use the modindices() function).\nPay close attention to the mi column (this is the “modification index”, which is the change in the \\(\\chi^2\\) statistic). The other interesting column is going to be the sepc.all column, which is the estimated parameter value of the proposed path, in a model where all the variables are standardised. This means we can evaluate whether the estimated parameter is relatively small/moderate/large, because these are all standardised correlations between -1 and 1!\nAre there any paths which the modification indices suggest might improve the model? Do they make theoretical sense to include them?\n\n\n\n\n\nSolution\n\n\n\n\nmodindices(burnoutfit)\n\n        lhs op      rhs    mi     epc sepc.lv sepc.all sepc.nox\n12  worksat ~~  burnout 0.268 -13.039 -13.039   -0.120   -0.120\n13  worksat ~~ conflict 0.842  -5.053  -5.053   -0.092   -0.092\n15  worksat ~~       op 1.102 -15.122 -15.122   -0.098   -0.098\n16  burnout ~~ conflict 3.461 -28.991 -28.991   -0.757   -0.757\n17  burnout ~~       hp 0.036  -2.403  -2.403   -0.020   -0.020\n18  burnout ~~       op 3.185  18.424  18.424    0.172    0.172\n21  worksat  ~  burnout 0.095  -0.101  -0.101   -0.067   -0.067\n22  worksat  ~ conflict 1.306  -0.314  -0.314   -0.103   -0.103\n23  worksat  ~       op 1.102  -0.113  -0.113   -0.101   -0.101\n24  burnout  ~       hp 0.268   0.035   0.035    0.054    0.054\n25  burnout  ~       op 3.471   0.137   0.137    0.184    0.184\n26 conflict  ~  worksat 0.842  -0.032  -0.032   -0.099   -0.099\n27 conflict  ~  burnout 0.225  -0.062  -0.062   -0.126   -0.126\n28       hp  ~  worksat 1.102   0.316   0.316    0.309    0.309\n29       hp  ~  burnout 0.096  -0.051  -0.051   -0.033   -0.033\n32       op  ~  worksat 1.102  -0.097  -0.097   -0.109   -0.109\n33       op  ~  burnout 4.058   0.264   0.264    0.196    0.196\n\n\nThere seems to be a suggested reasonably large correlation between burnout and conflict. If we were to fit this model, our fit indices may well improve and meet our cut-offs. But this may well just be overfitting.\n\nburnoutmod2 &lt;- \"\nworksat ~ hp\nburnout ~ worksat + conflict\nconflict ~ op + hp\nhp ~~ op\nburnout ~~ conflict\n\"\nsem(burnoutmod2, data=burnout)\n\nWe will not start adjusting models based on modification indices today (or indeed in this course at all). As a general rule for dapR3 course, we want you to specify and test a specific model, and not seek to use exploratory modifications."
  },
  {
    "objectID": "08_path2.html",
    "href": "08_path2.html",
    "title": "8. Mediation",
    "section": "",
    "text": "Relevant packages\n\nlavaan\n\nsemPlot or tidySEM\nmediation\nWe’ve seen how path analysis works in last week’s lab, and we can use that same logic to investigate models which have quite different structures such as those including mediating variables.\nBecause we have multiple endogenous variables here, then we’re immediately drawn to path analysis, because we’re in essence thinking of conducting several regression models. As we can’t fit our theoretical model into a nice straightforward regression model (we would need several), then we can use path analysis instead, and just smush lots of regressions together, allowing us to estimate things all at once."
  },
  {
    "objectID": "08_path2.html#footnotes",
    "href": "08_path2.html#footnotes",
    "title": "8. Mediation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nread y~m|x as y~m controlling for x↩︎"
  },
  {
    "objectID": "09_pca.html",
    "href": "09_pca.html",
    "title": "9. PCA",
    "section": "",
    "text": "Relevant packages\n\npsych"
  },
  {
    "objectID": "09_pca.html#explore",
    "href": "09_pca.html#explore",
    "title": "9. PCA",
    "section": "1. Explore",
    "text": "1. Explore\nFirst things first, we should always plot and describe our data. This is always a sensible thing to do - while many of the datasets we give you are nice and clean and organised, the data you get out of questionnaire tools, experiment software etc, are almost always quite a bit messier. It is also very useful to just eyeball the patterns of relationships between variables.\n\nQuestion 1\n\n\nLoad the job performance data into R and call it job. Check whether or not the data were read correctly into R - do the dimensions correspond to the description of the data above?\n\n\n\n\n\nSolution\n\n\n\nLet’s load the data:\n\nlibrary(tidyverse)\n\njob &lt;- read_csv('https://uoepsy.github.io/data/police_performance.csv')\ndim(job)\n\n[1] 50  7\n\n\nThere are 50 observations on 6 variables.\nThe top 6 rows in the data are:\n\nhead(job)\n\n# A tibble: 6 × 7\n  commun probl_solv logical learn physical appearance arrest_rate\n   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1     12         52      20    44       48         16      0.613 \n2     12         57      25    45       50         16      0.419 \n3     12         54      21    45       50         16      0     \n4     13         52      21    46       51         17      0.645 \n5     14         54      24    46       51         17      0.0645\n6     14         48      20    47       51         18      0.645 \n\n\n\n\n\n\nQuestion 2\n\n\nProvide descriptive statistics for each variable in the dataset.\n\n\n\n\n\nSolution\n\n\n\nWe now inspect some descriptive statistics for each variable in the dataset:\n\n# Quick summary\nsummary(job)\n\n     commun       probl_solv      logical       learn         physical   \n Min.   :12.0   Min.   :48.0   Min.   :20   Min.   :44.0   Min.   :48.0  \n 1st Qu.:16.0   1st Qu.:52.2   1st Qu.:22   1st Qu.:48.0   1st Qu.:52.2  \n Median :18.0   Median :54.0   Median :24   Median :50.0   Median :54.0  \n Mean   :17.7   Mean   :54.2   Mean   :24   Mean   :50.3   Mean   :54.2  \n 3rd Qu.:19.8   3rd Qu.:56.0   3rd Qu.:26   3rd Qu.:52.0   3rd Qu.:56.0  \n Max.   :24.0   Max.   :59.0   Max.   :31   Max.   :56.0   Max.   :59.0  \n   appearance    arrest_rate   \n Min.   :16.0   Min.   :0.000  \n 1st Qu.:19.0   1st Qu.:0.371  \n Median :21.0   Median :0.565  \n Mean   :21.1   Mean   :0.512  \n 3rd Qu.:23.0   3rd Qu.:0.669  \n Max.   :28.0   Max.   :0.935  \n\n\nOPTIONAL\nIf you wish to create a nice looking table for a report, you could try the following code. However, I should warn you: this code is quite difficult to understand so, if you are interested, attend a lab!\n\nlibrary(gt)\n\n# Mean and SD of each variable\njob %&gt;% \n  summarise(across(everything(), list(M = mean, SD = sd))) %&gt;%\n  pivot_longer(everything()) %&gt;% \n  mutate(\n    value = round(value, 2),\n    name = str_replace(name, '_M', '.M'),\n    name = str_replace(name, '_SD', '.SD')\n  ) %&gt;%\n  separate(name, into = c('variable', 'summary'), sep = '\\\\.') %&gt;%\n  pivot_wider(names_from = summary, values_from = value) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n    \n      variable\n      M\n      SD\n    \n  \n  \n    commun\n17.68\n2.74\n    probl_solv\n54.16\n2.41\n    logical\n24.02\n2.49\n    learn\n50.28\n2.84\n    physical\n54.16\n2.41\n    appearance\n21.06\n2.99\n    arrest_rate\n0.51\n0.23"
  },
  {
    "objectID": "09_pca.html#is-pca-needed",
    "href": "09_pca.html#is-pca-needed",
    "title": "9. PCA",
    "section": "2. Is PCA needed?",
    "text": "2. Is PCA needed?\n\n\n\n\n\n\nWhen might we want to reduce dimensionality?\n\n\n\n\n\nThere are many reasons we might want to reduce the dimensionality of data:\n\nTheory testing\n\nWhat are the number and nature of dimensions that best describe a theoretical construct?\n\nTest construction\n\nHow should I group my items into sub-scales?\nWhich items are the best measures of my constructs?\n\nPragmatic\n\nI have multicollinearity issues/too many variables, how can I defensibly combine my variables?\n\n\nPCA is most often used for the latter - we are less interested in the theory behind our items, we just want a useful way of simplifying lots of variables down to a smaller number.\nRecall that we are wanting to see how well the skills ratings predict arrest rate.\nWe might fit this model:\n\nmod &lt;- lm(arrest_rate ~ commun + probl_solv + logical + learn + physical + appearance, data = job)\n\nHowever, we might have reason to think that many of these predictors might be quite highly correlated with one another, and so we may be unable to draw accurate inferences. This is borne out in our variance inflation factor (VIF):\n\nlibrary(car)\nvif(mod)\n\n    commun probl_solv    logical      learn   physical appearance \n     34.67       1.17       1.23      43.56      34.98      21.78 \n\n\nAs the original variables are highly correlated, it is possible to reduce the dimensionality of the problem under investigation without losing too much information.\nOn the other side, if the correlation between the variables under study are weak, a larger number of components is needed in order to explain sufficient variability.\n\n\n\n\nQuestion 3\n\n\nWorking with only the skills ratings (not the arrest rate - we’ll come back to that right at the end), investigate whether or not the variables are highly correlated and explain whether or not you PCA might be useful in this case.\nHint: We only have 6 variables here, but if we had many, how might you visualise cor(job)?\n\n\n\n\n\nSolution\n\n\n\nLet’s start by looking at the correlation matrix of the data:\n\nlibrary(pheatmap)\n\njob_skills &lt;- job %&gt;% select(-arrest_rate)\n\nR &lt;- cor(job_skills)\n\npheatmap(R, breaks = seq(-1, 1, length.out = 100))\n\n\n\n\nFigure 1: Correlation between the variables in the ``Job’’ dataset\n\n\n\n\nThe correlation between the variables seems to be quite large (it doesn’t matter about direction here, only magnitude; if negative correlations were present, we would think in absolute value).\nThere appears to be a group of highly correlated variables comprising physical ability, appearance, communication skills, and learning ability which are correlated among themselves but uncorrelated with another group of variables. The second group comprises problem solving and logical ability.\nThis suggests that PCA might be useful in this problem to reduce the dimensionality without a significant loss of information."
  },
  {
    "objectID": "09_pca.html#cov-vs-cor",
    "href": "09_pca.html#cov-vs-cor",
    "title": "9. PCA",
    "section": "3. Cov vs Cor?",
    "text": "3. Cov vs Cor?\n\n\n\n\n\n\nShould we perform PCA on the covariance or the correlation matrix?\n\n\n\n\n\nThis depends on the variances of the variables in the dataset. If the variables have large differences in their variances, then the variables with the largest variances will tend to dominate the first few principal components.\nA solution to this is to standardise the variables prior to computing the covariance matrix - i.e., compute the correlation matrix!\n\n# show that the correlation matrix and the covariance matrix of the standardized variables are identical\nall.equal(cor(job_skills), cov(scale(job_skills)))\n\n[1] TRUE\n\n\n\n\n\n\nQuestion 4\n\n\nLook at the variance of the skills ratings in the data set. Do you think that PCA should be carried on the covariance matrix or the correlation matrix?\n\n\n\n\n\nSolution\n\n\n\nLet’s have a look at the standard deviation of each variable:\n\njob_skills %&gt;% \n  summarise(across(everything(), sd))\n\n# A tibble: 1 × 6\n  commun probl_solv logical learn physical appearance\n   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1   2.74       2.41    2.49  2.84     2.41       2.99\n\n\nAs the standard deviations appear to be fairly similar (and so will the variances) we can perform PCA using the covariance matrix."
  },
  {
    "objectID": "09_pca.html#perform-pca",
    "href": "09_pca.html#perform-pca",
    "title": "9. PCA",
    "section": "4. Perform PCA",
    "text": "4. Perform PCA\n\nQuestion 5\n\n\nUsing the principal() function from the psych package, we can perform a PCA of the job performance data, Call the output job_pca.\njob_pca &lt;- principal(job_skills, nfactors = ncol(job_skills), covar = ..., rotate = 'none')\nDepending on your answer to the previous question, either set covar = TRUE or covar = FALSE within the principal() function.\nWarning: the output of the function will be in terms of standardized variables nevertheless. So you will see output with standard deviation of 1.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(psych)\n\njob_pca &lt;- principal(job_skills, nfactors = ncol(job_skills), covar = TRUE, rotate = 'none')\n\n\n\n\n\n\n\n\n\n\nPCA OUTPUT\n\n\n\n\n\nWe can print the output by just typing the name of our PCA:\n\njob_pca\n\nPrincipal Components Analysis\nCall: principal(r = job_skills, nfactors = ncol(job_skills), rotate = \"none\", \n    covar = TRUE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n            PC1   PC2   PC3   PC4   PC5   PC6 h2       u2 com\ncommun     0.98 -0.12  0.02 -0.06  0.10 -0.05  1  6.7e-16 1.1\nprobl_solv 0.22  0.81  0.54  0.00  0.00  0.00  1  7.8e-16 1.9\nlogical    0.33  0.75 -0.58  0.00  0.00  0.00  1  8.9e-16 2.3\nlearn      0.99 -0.11  0.02 -0.05  0.00  0.11  1  0.0e+00 1.1\nphysical   0.99 -0.08  0.01 -0.05 -0.11 -0.05  1 -4.4e-16 1.0\nappearance 0.98 -0.13  0.02  0.16  0.01  0.00  1  3.3e-16 1.1\n\n                       PC1  PC2  PC3  PC4  PC5  PC6\nSS loadings           4.04 1.26 0.63 0.03 0.02 0.02\nProportion Var        0.67 0.21 0.11 0.01 0.00 0.00\nCumulative Var        0.67 0.88 0.99 0.99 1.00 1.00\nProportion Explained  0.67 0.21 0.11 0.01 0.00 0.00\nCumulative Proportion 0.67 0.88 0.99 0.99 1.00 1.00\n\nMean item complexity =  1.4\nTest of the hypothesis that 6 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0 \n with the empirical chi square  0  with prob &lt;  NA \n\nFit based upon off diagonal values = 1\n\n\nThe output is made up of two parts.\nFirst, it shows the loading matrix. In each column of the loading matrix we find how much each of the measured variables contributes to the computed new axis/direction (that is, the principal component). Notice that there are as many principal components as variables.\nThe second part of the output displays the contribution of each component to the total variance:\n\nSS loadings: The sum of the squared loadings. The eigenvalues (see Lecture).\n\nProportion Var: how much of the overall variance the component accounts for out of all the variables.\nCumulative Var: cumulative sum of Proportion Var.\nProportion Explained: relative amount of variance explained (\\(\\frac{\\text{Proportion Var}}{\\text{sum(Proportion Var)}}\\).\nCumulative Proportion: cumulative sum of Proportion Explained.\n\nLet’s focus on the row of that output called “Cumulative Var”. This displays the cumulative sum of the variances of each principal component. Taken all together, the six principal components taken explain all of the total variance in the original data. In other words, the total variance of the principal components (the sum of their variances) is equal to the total variance in the original data (the sum of the variances of the variables).\nHowever, our goal is to reduce the dimensionality of our data, so it comes natural to wonder which of the six principal components explain most of the variability, and which components instead do not contribute substantially to the total variance.\nTo that end, the second row “Proportion Var” displays the proportion of the total variance explained by each component, i.e. the variance of the principal component divided by the total variance.\nThe last row, as we saw, is the cumulative proportion of explained variance: 0.67, 0.67 + 0.21, 0.67 + 0.21 + 0.11, and so on.\nWe also notice that the first PC alone explains 67.3% of the total variability, while the first two components together explain almost 90% of the total variability. From the third component onwards, we do not see such a sharp increase in the proportion of explained variance, and the cumulative proportion slowly reaches the total ratio of 1 (or 100%).\n\n\n\n\n\n\n\n\n\nOptional: (some of) the math behind it\n\n\n\n\n\nDoing data reduction can feel a bit like magic, and in part that’s just because it’s quite complicated.\nThe intuition\nConsider one way we might construct a correlation matrix - as the product of vector \\(\\mathbf{f}\\) with \\(\\mathbf{f'}\\) (f transposed): \\[\n\\begin{equation*}\n\\mathbf{f} =\n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix}\n\\qquad\n\\mathbf{f} \\mathbf{f'} =\n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0.9, 0.8, 0.7, 0.6, 0.5, 0.4 \\\\\n\\end{bmatrix}\n\\qquad = \\qquad\n\\begin{bmatrix}\n0.81, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 0.64, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 0.49, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 0.36, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 0.25, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 0.16 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nBut we constrain this such that the diagonal has values of 1 (the correlation of a variable with itself is 1), and lets call it R. \\[\n\\begin{equation*}\n\\mathbf{R} =\n\\begin{bmatrix}\n1.00, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 1.00, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 1.00, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 1.00, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 1.00, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 1.00 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nPCA is about trying to determine a vector f which generates the correlation matrix R. a bit like unscrambling eggs!\nin PCA, we express \\(\\mathbf{R = CC'}\\), where \\(\\mathbf{C}\\) are our principal components.\nIf \\(n\\) is number of variables in \\(R\\), then \\(i^{th}\\) component \\(C_i\\) is the linear sum of each variable multiplied by some weighting:\n\\[\nC_i = \\sum_{j=1}^{n}w_{ij}x_{j}\n\\]\nHow do we find \\(C\\)?\nThis is where “eigen decomposition” comes in.\nFor the \\(n \\times n\\) correlation matrix \\(\\mathbf{R}\\), there is an eigenvector \\(x_i\\) that solves the equation \\[\n\\mathbf{x_i R} = \\lambda_i \\mathbf{x_i}\n\\] Where the vector multiplied by the correlation matrix is equal to some eigenvalue \\(\\lambda_i\\) multiplied by that vector.\nWe can write this without subscript \\(i\\) as: \\[\n\\begin{align}\n& \\mathbf{R X} = \\mathbf{X \\lambda} \\\\\n& \\text{where:} \\\\\n& \\mathbf{R} = \\text{correlation matrix} \\\\\n& \\mathbf{X} = \\text{matrix of eigenvectors} \\\\\n& \\mathbf{\\lambda} = \\text{vector of eigenvalues}\n\\end{align}\n\\] the vectors which make up \\(\\mathbf{X}\\) must be orthogonal (\\(\\mathbf{XX' = I}\\)), which means that \\(\\mathbf{R = X \\lambda X'}\\)\nWe can actually do this in R manually. Creating a correlation matrix\n\n# lets create a correlation matrix, as the produce of ff'\nf &lt;- seq(.9,.4,-.1)\nR &lt;- f %*% t(f)\n#give rownames and colnames\nrownames(R)&lt;-colnames(R)&lt;-paste0(\"V\",seq(1:6))\n#constrain diagonals to equal 1\ndiag(R)&lt;-1\nR\n\n     V1   V2   V3   V4   V5   V6\nV1 1.00 0.72 0.63 0.54 0.45 0.36\nV2 0.72 1.00 0.56 0.48 0.40 0.32\nV3 0.63 0.56 1.00 0.42 0.35 0.28\nV4 0.54 0.48 0.42 1.00 0.30 0.24\nV5 0.45 0.40 0.35 0.30 1.00 0.20\nV6 0.36 0.32 0.28 0.24 0.20 1.00\n\n\nEigen Decomposition\n\n# do eigen decomposition\ne &lt;- eigen(R)\nprint(e, digits=2)\n\neigen() decomposition\n$values\n[1] 3.16 0.82 0.72 0.59 0.44 0.26\n\n$vectors\n      [,1]   [,2]   [,3]  [,4]   [,5]   [,6]\n[1,] -0.50 -0.061  0.092  0.14  0.238  0.816\n[2,] -0.47 -0.074  0.121  0.21  0.657 -0.533\n[3,] -0.43 -0.096  0.182  0.53 -0.675 -0.184\n[4,] -0.39 -0.142  0.414 -0.78 -0.201 -0.104\n[5,] -0.34 -0.299 -0.860 -0.20 -0.108 -0.067\n[6,] -0.28  0.934 -0.178 -0.10 -0.067 -0.045\n\n\nThe eigenvectors are orthogonal (\\(\\mathbf{XX' = I}\\)):\n\nround(e$vectors %*% t(e$vectors),2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0    0\n[2,]    0    1    0    0    0    0\n[3,]    0    0    1    0    0    0\n[4,]    0    0    0    1    0    0\n[5,]    0    0    0    0    1    0\n[6,]    0    0    0    0    0    1\n\n\nThe Principal Components \\(\\mathbf{C}\\) are the eigenvectors scaled by the square root of the eigenvalues:\n\n#eigenvectors\ne$vectors\n\n       [,1]    [,2]    [,3]   [,4]    [,5]    [,6]\n[1,] -0.496 -0.0611  0.0923  0.139  0.2385  0.8155\n[2,] -0.468 -0.0743  0.1210  0.214  0.6566 -0.5327\n[3,] -0.433 -0.0963  0.1820  0.530 -0.6751 -0.1842\n[4,] -0.390 -0.1416  0.4143 -0.778 -0.2006 -0.1036\n[5,] -0.340 -0.2992 -0.8604 -0.197 -0.1076 -0.0669\n[6,] -0.282  0.9338 -0.1784 -0.100 -0.0667 -0.0452\n\n#scaled by sqrt of eigenvalues\ndiag(sqrt(e$values))\n\n     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] 1.78 0.000 0.000 0.000 0.000 0.000\n[2,] 0.00 0.906 0.000 0.000 0.000 0.000\n[3,] 0.00 0.000 0.848 0.000 0.000 0.000\n[4,] 0.00 0.000 0.000 0.769 0.000 0.000\n[5,] 0.00 0.000 0.000 0.000 0.664 0.000\n[6,] 0.00 0.000 0.000 0.000 0.000 0.512\n\nC &lt;- e$vectors %*% diag(sqrt(e$values))\nC\n\n       [,1]    [,2]    [,3]    [,4]    [,5]    [,6]\n[1,] -0.883 -0.0554  0.0782  0.1070  0.1584  0.4174\n[2,] -0.833 -0.0673  0.1025  0.1648  0.4361 -0.2727\n[3,] -0.770 -0.0873  0.1542  0.4077 -0.4483 -0.0943\n[4,] -0.694 -0.1284  0.3512 -0.5987 -0.1332 -0.0530\n[5,] -0.604 -0.2712 -0.7293 -0.1514 -0.0715 -0.0342\n[6,] -0.502  0.8464 -0.1513 -0.0771 -0.0443 -0.0231\n\n\nAnd we can reproduce our correlation matrix, because \\(\\mathbf{R = CC'}\\).\n\nC %*% t(C)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.72 0.63 0.54 0.45 0.36\n[2,] 0.72 1.00 0.56 0.48 0.40 0.32\n[3,] 0.63 0.56 1.00 0.42 0.35 0.28\n[4,] 0.54 0.48 0.42 1.00 0.30 0.24\n[5,] 0.45 0.40 0.35 0.30 1.00 0.20\n[6,] 0.36 0.32 0.28 0.24 0.20 1.00\n\n\nNow lets imagine we only consider 1 principal component.\nWe can do this with the principal() function:\n\nlibrary(psych)\npc1&lt;-principal(R, nfactors = 1, covar = FALSE, rotate = 'none')\npc1\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 1, rotate = \"none\", covar = FALSE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n    PC1   h2   u2 com\nV1 0.88 0.78 0.22   1\nV2 0.83 0.69 0.31   1\nV3 0.77 0.59 0.41   1\nV4 0.69 0.48 0.52   1\nV5 0.60 0.37 0.63   1\nV6 0.50 0.25 0.75   1\n\n                PC1\nSS loadings    3.16\nProportion Var 0.53\n\nMean item complexity =  1\nTest of the hypothesis that 1 component is sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n\nFit based upon off diagonal values = 0.95\n\n\nLook familiar? It looks like the first component we computed manually. The first column of \\(\\mathbf{C}\\):\n\ncbind(pc1$loadings, C=C[,1])\n\n     PC1      C\nV1 0.883 -0.883\nV2 0.833 -0.833\nV3 0.770 -0.770\nV4 0.694 -0.694\nV5 0.604 -0.604\nV6 0.502 -0.502\n\n\nWe can now ask “how well does this component (on its own) recreate our correlation matrix?”\n\nC[,1] %*% t(C[,1])\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] 0.780 0.735 0.680 0.613 0.534 0.444\n[2,] 0.735 0.693 0.641 0.578 0.503 0.418\n[3,] 0.680 0.641 0.592 0.534 0.465 0.387\n[4,] 0.613 0.578 0.534 0.481 0.419 0.348\n[5,] 0.534 0.503 0.465 0.419 0.365 0.304\n[6,] 0.444 0.418 0.387 0.348 0.304 0.252\n\n\nIt looks close, but not quite. How much not quite? Measurably so!\n\nR - (C[,1] %*% t(C[,1]))\n\n        V1      V2      V3      V4      V5      V6\nV1  0.2200 -0.0154 -0.0498 -0.0727 -0.0838 -0.0836\nV2 -0.0154  0.3067 -0.0809 -0.0976 -0.1033 -0.0982\nV3 -0.0498 -0.0809  0.4075 -0.1140 -0.1153 -0.1066\nV4 -0.0727 -0.0976 -0.1140  0.5187 -0.1193 -0.1085\nV5 -0.0838 -0.1033 -0.1153 -0.1193  0.6346 -0.1036\nV6 -0.0836 -0.0982 -0.1066 -0.1085 -0.1036  0.7477\n\n\nNotice the values on the diagonals of \\(\\mathbf{c_1}\\mathbf{c_1}'\\).\n\ndiag(C[,1] %*% t(C[,1]))\n\n[1] 0.780 0.693 0.592 0.481 0.365 0.252\n\n\nThese aren’t 1, like they are in \\(R\\). But they are proportional: this is the amount of variance in each observed variable that is explained by this first component. Sound familiar?\n\npc1$communality\n\n   V1    V2    V3    V4    V5    V6 \n0.780 0.693 0.592 0.481 0.365 0.252 \n\n\nAnd likewise the 1 minus these is the unexplained variance:\n\n1 - diag(C[,1] %*% t(C[,1]))\n\n[1] 0.220 0.307 0.408 0.519 0.635 0.748\n\npc1$uniquenesses\n\n   V1    V2    V3    V4    V5    V6 \n0.220 0.307 0.408 0.519 0.635 0.748"
  },
  {
    "objectID": "09_pca.html#how-many-components-to-keep",
    "href": "09_pca.html#how-many-components-to-keep",
    "title": "9. PCA",
    "section": "5. How many components to keep?",
    "text": "5. How many components to keep?\nThere is no single best method to select the optimal number of components to keep, while discarding the remaining ones (which are then considered as noise components).\nThe following three heuristic rules are commonly used in the literature:\n\nThe cumulative proportion of explained variance criterion\nKaiser’s rule\nThe scree plot\nVelicer’s Minimum Average Partial method\nParallel analysis\n\nIn the next sections we will look at each of them in turn.\n\n\n\n\n\n\nThe cumulative proportion of explained variance criterion\n\n\n\n\n\nThe rule suggests to keep as many principal components as needed in order to explain approximately 80-90% of the total variance.\n\n\n\n\nQuestion 6\n\n\nLooking again at the PCA output, how many principal components would you keep if you were following the cumulative proportion of explained variance criterion?\n\n\n\n\n\nSolution\n\n\n\nLet’s look again at the PCA summary:\n\njob_pca$loadings\n\n\nLoadings:\n           PC1    PC2    PC3    PC4    PC5    PC6   \ncommun      0.984 -0.120                0.101       \nprobl_solv  0.223  0.810  0.543                     \nlogical     0.329  0.747 -0.578                     \nlearn       0.987 -0.110                       0.105\nphysical    0.988                      -0.110       \nappearance  0.979 -0.125         0.161              \n\n                 PC1   PC2   PC3   PC4   PC5   PC6\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nProportion Var 0.673 0.210 0.105 0.006 0.004 0.003\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n\n\nThe following part of the output tells us that the first two components explain 88.3% of the total variance.\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\nAccording to this criterion, we should keep 2 principal components.\n\n\n\n\n\n\n\n\n\nKaiser’s rule\n\n\n\n\n\nAccording to Kaiser’s rule, we should keep the principal components having variance larger than 1. Standardized variables have a variance equal 1. Because we have 6 variables in the data set, and the total variance is 6, the value 1 represents the average variance in the data: \\[\n\\frac{1 + 1 + 1 + 1 + 1 + 1}{6} = 1\n\\]\nHint:\nThe variances of each PC are shown in the row of the output named SS loadings and also in job_pca$values. The average variance is:\n\nmean(job_pca$values)\n\n[1] 1\n\n\n\n\n\n\nQuestion 7\n\n\nLooking again at the PCA output, how many principal components would you keep if you were following Kaiser’s criterion?\n\n\n\n\n\nSolution\n\n\n\n\njob_pca$loadings\n\n\nLoadings:\n           PC1    PC2    PC3    PC4    PC5    PC6   \ncommun      0.984 -0.120                0.101       \nprobl_solv  0.223  0.810  0.543                     \nlogical     0.329  0.747 -0.578                     \nlearn       0.987 -0.110                       0.105\nphysical    0.988                      -0.110       \nappearance  0.979 -0.125         0.161              \n\n                 PC1   PC2   PC3   PC4   PC5   PC6\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nProportion Var 0.673 0.210 0.105 0.006 0.004 0.003\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n\n\nThe variances are shown in the row\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nFrom the result we see that only the first two principal components have variance greater than 1, so this rule suggests to keep 2 PCs only.\n\n\n\n\n\n\n\n\n\nThe scree plot\n\n\n\n\n\nThe scree plot is a graphical criterion which involves plotting the variance for each principal component. This can be easily done by calling plot on the variances, which are stored in job_pca$values\n\nplot(x = 1:length(job_pca$values), y = job_pca$values, \n     type = 'b', xlab = '', ylab = 'Variance', \n     main = 'Police officers: scree plot', frame.plot = FALSE)\n\n\n\n\n\n\n\n\nwhere the argument type = 'b' tells R that the plot should have both points and lines.\nA typical scree plot features higher variances for the initial components and quickly drops to small variances where the curve is almost flat. The flat part of the curve represents the noise components, which are not able to capture the main sources of variability in the system.\nAccording to the scree plot criterion, we should keep as many principal components as where the “elbow” in the plot occurs. By elbow we mean the variance before the curve looks almost flat.\nAlternatively, some people prefer to use the function scree() from the psych package:\n\nscree(job_skills, factors = FALSE)\n\n\n\n\n\n\n\n\nThis also draws a horizontal line at y = 1. So, if you are making a decision about how many PCs to keep by looking at where the plot falls below the y = 1 line, you are basically following Kaiser’s rule. In fact, Kaiser’s criterion tells you to keep as many PCs as are those with a variance (= eigenvalue) greater than 1.\nNOTE: Scree plots are subjective and may have multiple or no obvious kinks/elbows, making them hard to interpret\n\n\n\n\nQuestion 8\n\n\nAccording to the scree plot, how many principal components would you retain?\n\n\n\n\n\nSolution\n\n\n\nThis criterion then suggests to keep three principal components.\n\n\n\n\n\n\n\n\n\nVelicer’s Minimum Average Partial (MAP) method\n\n\n\n\n\nThe Minimum Average Partial (MAP) test computes the partial correlation matrix (removing and adjusting for a component from the correlation matrix), sequentially partialling out each component. At each step, the partial correlations are squared and their average is computed.\nAt first, the components which are removed will be those that are most representative of the shared variance between 2+ variables, meaning that the “average squared partial correlation” will decrease. At some point in the process, the components being removed will begin represent variance that is specific to individual variables, meaning that the average squared partial correlation will increase.\nThe MAP method is to keep the number of components for which the average squared partial correlation is at the minimum.\nWe can conduct MAP in R using:\n\nVSS(data, plot = FALSE, method=\"pc\", n = ncol(data))\n\n(be aware there is a lot of other information in this output too! For now just focus on the map column)\nNOTE: The MAP method will sometimes tend to under-extract (suggest too few components)\n\n\n\n\nQuestion 9\n\n\nHow many components should we keep according to the MAP method?\n\n\n\n\n\nSolution\n\n\n\n\nVSS(job_skills, plot=FALSE, method=\"pc\", n = ncol(job_skills))\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor, \n    method = \"pc\")\nVSS complexity 1 achieves a maximimum of 0.95  with  3  factors\nVSS complexity 2 achieves a maximimum of 0.98  with  3  factors\n\nThe Velicer MAP achieves a minimum of 0.12  with  2  factors \nBIC achieves a minimum of  -24.1  with  1  factors\nSample Size adjusted BIC achieves a minimum of  -2.87  with  2  factors\n\nStatistics by number of factors \n  vss1 vss2  map dof   chisq prob sqresid  fit RMSEA BIC SABIC complex  eChisq\n1 0.89 0.00 0.17   9 1.1e+01 0.27   2.046 0.89 0.065 -24   4.1     1.0 1.1e+01\n2 0.92 0.96 0.12   4 2.3e-01 0.99   0.802 0.96 0.000 -15  -2.9     1.1 1.5e-03\n3 0.95 0.98 0.29   0 1.5e-01   NA   0.044 1.00    NA  NA    NA     1.1 1.8e-04\n4 0.92 0.96 0.50  -3 9.2e-07   NA   0.777 0.96    NA  NA    NA     1.1 1.6e-09\n5 0.92 0.96 1.00  -5 5.1e-11   NA   0.788 0.96    NA  NA    NA     1.1 1.5e-13\n6 0.92 0.96   NA  -6 5.0e-11   NA   0.788 0.96    NA  NA    NA     1.1 1.5e-13\n     SRMR  eCRMS eBIC\n1 8.5e-02 0.1096  -24\n2 1.0e-03 0.0019  -16\n3 3.5e-04     NA   NA\n4 1.0e-06     NA   NA\n5 9.8e-09     NA   NA\n6 9.8e-09     NA   NA\n\n\nAccording to the MAP criterion we should keep 2 principal components.\n\n\n\n\n\n\n\n\n\nParallel analysis\n\n\n\n\n\nParallel analysis involves simulating lots of datasets of the same dimension but in which the variables are uncorrelated. For each of these simulations, a PCA is conducted on its correlation matrix, and the eigenvalues are extracted. We can then compare our eigenvalues from the PCA on our actual data to the average eigenvalues across these simulations. In theory, for uncorrelated variables, no components should explain more variance than any others, and eigenvalues should be equal to 1. In reality, variables are rarely truly uncorrelated, and so there will be slight variation in the magnitude of eigenvalues simply due to chance. The parallel analysis method suggests keeping those components for which the eigenvalues are greater than those from the simulations.\nIt can be conducted in R using:\n\nfa.parallel(job_skills, fa=\"pc\", n.iter = 500)\n\nNOTE: Parallel analysis will sometimes tend to over-extract (suggest too many components)\n\n\n\n\nQuestion 10\n\n\nHow many components should we keep according to parallel analysis?\n\n\n\n\n\nSolution\n\n\n\n\nfa.parallel(job_skills, fa=\"pc\", n.iter = 500)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1 \n\n\nParallel analysis suggests to keep 1 principal component only as there is only one PC with an eigenvalue higher than the simulated random ones in red."
  },
  {
    "objectID": "09_pca.html#retaining-n-components",
    "href": "09_pca.html#retaining-n-components",
    "title": "9. PCA",
    "section": "6. Retaining N Components",
    "text": "6. Retaining N Components\n\nQuestion 11\n\n\nBased on the set of criteria above, make a decision on how many components you will keep.\nSometimes, there may also be pragmatic reasons for keeping a certain number (e.g. if you want specifically 1 dimension, you may be willing to accept a lower proportion of explained variance).\n\n\n\n\n\nSolution\n\n\n\nIt’s a common thing to see disagreement between the methods to determine how many components we keep, and ultimately this is a decision that we as researchers have to make and explain.\n\n\n\nmethod\nrecommendation\n\n\n\n\nexplaining &gt;80% variance\nkeep 2 components\n\n\nkaiser’s rule\nkeep 2 components\n\n\nscree plot\nkeep 3 components? (subjective)\n\n\nMAP\nkeep 2 components\n\n\nparallel analysis\nkeep 1 component\n\n\n\nBecause three out of the five selection criteria above suggest to keep 2 principal components, here we will keep 2 components. This solution explains a reasonable proportion of the variance (88%), and it would be perfectly defensible to instead go for 3, explaining 98%\n\n\n\n\n\n\n\n\n\nExamining loadings\n\n\n\n\n\nLet’s have a look at the selected principal components:\n\njob_pca$loadings[, 1:2]\n\n             PC1     PC2\ncommun     0.984 -0.1197\nprobl_solv 0.223  0.8095\nlogical    0.329  0.7466\nlearn      0.987 -0.1097\nphysical   0.988 -0.0784\nappearance 0.979 -0.1253\n\n\nand at their corresponding proportion of total variance explained:\n\njob_pca$values / sum(job_pca$values)\n\n[1] 0.67253 0.21016 0.10510 0.00577 0.00372 0.00273\n\n\nWe see that the first PC accounts for 67.3% of the total variability. All loadings seem to have the same magnitude apart from probl_solv and logical which are closer to zero. The first component looks like a sort of average of the officers performance scores excluding problem solving and logical ability.\nThe second principal component, which explains only 21% of the total variance, has two loadings clearly distant from zero: the ones associated to problem solving and logical ability. It distinguishes police officers with strong logical and problem solving skills and low scores on other skills (note the negative magnitudes).\nWe have just seen how to interpret the first components by looking at the magnitude and sign of the coefficients for each measured variable.\n\nFor interpretation purposes, it might help hiding very small loadings. This can be done by specifying the cutoff value in the print() function. However, this only works when you pass the loadings for all the PCs:\n\nprint(job_pca$loadings, cutoff = 0.3)\n\n\nLoadings:\n           PC1    PC2    PC3    PC4    PC5    PC6   \ncommun      0.984                                   \nprobl_solv         0.810  0.543                     \nlogical     0.329  0.747 -0.578                     \nlearn       0.987                                   \nphysical    0.988                                   \nappearance  0.979                                   \n\n                 PC1   PC2   PC3   PC4   PC5   PC6\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nProportion Var 0.673 0.210 0.105 0.006 0.004 0.003\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n\n\n\n\n\n\n\n\n\n\n\n\nOptional: How well are the units represented in the reduced space?\n\n\n\n\n\nWe now focus our attention on the following question: Are all the statistical units (police officers) well represented in the 2D plot?\nThe 2D representation of the original data, which comprise 6 measured variables, is an approximation and henceforth it may happen that not all units are well represented in this new space.\nTypically, it is good to assess the approximation for each statistical unit by inspecting the scores on the discarded principal components. If a unit has a high score on those components, then this is a sign that the unit might be highly misplaced in the new space and misrepresented.\nConsider the 3D example below. There are three cases (= units or individuals). In the original space they are all very different from each other. For example, cases 1 and 2 are very different in their x and y values, but very similar in their z value. Cases 2 and 3 are very similar in their x and y values but very different in their z value. Cases 1 and 3 have very different values for all three variables x, y, and z.\nHowever, when represented in the 2D space given by the two principal components, units 2 and 3 seems like they are very similar when, in fact, they were very different in the original space which also accounted for the z variable.\n\n\n\n\n\n\n\n\n\nWe typically measure how badly a unit is represented in the new coordinate system by considering the sum of squared scores on the discarded principal components:\n\nscores_discarded &lt;- job_pca$scores[, -(1:2)]\nsum_sq &lt;- rowSums(scores_discarded^2)\nsum_sq\n\n [1]  28.51  46.89  63.69  64.24  36.58  17.39  49.24  35.10  18.56  19.27\n[11]  18.56  24.44  12.39  59.10  24.43  33.18  13.40  12.69  11.22  78.87\n[21]  14.16  34.18  95.57  18.40  16.45  14.41  31.97  33.52  40.12  32.48\n[31]  16.85  24.85  30.84  16.00  29.59  11.01   8.07  18.18  14.60  23.73\n[41]  29.82  41.37   9.30  65.42  21.98  63.97  36.09  84.98 129.65  88.00\n\n\nUnits with a high score should be considered for further inspection as it may happen that they are represented as close to another unit when, in fact, they might be very different.\n\nboxplot(sum_sq)\n\n\n\n\n\n\n\n\nThere seem to be only five outliers, and they are not too high compared to the rest of the scores. For this reason, we will consider the 2D representation of the data to be satisfactory."
  },
  {
    "objectID": "09_pca.html#using-pca-scores",
    "href": "09_pca.html#using-pca-scores",
    "title": "9. PCA",
    "section": "7. Using PCA scores",
    "text": "7. Using PCA scores\n\n\n\n\n\n\nPCA scores\n\n\n\n\n\nNow that we have decided to reduce our six variables down to two principal components, we can, for each of our observations, get their scores on each of our components.\n\njob_pca2 &lt;- principal(job_skills, nfactors = 2, covar = TRUE, rotate = 'none')\nhead(job_pca2$scores)\n\n       PC1    PC2\n[1,] -6.10 -1.796\n[2,] -4.69  4.164\n[3,] -5.18 -0.131\n[4,] -4.31 -1.758\n[5,] -3.71  1.207\n[6,] -3.88 -5.200\n\n\nPCA scores are essentially weighted combinations of an individuals responses to the items.\n\\[\n\\text{score}_{\\text{component j}} = w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 +\\, ... \\, + w_{pj}x_p\n\\] Where \\(w\\) are the weights, \\(x\\) the variable scores.\n\n\n\n\n\n\nOptional: How are weights calculated?\n\n\n\n\n\nThe weights are calculated from the eigenvectors as \\[\nw_{ij} = \\frac{a_{ij}}{\\sqrt(\\lambda_j)}\n\\] where \\(w_{ij}\\) is the weight for a given variable \\(i\\) on component \\(j\\) , \\(a_{ij}\\) is the value from the eigenvector for item \\(i\\) on component \\(j\\) and \\(\\lambda_{j}\\) is the eigenvalue for that component.\n\n\n\nIn the literature, some authors also suggest to look at the correlation between the principal component scores and the measured variables:\n\n# First PC\ncor(job_pca2$scores[,1], job_skills)\n\n     commun probl_solv logical learn physical appearance\n[1,]  0.985      0.214   0.319 0.988    0.989      0.981\n\n\nThe first PC is strongly correlated with all the measured variables except probl_solv and logical. As mentioned when looking at loadings, all these variables seem to contributed to the first PC.\n\n# Second PC\ncor(job_pca2$scores[,2], job_skills)\n\n     commun probl_solv logical  learn physical appearance\n[1,] -0.163      0.792   0.738 -0.154   -0.122     -0.169\n\n\nThe second PC is strongly correlated with probl_solv and logical, and slightly negatively correlated with the remaining variables. This separates police officers with clear logical and problem solving skills and a low rating on other skills.\nWe can also visualise our observations (the police officers) in the reduced space given by the retained principal component scores.\n\ntibble(pc1 = job_pca2$scores[, 1],\n       pc2 = job_pca2$scores[, 2]) %&gt;%\n  ggplot(.,aes(x=pc1,y=pc2))+\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 12\n\n\nWe have reduced our six variables down to two principal components, and we are now able to use the scores on each component in a subsequent analysis!\nJoin the principal component scores for your retained components to the original dataset which has the arrest rates in.\nThen fit a linear model to look at how the arrest rate of police officers is predicted by the two components representing different composites of the skills ratings by HR.\nCheck for multicollinearity between your predictors.\n\n\n\n\n\nSolution\n\n\n\n\n# add the PCA scores to the dataset\njob &lt;- \n  job %&gt;% mutate(\n    pc1 = job_pca2$scores[,1],\n    pc2 = job_pca2$scores[,2]\n  )\n# use the scores in an analysis\nmod &lt;- lm(arrest_rate ~ pc1 + pc2, data = job)\n\n# multicollinearity isn't a problem, because the components are orthogonal!! \nlibrary(car)\nvif(mod)\n\npc1 pc2 \n  1   1 \n\nsummary(mod)\n\n\nCall:\nlm(formula = arrest_rate ~ pc1 + pc2, data = job)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.385 -0.119 -0.016  0.160  0.384 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.51161    0.02943   17.38   &lt;2e-16 ***\npc1          0.03583    0.01089    3.29   0.0019 ** \npc2         -0.00923    0.01210   -0.76   0.4490    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.208 on 47 degrees of freedom\nMultiple R-squared:   0.2,  Adjusted R-squared:  0.166 \nF-statistic: 5.86 on 2 and 47 DF,  p-value: 0.00532"
  },
  {
    "objectID": "09_pca.html#footnotes",
    "href": "09_pca.html#footnotes",
    "title": "9. PCA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEven if we cut open someone’s brain, it’s unclear what we would be looking for in order to ‘measure’ it. It is unclear whether anxiety even exists as a physical thing, or rather if it is simply the overarching concept we apply to a set of behaviours and feelings↩︎"
  },
  {
    "objectID": "10_efa.html",
    "href": "10_efa.html",
    "title": "10. EFA 1",
    "section": "",
    "text": "Relevant packages\n\npsych\nGPArotation"
  },
  {
    "objectID": "10_efa.html#check-suitability",
    "href": "10_efa.html#check-suitability",
    "title": "10. EFA 1",
    "section": "1. Check Suitability",
    "text": "1. Check Suitability\n\nQuestion 1\n\n\nRead in the dataset from https://uoepsy.github.io/data/conduct_probs.csv.\nThe first column is clearly an ID column, and it is easiest just to discard this when we are doing factor analysis.\nCreate a correlation matrix for the items.\nInspect the items to check their suitability for exploratory factor analysis.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(psych)\ndf &lt;- read.csv(\"https://uoepsy.github.io/data/conduct_probs.csv\")\n# discard the first column\ndf &lt;- df[,-1]\n\ncorr.test(df)  \n\nCall:corr.test(x = df)\nCorrelation matrix \n       item1 item2 item3 item4 item5 item6 item7 item8 item9 item10\nitem1   1.00  0.59  0.49  0.48  0.60  0.17  0.30  0.32  0.26   0.20\nitem2   0.59  1.00  0.53  0.51  0.66  0.20  0.33  0.30  0.29   0.19\nitem3   0.49  0.53  1.00  0.49  0.55  0.15  0.25  0.24  0.25   0.15\nitem4   0.48  0.51  0.49  1.00  0.65  0.23  0.29  0.32  0.28   0.25\nitem5   0.60  0.66  0.55  0.65  1.00  0.21  0.30  0.29  0.27   0.21\nitem6   0.17  0.20  0.15  0.23  0.21  1.00  0.54  0.57  0.41   0.44\nitem7   0.30  0.33  0.25  0.29  0.30  0.54  1.00  0.83  0.61   0.58\nitem8   0.32  0.30  0.24  0.32  0.29  0.57  0.83  1.00  0.61   0.59\nitem9   0.26  0.29  0.25  0.28  0.27  0.41  0.61  0.61  1.00   0.44\nitem10  0.20  0.19  0.15  0.25  0.21  0.44  0.58  0.59  0.44   1.00\nSample Size \n[1] 450\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n       item1 item2 item3 item4 item5 item6 item7 item8 item9 item10\nitem1      0     0     0     0     0     0     0     0     0      0\nitem2      0     0     0     0     0     0     0     0     0      0\nitem3      0     0     0     0     0     0     0     0     0      0\nitem4      0     0     0     0     0     0     0     0     0      0\nitem5      0     0     0     0     0     0     0     0     0      0\nitem6      0     0     0     0     0     0     0     0     0      0\nitem7      0     0     0     0     0     0     0     0     0      0\nitem8      0     0     0     0     0     0     0     0     0      0\nitem9      0     0     0     0     0     0     0     0     0      0\nitem10     0     0     0     0     0     0     0     0     0      0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\ncortest.bartlett(cor(df), n=450)\n\n$chisq\n[1] 2238\n\n$p.value\n[1] 0\n\n$df\n[1] 45\n\nKMO(df)  \n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = df)\nOverall MSA =  0.87\nMSA for each item = \n item1  item2  item3  item4  item5  item6  item7  item8  item9 item10 \n  0.90   0.88   0.92   0.88   0.84   0.94   0.82   0.81   0.95   0.94 \n\npairs.panels(df)\n\n\n\n\n\n\n\n\nor alternatively, if you want a ggplot based approach:\n\nlibrary(GGally)\nggpairs(data=df, diag=list(continuous=\"density\"), axisLabels=\"show\")"
  },
  {
    "objectID": "10_efa.html#how-many-factors",
    "href": "10_efa.html#how-many-factors",
    "title": "10. EFA 1",
    "section": "2. How many factors?",
    "text": "2. How many factors?\n\nQuestion 2\n\n\nHow many dimensions should be retained? This question can be answered in the same way as we did above for PCA.\nUse a scree plot, parallel analysis, and MAP test to guide you.\nYou can use fa.parallel(data, fa = \"fa\") to conduct both parallel analysis and view the scree plot!\n\n\n\n\n\nSolution\n\n\n\n\nfa.parallel(df, fa = \"fa\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n\n\nIn this case the scree plot has a kink at the third factor, so we probably want to retain 2 factors.\nWe can conduct the MAP test using VSS(data).\n\nVSS(df, plot = FALSE, n = ncol(df))$map\n\n [1] 0.1058 0.0338 0.0576 0.1035 0.1494 0.2520 0.3974 0.4552 1.0000     NA\n\n\nThe MAP test suggests retaining 2 factors."
  },
  {
    "objectID": "10_efa.html#perform-efa",
    "href": "10_efa.html#perform-efa",
    "title": "10. EFA 1",
    "section": "3. Perform EFA",
    "text": "3. Perform EFA\nNow we need to perform the factor analysis. But there are two further things we need to consider, and they are:\n\nwhether we want to apply a rotation to our factor loadings, in order to make them easier to interpret, and\n\nhow do we want to extract our factors (it turns out there are loads of different approaches!).\n\n\nRotations?\nRotations are so called because they transform our loadings matrix in such a way that it can make it more easy to interpret. You can think of it as a transformation applied to our loadings in order to optimise interpretability, by maximising the loading of each item onto one factor, while minimising its loadings to others. We can do this by simple rotations, but maintaining our axes (the factors) as perpendicular (i.e., uncorrelated) as in Figure 3, or we can allow them to be transformed beyond a rotation to allow the factors to correlate (Figure 4).\n\n\n\n\n\nFigure 2: No rotation\n\n\n\n\n\n\n\n\n\nFigure 3: Orthogonal rotation\n\n\n\n\n\n\n\n\n\nFigure 4: Oblique rotation\n\n\n\n\nIn our path diagram of the model (Figure 5), all the factor loadings remain present, but some of them become negligible. We can also introduce the possible correlation between our factors, as indicated by the curved arrow between \\(F_1\\) and \\(F_2\\).\n\n\n\n\n\nFigure 5: Path diagrams for EFA with rotation\n\n\n\n\n\n\nFactor Extraction\nPCA (using eigendecomposition) is itself a method of extracting the different dimensions from our data. However, there are lots more available for factor analysis.\nYou can find a lot of discussion about different methods both in the help documentation for the fa() function from the psych package:\n\nFactoring method fm=“minres” will do a minimum residual as will fm=“uls”. Both of these use a first derivative. fm=“ols” differs very slightly from “minres” in that it minimizes the entire residual matrix using an OLS procedure but uses the empirical first derivative. This will be slower. fm=“wls” will do a weighted least squares (WLS) solution, fm=“gls” does a generalized weighted least squares (GLS), fm=“pa” will do the principal factor solution, fm=“ml” will do a maximum likelihood factor analysis. fm=“minchi” will minimize the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair. fm =“minrank” will do a minimum rank factor analysis. “old.min” will do minimal residual the way it was done prior to April, 2017 (see discussion below). fm=“alpha” will do alpha factor analysis as described in Kaiser and Coffey (1965)\n\nAnd there are lots of discussions both in papers and on forums.\nAs you can see, this is a complicated issue, but when you have a large sample size, a large number of variables, for which you have similar communalities, then the extraction methods tend to agree. For now, don’t fret too much about the factor extraction method.2\n\n\nQuestion 3\n\n\nUse the function fa() from the psych package to conduct and EFA to extract 2 factors (this is what we suggest based on the various tests above, but you might feel differently - the ideal number of factors is subjective!). Use a suitable rotation (rotate = ?) and extraction method (fm = ?).\n\nconduct_efa &lt;- fa(data, nfactors = ?, rotate = ?, fm = ?)\n\n\n\n\n\n\nSolution\n\n\n\nFor example, you could choose an oblimin rotation to allow factors to correlate and use minres as the extraction method.\n\nconduct_efa &lt;- fa(df, nfactors=2, rotate='oblimin', fm=\"minres\")"
  },
  {
    "objectID": "10_efa.html#inspect",
    "href": "10_efa.html#inspect",
    "title": "10. EFA 1",
    "section": "4. Inspect",
    "text": "4. Inspect\nWe can simply print the name of our model in order to see a lot of information. Let’s go through it in pieces.\n\nLoadings\n\n\nFactor Analysis using method =  minres\nCall: fa(r = df, nfactors = 2, rotate = \"oblimin\", fm = \"minres\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n         MR1   MR2   h2   u2 com\nitem1   0.03  0.71 0.52 0.48   1\nitem2   0.01  0.77 0.60 0.40   1\nitem3  -0.02  0.68 0.45 0.55   1\nitem4   0.06  0.68 0.50 0.50   1\nitem5  -0.04  0.87 0.73 0.27   1\nitem6   0.63 -0.02 0.39 0.61   1\nitem7   0.89  0.00 0.80 0.20   1\nitem8   0.92 -0.01 0.84 0.16   1\nitem9   0.63  0.09 0.45 0.55   1\nitem10  0.67 -0.03 0.43 0.57   1\n\n\nFactor loading’s, like PCA loading’s, show the relationship of each measured variable to each factor. They range between -1.00 and 1.00 Larger absolute values represent stronger relationship between measured variable and factor.\n\nThe columns that (depending upon estimation method) might be called MR/ML/PC are the factors. The number assigned to is arbitrary, and they might not always be in a numeric order (this has to do with a rotated solution). Typically, the numbering maps to how much variance each factor account for.\nh2: This is the “communality”, which is how much variance in the item is explained by the factors. It is calculated as the sum of the squared loadings.\n\nu2: This is \\(1 - h2\\). It is the residual variance, or the “uniqueness” for that item (i.e. the amount left unexplained).\n\ncom: This is the “Item complexity”. It tells us how much a given item reflects a single factor (vs being “more complex” in that it represents multiple factors). It equals one if an item loads only on one factor, 2 if evenly loads on two factors, etc.\n\nYou can get these on their own using\n\nconduct_efa$loadings\n\n\n\nVariance Accounted For\n\n\n                       MR1  MR2\nSS loadings           2.92 2.80\nProportion Var        0.29 0.28\nCumulative Var        0.29 0.57\nProportion Explained  0.51 0.49\nCumulative Proportion 0.51 1.00\n\n\nBelow the factor loadings, we have a familiar set of measures of the variance in the data accounted for by each factor. This is very similar to what we saw with PCA.\n\nSS loadings: The sum of the squared loadings. The eigenvalues.\n\nProportion Var: how much of the overall variance the factor accounts for out of all the variables.\nCumulative Var: cumulative sum of Proportion Var.\nProportion Explained: relative amount of variance explained (\\(\\frac{\\text{Proportion Var}}{\\text{sum(Proportion Var)}}\\).\nCumulative Proportion: cumulative sum of Proportion Explained.\n\nYou can get these on their own using\n\nconduct_efa$Vaccounted\n\n\n\nFactor Correlations\n\n\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.43\nMR2 0.43 1.00\n\nMean item complexity =  1\n\n\nWhether we see this section will depend if we have run a factor analysis with \\(\\geq 2\\) factors and a rotation.\n\nfactor correlations: shows the correlation matrix between the factors.\nmean item complexity: shows the mean of the com column from the loadings above.\n\nYou can get these on their own using\n\nconduct_efa$Phi\n\n\n\nTests, Fit Indices etc\nWe also get a whole load of other stuff that can sometimes be useful. These include: a test of an hypothesis that the 2 factors are sufficient; information on the number of observations; fit indices such as RMSEA, TLI RMSR etc; and measures of factor score adequacy (we’ll get to talking about factor scores next week).\n\n\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  45  with the objective function =  5.03 with Chi Square =  2238\ndf of  the model are 26  and the objective function was  0.09 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  450 with the empirical chi square  13.7  with prob &lt;  0.98 \nThe total n.obs was  450  with Likelihood Chi Square =  40  with prob &lt;  0.039 \n\nTucker Lewis Index of factoring reliability =  0.989\nRMSEA index =  0.035  and the 90 % confidence intervals are  0.008 0.055\nBIC =  -119\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.96 0.94\nMultiple R square of scores with factors          0.92 0.88\nMinimum correlation of possible factor scores     0.84 0.76\n\n\n\n\nQuestion 4\n\n\nInspect the loadings (conduct_efa$loadings) and give the factors you extracted labels based on the patterns of loadings.\nLook back to the description of the items, and suggest a name for your factors\n\n\n\n\n\nSolution\n\n\n\nYou can inspect the loadings using:\n\nprint(conduct_efa$loadings, sort=TRUE)\n\n\nLoadings:\n       MR1    MR2   \nitem6   0.634       \nitem7   0.890       \nitem8   0.924       \nitem9   0.629       \nitem10  0.669       \nitem1          0.706\nitem2          0.772\nitem3          0.681\nitem4          0.676\nitem5          0.872\n\n                MR1   MR2\nSS loadings    2.90 2.784\nProportion Var 0.29 0.278\nCumulative Var 0.29 0.568\n\n\nWe can see that the first five items have high loadings for one factor and the second five items have high loadings for the other.\nThe first five items all have in common that they are non-aggressive forms of conduct problems, while the last five items are all aggressive behaviours. We could, therefore, label our factors: ‘non-aggressive’ and ‘aggressive’ conduct problems.\n\n\n\n\nQuestion 5\n\n\nHow correlated are your factors?\nWe can inspect the factor correlations (if we used an oblique rotation) using:\n\nconduct_efa$Phi\n\n\n\n\n\n\nSolution\n\n\n\n\nconduct_efa$Phi\n\n     MR1  MR2\nMR1 1.00 0.43\nMR2 0.43 1.00\n\n\nWe can see here that there is a moderate correlation between the two factors. An oblique rotation would be appropriate here."
  },
  {
    "objectID": "10_efa.html#write-up",
    "href": "10_efa.html#write-up",
    "title": "10. EFA 1",
    "section": "5. Write-up",
    "text": "5. Write-up\n\nQuestion 6\n\n\nDrawing on your previous answers and conducting any additional analyses you believe would be necessary to identify an optimal factor structure for the 10 conduct problems, write a brief text that summarises your method and the results from your chosen optimal model.\n\n\n\n\n\nSolution\n\n\n\nThe main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description).\nAn example summary would be:\n\nFirst, the data were checked for their suitability for factor analysis. Normality was checked using visual inspection of histograms, linearity was checked through the inspection of the linear and lowess lines for the pairwise relations of the variables, and factorability was confirmed using a KMO test, which yielded an overall KMO of \\(.87\\) with no variable KMOs \\(&lt;.50\\). An exploratory factor analysis was conducted to inform the structure of a new conduct problems test. Inspection of a scree plot alongside parallel analysis (using principal components analysis; PA-PCA) and the MAP test were used to guide the number of factors to retain. All three methods suggested retaining two factors; however, a one-factor and three-factor solution were inspected to confirm that the two-factor solution was optimal from a substantive and practical perspective, e.g., that it neither blurred important factor distinctions nor included a minor factor that would be better combined with the other in a one-factor solution. These factor analyses were conducted using minres extraction and (for the two- and three-factor solutions) an oblimin rotation, because it was expected that the factors would correlate. Inspection of the factor loadings and correlations reinforced that the two-factor solution was optimal: both factors were well-determined, including 5 loadings \\(&gt;|0.3|\\) and the one-factor model blurred the distinction between different forms of conduct problems. The factor loadings are provided in Table 13. Based on the pattern of factor loadings, the two factors were labelled ‘aggressive conduct problems’ and ‘non-aggressive conduct problems’. These factors had a correlation of \\(r=.43\\). Overall, they accounted for 57% of the variance in the items, suggesting that a two-factor solution effectively summarised the variation in the items.\n\n\n\n\nTable 1: Factor Loadings\n\n\n\nMR1\nMR2\n\n\n\n\nitem1\n\n0.71\n\n\nitem2\n\n0.77\n\n\nitem3\n\n0.68\n\n\nitem4\n\n0.68\n\n\nitem5\n\n0.87\n\n\nitem6\n0.63\n\n\n\nitem7\n0.89\n\n\n\nitem8\n0.92\n\n\n\nitem9\n0.63\n\n\n\nitem10\n0.67"
  },
  {
    "objectID": "10_efa.html#footnotes",
    "href": "10_efa.html#footnotes",
    "title": "10. EFA 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen we have some clear hypothesis about relationships between measured variables and latent factors, we might want to impose a specific factor structure on the data (e.g., items 1 to 10 all measure social anxiety, items 11 to 15 measure health anxiety, and so on). When we impose a specific factor structure, we are doing Confirmatory Factor Analysis (CFA). This is not covered in this course, but it’s important to note that in practice EFA is not wholly “exploratory” (your theory will influence the decisions you make) nor is CFA wholly “confirmatory” (in which you will inevitably get tempted to explore how changing your factor structure might improve fit).↩︎\n(It’s a bit like the optimiser issue in the multi-level model block)↩︎\nYou should provide the table of factor loadings. It is conventional to omit factor loadings \\(&lt;|0.3|\\); however, be sure to ensure that you mention this in a table note.↩︎"
  },
  {
    "objectID": "11_efa2.html",
    "href": "11_efa2.html",
    "title": "11. EFA 2",
    "section": "",
    "text": "Relevant packages\n\ntidyverse\npsych\nGPArotation"
  },
  {
    "objectID": "11_efa2.html#footnotes",
    "href": "11_efa2.html#footnotes",
    "title": "11. EFA 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOf course this all assuming that the scales aren’t completely miscalibrated↩︎"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n\nSolution\n\n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "example_00_anova.html",
    "href": "example_00_anova.html",
    "title": "Analysis Example: Rpt & Mixed ANOVA",
    "section": "",
    "text": "This is optional for the DAPR3 course, but may be useful for your dissertations should your field/supervisor prefer the ANOVA framework to that of the linear model.\nThis walks briefly through these models with the ez package. There are many other packages available, and many good tutorials online should you desire extra resources in the future:\n\nhttps://www.datanovia.com/en/lessons/repeated-measures-anova-in-r\nhttps://www.r-bloggers.com/2021/04/repeated-measures-of-anova-in-r-complete-tutorial/\nhttps://stats.idre.ucla.edu/r/seminars/repeated-measures-analysis-with-r/\nhttps://www.datanovia.com/en/lessons/mixed-anova-in-r/\n\n\n\nData: Audio interference in executive functioning\nThis data is from a simulated study that aims to investigate the following research questions:\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n\n24 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room.\nThe data is in stored in two separate files - the research administering the tests recorded the SDMT score in one spreadsheet, while details of the audio used in the experiment are held in a separate sheet\n\nInformation about the audio condition for each trial of each participant is stored in .csv format at https://uoepsy.github.io/data/ef_music.csv. The data is in long format (1 row per participant-trial).\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\ntrial_n\nTrial Number (1-15)\n\n\naudio\nAudio heard during the test (‘no_audio’, ‘white_noise’,‘music’)\n\n\nheadphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n\n\n\n\n\n\nInformation on participants’ Symbol Digit Modalities Test (SDMT) for each trial is stored in .xlsx format at https://uoepsy.github.io/data/ef_sdmt.xlsx. The data is in wide format (1 row per participant, 1 column per trial).\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\nTrial_01\nSDMT score in trial 1\n\n\nTrial_02\nSDMT score in trial 2\n\n\nTrial_03\nSDMT score in trial 3\n\n\n…\nSDMT score in trial …\n\n\n…\nSDMT score in trial …\n\n\nTrial_15\nSDMT score in trial 15\n\n\n\n\n\n\nThe code below will read in both datasets and join them for you:\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\ndownload.file(url = \"https://uoepsy.github.io/data/ef_sdmt.xlsx\",\n              destfile = \"ef_sdmt.xlsx\",\n              mode = \"wb\")\nefdata &lt;- \n  left_join(\n    read_csv(\"https://uoepsy.github.io/data/ef_music.csv\"),\n    read_xlsx(\"ef_sdmt.xlsx\") %&gt;%\n      pivot_longer(Trial_01:Trial_15, names_to = \"trial_n\", values_to = \"SDMT\")\n  )\n\n\n\nOne-Way Repeated Measures ANOVA\nFor a repeated measures ANOVA, we have one independent variable that is within group.\nThis would be appropriate if our research question were the following:\n\nHow do different types of audio interfere with executive functioning?\n\nMapping this to the variables in our dataset, our model is going to be SDMT ~ audio, and we want to account for PID differences. So for now we will ignore the headphones variable.\n\n\nCode\nhead(efdata)\n\n\n# A tibble: 6 × 5\n  PID    trial_n  audio       headphones  SDMT\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 PPT_01 Trial_02 no_audio    speakers      31\n2 PPT_01 Trial_08 no_audio    speakers      23\n3 PPT_01 Trial_11 no_audio    speakers      23\n4 PPT_01 Trial_13 no_audio    speakers      24\n5 PPT_01 Trial_15 no_audio    speakers      34\n6 PPT_01 Trial_01 white_noise speakers      38\n\n\nThe easiest way to conduct a repeated measures ANOVA in R is to use the ez package, which comes with some handy functions to visualise the experimental design.\nWe can see from below that every participant completed 5 trials for each type of audio interference:\n\n\nCode\nlibrary(ez)\nezDesign(data = efdata, x = audio, y = PID)\n\n\n\n\n\n\n\n\n\nThe ezANOVA() function takes a few arguments.\nThe ones you will need for this are:\n\ndata the name of the dataframe\ndv the column name for the dependent variable\nwid the column name for the participant id variable\nwithin the column name(s) for the predictor variable(s) that vary within participants\nbetween the column name(s) for any predictor variable(s) that vary between participants\n\nFit a repeated measures ANOVA to examine the effect of the audio type on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio)\n\n\n$ANOVA\n  Effect DFn DFd        F            p p&lt;.05       ges\n2  audio   2  46 44.69618 1.647271e-11     * 0.2534633\n\n$`Mauchly's Test for Sphericity`\n  Effect         W          p p&lt;.05\n2  audio 0.8105961 0.09927715      \n\n$`Sphericity Corrections`\n  Effect       GGe      p[GG] p[GG]&lt;.05      HFe        p[HF] p[HF]&lt;.05\n2  audio 0.8407573 5.0677e-10         * 0.899603 1.427119e-10         *\n\n\n\n\nMixed ANOVA\nMixed ANOVA can be used to investigate effects of independent variables that are at two different levels, i.e. some are within clusters and some are between.\n\nDoes the effect of audio interference on executive functioning differ depending upon whether or not noise-cancelling headphones are used?\n\nLook at the two lines below. Can you work out what the plots will look like before you run them?\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\nezDesign(data = efdata, x = headphones, y = audio)\n\n\nParticipants 1-20 are in one condition, and 21-40 are in another.\nThis should look like a two big blocks on the diagonal.\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\n\n\n\n\n\n\n\n\n\nIn each condition, all different types of audio were observed in the same number of trials. This should be a full grid:\n\n\nCode\nezDesign(data = efdata, x = headphones, y = audio)\n\n\n\n\n\n\n\n\n\nFit a mixed ANOVA to examine the interaction between audio and headphone use on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio, between = headphones)\n\n\n$ANOVA\n            Effect DFn DFd         F            p p&lt;.05        ges\n2       headphones   1  22  9.815545 4.836945e-03     * 0.26784992\n3            audio   2  44 59.615596 2.980503e-13     * 0.32788320\n4 headphones:audio   2  44  8.677316 6.657590e-04     * 0.06629911\n\n$`Mauchly's Test for Sphericity`\n            Effect         W         p p&lt;.05\n3            audio 0.9422531 0.5355001      \n4 headphones:audio 0.9422531 0.5355001      \n\n$`Sphericity Corrections`\n            Effect       GGe        p[GG] p[GG]&lt;.05      HFe        p[HF]\n3            audio 0.9454057 1.196469e-12         * 1.031585 2.980503e-13\n4 headphones:audio 0.9454057 8.648057e-04         * 1.031585 6.657590e-04\n  p[HF]&lt;.05\n3         *\n4         *\n\n\nThe ez package also contains some easy plotting functions for factorial experiments, such as ezPlot(). It takes similar arguments to the ezANOVA() function.\n\nlook up the help documentation for ezPlot().\nlet’s use ezPlot() to make a nice plot\n\n\n\nCode\nezPlot(data = efdata, dv = SDMT, \n       wid = PID, within = audio, between = headphones,\n       x = audio, split = headphones)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same thing in lmer\n\n\n\n\n\n\n\nCode\nlibrary(lme4)\nlibrary(lmerTest)\nmod &lt;- lmer(SDMT ~ 1 + headphones * audio + (1 + audio | PID), \n            data = efdata)\nanova(mod, type=\"III\")\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                 Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nheadphones        325.0  325.04     1    22  9.8155  0.004837 ** \naudio            3212.0 1606.01     2    22 48.4976 8.626e-09 ***\nheadphones:audio  490.1  245.06     2    22  7.4001  0.003486 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nlibrary(sjPlot)\nplot_model(mod, type=\"eff\", terms=c(\"audio\",\"headphones\"))"
  },
  {
    "objectID": "example_01_EFA.html",
    "href": "example_01_EFA.html",
    "title": "Analysis Example: Exploratory Factor Analysis",
    "section": "",
    "text": "Please note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ug.ppls.stats@ed.ac.uk.\n\n\nIntro\nThis is a quick demonstration of one way of dealing with these tasks. It is by no means the only correct way. There is a substantial level of subjectivity in Exploratory Factor Analysis and the method involves repeated evaluation and re-evaluation of the model in light of the extracted factors and their conceptual relationships with the analysed items. In other words, a good EFA requires you to get your hands dirty.\n\nData: Work Pressures Survey\nThe Work Pressures Survey (WPS) data is available at the following link: https://uoepsy.github.io/data/WPS_data.csv.\nThe data contains responses from 946 workers from a variety of companies to the Work Pressures Survey. You can look at the survey taken by the study participants at the following link: https://uoepsy.github.io/data/WPS_data_codebook.pdf\nWe will be performing a factor analysis of the main section of this survey (Job1 to Job50).\n\n\n\nRead in Data\nRead the WPS data into R. Make sure to take a look at the variable names and data structure.\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- read.csv(\"https://uoepsy.github.io/data/WPS_data.csv\")\nhead(df)\n\n\n  job_yrs job_mths worktype cont_hrs act_hrs gender dobm doby status youngdep\n1       1        6        1       45      45      1    7   83      1        1\n2       5        8        1       36      36      2    1   58      2        2\n3       2        8        1       40      40      2    2   82      1        1\n4       4        3        1       35      35      1    4   78      1        1\n5       7        1        1       40      41      1    2   73      2        2\n6       3       11        1       32      32      1    8   75      2        2\n  elderdep qualif exercise cigs alcohol time_rel when_rel job1 job2 job3 job4\n1        1      5        4    1       1        2        1    5    4    5    4\n2        1      5        2    2       0        2        2    5    7    4    3\n3        1      4        6    2       0        1        1    1    4    7    1\n4        2      5        3    1       2        2        1    6    4    5    5\n5        1      4        2    1       3        2        1    1    1    5    5\n6        2      5        6    1       4        5        2    3    4    5    3\n  job5 job6 job7 job8 job9 job10 job11 job12 job13 job14 job15 job16 job17\n1    6    4    7    7    4     7     4     4     1     4     4     6     4\n2    6    2    1    4    5     6     1     5     4     5     7     5     1\n3    1    1    7    7    3     1     7     4     1     7     3     1     1\n4    6    4    5    5    5     6     5     3     4     2     7     2     4\n5    1    2    2    2    5     2     3     3     4     6     2     2     2\n6    4    3    4    3    5     4     3     3     4     1     6     3     5\n  job18 job19 job20 job21 job22 job23 job24 job25 job26 job27 job28 job29 job30\n1     7     7     3     4     3     5     4     4     7     2     3     3     4\n2     7     1     5     1     1     1     4     5     6     4     6     3     3\n3     7     7     7     7     1     2     5     6     7     1     7     5     6\n4     6     7     5     5     2     4     4     5     3     2     5     7     5\n5     3     2     1     1     4     6     2     3     6     6     2     5     1\n6     5     6     3     3     1     3     4     2     5     5     5     6     2\n  job31 job32 job33 job34 job35 job36 job37 job38 job39 job40 job41 job42 job43\n1     5     5     6     4     4     4     4     4     5     4     6     6     6\n2     1     7     4     6     6     1     5     5     7     3     5     4     1\n3     5     3     6     5     3     2     1     5     5     5     3     5     7\n4     3     2     5     7     5     3     3     2     7     3     5     4     3\n5     4     2     4     4     4     2     2     2     2     4     1     5     4\n6     5     1     3     5     1     5     3     1     5     3     1     3     5\n  job44 job45 job46 job47 job48 job49 job50\n1     6     6     3     3     3     6     3\n2     4     6     1     4     7     5     5\n3     6     7     2     5     7     5     1\n4     5     7     3     5     7     6     3\n5     2     6     6     1     6     2     1\n6     3     3     6     3     4     2     7\n\n\nVariable names - Option 1:\n\n\nCode\nnames(df)\n\n\n [1] \"job_yrs\"  \"job_mths\" \"worktype\" \"cont_hrs\" \"act_hrs\"  \"gender\"  \n [7] \"dobm\"     \"doby\"     \"status\"   \"youngdep\" \"elderdep\" \"qualif\"  \n[13] \"exercise\" \"cigs\"     \"alcohol\"  \"time_rel\" \"when_rel\" \"job1\"    \n[19] \"job2\"     \"job3\"     \"job4\"     \"job5\"     \"job6\"     \"job7\"    \n[25] \"job8\"     \"job9\"     \"job10\"    \"job11\"    \"job12\"    \"job13\"   \n[31] \"job14\"    \"job15\"    \"job16\"    \"job17\"    \"job18\"    \"job19\"   \n[37] \"job20\"    \"job21\"    \"job22\"    \"job23\"    \"job24\"    \"job25\"   \n[43] \"job26\"    \"job27\"    \"job28\"    \"job29\"    \"job30\"    \"job31\"   \n[49] \"job32\"    \"job33\"    \"job34\"    \"job35\"    \"job36\"    \"job37\"   \n[55] \"job38\"    \"job39\"    \"job40\"    \"job41\"    \"job42\"    \"job43\"   \n[61] \"job44\"    \"job45\"    \"job46\"    \"job47\"    \"job48\"    \"job49\"   \n[67] \"job50\"   \n\n\nVariable names - Option 2:\n\n\nCode\ncolnames(df)\n\n\nData structure - Option 1:\n\n\nCode\nstr(df)\n\n\n'data.frame':   946 obs. of  67 variables:\n $ job_yrs : int  1 5 2 4 7 3 4 4 1 0 ...\n $ job_mths: num  6 8 8 3 1 11 2 0 1 11 ...\n $ worktype: int  1 1 1 1 1 1 1 1 1 1 ...\n $ cont_hrs: num  45 36 40 35 40 32 30 54 37 37 ...\n $ act_hrs : num  45 36 40 35 41 32 30 54 37 37 ...\n $ gender  : int  1 2 2 1 1 1 2 1 2 2 ...\n $ dobm    : int  7 1 2 4 2 8 3 5 12 11 ...\n $ doby    : int  83 58 82 78 73 75 72 85 70 70 ...\n $ status  : int  1 2 1 1 2 2 2 2 2 2 ...\n $ youngdep: int  1 2 1 1 2 2 2 2 2 2 ...\n $ elderdep: int  1 1 1 2 1 2 1 1 1 1 ...\n $ qualif  : int  5 5 4 5 4 5 4 5 2 2 ...\n $ exercise: int  4 2 6 3 2 6 4 3 1 3 ...\n $ cigs    : int  1 2 2 1 1 1 2 2 1 1 ...\n $ alcohol : num  1 0 0 2 3 4 0 0 1 4 ...\n $ time_rel: int  2 2 1 2 2 5 1 3 1 2 ...\n $ when_rel: int  1 2 1 1 1 2 1 2 2 2 ...\n $ job1    : int  5 5 1 6 1 3 5 4 6 6 ...\n $ job2    : int  4 7 4 4 1 4 4 1 2 6 ...\n $ job3    : int  5 4 7 5 5 5 6 7 4 7 ...\n $ job4    : int  4 3 1 5 5 3 7 7 5 6 ...\n $ job5    : int  6 6 1 6 1 4 5 7 4 7 ...\n $ job6    : int  4 2 1 4 2 3 5 1 2 7 ...\n $ job7    : int  7 1 7 5 2 4 4 5 4 6 ...\n $ job8    : int  7 4 7 5 2 3 5 5 6 7 ...\n $ job9    : int  4 5 3 5 5 5 6 7 2 7 ...\n $ job10   : int  7 6 1 6 2 4 5 7 4 7 ...\n $ job11   : int  4 1 7 5 3 3 6 1 5 7 ...\n $ job12   : int  4 5 4 3 3 3 3 1 4 7 ...\n $ job13   : int  1 4 1 4 4 4 4 1 5 7 ...\n $ job14   : int  4 5 7 2 6 1 2 1 1 7 ...\n $ job15   : int  4 7 3 7 2 6 6 7 5 5 ...\n $ job16   : int  6 5 1 2 2 3 3 7 4 5 ...\n $ job17   : int  4 1 1 4 2 5 3 7 3 7 ...\n $ job18   : int  7 7 7 6 3 5 5 3 6 2 ...\n $ job19   : int  7 1 7 7 2 6 7 3 6 2 ...\n $ job20   : int  3 5 7 5 1 3 7 6 3 3 ...\n $ job21   : int  4 1 7 5 1 3 3 1 5 6 ...\n $ job22   : int  3 1 1 2 4 1 2 1 2 6 ...\n $ job23   : int  5 1 2 4 6 3 6 1 5 7 ...\n $ job24   : int  4 4 5 4 2 4 4 4 5 6 ...\n $ job25   : int  4 5 6 5 3 2 3 7 5 4 ...\n $ job26   : int  7 6 7 3 6 5 6 7 6 4 ...\n $ job27   : int  2 4 1 2 6 5 1 1 3 7 ...\n $ job28   : int  3 6 7 5 2 5 6 1 4 7 ...\n $ job29   : int  3 3 5 7 5 6 5 1 3 7 ...\n $ job30   : int  4 3 6 5 1 2 3 6 6 4 ...\n $ job31   : int  5 1 5 3 4 5 4 1 5 5 ...\n $ job32   : int  5 7 3 2 2 1 3 2 6 5 ...\n $ job33   : int  6 4 6 5 4 3 7 4 6 4 ...\n $ job34   : int  4 6 5 7 4 5 6 1 5 5 ...\n $ job35   : int  4 6 3 5 4 1 3 1 5 6 ...\n $ job36   : int  4 1 2 3 2 5 2 1 5 6 ...\n $ job37   : int  4 5 1 3 2 3 3 7 3 6 ...\n $ job38   : int  4 5 5 2 2 1 3 1 3 7 ...\n $ job39   : int  5 7 5 7 2 5 6 7 6 5 ...\n $ job40   : int  4 3 5 3 4 3 3 7 3 6 ...\n $ job41   : int  6 5 3 5 1 1 3 1 5 7 ...\n $ job42   : int  6 4 5 4 5 3 4 1 4 5 ...\n $ job43   : int  6 1 7 3 4 5 1 2 3 7 ...\n $ job44   : int  6 4 6 5 2 3 7 1 5 7 ...\n $ job45   : int  6 6 7 7 6 3 6 7 6 7 ...\n $ job46   : int  3 1 2 3 6 6 3 7 3 5 ...\n $ job47   : int  3 4 5 5 1 3 3 3 3 6 ...\n $ job48   : int  3 7 7 7 6 4 6 7 5 6 ...\n $ job49   : int  6 5 5 6 2 2 5 2 6 7 ...\n $ job50   : int  3 5 1 3 1 7 4 1 5 6 ...\n\n\nData structure - Option 2:\n\n\nCode\nglimpse(df)\n\n\nRows: 946\nColumns: 67\n$ job_yrs  &lt;int&gt; 1, 5, 2, 4, 7, 3, 4, 4, 1, 0, 2, 2, 11, 4, 1, 2, 1, 0, 1, 0, …\n$ job_mths &lt;dbl&gt; 6, 8, 8, 3, 1, 11, 2, 0, 1, 11, 0, 5, 1, 2, 10, 10, 4, 3, 0, …\n$ worktype &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1…\n$ cont_hrs &lt;dbl&gt; 45.0, 36.0, 40.0, 35.0, 40.0, 32.0, 30.0, 54.0, 37.0, 37.0, 3…\n$ act_hrs  &lt;dbl&gt; 45.0, 36.0, 40.0, 35.0, 41.0, 32.0, 30.0, 54.0, 37.0, 37.0, 3…\n$ gender   &lt;int&gt; 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1…\n$ dobm     &lt;int&gt; 7, 1, 2, 4, 2, 8, 3, 5, 12, 11, 1, 12, 11, 3, 6, 6, 3, 5, 3, …\n$ doby     &lt;int&gt; 83, 58, 82, 78, 73, 75, 72, 85, 70, 70, 77, 77, 74, 68, 88, 8…\n$ status   &lt;int&gt; 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2…\n$ youngdep &lt;int&gt; 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2…\n$ elderdep &lt;int&gt; 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ qualif   &lt;int&gt; 5, 5, 4, 5, 4, 5, 4, 5, 2, 2, 2, 3, 2, 5, 2, 1, 1, 2, 3, 5, 2…\n$ exercise &lt;int&gt; 4, 2, 6, 3, 2, 6, 4, 3, 1, 3, 3, 2, 4, 1, 4, 3, 3, 4, 5, 2, 4…\n$ cigs     &lt;int&gt; 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 4…\n$ alcohol  &lt;dbl&gt; 1, 0, 0, 2, 3, 4, 0, 0, 1, 4, 0, 10, 40, 30, 10, 0, 15, 0, 15…\n$ time_rel &lt;int&gt; 2, 2, 1, 2, 2, 5, 1, 3, 1, 2, 3, 2, 2, 1, 2, 4, 2, 3, 1, 2, 3…\n$ when_rel &lt;int&gt; 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 4, 2, 1, 2, 1, 1…\n$ job1     &lt;int&gt; 5, 5, 1, 6, 1, 3, 5, 4, 6, 6, 5, 5, 4, 6, 4, 5, 5, 5, 4, 5, 5…\n$ job2     &lt;int&gt; 4, 7, 4, 4, 1, 4, 4, 1, 2, 6, 3, 2, 3, 2, 1, 2, 4, 6, 4, 5, 3…\n$ job3     &lt;int&gt; 5, 4, 7, 5, 5, 5, 6, 7, 4, 7, 1, 4, 3, 2, 4, 3, 1, 6, 3, 5, 6…\n$ job4     &lt;int&gt; 4, 3, 1, 5, 5, 3, 7, 7, 5, 6, 7, 7, 3, 6, 6, 6, 6, 6, 4, 3, 2…\n$ job5     &lt;int&gt; 6, 6, 1, 6, 1, 4, 5, 7, 4, 7, 6, 6, 6, 6, 6, 6, 4, 6, 4, 6, 5…\n$ job6     &lt;int&gt; 4, 2, 1, 4, 2, 3, 5, 1, 2, 7, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1…\n$ job7     &lt;int&gt; 7, 1, 7, 5, 2, 4, 4, 5, 4, 6, 5, 6, 5, 7, 4, 4, 7, 5, 6, 7, 7…\n$ job8     &lt;int&gt; 7, 4, 7, 5, 2, 3, 5, 5, 6, 7, 7, 6, 4, 6, 5, 4, 6, 6, 5, 3, 6…\n$ job9     &lt;int&gt; 4, 5, 3, 5, 5, 5, 6, 7, 2, 7, 3, 4, 6, 2, 1, 2, 1, 5, 2, 6, 3…\n$ job10    &lt;int&gt; 7, 6, 1, 6, 2, 4, 5, 7, 4, 7, 6, 6, 6, 6, 6, 4, 4, 5, 5, 6, 4…\n$ job11    &lt;int&gt; 4, 1, 7, 5, 3, 3, 6, 1, 5, 7, 3, 5, 5, 6, 6, 4, 5, 5, 6, 4, 6…\n$ job12    &lt;int&gt; 4, 5, 4, 3, 3, 3, 3, 1, 4, 7, 4, 2, 1, 4, 2, 2, 2, 2, 2, 2, 2…\n$ job13    &lt;int&gt; 1, 4, 1, 4, 4, 4, 4, 1, 5, 7, 3, 4, 2, 7, 1, 4, 2, 4, 4, 6, 4…\n$ job14    &lt;int&gt; 4, 5, 7, 2, 6, 1, 2, 1, 1, 7, 7, 6, 4, 6, 4, 4, 7, 4, 6, 6, 4…\n$ job15    &lt;int&gt; 4, 7, 3, 7, 2, 6, 6, 7, 5, 5, 4, 4, 7, 6, 3, 5, 5, 4, 4, 3, 5…\n$ job16    &lt;int&gt; 6, 5, 1, 2, 2, 3, 3, 7, 4, 5, 6, 6, 6, 6, 5, 4, 4, 6, 5, 6, 3…\n$ job17    &lt;int&gt; 4, 1, 1, 4, 2, 5, 3, 7, 3, 7, 2, 2, 3, 2, 1, 2, 2, 2, 1, 2, 3…\n$ job18    &lt;int&gt; 7, 7, 7, 6, 3, 5, 5, 3, 6, 2, 3, 5, 5, 6, 5, 5, 6, 6, 4, 5, 3…\n$ job19    &lt;int&gt; 7, 1, 7, 7, 2, 6, 7, 3, 6, 2, 1, 2, 2, 6, 1, 5, 5, 6, 2, 2, 2…\n$ job20    &lt;int&gt; 3, 5, 7, 5, 1, 3, 7, 6, 3, 3, 4, 3, 3, 4, 3, 6, 1, 5, 3, 6, 5…\n$ job21    &lt;int&gt; 4, 1, 7, 5, 1, 3, 3, 1, 5, 6, 3, 5, 5, 6, 4, 4, 3, 5, 4, 2, 5…\n$ job22    &lt;int&gt; 3, 1, 1, 2, 4, 1, 2, 1, 2, 6, 2, 5, 2, 2, 1, 6, 1, 5, 3, 5, 3…\n$ job23    &lt;int&gt; 5, 1, 2, 4, 6, 3, 6, 1, 5, 7, 2, 5, 6, 4, 4, 2, 4, 5, 5, 6, 6…\n$ job24    &lt;int&gt; 4, 4, 5, 4, 2, 4, 4, 4, 5, 6, 4, 6, 4, 2, 3, 6, 5, 6, 5, 4, 2…\n$ job25    &lt;int&gt; 4, 5, 6, 5, 3, 2, 3, 7, 5, 4, 5, 5, 5, 3, 4, 6, 4, 6, 4, 6, 3…\n$ job26    &lt;int&gt; 7, 6, 7, 3, 6, 5, 6, 7, 6, 4, 1, 5, 6, 6, 4, 6, 4, 4, 5, 4, 4…\n$ job27    &lt;int&gt; 2, 4, 1, 2, 6, 5, 1, 1, 3, 7, 6, 3, 1, 1, 1, 2, 2, 4, 6, 6, 2…\n$ job28    &lt;int&gt; 3, 6, 7, 5, 2, 5, 6, 1, 4, 7, 3, 3, 5, 1, 3, 4, 1, 3, 2, 6, 5…\n$ job29    &lt;int&gt; 3, 3, 5, 7, 5, 6, 5, 1, 3, 7, 4, 4, 3, 2, 2, 4, 3, 3, 5, 5, 3…\n$ job30    &lt;int&gt; 4, 3, 6, 5, 1, 2, 3, 6, 6, 4, 7, 6, 4, 4, 3, 4, 4, 6, 4, 2, 3…\n$ job31    &lt;int&gt; 5, 1, 5, 3, 4, 5, 4, 1, 5, 5, 5, 3, 4, 6, 5, 6, 6, 6, 5, 4, 2…\n$ job32    &lt;int&gt; 5, 7, 3, 2, 2, 1, 3, 2, 6, 5, 5, 5, 4, 7, 5, 5, 5, 6, 5, 5, 5…\n$ job33    &lt;int&gt; 6, 4, 6, 5, 4, 3, 7, 4, 6, 4, 6, 3, 4, 2, 3, 6, 6, 6, 4, 2, 2…\n$ job34    &lt;int&gt; 4, 6, 5, 7, 4, 5, 6, 1, 5, 5, 2, 2, 2, 2, 3, 5, 5, 5, 1, 2, 4…\n$ job35    &lt;int&gt; 4, 6, 3, 5, 4, 1, 3, 1, 5, 6, 5, 5, 4, 6, 5, 4, 5, 5, 7, 5, 6…\n$ job36    &lt;int&gt; 4, 1, 2, 3, 2, 5, 2, 1, 5, 6, 5, 3, 4, 6, 5, 6, 6, 5, 7, 5, 5…\n$ job37    &lt;int&gt; 4, 5, 1, 3, 2, 3, 3, 7, 3, 6, 7, 4, 6, 7, 4, 5, 6, 5, 6, 5, 5…\n$ job38    &lt;int&gt; 4, 5, 5, 2, 2, 1, 3, 1, 3, 7, 7, 5, 6, 7, 3, 6, 6, 5, 6, 5, 5…\n$ job39    &lt;int&gt; 5, 7, 5, 7, 2, 5, 6, 7, 6, 5, 4, 5, 6, 7, 3, 5, 5, 5, 2, 1, 4…\n$ job40    &lt;int&gt; 4, 3, 5, 3, 4, 3, 3, 7, 3, 6, 3, 5, 6, 7, 4, 4, 6, 6, 4, 6, 1…\n$ job41    &lt;int&gt; 6, 5, 3, 5, 1, 1, 3, 1, 5, 7, 3, 5, 5, 6, 4, 5, 7, 6, 5, 6, 5…\n$ job42    &lt;int&gt; 6, 4, 5, 4, 5, 3, 4, 1, 4, 5, 4, 6, 5, 7, 4, 5, 5, 6, 4, 3, 5…\n$ job43    &lt;int&gt; 6, 1, 7, 3, 4, 5, 1, 2, 3, 7, 2, 6, 4, 6, 5, 4, 7, 6, 2, 6, 4…\n$ job44    &lt;int&gt; 6, 4, 6, 5, 2, 3, 7, 1, 5, 7, 7, 6, 4, 5, 6, 5, 6, 6, 6, 5, 3…\n$ job45    &lt;int&gt; 6, 6, 7, 7, 6, 3, 6, 7, 6, 7, 5, 4, 6, 6, 5, 6, 4, 6, 4, 2, 6…\n$ job46    &lt;int&gt; 3, 1, 2, 3, 6, 6, 3, 7, 3, 5, 5, 2, 2, 2, 1, 2, 2, 5, 1, 1, 2…\n$ job47    &lt;int&gt; 3, 4, 5, 5, 1, 3, 3, 3, 3, 6, 5, 5, 4, 4, 3, 6, 5, 5, 4, 4, 3…\n$ job48    &lt;int&gt; 3, 7, 7, 7, 6, 4, 6, 7, 5, 6, 5, 3, 6, 3, 4, 5, 4, 5, 3, 1, 6…\n$ job49    &lt;int&gt; 6, 5, 5, 6, 2, 2, 5, 2, 6, 7, 5, 3, 6, 6, 7, 6, 7, 6, 6, 6, 7…\n$ job50    &lt;int&gt; 3, 5, 1, 3, 1, 7, 4, 1, 5, 6, 5, 4, 4, 5, 1, 2, 5, 3, 4, 5, 7…\n\n\n\n\nSanity Checks\nProduce a table of summary statistics for the variables in the data.\n\n\nCode\ndf %&gt;%\n    summarise(across(everything(), \n                     list(M = mean, SD = sd, MIN = min, MAX = max))) %&gt;%\n    pivot_longer(everything())\n\n\n# A tibble: 268 × 2\n   name          value\n   &lt;chr&gt;         &lt;dbl&gt;\n 1 job_yrs_M     4.03 \n 2 job_yrs_SD    5.86 \n 3 job_yrs_MIN  -1    \n 4 job_yrs_MAX  37    \n 5 job_mths_M   NA    \n 6 job_mths_SD  NA    \n 7 job_mths_MIN NA    \n 8 job_mths_MAX NA    \n 9 worktype_M    1.24 \n10 worktype_SD   0.427\n# ℹ 258 more rows\n\n\nWe can see that there are a few missing values in some variables.\nIf you were to analyse this data for a research project hopefully leading to a paper, you would probably want to perform sanity check on the variables, such as check if everyone is an adult (assuming this was a requirement for partaking of the study).\nCheck whether all participants in the study are adults.\n\n\nCode\nunique(df$doby)\n\n\n [1]   83   58   82   78   73   75   72   85   70   77   74   68   88   81   90\n[16]   59   89   87   80   86   84   67   79   55   71   56   57   62   61   91\n[31]   52   63   69   60   53   76   50   49   64   NA   54    0   66   35   65\n[46]   47   48   45   51   46 1979 1982 1980 1975 1969 1981 1973 1946   -1   43\n[61]   44   42 1971 1976 1983 1965 1985 1955 1950 1974 1984\n\n\nThe data look like a bit of a mess.. Some participants have the full year of birth, some only the last 2 digits. Let’s only extract the last 2 digits from all rows then.\nLet’s use the str_sub() function to take only the last 2 characters.\n\n\nCode\ndf &lt;- df %&gt;%\n    mutate(doby = str_sub(doby, -2, -1))\n\n\nIt seems like it’s now a character rather than a number:\n\n\nCode\nclass(df$doby)\n\n\n[1] \"character\"\n\n\nLet’s make it a number again:\n\n\nCode\ndf$doby &lt;- as.numeric(df$doby)\n\n\nVisualise the distribution of birth year. Do we notice anything strange?\n\n\nCode\nggplot(df, aes(x = doby)) + \n    geom_histogram(color = 'white')\n\n\n\n\n\n\n\n\n\nOr a dotplot if you prefer:\n\n\nCode\nggplot(df, aes(x = doby)) + \n    geom_dotplot(dotsize = 0.6, binwidth = 1, fill = 'dodgerblue', color = NA)\n\n\n\n\n\n\n\n\n\nA year of birth equal to −1 doesn’t make any sense and, since we only want to keep adults, we will remove the rows in the data set having a year of birth equal to -1.\nIn the meantime, we will also remove those participants who don’t have a value for doby.\n\n\nCode\ndf &lt;- df %&gt;%\n    filter(!is.na(doby) | doby &gt; 0)\n\nhist(df$doby)\n\n\n\n\n\n\n\n\n\nThat looks much better!\nNormally, you’d want to check other variables too. For now, because we are focusing on EFA, we’ll just assume that the other variables are okay.\n\n\nSubset\nRemember that the only variables we are interested in for our EFA are the job1 to job50 variables. We need to subset the data set to only include those variables.\n\n\nCode\ndf &lt;- df %&gt;%\n    select(job1:job50)\n\n\nCreate a table of descriptive summary statistics for each variable.\n\n\nCode\nlibrary(psych)\ndescribe(df)\n\n\n      vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\njob1     1 943 4.20 2.43      4    4.18 1.48  -1  55    56  9.62   201.28 0.08\njob2     2 943 3.76 2.00      4    3.71 2.97  -1   7     8  0.07    -1.29 0.06\njob3     3 943 4.33 2.05      5    4.41 2.97  -1   7     8 -0.24    -1.24 0.07\njob4     4 943 4.05 2.04      4    4.06 2.97  -1   7     8 -0.08    -1.29 0.07\njob5     5 943 4.77 2.11      5    4.89 1.48   0  36    36  3.05    49.63 0.07\njob6     6 943 3.38 2.00      3    3.25 2.97   1   7     6  0.34    -1.20 0.07\njob7     7 943 3.79 2.13      4    3.74 2.97  -1   7     8  0.04    -1.38 0.07\njob8     8 943 4.47 1.72      5    4.55 1.48  -1   7     8 -0.47    -0.66 0.06\njob9     9 943 4.06 1.99      4    4.08 2.97  -1   7     8 -0.06    -1.18 0.06\njob10   10 943 4.52 1.80      5    4.63 1.48   0   7     7 -0.44    -0.81 0.06\njob11   11 943 4.68 1.78      5    4.81 1.48   1   7     6 -0.60    -0.69 0.06\njob12   12 943 3.67 2.03      4    3.59 2.97   0   7     7  0.21    -1.25 0.07\njob13   13 943 3.76 1.94      4    3.71 2.97  -1   7     8  0.10    -1.15 0.06\njob14   14 943 4.77 2.45      5    4.88 1.48   1  52    51  7.34   144.84 0.08\njob15   15 943 3.91 1.91      4    3.89 2.97  -1  14    15  0.08    -0.34 0.06\njob16   16 943 4.11 1.77      4    4.17 1.48  -1   7     8 -0.23    -0.97 0.06\njob17   17 943 3.73 1.97      3    3.66 2.97   1   7     6  0.18    -1.27 0.06\njob18   18 943 4.20 1.79      5    4.26 1.48  -1   7     8 -0.29    -0.97 0.06\njob19   19 943 2.83 1.89      2    2.61 1.48  -1   7     8  0.73    -0.67 0.06\njob20   20 943 4.02 1.98      4    4.03 2.97  -1   7     8 -0.07    -1.15 0.06\njob21   21 943 4.35 1.66      5    4.42 1.48  -1   7     8 -0.40    -0.50 0.05\njob22   22 943 3.60 1.90      3    3.54 1.48  -1   7     8  0.24    -1.11 0.06\njob23   23 943 4.06 1.88      4    4.08 2.97  -1   7     8 -0.10    -1.15 0.06\njob24   24 943 3.83 1.69      4    3.86 1.48  -1   7     8 -0.11    -0.76 0.06\njob25   25 943 3.97 1.77      4    3.99 1.48  -1   7     8 -0.15    -0.65 0.06\njob26   26 943 4.13 1.81      4    4.17 2.97   0   7     7 -0.20    -1.07 0.06\njob27   27 943 3.84 2.22      4    3.80 2.97   1   7     6  0.11    -1.45 0.07\njob28   28 943 3.79 2.02      4    3.74 2.97   1   7     6  0.16    -1.26 0.07\njob29   29 943 4.55 1.76      5    4.63 1.48  -1   7     8 -0.38    -0.80 0.06\njob30   30 943 3.99 1.92      4    3.98 2.97   1   7     6 -0.10    -1.16 0.06\njob31   31 943 4.20 2.00      4    4.25 2.97  -1   7     8 -0.22    -1.21 0.07\njob32   32 943 5.14 1.47      5    5.29 1.48   0   7     7 -0.78     0.24 0.05\njob33   33 943 3.59 1.80      4    3.55 2.97  -1   7     8  0.13    -0.99 0.06\njob34   34 943 3.56 1.86      4    3.50 2.97   0   7     7  0.14    -1.13 0.06\njob35   35 943 4.89 1.51      5    5.02 1.48  -1   7     8 -0.84     0.51 0.05\njob36   36 943 4.52 1.81      5    4.63 1.48  -1   7     8 -0.46    -0.79 0.06\njob37   37 943 4.32 1.96      5    4.40 1.48   1   7     6 -0.33    -1.13 0.06\njob38   38 943 4.44 1.87      5    4.54 1.48  -1   7     8 -0.45    -0.93 0.06\njob39   39 943 3.83 1.85      4    3.83 1.48  -1   7     8 -0.07    -1.08 0.06\njob40   40 943 4.29 1.86      4    4.36 2.97  -1   7     8 -0.30    -0.96 0.06\njob41   41 943 4.73 1.55      5    4.84 1.48   1   7     6 -0.60    -0.17 0.05\njob42   42 943 4.20 1.59      5    4.27 1.48   1   8     7 -0.36    -0.72 0.05\njob43   43 943 3.52 1.96      3    3.43 2.97  -1   7     8  0.21    -1.16 0.06\njob44   44 943 3.75 1.88      4    3.71 2.97  -1   7     8  0.07    -1.06 0.06\njob45   45 943 4.50 1.77      5    4.60 1.48  -1   7     8 -0.50    -0.60 0.06\njob46   46 943 3.46 2.05      3    3.36 2.97  -1   7     8  0.23    -1.29 0.07\njob47   47 943 3.71 1.61      4    3.74 1.48  -1   7     8 -0.08    -0.81 0.05\njob48   48 943 4.38 1.94      5    4.48 1.48  -1   7     8 -0.36    -1.09 0.06\njob49   49 943 5.68 1.33      6    5.88 1.48  -1   7     8 -1.34     1.93 0.04\njob50   50 943 4.18 1.97      5    4.23 1.48  -1   7     8 -0.27    -1.17 0.06\n\n\nSome of the variables appear to have values of 0, −1, as well as values larger than 7, even though all the questionnaire items are on a 7-point Likert scale.\nGet rid of infeasible values:\n\n\nCode\ndf[df &lt; 1 | df &gt; 7] &lt;- NA\n\n\n\n\nDescriptives & Visualising\nLet’s now look at the score distributions per item and the correlations between pairs of items.\nSometimes easier to use subsets of ten variables each time. For example, look at the pairwise plots of the first 10 variables, then the next 10, and so on.\nThere’s a lot here to look at. the bottom triangle shows the scatterplots for pairs of variables. Because we have lots of data, and everything is likert data (1-7), the points themselves don’t tell us much, but we can eyeball the lines to look at the relationships. And we can look in the top triangle to see these too. It’s also good to take a look at the diagonals, which show the distributions of each item. This will help inform us about what type of correlations and factor extraction we might use.\n\n\nCode\npairs.panels(df[, 1:10])\n\n\n\n\n\n\n\n\n\nCode\npairs.panels(df[, 11:20])\n\n\n\n\n\n\n\n\n\nCode\npairs.panels(df[, 21:30])\n\n\n\n\n\n\n\n\n\nCode\npairs.panels(df[, 31:40])\n\n\n\n\n\n\n\n\n\nCode\npairs.panels(df[, 41:50])\n\n\n\n\n\n\n\n\n\nAs you can see, while some of the items have pretty much bell-shaped distributions, some others are massively skewed (looking at you job49) or close to uniform (job9). At this stage, you’d want to have a closer look at the wording of these troublesome items and see if you can spot any methodological issues that might account for these distributions. If the items look fine, you might want to consider alternative correlation coefficients (e.g., polychoric correlations) that might be more suitable to items with weird distributions. For now, let’s stick to Pearson’s correlation (r). Since we have NAs in the data, let’s just use complete observations.\nCompute the correlation matrix of the variables.\nInstead of looking at the \\(50 \\times 50\\) matrix of correlations, look at the distribution of correlation coefficients from the lower triangular part of the matrix.\nOption 1:\n\n\nCode\nR &lt;- cor(df, use = \"complete.obs\")\n\n\nOption 2:\n\n\nCode\nR &lt;- cor(na.omit(df))\n\n\n\n\nCode\nhist(R[lower.tri(R)])\n\n\n\n\n\n\n\n\n\nIf you want to be a little fancier, you can categorise the coefficients into negligible, weak, moderate, and strong correlations and plot a bar plot like this:\n\n\nCode\nRc &lt;- cut(abs(R), \n          breaks = c(0, .2, .5, .7, 1), \n          labels = c(\"negligible\", \"weak\", \"moderate\", \"strong\"))\n\nbarplot(table(Rc[lower.tri(Rc)]))\n\n\n\n\n\n\n\n\n\nAs we can see, most of the correlations are negligible and many are weak. There are some moderate and strong relationships in the data. This suggests that there might be multiple independent factors.\n\n\nSuitability for FA\nCheck if the correlations are sufficient for EFA with Bartlett’s test of sphericity and if the sample was adequate with KMO.\n\n\nCode\ncortest.bartlett(R, \n                 n = sum(complete.cases(df))) # we have 917 complete cases\n\n\n$chisq\n[1] 18702.7\n\n$p.value\n[1] 0\n\n$df\n[1] 1225\n\n\n\n\nCode\nKMO(R)\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = R)\nOverall MSA =  0.89\nMSA for each item = \n job1  job2  job3  job4  job5  job6  job7  job8  job9 job10 job11 job12 job13 \n 0.95  0.77  0.76  0.88  0.88  0.81  0.91  0.93  0.77  0.84  0.74  0.93  0.80 \njob14 job15 job16 job17 job18 job19 job20 job21 job22 job23 job24 job25 job26 \n 0.76  0.93  0.90  0.76  0.92  0.87  0.87  0.89  0.92  0.79  0.93  0.95  0.93 \njob27 job28 job29 job30 job31 job32 job33 job34 job35 job36 job37 job38 job39 \n 0.92  0.85  0.78  0.95  0.80  0.79  0.90  0.88  0.82  0.76  0.76  0.83  0.91 \njob40 job41 job42 job43 job44 job45 job46 job47 job48 job49 job50 \n 0.93  0.84  0.96  0.91  0.91  0.95  0.81  0.94  0.94  0.85  0.90 \n\n\nA significant Bartlett’s test of sphericity means that our correlation matrix is not proportional to an identity matrix (a matrix with only 1s on the diagonal and 0s everywhere else). This is exactly what we want, so we’re happy!\nLikewise, the sampling adequacy is pretty good. All items have a measure of sampling adequacy (MSA) in the \\(&gt;.7\\) “middling” region and the overall KMO is bordering on the \\(&gt;.9\\) “marvellous” level (I kid you not).\nGiven these results, we can merrily factor-analyse!\n\n\nNumber of Factors\nIn order to decide how many factors to use, look at the suggestions given by parallel analysis and MAP.\n\n\nCode\nfa.parallel(df, fa = 'fa')\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  11  and the number of components =  NA \n\n\n\n\nCode\nVSS(df)\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.68  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.78  with  5  factors\n\nThe Velicer MAP achieves a minimum of 0.01  with  7  factors \nBIC achieves a minimum of  -2604.78  with  8  factors\nSample Size adjusted BIC achieves a minimum of  104.3  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2    map  dof chisq     prob sqresid  fit RMSEA   BIC SABIC complex\n1 0.63 0.00 0.0149 1175 11124  0.0e+00      59 0.63 0.095  3076  6808     1.0\n2 0.68 0.71 0.0136 1126  9339  0.0e+00      47 0.71 0.088  1627  5203     1.1\n3 0.59 0.75 0.0119 1078  7703  0.0e+00      37 0.77 0.081   320  3743     1.4\n4 0.51 0.76 0.0103 1031  6309  0.0e+00      30 0.81 0.074  -753  2522     1.6\n5 0.51 0.78 0.0085  985  4953  0.0e+00      25 0.85 0.065 -1793  1335     1.7\n6 0.50 0.75 0.0086  940  4248  0.0e+00      22 0.86 0.061 -2191   795     1.9\n7 0.46 0.72 0.0084  896  3660  0.0e+00      20 0.87 0.057 -2477   369     2.0\n8 0.46 0.70 0.0086  853  3237 1.3e-273      19 0.88 0.054 -2605   104     2.0\n  eChisq  SRMR eCRMS  eBIC\n1  23561 0.101 0.103 15513\n2  16220 0.084 0.087  8508\n3  10627 0.068 0.072  3243\n4   7270 0.056 0.061   208\n5   4414 0.044 0.049 -2332\n6   3488 0.039 0.044 -2950\n7   2746 0.034 0.040 -3391\n8   2167 0.031 0.037 -3675\n\n\nSince parallel analysis (suggesting 11 factor) tends to overextract and MAP (suggesting 7 factors) can sometimes underextract, it is reasonable to look at solutions with 7-10 factors. However, looking at the scree plot, it might be reasonable to cast a glance on a 5- or 6-factor solution.\n\n\nPerform EFA\nFit a factor analysis model to the data using 10 factors. Since there is no good reason to expect the factors to be uncorrelated (orthogonal), let’s use the oblimin rotation.\n\n\nCode\nm_10f &lt;- fa(df, nfactors = 10, rotate = \"oblimin\", fm = \"ml\")\n\n\nPrint loadings sorted according to loadings\n\n\nCode\nfa.sort(m_10f)\n\n\nFactor Analysis using method =  ml\nCall: fa(r = df, nfactors = 10, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n        ML6   ML1   ML8   ML3   ML5   ML2   ML7   ML9   ML4  ML10   h2   u2 com\njob15  0.77  0.05  0.03  0.05 -0.03  0.00  0.01  0.02  0.00  0.01 0.67 0.33 1.0\njob39  0.67  0.03  0.01  0.00  0.08  0.13  0.01  0.06  0.00 -0.09 0.58 0.42 1.2\njob34  0.60  0.01  0.01 -0.02  0.20 -0.01 -0.02  0.08 -0.12  0.10 0.41 0.59 1.4\njob45  0.56  0.04  0.14  0.01 -0.07  0.11  0.03  0.08 -0.02  0.01 0.54 0.46 1.3\njob22 -0.51 -0.03  0.01  0.02  0.16  0.00  0.03  0.01 -0.14 -0.11 0.35 0.65 1.5\njob12 -0.44  0.02 -0.06  0.10  0.38  0.01 -0.09  0.07 -0.05  0.03 0.47 0.53 2.3\njob27 -0.41  0.09 -0.20 -0.19  0.23  0.02 -0.04  0.06 -0.06  0.00 0.33 0.67 2.9\njob48  0.37  0.06  0.03  0.02  0.01  0.11  0.17  0.00  0.10 -0.10 0.35 0.65 2.1\njob25  0.35  0.27  0.07  0.00 -0.07  0.00  0.15 -0.01  0.15 -0.17 0.53 0.47 3.5\njob26  0.33  0.30 -0.05 -0.03  0.11  0.32  0.02 -0.12  0.01 -0.01 0.49 0.51 3.6\njob18  0.02  0.73  0.04  0.02 -0.08 -0.01  0.06  0.04  0.02  0.10 0.64 0.36 1.1\njob8  -0.03  0.57  0.06  0.00 -0.11  0.05  0.07  0.12  0.07  0.13 0.49 0.51 1.4\njob42  0.17  0.51  0.08 -0.04 -0.07  0.12  0.03  0.04  0.09 -0.06 0.59 0.41 1.6\njob43 -0.11  0.49  0.09  0.10  0.01  0.05  0.11 -0.03  0.06 -0.17 0.37 0.63 1.7\njob19  0.05  0.49  0.20 -0.01  0.16 -0.09 -0.01 -0.17 -0.09  0.07 0.40 0.60 2.1\njob1   0.19  0.45  0.06 -0.05 -0.02 -0.05  0.14  0.08 -0.05 -0.25 0.50 0.50 2.4\njob47  0.19  0.40  0.03 -0.01  0.08  0.22  0.06 -0.13  0.13 -0.07 0.48 0.52 2.8\njob30  0.11  0.38  0.23 -0.03 -0.06 -0.02 -0.04  0.14 -0.07 -0.19 0.38 0.62 3.0\njob7  -0.01  0.33  0.15 -0.05  0.14  0.03 -0.03  0.03  0.04 -0.08 0.20 0.80 2.1\njob50 -0.05 -0.15 -0.12  0.13  0.10 -0.10 -0.07  0.13 -0.03 -0.06 0.16 0.84 6.9\njob33  0.06  0.08  0.76 -0.06  0.05  0.02 -0.04 -0.03 -0.01  0.06 0.65 0.35 1.1\njob44 -0.01  0.02  0.75  0.01  0.05  0.06  0.04  0.02  0.04  0.05 0.63 0.37 1.0\njob4  -0.03 -0.10  0.70  0.10 -0.05 -0.03  0.14  0.02  0.04 -0.05 0.57 0.43 1.2\njob24  0.05  0.19  0.52 -0.06  0.03  0.06  0.07 -0.05  0.07 -0.05 0.53 0.47 1.5\njob20 -0.02 -0.01 -0.43  0.02  0.12  0.04  0.03  0.06  0.03  0.20 0.26 0.74 1.7\njob3   0.01  0.05 -0.02  0.77  0.02  0.01  0.05 -0.04  0.02 -0.05 0.61 0.39 1.0\njob9   0.11 -0.03  0.03  0.71 -0.04 -0.05  0.09  0.02  0.01 -0.02 0.55 0.45 1.1\njob13 -0.10 -0.03  0.03  0.68  0.01 -0.04 -0.04  0.10 -0.03 -0.13 0.53 0.47 1.2\njob28  0.09 -0.01  0.00  0.60  0.14 -0.01 -0.07 -0.01 -0.03  0.13 0.44 0.56 1.3\njob2  -0.06  0.06 -0.05  0.60 -0.01  0.11 -0.08 -0.09 -0.01  0.23 0.40 0.60 1.5\njob17  0.02 -0.01 -0.02 -0.02  0.80 -0.07  0.01 -0.04  0.04 -0.01 0.63 0.37 1.0\njob46  0.08 -0.07  0.00  0.07  0.67  0.06  0.05 -0.03 -0.02  0.00 0.47 0.53 1.1\njob6  -0.10 -0.01  0.10  0.08  0.67 -0.03 -0.05  0.03  0.02  0.05 0.53 0.47 1.2\njob29  0.03  0.01  0.12  0.08  0.37 -0.06 -0.08  0.24 -0.15 -0.12 0.26 0.74 3.0\njob37  0.02 -0.05  0.02 -0.04  0.03  0.90  0.01 -0.01 -0.07  0.12 0.82 0.18 1.1\njob38 -0.01  0.03  0.03  0.06 -0.08  0.80  0.02  0.08  0.04 -0.18 0.75 0.25 1.2\njob40  0.15  0.14 -0.04  0.00 -0.12  0.37  0.03 -0.03  0.13  0.01 0.34 0.66 2.2\njob10 -0.08 -0.01  0.00  0.03  0.01  0.03  0.83  0.05 -0.01  0.04 0.67 0.33 1.0\njob5   0.02 -0.04  0.11 -0.02  0.02 -0.03  0.75 -0.02 -0.01  0.00 0.61 0.39 1.1\njob16  0.08  0.10 -0.09  0.01  0.00  0.01  0.64 -0.07  0.04 -0.02 0.47 0.53 1.2\njob35  0.03  0.03  0.00  0.00  0.01  0.00 -0.01  0.65  0.12  0.02 0.51 0.49 1.1\njob32  0.00  0.02 -0.02  0.01 -0.06  0.03  0.05  0.65  0.01 -0.03 0.45 0.55 1.0\njob41  0.09  0.01 -0.04 -0.04  0.00  0.09 -0.03  0.55  0.15  0.09 0.45 0.55 1.3\njob49  0.01  0.07  0.02  0.02 -0.18  0.08  0.05  0.38  0.13  0.18 0.33 0.67 2.4\njob21  0.25  0.06 -0.02  0.02  0.11  0.10  0.06  0.35 -0.09  0.14 0.30 0.70 3.0\njob14  0.00 -0.14 -0.03  0.09  0.16  0.05 -0.07  0.34  0.20 -0.15 0.27 0.73 3.4\njob36 -0.01 -0.03 -0.01  0.01  0.05 -0.02  0.01  0.09  0.84  0.03 0.74 0.26 1.0\njob31 -0.02  0.07  0.11 -0.04 -0.01 -0.06 -0.02 -0.05  0.63 -0.03 0.44 0.56 1.1\njob11  0.06  0.09  0.05 -0.05  0.02  0.01  0.06  0.24  0.02  0.50 0.37 0.63 1.6\njob23  0.00  0.23  0.12 -0.04  0.04 -0.07 -0.01  0.15 -0.02  0.29 0.19 0.81 3.2\n\n                       ML6  ML1  ML8  ML3  ML5  ML2  ML7  ML9  ML4 ML10\nSS loadings           3.69 3.42 2.89 2.49 2.31 2.21 2.19 2.03 1.58 0.92\nProportion Var        0.07 0.07 0.06 0.05 0.05 0.04 0.04 0.04 0.03 0.02\nCumulative Var        0.07 0.14 0.20 0.25 0.30 0.34 0.38 0.42 0.46 0.47\nProportion Explained  0.16 0.14 0.12 0.10 0.10 0.09 0.09 0.09 0.07 0.04\nCumulative Proportion 0.16 0.30 0.42 0.53 0.62 0.72 0.81 0.89 0.96 1.00\n\n With factor correlations of \n       ML6   ML1   ML8   ML3   ML5   ML2   ML7  ML9   ML4  ML10\nML6   1.00  0.47  0.32  0.00 -0.10  0.39  0.25 0.13  0.11 -0.04\nML1   0.47  1.00  0.47 -0.09 -0.11  0.27  0.34 0.06  0.20 -0.04\nML8   0.32  0.47  1.00  0.05  0.02  0.09  0.45 0.02  0.20 -0.09\nML3   0.00 -0.09  0.05  1.00  0.21  0.00  0.09 0.07 -0.02 -0.06\nML5  -0.10 -0.11  0.02  0.21  1.00 -0.13 -0.20 0.00 -0.17  0.10\nML2   0.39  0.27  0.09  0.00 -0.13  1.00  0.21 0.18  0.07  0.05\nML7   0.25  0.34  0.45  0.09 -0.20  0.21  1.00 0.06  0.23 -0.10\nML9   0.13  0.06  0.02  0.07  0.00  0.18  0.06 1.00  0.36  0.10\nML4   0.11  0.20  0.20 -0.02 -0.17  0.07  0.23 0.36  1.00 -0.06\nML10 -0.04 -0.04 -0.09 -0.06  0.10  0.05 -0.10 0.10 -0.06  1.00\n\nMean item complexity =  1.8\nTest of the hypothesis that 10 factors are sufficient.\n\ndf null model =  1225  with the objective function =  20.66 with Chi Square =  19096.72\ndf of  the model are 770  and the objective function was  2.45 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  940 with the empirical chi square  1420.85  with prob &lt;  3e-41 \nThe total n.obs was  943  with Likelihood Chi Square =  2249.02  with prob &lt;  1.2e-144 \n\nTucker Lewis Index of factoring reliability =  0.867\nRMSEA index =  0.045  and the 90 % confidence intervals are  0.043 0.047\nBIC =  -3024.76\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   ML6  ML1  ML8  ML3  ML5  ML2\nCorrelation of (regression) scores with factors   0.93 0.92 0.93 0.92 0.91 0.94\nMultiple R square of scores with factors          0.87 0.85 0.86 0.84 0.83 0.89\nMinimum correlation of possible factor scores     0.74 0.71 0.72 0.68 0.65 0.78\n                                                   ML7  ML9  ML4 ML10\nCorrelation of (regression) scores with factors   0.91 0.88 0.90 0.79\nMultiple R square of scores with factors          0.84 0.78 0.81 0.62\nMinimum correlation of possible factor scores     0.67 0.56 0.61 0.24\n\n\nOK, 10 factors looks like way too many as the last 2 have very few substantive loadings (&gt;.33). Let’s look at a smaller solution, e.g. 9 or 8 factors and see if it’s still the case…\n\n\nCode\nm_9f &lt;- fa(df, nfactors = 9, rotate = \"oblimin\", fm = \"ml\")\nm_8f &lt;- fa(df, nfactors = 8, rotate = \"oblimin\", fm = \"ml\")\n\n\nIt was the case and even the 9-factor solution has only 1 substantive loading on the last factor. These are however quite large so let’s look at this solution a little closer. We can see that a few items don’t have any loadings larger than our .33 cut-off: We can see that a few items don’t have any loadings larger than our .33 cut-off:\n\n\nCode\n# get loadings\nx &lt;- m_9f$loadings\nwhich(rowSums(x &lt; .33) == 9)\n\n\njob20 job21 job22 job23 job26 job27 job50 \n   20    21    22    23    26    27    50 \n\n\nIt was the case and even the 8-factor solution has only 2 substantive loadings on the last factor. These are however quite large so let’s look at this solution a little closer. We can see that a few items don’t have any loadings larger than our .33 cut-off:\n\n\nCode\n# get loadings\nx &lt;- m_8f$loadings\nwhich(rowSums(x &lt; .33) == 8)\n\n\njob11 job20 job21 job22 job23 job27 job31 job40 job50 \n   11    20    21    22    23    27    31    40    50 \n\n\nHere is when we would go back to the item wordings and try to see why these items might not really correlate with any other items. For instance, job11 (“I regularly discuss problems at work with my colleagues.”) might be ambiguous: does it mean that there are often problems or that if there are problems, I discuss them regularly?\nFor argument’s sake, let’s say, all of these identified items are deemed problematic so we should remove them:\n\n\nCode\ncols_to_remove &lt;- names(which(rowSums(x &lt; .33) == 8))\ndf2 &lt;- df[ , !names(df) %in% cols_to_remove]\n\n\nCheck parallel analysis and MAP again.\n\n\nCode\nfa.parallel(df2, fa = 'fa')\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  9  and the number of components =  NA \n\n\n\n\nCode\nVSS(df2)\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.69  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.81  with  5  factors\n\nThe Velicer MAP achieves a minimum of 0.01  with  8  factors \nBIC achieves a minimum of  -1789.95  with  8  factors\nSample Size adjusted BIC achieves a minimum of  -138.46  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq     prob sqresid  fit RMSEA   BIC SABIC complex\n1 0.63 0.00 0.018 779  9008  0.0e+00      47 0.63 0.106  3673  6147     1.0\n2 0.69 0.72 0.016 739  7295  0.0e+00      35 0.72 0.097  2234  4581     1.1\n3 0.67 0.76 0.014 700  5966  0.0e+00      27 0.78 0.089  1171  3394     1.3\n4 0.51 0.78 0.013 662  4769  0.0e+00      22 0.83 0.081   235  2337     1.6\n5 0.55 0.81 0.010 625  3556  0.0e+00      17 0.86 0.071  -725  1260     1.6\n6 0.54 0.80 0.010 589  2973 1.2e-313      16 0.88 0.066 -1061   810     1.6\n7 0.51 0.76 0.010 554  2329 1.5e-215      14 0.89 0.058 -1465   294     1.7\n8 0.47 0.75 0.010 520  1772 4.5e-136      13 0.90 0.051 -1790  -138     1.7\n  eChisq  SRMR eCRMS  eBIC\n1  18558 0.110 0.112 13222\n2  11704 0.087 0.092  6642\n3   7606 0.070 0.076  2811\n4   4968 0.057 0.063   434\n5   2766 0.042 0.048 -1515\n6   2056 0.036 0.043 -1978\n7   1459 0.031 0.037 -2335\n8   1035 0.026 0.032 -2527\n\n\nFit an 8-factor model to the new dataset.\n\n\nCode\nm_8f &lt;- fa(df2, nfactors = 8, rotate = \"oblimin\", fm = \"ml\")\nfa.sort(m_8f)\n\n\nFactor Analysis using method =  ml\nCall: fa(r = df2, nfactors = 8, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n        ML2   ML5   ML8   ML3   ML6   ML4   ML7   ML1   h2    u2 com\njob18  0.76 -0.03  0.03  0.00  0.04 -0.03  0.03  0.03 0.61 0.386 1.0\njob8   0.59 -0.06  0.05 -0.02  0.17 -0.06  0.04  0.06 0.46 0.539 1.3\njob42  0.56  0.17  0.07 -0.03  0.09 -0.09  0.02  0.06 0.59 0.407 1.4\njob43  0.51 -0.08  0.09  0.12 -0.04 -0.01  0.12 -0.02 0.34 0.656 1.3\njob19  0.47  0.05  0.19 -0.02 -0.25  0.16 -0.05 -0.04 0.39 0.609 2.3\njob47  0.47  0.19  0.03  0.00 -0.04  0.04  0.07  0.12 0.45 0.550 1.6\njob1   0.47  0.22  0.04 -0.01  0.00 -0.04  0.13 -0.09 0.46 0.545 1.7\njob30  0.38  0.14  0.19 -0.01  0.04 -0.07 -0.05 -0.05 0.34 0.663 2.0\njob7   0.37 -0.01  0.12 -0.04  0.03  0.15 -0.01  0.01 0.19 0.805 1.6\njob26  0.35  0.32 -0.07 -0.02 -0.08  0.08  0.02  0.27 0.47 0.530 3.2\njob15  0.03  0.78  0.04  0.04  0.01 -0.06 -0.01  0.00 0.67 0.330 1.0\njob39  0.01  0.73  0.01  0.01  0.04  0.03  0.01  0.07 0.60 0.399 1.0\njob34 -0.05  0.62  0.00 -0.03  0.02  0.20 -0.05  0.02 0.39 0.614 1.2\njob45  0.05  0.57  0.12  0.01  0.07 -0.08  0.03  0.09 0.53 0.472 1.2\njob12 -0.03 -0.39 -0.04  0.11  0.03  0.37 -0.11  0.02 0.42 0.582 2.4\njob48  0.12  0.38  0.00  0.03  0.06 -0.02  0.19  0.04 0.34 0.662 1.8\njob25  0.31  0.35  0.08  0.01  0.06 -0.11  0.16 -0.07 0.51 0.491 3.0\njob44  0.01  0.00  0.80  0.00  0.04  0.03  0.00  0.05 0.67 0.328 1.0\njob4  -0.10 -0.01  0.73  0.10  0.01 -0.08  0.13 -0.04 0.58 0.424 1.2\njob33  0.10  0.06  0.72 -0.07 -0.05  0.05 -0.04  0.02 0.61 0.388 1.1\njob24  0.24  0.06  0.50 -0.05 -0.03  0.00  0.08  0.00 0.52 0.480 1.6\njob3   0.07  0.00 -0.03  0.78 -0.05  0.01  0.05  0.00 0.61 0.390 1.0\njob9  -0.02  0.09  0.03  0.71  0.03 -0.03  0.09 -0.04 0.53 0.465 1.1\njob13 -0.06 -0.05  0.03  0.70  0.06 -0.01 -0.04 -0.07 0.52 0.484 1.1\njob28 -0.03  0.08  0.00  0.58 -0.02  0.16 -0.08  0.03 0.42 0.584 1.2\njob2   0.03 -0.08 -0.02  0.57 -0.06  0.00 -0.11  0.15 0.34 0.658 1.3\njob35  0.02  0.04  0.01  0.00  0.72  0.05 -0.02 -0.01 0.52 0.480 1.0\njob41  0.01  0.08 -0.01 -0.05  0.65  0.03 -0.04  0.08 0.46 0.535 1.1\njob32  0.01  0.00 -0.03  0.02  0.63 -0.02  0.03  0.03 0.41 0.592 1.0\njob36  0.10 -0.07  0.07 -0.02  0.51 -0.03  0.10 -0.12 0.31 0.688 1.4\njob49  0.08 -0.02  0.00  0.00  0.45 -0.13  0.05  0.08 0.28 0.724 1.3\njob14 -0.12  0.02 -0.01  0.10  0.44  0.12 -0.04 -0.02 0.23 0.766 1.5\njob17  0.01  0.03 -0.04 -0.04 -0.01  0.82  0.04 -0.07 0.65 0.351 1.0\njob6   0.01 -0.11  0.08  0.06  0.06  0.71 -0.04 -0.02 0.56 0.443 1.1\njob46 -0.06  0.09 -0.01  0.06 -0.04  0.66  0.05  0.08 0.46 0.535 1.1\njob29 -0.01  0.06  0.09  0.10  0.13  0.39 -0.09 -0.04 0.22 0.784 1.7\njob10  0.00 -0.09  0.01  0.02  0.05  0.03  0.81  0.05 0.66 0.342 1.0\njob5  -0.05  0.03  0.12 -0.02 -0.04  0.02  0.74 -0.02 0.61 0.390 1.1\njob16  0.11  0.09 -0.08  0.01 -0.05 -0.01  0.64  0.00 0.47 0.528 1.1\njob37 -0.03  0.00  0.02 -0.03 -0.01  0.01  0.01  1.01 1.00 0.005 1.0\njob38  0.11  0.07 -0.01  0.10  0.11 -0.13  0.06  0.63 0.59 0.407 1.3\n\n                       ML2  ML5  ML8  ML3  ML6  ML4  ML7  ML1\nSS loadings           3.57 3.17 2.53 2.41 2.23 2.21 2.11 1.76\nProportion Var        0.09 0.08 0.06 0.06 0.05 0.05 0.05 0.04\nCumulative Var        0.09 0.16 0.23 0.28 0.34 0.39 0.44 0.49\nProportion Explained  0.18 0.16 0.13 0.12 0.11 0.11 0.11 0.09\nCumulative Proportion 0.18 0.34 0.46 0.58 0.70 0.81 0.91 1.00\n\n With factor correlations of \n      ML2   ML5  ML8   ML3   ML6   ML4   ML7   ML1\nML2  1.00  0.51 0.50 -0.08  0.12 -0.16  0.37  0.24\nML5  0.51  1.00 0.32  0.01  0.15 -0.09  0.24  0.36\nML8  0.50  0.32 1.00  0.07  0.07  0.03  0.46  0.06\nML3 -0.08  0.01 0.07  1.00  0.06  0.22  0.10 -0.02\nML6  0.12  0.15 0.07  0.06  1.00 -0.10  0.11  0.16\nML4 -0.16 -0.09 0.03  0.22 -0.10  1.00 -0.22 -0.10\nML7  0.37  0.24 0.46  0.10  0.11 -0.22  1.00  0.14\nML1  0.24  0.36 0.06 -0.02  0.16 -0.10  0.14  1.00\n\nMean item complexity =  1.4\nTest of the hypothesis that 8 factors are sufficient.\n\ndf null model =  820  with the objective function =  17.24 with Chi Square =  15994.42\ndf of  the model are 520  and the objective function was  1.89 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  940 with the empirical chi square  1071.5  with prob &lt;  1.8e-40 \nThe total n.obs was  943  with Likelihood Chi Square =  1744.49  with prob &lt;  6.3e-132 \n\nTucker Lewis Index of factoring reliability =  0.872\nRMSEA index =  0.05  and the 90 % confidence intervals are  0.047 0.053\nBIC =  -1817.03\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   ML2  ML5  ML8  ML3  ML6  ML4\nCorrelation of (regression) scores with factors   0.93 0.93 0.93 0.91 0.89 0.91\nMultiple R square of scores with factors          0.86 0.86 0.86 0.83 0.79 0.83\nMinimum correlation of possible factor scores     0.73 0.73 0.71 0.67 0.59 0.66\n                                                   ML7  ML1\nCorrelation of (regression) scores with factors   0.91 1.00\nMultiple R square of scores with factors          0.83 0.99\nMinimum correlation of possible factor scores     0.66 0.99\n\n\nWe still only get 2 substantive loadings on the last factor, while we want at least 3. This also happens with the 7- and 6-factor solutions. But once we hit the 5-factor solution, we find a better structure:\n\n\nCode\nm_5f &lt;- fa(df2, nfactors = 5, rotate = \"oblimin\", fm = \"ml\")\nfa.sort(m_5f)\n\n\nFactor Analysis using method =  ml\nCall: fa(r = df2, nfactors = 5, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n        ML1   ML5   ML4   ML2   ML3   h2   u2 com\njob39  0.74 -0.03  0.04  0.04  0.04 0.54 0.46 1.0\njob15  0.73  0.03 -0.03  0.06  0.00 0.56 0.44 1.0\njob26  0.67  0.04  0.03 -0.03 -0.06 0.46 0.54 1.0\njob45  0.62  0.11 -0.06  0.03  0.07 0.50 0.50 1.1\njob34  0.60 -0.09  0.23  0.00  0.01 0.34 0.66 1.3\njob37  0.55 -0.19 -0.07  0.01  0.10 0.29 0.71 1.3\njob38  0.50 -0.06 -0.19  0.10  0.17 0.37 0.63 1.7\njob47  0.49  0.28 -0.01 -0.03 -0.03 0.43 0.57 1.6\njob42  0.45  0.35 -0.11 -0.09  0.10 0.54 0.46 2.2\njob48  0.45  0.16 -0.08  0.06  0.07 0.33 0.67 1.4\njob25  0.41  0.35 -0.15  0.01  0.06 0.49 0.51 2.3\njob44 -0.01  0.74  0.13  0.00  0.05 0.54 0.46 1.1\njob4  -0.15  0.72 -0.01  0.14  0.02 0.48 0.52 1.2\njob33  0.07  0.71  0.15 -0.08 -0.03 0.54 0.46 1.1\njob24  0.12  0.66  0.03 -0.06 -0.02 0.52 0.48 1.1\njob5  -0.03  0.54 -0.20  0.13 -0.01 0.35 0.65 1.4\njob10 -0.07  0.49 -0.23  0.17  0.08 0.32 0.68 1.8\njob43  0.13  0.45 -0.07  0.07 -0.03 0.28 0.72 1.3\njob18  0.31  0.44 -0.08 -0.09  0.05 0.45 0.55 2.0\njob19  0.22  0.42  0.18 -0.08 -0.24 0.34 0.66 2.7\njob16  0.11  0.39 -0.24  0.13 -0.02 0.29 0.71 2.1\njob1   0.35  0.39 -0.09 -0.04 -0.01 0.41 0.59 2.1\njob30  0.25  0.38 -0.03 -0.07  0.04 0.31 0.69 1.8\njob8   0.23  0.37 -0.10 -0.09  0.18 0.37 0.63 2.6\njob7   0.14  0.32  0.14 -0.09  0.04 0.17 0.83 2.1\njob17  0.00  0.03  0.77  0.00 -0.02 0.58 0.42 1.0\njob6  -0.10  0.09  0.72  0.07  0.05 0.55 0.45 1.1\njob46  0.13 -0.01  0.63  0.11 -0.03 0.43 0.57 1.1\njob29  0.02  0.05  0.44  0.09  0.12 0.22 0.78 1.3\njob12 -0.36 -0.10  0.39  0.09  0.03 0.39 0.61 2.3\njob3   0.04  0.05  0.00  0.77 -0.05 0.60 0.40 1.0\njob9   0.06  0.08 -0.03  0.72  0.02 0.54 0.46 1.0\njob13 -0.10 -0.01  0.04  0.68  0.05 0.49 0.51 1.1\njob28  0.10 -0.08  0.20  0.58 -0.02 0.41 0.59 1.4\njob2   0.07 -0.13  0.03  0.54 -0.05 0.31 0.69 1.2\njob35  0.02  0.00  0.07 -0.01  0.72 0.52 0.48 1.0\njob41  0.12 -0.06  0.04 -0.06  0.67 0.47 0.53 1.1\njob32  0.00 -0.02 -0.02  0.02  0.64 0.41 0.59 1.0\njob36 -0.14  0.22 -0.05 -0.02  0.51 0.30 0.70 1.6\njob49  0.05  0.05 -0.15 -0.01  0.46 0.27 0.73 1.3\njob14 -0.06 -0.09  0.15  0.11  0.44 0.23 0.77 1.5\n\n                       ML1  ML5  ML4  ML2  ML3\nSS loadings           4.91 4.79 2.47 2.45 2.31\nProportion Var        0.12 0.12 0.06 0.06 0.06\nCumulative Var        0.12 0.24 0.30 0.36 0.41\nProportion Explained  0.29 0.28 0.15 0.14 0.14\nCumulative Proportion 0.29 0.57 0.72 0.86 1.00\n\n With factor correlations of \n      ML1   ML5   ML4   ML2   ML3\nML1  1.00  0.44 -0.15 -0.03  0.20\nML5  0.44  1.00 -0.11  0.04  0.11\nML4 -0.15 -0.11  1.00  0.19 -0.11\nML2 -0.03  0.04  0.19  1.00  0.06\nML3  0.20  0.11 -0.11  0.06  1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 5 factors are sufficient.\n\ndf null model =  820  with the objective function =  17.24 with Chi Square =  15994.42\ndf of  the model are 625  and the objective function was  3.82 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  940 with the empirical chi square  2821.86  with prob &lt;  2.2e-275 \nThe total n.obs was  943  with Likelihood Chi Square =  3530.88  with prob &lt;  0 \n\nTucker Lewis Index of factoring reliability =  0.748\nRMSEA index =  0.07  and the 90 % confidence intervals are  0.068 0.073\nBIC =  -749.79\nFit based upon off diagonal values = 0.96\nMeasures of factor score adequacy             \n                                                   ML1  ML5  ML4  ML2  ML3\nCorrelation of (regression) scores with factors   0.94 0.94 0.91 0.91 0.89\nMultiple R square of scores with factors          0.89 0.89 0.82 0.83 0.80\nMinimum correlation of possible factor scores     0.78 0.77 0.64 0.66 0.59\n\n\nAlso notice that the 8-factor solution accounted for 49% of the variance and the 5-factor one explains 41%. That is not a huge drop considering that, by choosing the 5-factor over the 8-factor solution, we reduce the dimensionality of the data (number of variables we have to deal with) by further 3 dimensions!\nLooking at the loadings, we can see that only 4 items have substantial cross-loadings (on exactly 2 factors), which is not terrible.\nGlance at the factor correlations of this final model, we see that only factor 1 and 5 are weakly-to-moderately correlated, which is not too bad! It allows us to claim that the factors (except for one) are largely independent of each other. This model accounts for about 41% of the variance.\nAt this stage, we would go to the individual items, look at which factors load on which items, and try to figure out what is the common theme linking these items. For instance, let’s look at the factor ML5. I would start by looking at the items with the highest loadings, i.e., items 44, 4, 33, and 24. They all have three things in common: they address fairness, openness, and promotions/pay rises. Since we have multiple themes going on here, let’s look at the items with loadings in the .4-.6 range. A stronger theme of fair acknowledgement of performance emerges. Not all of the lower-loading items chime with this theme terribly well, but those that do not tend to have cross-loadings with other factor. I would therefore be reasonable confident that the factor taps into something that could be called “Fair recognition” (apparently this is referred to in the OrgPsych jargon as “Procedural justice”)."
  },
  {
    "objectID": "explainer_pcavar.html",
    "href": "explainer_pcavar.html",
    "title": "PCA and unequal variances",
    "section": "",
    "text": "We’re including this code if you want to create some data and play around with it yourself, but do not worry about understanding it! In brief, what it does is 1) create a covariance matrix 2) generate data based on the covariance matrix and 3) rename the columns to “item1”, “item2”.. etc.\n\n\nCode\nlibrary(tidyverse)\nset.seed(777)\nnitem &lt;- 5  \nA &lt;- matrix(runif(nitem^2)*2-1, ncol=nitem) \nscor &lt;- t(A) %*% A\ndf &lt;- MASS::mvrnorm(n=200,mu=rep(0,5),Sigma = scor) %&gt;% as_tibble()\nnames(df)&lt;-paste0(\"item\",1:5)\n\n\nThe data we created has 5 items, all on similar scales:\n\n\nCode\nlibrary(psych)\nlibrary(knitr)\nkable(describe(df)[,c(3:4)])\n\n\n\n\n\n\nmean\nsd\n\n\n\n\nitem1\n0.054\n1.126\n\n\nitem2\n-0.098\n1.626\n\n\nitem3\n0.098\n0.957\n\n\nitem4\n-0.179\n1.180\n\n\nitem5\n-0.071\n1.141"
  },
  {
    "objectID": "explainer_pcavar.html#pca-on-the-covariance-matrix",
    "href": "explainer_pcavar.html#pca-on-the-covariance-matrix",
    "title": "PCA and unequal variances",
    "section": "PCA on the covariance matrix",
    "text": "PCA on the covariance matrix\nIf we use the covariance matrix, we get slightly different results, because the loadings are proportional to the scale of the variance for each item.\n\n\nCode\nprincipal(cov(df), nfactors = 1, covar = TRUE)$loadings\n\n\n\nLoadings:\n      PC1   \nitem1 -0.796\nitem2  0.772\nitem3 -0.874\nitem4  0.860\nitem5  0.898\n\n                 PC1\nSS loadings    3.540\nProportion Var 0.708\n\n\n\n\n\n\n\n\nvariance of item\nloadings cor PCA\nloadings cov PCA\n\n\n\n\nitem1\n1.268\n-0.861\n-0.796\n\n\nitem2\n2.643\n0.222\n0.772\n\n\nitem3\n0.915\n-0.834\n-0.874\n\n\nitem4\n1.392\n0.765\n0.860\n\n\nitem5\n1.302\n0.863\n0.898\n\n\n\n\n\n\n\nThis means that if the items are measured on very different scales, using the covariance matrix will lead to the components being dominated by the items with the largest variance.\nLet’s make another dataset in which item2 is just measured on a completely different scale\n\n\nCode\ndfb &lt;- df %&gt;% mutate(item2 = item2*20)\nkable(describe(dfb)[,c(3:4)])\n\n\n\n\n\n\nmean\nsd\n\n\n\n\nitem1\n0.054\n1.126\n\n\nitem2\n-1.964\n32.515\n\n\nitem3\n0.098\n0.957\n\n\nitem4\n-0.179\n1.180\n\n\nitem5\n-0.071\n1.141\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariance of item\nloadings cor PCA\nloadings cov PCA\n\n\n\n\nitem1\n1.268\n-0.861\n0.288\n\n\nitem2\n1057.242\n0.222\n32.515\n\n\nitem3\n0.915\n-0.834\n-0.593\n\n\nitem4\n1.392\n0.765\n0.091\n\n\nitem5\n1.302\n0.863\n0.064"
  },
  {
    "objectID": "explainer_pcavar.html#use-of-covar..",
    "href": "explainer_pcavar.html#use-of-covar..",
    "title": "PCA and unequal variances",
    "section": "Use of covar=..",
    "text": "Use of covar=..\nThe covar=TRUE/FALSE argument of principal() only makes a difference if you give the function a covariance matrix.\nIf you give the principal() function the raw data, then it will automatically conduct the PCA on the correlation matrix regardless of whether you put covar=TRUE or covar=FALSE\n\n\n\n\n\nprincipal(dfb, nfactors = 1, covar = FALSE)\ndfb, nfactors = 1, covar = TRUE)\nprincipal(cor(dfb), nfactors = 1)\nprincipal(cov(dfb), nfactors = 1, covar = FALSE)\nprincipal(cov(dfb), nfactors = 1, covar = TRUE)\n\n\n\n\n-0.861\n-0.861\n-0.861\n-0.861\n0.288\n\n\n0.222\n0.222\n0.222\n0.222\n32.515\n\n\n-0.834\n-0.834\n-0.834\n-0.834\n-0.593\n\n\n0.765\n0.765\n0.765\n0.765\n0.091\n\n\n0.863\n0.863\n0.863\n0.863\n0.064"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DAPR3",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 3 (DAPR3) lab workbook. Using the menu above, you can find lab materials for each week. These include sets of exercises along with walkthrough readings in which we introduce some of the more important R code. It is strongly recommended that students have taken Data Analysis for Psychology in R 1 and 2 (DAPR1 & DAPR2)."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Welcome to DAPR3",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Welcome to DAPR3",
    "section": "Tips on googling statistics and R",
    "text": "Tips on googling statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Welcome to DAPR3",
    "section": "Feedback on labs",
    "text": "Feedback on labs\nIf you wish to make suggestions for improvements to these workbooks, please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin toss. For a fair coin, the chance of getting a heads/tails for any given toss is 0.5.\nWe can simulate the number of “heads” in a single fair coin toss with the following code (because it is a single toss, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 1\n\n\nWe can simulate the number of “heads” in 8 fair coin tosses with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 3\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin tosses? Answer: 4! Doing another 8 tosses:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 6\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 3\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin tosses, but to do 1000 sets of 8 tosses:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  2  36  97 214 287 227 108  21   8"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nWe can get to the probability of observing \\(k\\) heads in 8 tosses of a fair coin using dbinom().\nLet’s calculate the probability of observing 2 heads in 8 tosses.\nAs coin tosses are independent, we can calculate probability using the product rule (“\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent). So the probability of observing 2 heads in 2 tosses is \\(0.5 \\cdot 0.5 = 0.25\\):\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 tosses, those two heads could occur in various ways:\n\n\n# A tibble: 11 × 1\n   `Ways to get 2 heads in 8 tosses`\n   &lt;chr&gt;                            \n 1 THTTTHTT                         \n 2 TTTTHHTT                         \n 3 TTHHTTTT                         \n 4 THTTTTHT                         \n 5 TTHTTTTH                         \n 6 TTTHHTTT                         \n 7 TTTHTTHT                         \n 8 TTTTHTTH                         \n 9 THHTTTTT                         \n10 THTHTTTT                         \n11 ...                              \n\n\nIn fact there are 28 different ways this could happen:\n\ndim(combn(8, 2))\n\n[1]  2 28\n\n\nThe probability of getting 2 heads in 8 tosses of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin tosses (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given toss)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin tosses is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nFor likelihood, we are interested in hypotheses about our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?).\nTo consider these hypotheses, we need to observe some data, and so we need to have a given number of tosses, and a given number of heads. Whereas above we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given toss, for the likelihood we fix the number of heads observed, and can make statements about different possible parameters that might govern the coins behaviour.\nFor example, if we did observe 2 heads in 8 tosses, what is the likelihood of this data given various parameters?\nOur parameter can take any real number between from 0 to 1, but let’s do it for a selection:\n\nposs_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, poss_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin tosses, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). You can visualise this as below:\n\n\n\n\n\n\n\n\n\nFormalizing the intuition We have a stochastic process that takes discrete values (i.e. outcomes of tossing a coin 10 times). We calculated the probability of observing a particular set of outcomes (8 correct predictions) by making assumptions about the underlying stochastic process, that is, the probability that our test subject can correctly predict the outcome of the coin toss is (p) (e.g. 0.8). We also assumed implicitly that the coin tosses are independent."
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\). Given that we know that if we have a specific value for \\(\\theta\\), then \\(P(d | \\theta)\\) gives us the probability of observing \\(d\\), it follows that we would like to figure out what values of \\(\\theta\\) maximise \\(\\mathcal{L}(\\theta \\vert d) = P(d \\vert \\theta)\\), where \\(\\mathcal{L}(\\theta \\vert d)\\) is the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. In Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎"
  }
]