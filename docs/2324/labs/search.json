[
  {
    "objectID": "01_regressionrefresh.html",
    "href": "01_regressionrefresh.html",
    "title": "1. Regression Refresh | Clustered Data",
    "section": "",
    "text": "Preliminaries\n\nOpen Rstudio!\n\nCreate a new RMarkdown document or R script (whichever you like) for this week.\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already.\n\n\ntidyverse : for organising data\npatchwork: for organising plots\nICC : for quickly calculating intraclass correlation coefficient\nlme4 : for fitting generalised linear mixed effects models\nparameters : inference!\npbkrTest : more inference!\nHLMdiag : for examining case diagnostics at multiple levels\nlmeresampler : for bootstrapping!\neffects : for tables/plots\nsjPlot : for tables/plots\nbroom.mixed : tidying methods for mixed models\n\nYou can install all of these at once using:\n\ninstall.packages(c(\"tidyverse\",\"ICC\",\"lme4\",\"parameters\",\"pbkrTest\",\"effects\",\"broom.mixed\",\"sjPlot\",\"HLMdiag\"))\n# the lmeresampler package has had some recent updates. better to install the most recent version:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"aloy/lmeresampler\")"
  },
  {
    "objectID": "01_regressionrefresh.html#footnotes",
    "href": "01_regressionrefresh.html#footnotes",
    "title": "1. Regression Refresh | Clustered Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nANOVA is just a special case of the linear model↩︎"
  },
  {
    "objectID": "02_intromlm.html",
    "href": "02_intromlm.html",
    "title": "2. Intro to Multilevel Models",
    "section": "",
    "text": "A Note on terminology\n\n\n\n\n\nThe methods we’re going to learn about in the first five weeks of this course are known by lots of different names: “multilevel models”; “hierarchical linear models”; “mixed-effect models”; “mixed models”; “nested data models”; “random coefficient models”; “random-effects models”; “random parameter models”… and so on).\nWhat the idea boils down to is that model parameters vary at more than one level. This week, we’re going to explore what that means.\nThroughout this course, we will tend to use the terms “mixed effect model”, “linear mixed model (LMM)” and “multilevel model (MLM)” interchangeably."
  },
  {
    "objectID": "02_intromlm.html#footnotes",
    "href": "02_intromlm.html#footnotes",
    "title": "2. Intro to Multilevel Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“(g)lmer” here stands for “(generalised) linear mixed effects regression”.↩︎"
  },
  {
    "objectID": "03_assumptcent.html",
    "href": "03_assumptcent.html",
    "title": "3. Assumptions and Diagnostics | Centering",
    "section": "",
    "text": "Centering & Scaling in LM\n\n\n\n\n\nWe have some data from a study investigating how perceived persuasiveness of a speaker is influenced by the rate at which they speak.\n\ndap2 &lt;- read_csv(\"https://uoepsy.github.io/data/dapr2_2122_report1.csv\")\n\nWe can fit a simple linear regression (one predictor) to evaluate how speech rate (variable sp_rate in the dataset) influences perceived persuasiveness (variable persuasive in the dataset). There are various ways in which we can transform the predictor variable sp_rate, which in turn can alter the interpretation of some of our estimates:\n\n\nRaw X\n\nm1 &lt;- lm(persuasive ~ sp_rate, data = dap2)\nsummary(m1)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 55.532060  6.4016670  8.674625 6.848945e-15\nsp_rate     -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\nThe intercept and the coefficient for neuroticism are interpreted as:\n\n(Intercept): A audio clip of someone speaking at zero phones per second is estimated as having an average persuasive rating of 55.53.\n\nsp_rate: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nMean-Centered X\nWe can mean center our predictor and fit the model again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_mc = sp_rate - mean(sp_rate))\nm2 &lt;- lm(persuasive ~ sp_rate_mc, data = dap2)\nsummary(m2)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667  1.3519418 39.110165 6.429541e-80\nsp_rate_mc  -0.190987  0.4497113 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_mc: For every increase of one phone per second, perceived persuasiveness is estimated to decrease by -0.19.\n\n\n\nStandardised X\nWe can standardise our predictor and fit the model yet again:\n\ndap2 &lt;- dap2 %&gt;% mutate(sp_rate_z = scale(sp_rate))\nm3 &lt;- lm(persuasive ~ sp_rate_z, data = dap2)\nsummary(m3)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 52.874667   1.351942 39.110165 6.429541e-80\nsp_rate_z   -0.576077   1.356471 -0.424688 6.716809e-01\n\n\n\n(Intercept): A audio clip of someone speaking at the mean phones per second is estimated as having an average persuasive rating of 52.87.\n\nsp_rate_z: For every increase of one standard deviation in phones per second, perceived persuasiveness is estimated to decrease by -0.58.\n\nRemember that the scale(sp_rate) is subtracting the mean from each value, then dividing those by the standard deviation. The standard deviation of dap2$sp_rate is:\n\nsd(dap2$sp_rate)\n\n[1] 3.016315\n\n\nso in our variable dap2$sp_rate_z, a change of 3.02 gets scaled to be a change of 1 (because we are dividing by sd(dap2$sp_rate)).\n\ncoef(m1)[2] * sd(dap2$sp_rate)\n\n  sp_rate \n-0.576077 \n\ncoef(m3)[2]\n\nsp_rate_z \n-0.576077 \n\n\n\n\nNote that these models are identical. When we conduct a model comparison between the 3 models, the residual sums of squares is identical for all models:\n\nanova(m1,m2,m3)\n\nAnalysis of Variance Table\n\nModel 1: persuasive ~ sp_rate\nModel 2: persuasive ~ sp_rate_mc\nModel 3: persuasive ~ sp_rate_z\n  Res.Df   RSS Df Sum of Sq F Pr(&gt;F)\n1    148 40576                      \n2    148 40576  0         0         \n3    148 40576  0         0         \n\n\nWhat changes when you center or scale a predictor in a standard regression model (one fitted with lm())?\n\nThe variance explained by the predictor remains exactly the same\nThe intercept will change to be the estimated mean outcome where that predictor is “0”. Scaling and centering changes what “0” represents, thereby changing this estimate (the significance test will therefore also change because the intercept now has a different meaning)\nThe slope of the predictor will change according to any scaling (e.g. if you divide your predictor by 10, the slope will multiply by 10).\nThe test of the slope of the predictor remains exactly the same.\n\n\n\n\n\n\n\n\n\nExercises: Assumptions & Diagnostics\n\nData: Wellbeing Across Scotland\nFor these next set of exercises we continue with our recurring study in which researchers want to look at the relationship between time spent outdoors and mental wellbeing, across all of Scotland. Data is collected from 20 of the Local Authority Areas and is accessible at https://uoepsy.github.io/data/LAAwellbeing.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant ID\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nSelf report estimated number of hours per week spent outdoors\n    wellbeing\nWarwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n    density\nLAA Population Density (people per square km)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nThe code below will read in the data and fit the model with by-LAA random intercepts and slopes of outdoor time.\n\nlibrary(tidyverse)\nlibrary(lme4)\nscotmw &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\nrs_model &lt;- lmer(wellbeing ~ 1 + outdoor_time + (1 + outdoor_time | laa), data = scotmw)\n\n\nPlot the residuals vs fitted model, and assess the extend to which the assumption holds that the residuals are zero mean.\nConstruct a scale-location plot. This is where the square-root of the absolute value of the standardised residuals is plotted against the fitted values, and allows you to more easily assess the assumption of constant variance.\n\n\nOptional: can you create the same plot using ggplot, starting with the augment() function from the broom.mixed package?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nplot(model) will give you this plot, but you might want to play with the type = c(......) argument to get the smoothing line\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nplot(rs_model, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nAs we can see, the mean value of the residuals is quite close to zero, right the way across the fitted values. This is good.\n\nplot(rs_model,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nIn this plot we can see that the variance of the residuals is fairly constant across the fitted values. There is a slight dip at the lower end. We can see this in the previous plot too - all the points at the LHS of the plot are slightly more tightly grouped around the line. This is not enough to worry me, personally.\n\nlibrary(broom.mixed)\naugment(rs_model) %&gt;%\n  mutate(\n    sqrtr = sqrt(abs(.resid))\n  ) %&gt;%\n  ggplot(aes(x=.fitted, y=sqrtr)) + \n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nExamine the normality of both the level 1 and level 2 residuals.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nUse hist() if you like, or qqnorm(residuals) followed by qqline(residuals)\nExtracting the level 2 residuals (the random effects) can be difficult. ranef(model) will get you some of the way.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nLevel 1\n\nhist(resid(rs_model))\n\n\n\n\n\n\n\nqqnorm(resid(rs_model))\nqqline(resid(rs_model))\n\n\n\n\n\n\n\n\nLevel 2\n\nqqnorm(ranef(rs_model)$laa[, 1], main = \"Random intercept\")\nqqline(ranef(rs_model)$laa[, 1])\n\n\n\n\n\n\n\nqqnorm(ranef(rs_model)$laa[, 2], main = \"Random slope\")\nqqline(ranef(rs_model)$laa[, 2])\n\n\n\n\n\n\n\n\nThe normality of the residuals at both levels looks pretty decent here. This is especially good given that we only actually have 20 clusters (the LAAs). We have quite a small sample at this level.\n\n\n\n\nQuestion 3\n\n\n\nWhich person in the dataset has the greatest influence on our model?\n\nFor which person is the model fit the worst (i.e., who has the highest residual?)\nWhich LAA has the greatest influence on our model?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nas well as hlm_influence() in the HLMdiag package there is another nice function, hlm_augment()\nwe can often end up in confusion because the \\(i^{th}\\) observation inputted to our model (and therefore the \\(i^{th}\\) observation of hlm_influence() output) might not be the \\(i^{th}\\) observation in our original dataset - there may be missing data! (Luckily, we have no missing data in this dataset).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(HLMdiag)\nl1_inf &lt;- hlm_influence(rs_model,level=1)\ndotplot_diag(l1_inf$cooksd, cutoff=\"internal\")+\n  ylim(0,.15)\n\n\n\n\n\n\n\n\nGreatest influence:\n\nhlm_augment(rs_model, level=1) %&gt;% arrange(desc(cooksd))\n\n# A tibble: 132 × 15\n      id wellbeing outdoor_time laa          .resid .fitted .ls.resid .ls.fitted\n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1    74        35           33 Scottish Bo…  -5.15    40.2    -3.73        38.7\n 2   129        60            5 City of Edi…   8.89    51.1     6.43        53.6\n 3   109        31            8 Inverclyde    -9.22    40.2    -3.03        34.0\n 4    59        32            7 Scottish Bo…  -6.43    38.4    -7.45        39.5\n 5    31        35            7 Moray         -3.44    38.4     0.198       34.8\n 6    90        70           34 Na h-Eilean…  -3.16    73.2    -1.19        71.2\n 7    87        54           29 Highland      -5.33    59.3    -5.66        59.7\n 8    62        26           21 Midlothian    -4.77    30.8     0.214       25.8\n 9    67        37            7 East Ayrshi…   4.58    32.4     4.62        32.4\n10    64        46           18 City of Edi… -10.5     56.5   -10.1         56.1\n# ℹ 122 more rows\n# ℹ 7 more variables: .mar.resid &lt;dbl&gt;, .mar.fitted &lt;dbl&gt;, cooksd &lt;dbl&gt;,\n#   mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;, covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\nscotmw[74, ]\n\n# A tibble: 1 × 6\n  ppt   name                 laa              outdoor_time wellbeing density\n  &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ID46  Groundskeeper Willie Scottish Borders           33        35      29\n\n\n\n\n\n\n\n\n\n\n\nHighest residual:\n\nhlm_augment(rs_model, level=1) %&gt;% arrange(desc(abs(.resid)))\n\n# A tibble: 132 × 15\n      id wellbeing outdoor_time laa          .resid .fitted .ls.resid .ls.fitted\n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1    64        46           18 City of Edi… -10.5     56.5    -10.1        56.1\n 2   107        22           12 East Ayrshi… -10.3     32.3    -10.1        32.1\n 3    72        22           24 West Lothian -10.0     32.0     -9.24       31.2\n 4   109        31            8 Inverclyde    -9.22    40.2     -3.03       34.0\n 5   130        22           16 West Dunbar…  -8.98    31.0     -8.61       30.6\n 6   129        60            5 City of Edi…   8.89    51.1      6.43       53.6\n 7    93        65           18 City of Edi…   8.51    56.5      8.90       56.1\n 8    85        47           15 City of Edi…  -8.25    55.2     -8.52       55.5\n 9     7        38           13 Perth and K…  -7.84    45.8     -5.28       43.3\n10   121        31           16 Dumfries an…  -7.78    38.8     -7.70       38.7\n# ℹ 122 more rows\n# ℹ 7 more variables: .mar.resid &lt;dbl&gt;, .mar.fitted &lt;dbl&gt;, cooksd &lt;dbl&gt;,\n#   mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;, covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\nscotmw[64, ]\n\n# A tibble: 1 × 6\n  ppt   name            laa               outdoor_time wellbeing density\n  &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ID37  Nicola Sturgeon City of Edinburgh           18        46    1958\n\n\nMost influential LAA:\n\nhlm_augment(rs_model, level=\"laa\") %&gt;% arrange(desc(cooksd))\n\n# A tibble: 20 × 10\n   laa       .ranef.intercept .ranef.outdoor_time .ls.intercept .ls.outdoor_time\n   &lt;chr&gt;                &lt;dbl&gt;               &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n 1 Midlothi…           -1.76             -0.486           8.75           -1.26  \n 2 Na h-Eil…           14.1               0.399          21.4             0.0553\n 3 Glasgow …          -10.9              -0.464          -9.45           -0.593 \n 4 City of …           10.8               0.199          16.2            -0.144 \n 5 Stirling             6.51             -0.0758         13.3            -0.500 \n 6 Shetland…            4.73              0.389           3.69            0.424 \n 7 Angus               -4.86              0.0959         -6.82            0.265 \n 8 West Lot…           -3.13             -0.344           4.13           -0.725 \n 9 Falkirk             -7.45             -0.157         -12.3            -0.0360\n10 Invercly…           -1.30              0.197         -14.5             1.17  \n11 Highland             6.73              0.281           8.95            0.155 \n12 West Dun…           -6.41             -0.267          -4.87           -0.395 \n13 Moray               -2.22              0.133          -5.81            0.267 \n14 Perth an…            1.49              0.257         -20.4             1.76  \n15 East Ayr…           -5.67             -0.233          -3.63           -0.391 \n16 Orkney I…            0.179             0.155          -0.816           0.212 \n17 Scottish…           -0.264            -0.148           3.28           -0.367 \n18 Dumfries…           -2.69             -0.0120         -7.27            0.261 \n19 Argyll a…            1.65             -0.00518         4.67           -0.187 \n20 East Ren…            0.502             0.0849          1.40            0.0247\n# ℹ 5 more variables: cooksd &lt;dbl&gt;, mdffits &lt;dbl&gt;, covtrace &lt;dbl&gt;,\n#   covratio &lt;dbl&gt;, leverage.overall &lt;dbl&gt;\n\n\n\n\n\n\nQuestion 4\n\n\n\nLooking at the random effects, which LAA shows the least improvement in wellbeing as outdoor time increases, and which shows the greatest improvement?\n\nWhat is the estimated wellbeing for people from City of Edinburgh with zero hours of outdoor time per week, and what is their associated increases in wellbeing for every hour per week increase in outdoor time?\n\n\n\n\n\n\nSolution\n\n\n\nIt looks like the residents of Midlothian have the least improvement, and the Western Isles (Na h-Eileanan Siar) show the most increases of wellbeing with outdoor time. We can see this from the LAA-random slopes of outdoor time:\n\nranef(rs_model)\n\n$laa\n                      (Intercept) outdoor_time\nAngus                  -4.8568066  0.095850303\nArgyll and Bute         1.6488121 -0.005181049\nCity of Edinburgh      10.8163125  0.199034174\nDumfries and Galloway  -2.6893688 -0.012005965\nEast Ayrshire          -5.6749200 -0.232750990\nEast Renfrewshire       0.5024800  0.084907037\nFalkirk                -7.4525578 -0.156694328\nGlasgow City          -10.9101439 -0.464232183\nHighland                6.7315989  0.280992008\nInverclyde             -1.2966048  0.197142062\nMidlothian             -1.7585791 -0.485961786\nMoray                  -2.2165380  0.133392034\nNa h-Eileanan Siar     14.0595006  0.399493656\nOrkney Islands          0.1789928  0.154590585\nPerth and Kinross       1.4894924  0.256689754\nScottish Borders       -0.2638474 -0.148174460\nShetland Islands        4.7262680  0.388873631\nStirling                6.5060959 -0.075781592\nWest Dunbartonshire    -6.4127140 -0.266515096\nWest Lothian           -3.1274727 -0.343667797\n\nwith conditional variances for \"laa\" \n\n\nWe can get the cluster-specific coefficients either by adding the fixef() and ranef() together, or using coef():\n\ncoef(rs_model)\n\n$laa\n                      (Intercept) outdoor_time\nAngus                    33.36700   0.31050188\nArgyll and Bute          39.87261   0.20947052\nCity of Edinburgh        49.04011   0.41368575\nDumfries and Galloway    35.53443   0.20264561\nEast Ayrshire            32.54888  -0.01809942\nEast Renfrewshire        38.72628   0.29955861\nFalkirk                  30.77124   0.05795725\nGlasgow City             27.31366  -0.24958061\nHighland                 44.95540   0.49564358\nInverclyde               36.92720   0.41179364\nMidlothian               36.46522  -0.27131021\nMoray                    36.00726   0.34804361\nNa h-Eileanan Siar       52.28330   0.61414523\nOrkney Islands           38.40280   0.36924216\nPerth and Kinross        39.71329   0.47134133\nScottish Borders         37.95995   0.06647711\nShetland Islands         42.95007   0.60352520\nStirling                 44.72990   0.13886998\nWest Dunbartonshire      31.81109  -0.05186352\nWest Lothian             35.09633  -0.12901622\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\n\ncoef(rs_model)$laa[\"City of Edinburgh\",]\n\n                  (Intercept) outdoor_time\nCity of Edinburgh    49.04011    0.4136857\n\n\n\n\n\n\n\n\n\n\n\nExercises: Centering in the MLM\n\nData: Hangry\nThe study is interested in evaluating whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting. 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week. 46 of the participants were following a five-two diet (five days of normal eating, 2 days of fasting), and the remaining 35 were following no specific diet.\nThe data are available at: https://uoepsy.github.io/data/hangry.csv.\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant\n\n\nfivetwo\nWhether the participant follows the five-two diet\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead carefully the description of the study above, and try to write out (in lmer syntax) an appropriate model to test the research aims.\ne.g.:\noutcome ~ explanatory variables + (???? | grouping)\nTry to think about the maximal random effect structure (i.e. everything that can vary by-grouping is estimated as doing so).\nTo help you think through the steps to get from a description of a research study to a model specification, think about your answers to the following questions.\nQ: What is our outcome variable?\nQ: What are our explanatory variables?\nQ: Is there any grouping (or “clustering”) of our data that we consider to be a random sample? If so, what are the groups?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe research is looking at how hunger influences irritability, and whether this is different for people on the fivetwo diet.\nWe can split our data in to groups of each participant. We can also split it into groups of each diet. Which of these groups have we randomly sampled? Do we have a random sample of participants? Do we have a random sample of diets? Another way to think of this is “if i repeated the experiment, what these groups be different?”\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nOur outcome is irritability here, because it is the thing that we are trying to explain through peoples’ hunger levels and diets.\n\nlmer(irritability ~  explanatory variables + (???? | grouping))\n\nWe are interested in the effect of hunger on irritability, and whether this effect is different for the five-two diet. So we are interested in the interaction:\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | grouping))\n\n(remember that hunger + diet + hunger:diet is just a more explicit way of writing hunger*diet).\nIf we did this experiment again, would we have different participants?\nYes. If we did this experiment again, would we have different diets? No, because we’re interested in the specific differences between the five-two diet and no dieting. This means we will likely want to by-participant random deviations (e.g. the ( ... | participant) bit in lmer). But we won’t have by-diet random effects (1 | diet) because the diet differences are the specific differences that we wish to test.\n\nlmer(irritability ~  hunger + diet + hunger:diet + (???? | participant))\n\nThinking about what can be modelled as randomly varying between participants, we have some options:\n\nparticipants vary in how irritable they are on average\n(the intercept, 1 | participant)\nparticipants vary in how much hunger influences their irritability\n(the effect of hunger, hunger | participant)\nparticipants vary in how much diet influences irritability\n(the effect of diet, diet | participant)\nparticipants vary in how much diet effects hunger’s influence on irritability\n(the interaction between diet and hunger, diet:hunger | participant)\n\nWe can vary 1 and 2, but not 3 and 4. This is because each participant is either following the five-two diet or they are not. So for a single participant, we can’t assess “the effect diet has” on anything, because we haven’t seen that participant under different diets. if we try to plot a single participants’ data, we can see that it is impossible for us to assess “the effect of diet”:\n\n\n\n\n\n\n\n\n\nBy contrast, we can vary the intercept and the effect of hunger, because each participant has multiple values of irritability, and multiple different observations of hunger. We can think about a single participant’s “effect of hunger on irritability” and how we might fit a line to their data:\n\n\n\n\n\n\n\n\n\n\nlmer(irritability ~  hunger + diet + hunger:diet + (1 + hunger | participant))\n\n\n\n\n\n\n\n\n\n\nTotal, Within, Between\n\n\n\n\n\nRecall our research aim:\n\n… whether hunger influences peoples’ levels of irritability (i.e., “the hangry hypothesis”), and whether this is different for people following a diet that includes fasting.\n\nForgetting about any differences due to diet, let’s just think about the relationship between irritability and hunger. How should we interpret this research aim?\nWas it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a composite of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that the ‘total effect’ isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it about how person i changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are.).\n\n\n\n\nQuestion 2\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll find group_by() %&gt;% mutate() very useful here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nhangry &lt;- \n    hangry %&gt;% group_by(ppt) %&gt;%\n        mutate(\n            avg_hunger = mean(q_hunger),\n            hunger_gc = q_hunger - avg_hunger\n        )\nhead(hangry)\n\n# A tibble: 6 × 6\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   fivetwo avg_hunger hunger_gc\n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1             17       30 N1p1  1             26.6     3.4  \n2             19       27 N1p1  1             26.6     0.400\n3             19       29 N1p1  1             26.6     2.4  \n4             20       33 N1p1  1             26.6     6.4  \n5             24       14 N1p1  1             26.6   -12.6  \n6             30       28 N1p2  1             32.6    -4.6  \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people?\n\nDoes it look like when people are more hungry than normal, they are more irritable?\n\n\n\n\n\n\nSolution\n\n\n\nWe might find it easier to look at a plot where each participant is represented as their mean plus an indication of their range of irritability scores:\n\nggplot(hangry,aes(x=avg_hunger,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nThere appears to be a slight positive relationship between a persons’ average hunger and their irritability scores.\nIt is harder to tell what the relationship is between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data:\n\nggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt))+\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2)\n\n\n\n\n\n\n\n\nI think there might be a positive trend in here, in that participants tend to be higher irritability when they are higher (for them) on the hunger score.\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation level deviations from those averages), that represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables as predictors, instead of the raw hunger scores.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nhunger * diet could be replaced by (hunger1 + hunger2) * diet, thereby allowing each aspect of hunger to interact with diet.\nWe can only put one of these variables in the random effects (1 + hunger | participant). Recall that above we discussed how we cannot have (diet | participant), because “an effect of diet” makes no sense for a single participant (they are either on the diet or they are not, so there is no ‘effect’). Similarly, each participant has only one value for their average hungriness.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(lme4)\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc)* fivetwo + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                control = lmerControl(optimizer=\"bobyqa\"))\n\n\n\n\n\nQuestion 5\n\n\nHopefully, you have fitted a model similar to the below:\n\nhangrywb &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc) * fivetwo + \n                (1 + hunger_gc | ppt), data = hangry,\n            control = lmerControl(optimizer=\"bobyqa\"))\n\nBelow, we have obtained p-values using the Kenward Rogers Approximation of \\(df\\) for the test of whether the fixed effects are zero, so we can see the significance of each estimate.\nProvide an answer for each of these questions:\n\nFor those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\nDo people following no diet tend to be more irritable when they are more hungry than they usually are?\nIs there evidence to suggest that this is different for those following the five-two diet? In what way?\n(Trickier:) What does the fivetwo coefficient represent?\n\n\n\n\n\n\n\n  \n    \n      Model Summary\n    \n    \n    \n      Parameter\n      Coefficient\n      SE\n      95% CI\n      t\n      df\n      p\n    \n  \n  \n    \n      Fixed Effects \n    \n    (Intercept)\n17.13\n5.21\n(6.75, 27.51)\n3.29\n77.00\n0.002 \n    avg hunger\n3.86e-03\n0.11\n(-0.21, 0.22)\n0.04\n77.00\n0.971 \n    hunger gc\n0.19\n0.08\n(0.03, 0.34)\n2.45\n70.40\n0.017 \n    fivetwo (1)\n-10.85\n6.62\n(-24.03, 2.32)\n-1.64\n77.00\n0.105 \n    avg hunger × fivetwo (1)\n0.47\n0.14\n(0.20, 0.74)\n3.44\n77.00\n&lt; .001\n    hunger gc × fivetwo (1)\n0.38\n0.10\n(0.18, 0.58)\n3.75\n73.64\n&lt; .001\n    \n      Random Effects \n    \n    SD (Intercept: ppt)\n6.93\n\n\n\n\n\n    SD (hunger_gc: ppt)\n0.38\n\n\n\n\n\n    Cor (Intercept~hunger_gc: ppt)\n-0.01\n\n\n\n\n\n    SD (Residual)\n4.83\n\n\n\n\n\n  \n  \n    \n      \n    \n  \n  \n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n1: For those following no diet, is there evidence to suggest that people who are on average more hungry are more irritable?\nA: ‘No diet’ is the reference level of the five-two variable, and because we have an interaction, that means the avg_hunger coefficient will provide the relevant estimate. There is no evidence (\\(p&gt;.05\\)) to suggest that when not dieting, hungrier people are more irritable than less hungry people.\n2: Is there evidence to suggest that this is different for those following the five-two diet? In what way?\nA: This is the interaction between avg_hunger:fivetwo1. We can see that, for every increase of 1 in average hunger, irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet.\nThese units are still in terms of the original scale (i.e. 0 to 100).\n3: Do people following no diet tend to be more irritable when they are more hungry than they usually are? A: This is the estimate for the coefficient of hunger_gc. For people following no diet, there is an estimated 0.19 increase in irritability for every 1 unit more hungry they become.\n4: Is there evidence to suggest that this is different for those following the five-two diet? In what way? A: This effect of a 1 unit change on within-person hunger increasing irritability is increased for those who are following the five-two diet by an additional 0.38\n5: What does the fivetwo1 coefficient represent? A: This represents the group difference of irritability between those on the five-two diet vs those not dieting, for someone who has an average hunger score of 0.\n\n\n\n\nQuestion 6\n\n\nConstruct two plots showing the two model estimated interactions. Think about your answers to the previous question, and check that they match with what you are seeing in the plots (do not underestimate the utility of this activity for helping understanding!).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis isn’t as difficult as it sounds. the sjPlot package can do it in one line of code!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(sjPlot)\nplot_model(hangrywb, type = \"int\")[[1]]\n\n\n\n\n\n\n\n\nWe saw in the model coefficients that for the reference level of fivetwo, the “No Diet” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. We also saw the interaction avg_hunger:fivetwo1 indicates that irritability is estimated to increase by 0.47 more for those in the five-two diet than it does for those following no diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\nplot_model(hangrywb, type = \"int\")[[2]]\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_gc we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re on “No Diet”). This is the slope of the red line. The interaction hunger_gc:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive and significant, which matches with the fact that the blue line is clearly steeper in this plot.\n\n\n\n\nQuestion 7\n\n\nProvide tests or confidence intervals for the parameters of interest, and write-up the results.\n\n\n\n\n\n\nRemember: some options for inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\n\n\n\n\ntests or CIs for model parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\n\n\nmodel comparison(different fixed effects, same random effects)\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(parameters)\nmodel_parameters(hangrywb, ci_method = \"kr\", ci_random = FALSE)\n\n# Fixed Effects\n\nParameter                | Coefficient |   SE |          95% CI |     t |    df |      p\n----------------------------------------------------------------------------------------\n(Intercept)              |       17.13 | 5.21 | [  6.75, 27.51] |  3.29 | 77.00 | 0.002 \navg hunger               |    3.86e-03 | 0.11 | [ -0.21,  0.22] |  0.04 | 77.00 | 0.971 \nhunger gc                |        0.19 | 0.08 | [  0.03,  0.34] |  2.45 | 70.40 | 0.017 \nfivetwo [1]              |      -10.85 | 6.62 | [-24.03,  2.32] | -1.64 | 77.00 | 0.105 \navg hunger × fivetwo [1] |        0.47 | 0.14 | [  0.20,  0.74] |  3.44 | 77.00 | &lt; .001\nhunger gc × fivetwo [1]  |        0.38 | 0.10 | [  0.18,  0.58] |  3.75 | 73.64 | &lt; .001\n\n# Random Effects\n\nParameter                      | Coefficient\n--------------------------------------------\nSD (Intercept: ppt)            |        6.93\nSD (hunger_gc: ppt)            |        0.38\nCor (Intercept~hunger_gc: ppt) |       -0.01\nSD (Residual)                  |        4.83\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on a restricted diet such as the five-two, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes of relative-hunger level were included for participants. The model was fitting with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser from the lme4. \\(P\\)-values were obtained using Wald tests with Kenward-Roger approximation of denominator degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(\\beta = 0.19,\\ SE = 0.08,\\ t(2.45) = 70.4,\\ p=0.017\\)), and that this was increased for those following the five-two diet (\\(\\beta = 0.38,\\ SE = 0.1,\\ t(3.75) = 73.64,\\ p&lt;0.001\\)). Although for those not on a specific diet there was no evidence for an association between irritability and being generally a more hungry person (\\(p=0.971\\)), there a significant interaction was found between average hunger and being on the five-two diet (\\(\\beta = 0.47,\\ SE = 0.14,\\ t(3.44) = 77,\\ p&lt;0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association of both between-person hunger and within-person hunger with irritability.\n\n\n\n\nOther within-group transformations\nAs well as within-group mean centering a predictor (like we have done above), we can within-group standardise a predictor. This would disagregate within and between effects, but interpretation would of the within effect would be the estimated change in \\(y\\) associated with being 1 standard deviation higher in \\(x\\) for that group."
  },
  {
    "objectID": "04_ranefglmer.html",
    "href": "04_ranefglmer.html",
    "title": "4. Random Effect Structures | Logistic MLM",
    "section": "",
    "text": "Nested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results."
  },
  {
    "objectID": "04_ranefglmer.html#footnotes",
    "href": "04_ranefglmer.html#footnotes",
    "title": "4. Random Effect Structures | Logistic MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s always going to be debateable about what is ‘too high’ because in certain situations you might expect correlations close to 1. It’s best to think through whether it is a feasible value given the study itself↩︎"
  },
  {
    "objectID": "05_recap.html",
    "href": "05_recap.html",
    "title": "5. Recap & Practice Datasets",
    "section": "",
    "text": "Flashcards: lm to lmer\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\n\nFormula:\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustered (multi-level) data\n\n\n\n\n\nWhen our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nIf we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nBy including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nWe can now see that the intercept estimate \\(\\beta_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has an adjustment for each subject.\nEach subject’s line has been moved up or down accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage\n\n\n\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different:\n\n\n\n\n\n\n\n\n\nWhy? One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters.\n\n\n\n\n\n\n\n\n\nRandom slopes\n\n\n\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(\\beta_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(\\beta_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nAs with the intercept \\(\\beta_{0i}\\), the slope of the predictor \\(\\beta_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed effects\n\n\n\n\n\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the groups all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\nThese are the overall intercept and slope.\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n405.7897675  -0.6722654 \n\n\n\n\n\n\n\n\n\n\n\nRandom effects\n\n\n\n\n\nThe plots below show the fitted values for each subject from each model that we have gone through in these expandable boxes (simple linear regression, random intercept, and random intercept & slope):\n\n\n\n\n\n\n\n\n\nIn the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_0\\). The standard deviation (and variance, which is \\(\\sigma_0^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function).\n\n\n\n\n\n\n\n\n\nIn the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. We can extract the deviations for each group from the fixed effect estimates using the ranef() function.\nThese are the deviations from the overall intercept (\\(\\widehat \\gamma_{00} = 405.79\\)) and slope (\\(\\widehat \\gamma_{10} = -0.672\\)) for each subject \\(i\\).\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)          x1\nsub_308   31.327291 -1.43995253\nsub_309  -28.832219  0.41839420\nsub_310    2.711822  0.05993766\nsub_330   59.398971  0.38526670\nsub_331   74.958481  0.17391602\nsub_332   91.086535 -0.23461836\nsub_333   97.852988 -0.19057838\nsub_334  -54.185688 -0.55846794\nsub_335  -16.902018  0.92071637\nsub_337   52.217859 -1.16602280\nsub_349  -67.760246 -0.68438960\nsub_350   -5.821271 -1.23788002\nsub_351   61.198823  0.05499816\nsub_352   -7.905596 -0.66495059\nsub_369  -47.636645 -0.46810258\nsub_370  -33.121093 -1.11001234\nsub_371   77.576205 -0.20402571\nsub_372  -36.389281 -0.45829505\nsub_373 -197.579562  1.79897904\nsub_374  -52.195357  4.60508775\n\nwith conditional variances for \"subject\" \n\n\n\n\n\n\n\n\n\n\n\nGroup-level coefficients\n\n\n\n\n\nWe can see the estimated intercept and slope for each subject \\(i\\) specifically, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)         x1\nsub_308    437.1171 -2.1122179\nsub_309    376.9575 -0.2538712\nsub_310    408.5016 -0.6123277\nsub_330    465.1887 -0.2869987\nsub_331    480.7482 -0.4983494\nsub_332    496.8763 -0.9068837\nsub_333    503.6428 -0.8628438\nsub_334    351.6041 -1.2307333\nsub_335    388.8877  0.2484510\nsub_337    458.0076 -1.8382882\nsub_349    338.0295 -1.3566550\nsub_350    399.9685 -1.9101454\nsub_351    466.9886 -0.6172672\nsub_352    397.8842 -1.3372160\nsub_369    358.1531 -1.1403680\nsub_370    372.6687 -1.7822777\nsub_371    483.3660 -0.8762911\nsub_372    369.4005 -1.1305604\nsub_373    208.2102  1.1267137\nsub_374    353.5944  3.9328224\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nNotice that the above are the fixed effects + random effects estimates, i.e. the overall intercept and slope + deviations for each subject.\n\ncbind(\n  int = fixef(random_slopes_model)[1] + \n    ranef(random_slopes_model)$subject[,1],\n  slope = fixef(random_slopes_model)[2] + \n    ranef(random_slopes_model)$subject[,2]\n)\n\n           int      slope\n [1,] 437.1171 -2.1122179\n [2,] 376.9575 -0.2538712\n [3,] 408.5016 -0.6123277\n [4,] 465.1887 -0.2869987\n [5,] 480.7482 -0.4983494\n [6,] 496.8763 -0.9068837\n [7,] 503.6428 -0.8628438\n [8,] 351.6041 -1.2307333\n [9,] 388.8877  0.2484510\n[10,] 458.0076 -1.8382882\n[11,] 338.0295 -1.3566550\n[12,] 399.9685 -1.9101454\n[13,] 466.9886 -0.6172672\n[14,] 397.8842 -1.3372160\n[15,] 358.1531 -1.1403680\n[16,] 372.6687 -1.7822777\n[17,] 483.3660 -0.8762911\n[18,] 369.4005 -1.1305604\n[19,] 208.2102  1.1267137\n[20,] 353.5944  3.9328224\n\n\n\n\n\n\n\n\n\n\n\nAssumptions, Influence\n\n\n\n\n\nIn the simple linear model \\(\\color{red}{y} = \\color{blue}{\\beta_0 + \\beta_1(x)} + \\varepsilon\\), we distinguished between the systematic model part \\(\\beta_0 + \\beta_1(x)\\), around which observations randomly vary (the \\(\\varepsilon\\) part) - i.e. \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}} + \\text{error}\\).\nIn the multi-level model, our random effects are another source of random variation - \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}} + \\text{group_error} + \\text{individual_error}\\). As such, random effects are another form of residual, and our assumptions of zero mean constant variance apply at both levels of residuals (see Figure 1).\n\n\n\n\n\nFigure 1: The black dashed lines show our model assumptions.\n\n\n\n\n\nWe can assess these normality of both resid(model) and ranef(model) by constructing plots using functions such as hist(), qqnorm() and qqline().\n\nWe can also use plot(model, type=c(\"p\",\"smooth\")) to give us our residuals vs fitted plot (smooth line should be horizontal at approx zero, showing zero mean).\n\nplot(model, form = sqrt(abs(resid(.))) ~ fitted(.), type = c(\"p\",\"smooth\")) will give us our scale-location plot (smooth line should be horizontal, showing constant variance).\n\nWe can also use the check_model() function from the performance package to get lots of info at once:\n\nlibrary(performance)\ncheck_model(random_slopes_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood-based\ncase-based bootstrap\n\n\n\n\ntests or CIs for model parameters\nlibrary(parameters)model_parameters(model, ci_method=\"kr\")\nconfint(model, type=\"profile\")\nlibrary(lmeresampler)bootstrap(model, .f=fixef, type=\"case\", B = 2000, resample = c(??,??))\n\n\nmodel comparison(different fixed effects, same random effects)\nlibrary(pbkrtest)KRmodcomp(model1,model0)\nanova(model0,model)\n\n\n\n\nfit models with REML=TRUE.good option for small samples\nfit models with REML=FALSE.needs large N at both levels (40+)\ntakes time, needs careful thought about which levels to resample, but means we can relax distributional assumptions (e.g. about normality of residuals)\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Model Fitted values\n\n\n\n\n\nThe model fitted (or “model predicted”) values can be obtained using predict() (returning just the values) or broom.mixed::augment() (returning the values attached to the data that is inputted to the model).\nTo plot, them, we would typically like to plot the fitted values for each group (e.g. subject)\n\nlibrary(broom.mixed)\naugment(random_slopes_model) %&gt;%\n  ggplot(.,aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Fixed Effects\n\n\n\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. Packages like sjPlot make it incredibly easy (but sometimes too easy), so a nice option is to use the effects package to construct a dataframe of the linear prediction accross the values of a predictor, plus standard errors and confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\n# a quick option:  \nlibrary(sjPlot)\nplot_model(random_slopes_model, type = \"eff\")\n\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting random effects\n\n\n\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms &lt;- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor, more succinctly:\n(1 | school/class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school/class) would give the same results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\n\n\n\n\nMLM allows us to model effects in the linear model as varying between groups. Our coefficients we remember from simple linear models (the \\(\\beta\\)’s) are modelled as a distribution that has an overall mean around which our groups vary. We can see this in Figure 2, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 2 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\nFigure 2: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(\\beta\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{\\beta_{0i} \\cdot 1 + \\beta_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{\\beta_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{\\beta_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is \\(\\rho = \\text{Cor}(\\zeta_{0i}, \\zeta_{1i}) = \\frac{\\sigma_{01}}{\\sigma_0 \\sigma_1}\\):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\nWe fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\nlibrary(lme4)\nm1 &lt;- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 637.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.49449 -0.57223 -0.01353  0.62544  2.39122 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.2616   1.5038       \n          x           0.7958   0.8921   0.55\n Residual             4.3672   2.0898       \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   1.7261     0.9673   1.785\nx             1.1506     0.2968   3.877\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)\n\n\n\n\n\n\n\n\n\nPractice Datasets Weeks 4 and 5\nBelow are various datasets on which you can try out your newfound modelling skills. Read the descriptions carefully, keeping in mind the explanation of how the data is collected and the research question that motivates the study design.\n\n\n\n\n\n\nPractice 1: Music and Driving\n\n\n\n\n\nThese data are simulated to represent data from a fake experiment, in which participants were asked to drive around a route in a 30mph zone. Each participant completed the route 3 times (i.e. “repeated measures”), but each time they were listening to different audio (either speech, classical music or rap music). Their average speed across the route was recorded. This is a fairly simple design, that we might use to ask “how is the type of audio being listened to associated with driving speeds?”\nThe data are available at https://uoepsy.github.io/data/drivingmusicwithin.csv.\n\n\n\n\n\n\n\n\n\nPractice 2: CBT and Stress\n\n\n\n\n\nThese data are simulated to represent data from 50 participants, each measured at 3 different time-points (pre, during, and post) on a measure of stress. Participants were randomly allocated such that half received some cognitive behavioural therapy (CBT) treatment, and half did not. This study is interested in assessing whether the two groups (control vs treatment) differ in how stress changes across the 3 time points.\nThe data are available at https://uoepsy.github.io/data/stressint.csv.\n\n\n\n\n\n\n\n\n\nPractice 3: Erm.. I don’t believe you\n\n\n\n\n\nThese data are simulated to represent data from 30 participants who took part in an experiment designed to investigate whether fluency of speech influences how believable an utterance is perceived to be.\nEach participant listened to the same 20 statements, with 10 being presented in fluent speech, and 10 being presented with a disfluency (an “erm, …”). Fluency of the statements was counterbalanced such that 15 participants heard statements 1 to 10 as fluent and 11 to 20 as disfluent, and the remaining 15 participants heard statements 1 to 10 as disfluent, and 11 to 20 as fluent. The order of the statements presented to each participant was random.\nThe data are available at https://uoepsy.github.io/data/erm_belief.csv.\n\n\n\n\n\n\n\n\n\nPractice 4: Cognitive Aging\n\n\n\n\n\nThese data are simulated to represent a large scale international study of cognitive aging, for which data from 17 research centers has been combined. The study team are interested in whether different cognitive domains have different trajectories as people age. Do all cognitive domains decline at the same rate? Do some decline more steeply, and some less? The literature suggests that scores on cognitive ability are predicted by educational attainment, so they would like to control for this.\nEach of the 17 research centers recruited a minimum of 14 participants (Median = 21, Range 14-29) at age 45, and recorded their level of education (in years). Participants were then tested on 5 cognitive domains: processing speed, spatial visualisation, memory, reasoning, and vocabulary. Participants were contacted for follow-up on a further 9 occasions (resulting in 10 datapoints for each participant), and at every follow-up they were tested on the same 5 cognitive domains. Follow-ups were on average 3 years apart (Mean = 3, SD = 0.8).\nThe data are available at https://uoepsy.github.io/data/cogdecline.csv."
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n\nSolution\n\n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "example_00_anova.html",
    "href": "example_00_anova.html",
    "title": "Analysis Example: Rpt & Mixed ANOVA",
    "section": "",
    "text": "This is optional for the DAPR3 course, but may be useful for your dissertations should your field/supervisor prefer the ANOVA framework to that of the linear model.\nThis walks briefly through these models with the ez package. There are many other packages available, and many good tutorials online should you desire extra resources in the future:\n\nhttps://www.datanovia.com/en/lessons/repeated-measures-anova-in-r\nhttps://www.r-bloggers.com/2021/04/repeated-measures-of-anova-in-r-complete-tutorial/\nhttps://stats.idre.ucla.edu/r/seminars/repeated-measures-analysis-with-r/\nhttps://www.datanovia.com/en/lessons/mixed-anova-in-r/\n\n\n\nData: Audio interference in executive functioning\nThis data is from a simulated study that aims to investigate the following research questions:\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n\n24 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room.\nThe data is in stored in two separate files - the research administering the tests recorded the SDMT score in one spreadsheet, while details of the audio used in the experiment are held in a separate sheet\n\nInformation about the audio condition for each trial of each participant is stored in .csv format at https://uoepsy.github.io/data/ef_music.csv. The data is in long format (1 row per participant-trial).\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\ntrial_n\nTrial Number (1-15)\n\n\naudio\nAudio heard during the test (‘no_audio’, ‘white_noise’,‘music’)\n\n\nheadphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n\n\n\n\n\n\nInformation on participants’ Symbol Digit Modalities Test (SDMT) for each trial is stored in .xlsx format at https://uoepsy.github.io/data/ef_sdmt.xlsx. The data is in wide format (1 row per participant, 1 column per trial).\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant ID\n\n\nTrial_01\nSDMT score in trial 1\n\n\nTrial_02\nSDMT score in trial 2\n\n\nTrial_03\nSDMT score in trial 3\n\n\n…\nSDMT score in trial …\n\n\n…\nSDMT score in trial …\n\n\nTrial_15\nSDMT score in trial 15\n\n\n\n\n\n\nThe code below will read in both datasets and join them for you:\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\ndownload.file(url = \"https://uoepsy.github.io/data/ef_sdmt.xlsx\",\n              destfile = \"ef_sdmt.xlsx\",\n              mode = \"wb\")\nefdata &lt;- \n  left_join(\n    read_csv(\"https://uoepsy.github.io/data/ef_music.csv\"),\n    read_xlsx(\"ef_sdmt.xlsx\") %&gt;%\n      pivot_longer(Trial_01:Trial_15, names_to = \"trial_n\", values_to = \"SDMT\")\n  )\n\n\n\nOne-Way Repeated Measures ANOVA\nFor a repeated measures ANOVA, we have one independent variable that is within group.\nThis would be appropriate if our research question were the following:\n\nHow do different types of audio interfere with executive functioning?\n\nMapping this to the variables in our dataset, our model is going to be SDMT ~ audio, and we want to account for PID differences. So for now we will ignore the headphones variable.\n\n\nCode\nhead(efdata)\n\n\n# A tibble: 6 × 5\n  PID    trial_n  audio       headphones  SDMT\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 PPT_01 Trial_02 no_audio    speakers      31\n2 PPT_01 Trial_08 no_audio    speakers      23\n3 PPT_01 Trial_11 no_audio    speakers      23\n4 PPT_01 Trial_13 no_audio    speakers      24\n5 PPT_01 Trial_15 no_audio    speakers      34\n6 PPT_01 Trial_01 white_noise speakers      38\n\n\nThe easiest way to conduct a repeated measures ANOVA in R is to use the ez package, which comes with some handy functions to visualise the experimental design.\nWe can see from below that every participant completed 5 trials for each type of audio interference:\n\n\nCode\nlibrary(ez)\nezDesign(data = efdata, x = audio, y = PID)\n\n\n\n\n\n\n\n\n\nThe ezANOVA() function takes a few arguments.\nThe ones you will need for this are:\n\ndata the name of the dataframe\ndv the column name for the dependent variable\nwid the column name for the participant id variable\nwithin the column name(s) for the predictor variable(s) that vary within participants\nbetween the column name(s) for any predictor variable(s) that vary between participants\n\nFit a repeated measures ANOVA to examine the effect of the audio type on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio)\n\n\n$ANOVA\n  Effect DFn DFd        F            p p&lt;.05       ges\n2  audio   2  46 44.69618 1.647271e-11     * 0.2534633\n\n$`Mauchly's Test for Sphericity`\n  Effect         W          p p&lt;.05\n2  audio 0.8105961 0.09927715      \n\n$`Sphericity Corrections`\n  Effect       GGe      p[GG] p[GG]&lt;.05      HFe        p[HF] p[HF]&lt;.05\n2  audio 0.8407573 5.0677e-10         * 0.899603 1.427119e-10         *\n\n\n\n\nMixed ANOVA\nMixed ANOVA can be used to investigate effects of independent variables that are at two different levels, i.e. some are within clusters and some are between.\n\nDoes the effect of audio interference on executive functioning differ depending upon whether or not noise-cancelling headphones are used?\n\nLook at the two lines below. Can you work out what the plots will look like before you run them?\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\nezDesign(data = efdata, x = headphones, y = audio)\n\n\nParticipants 1-20 are in one condition, and 21-40 are in another.\nThis should look like a two big blocks on the diagonal.\n\n\nCode\nezDesign(data = efdata, x = headphones, y = PID)\n\n\n\n\n\n\n\n\n\nIn each condition, all different types of audio were observed in the same number of trials. This should be a full grid:\n\n\nCode\nezDesign(data = efdata, x = headphones, y = audio)\n\n\n\n\n\n\n\n\n\nFit a mixed ANOVA to examine the interaction between audio and headphone use on SDMT:\n\n\nCode\nezANOVA(data = efdata, dv = SDMT, wid = PID, within = audio, between = headphones)\n\n\n$ANOVA\n            Effect DFn DFd         F            p p&lt;.05        ges\n2       headphones   1  22  9.815545 4.836945e-03     * 0.26784992\n3            audio   2  44 59.615596 2.980503e-13     * 0.32788320\n4 headphones:audio   2  44  8.677316 6.657590e-04     * 0.06629911\n\n$`Mauchly's Test for Sphericity`\n            Effect         W         p p&lt;.05\n3            audio 0.9422531 0.5355001      \n4 headphones:audio 0.9422531 0.5355001      \n\n$`Sphericity Corrections`\n            Effect       GGe        p[GG] p[GG]&lt;.05      HFe        p[HF]\n3            audio 0.9454057 1.196469e-12         * 1.031585 2.980503e-13\n4 headphones:audio 0.9454057 8.648057e-04         * 1.031585 6.657590e-04\n  p[HF]&lt;.05\n3         *\n4         *\n\n\nThe ez package also contains some easy plotting functions for factorial experiments, such as ezPlot(). It takes similar arguments to the ezANOVA() function.\n\nlook up the help documentation for ezPlot().\nlet’s use ezPlot() to make a nice plot\n\n\n\nCode\nezPlot(data = efdata, dv = SDMT, \n       wid = PID, within = audio, between = headphones,\n       x = audio, split = headphones)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same thing in lmer\n\n\n\n\n\n\n\nCode\nlibrary(lme4)\nlibrary(lmerTest)\nmod &lt;- lmer(SDMT ~ 1 + headphones * audio + (1 + audio | PID), \n            data = efdata)\nanova(mod, type=\"III\")\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                 Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nheadphones        325.0  325.04     1    22  9.8155  0.004837 ** \naudio            3212.0 1606.01     2    22 48.4976 8.626e-09 ***\nheadphones:audio  490.1  245.06     2    22  7.4001  0.003486 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nlibrary(sjPlot)\nplot_model(mod, type=\"eff\", terms=c(\"audio\",\"headphones\"))"
  },
  {
    "objectID": "example_01_repeated_measures.html",
    "href": "example_01_repeated_measures.html",
    "title": "Analysis Walkthrough 1",
    "section": "",
    "text": "Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:\n\nFirst the data and research context is introduced. For the purpose of these tutorials, we will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated.\nSecond, we go through any tidying of the data that is required, before creating some brief descriptives and visualizations of the raw data.\nThen, we conduct an analysis. Where possible, we translate the research questions into formal equations prior to fitting the models in lme4. Model comparisons are conducted, along with checks of distributional assumptions on our model residuals.\nFinally, we visualize and interpret our analysis.\n\nPlease note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ppls.psych.stats@ed.ac.uk."
  },
  {
    "objectID": "example_01_repeated_measures.html#equations",
    "href": "example_01_repeated_measures.html#equations",
    "title": "Analysis Walkthrough 1",
    "section": "Equations",
    "text": "Equations\nWe’re going to fit the model below, and examine the change in speed associated with moving from speech (our reference level) to both classical, and rap conditions.\nRecall that because music is categorical with 3 levels, we’re going to be estimating 2 (\\(3-1\\)) coefficients.\n\\[\\begin{aligned}\n&\\text{for trial }j \\text{ from participant } i \\\\\n  \\operatorname{speed}_{i[j]} =& \\beta_{0i} + \\beta_1(\\operatorname{music}_{\\operatorname{classical}_j}) + \\beta_2(\\operatorname{music}_{\\operatorname{rap}_j}) + \\varepsilon_{i[j]} \\\\\n    \\beta_{0i} =& \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "example_01_repeated_measures.html#fitting-the-models",
    "href": "example_01_repeated_measures.html#fitting-the-models",
    "title": "Analysis Walkthrough 1",
    "section": "Fitting the models",
    "text": "Fitting the models\n\n\nCode\nlibrary(lme4)\n\n\nHere we run an empty model so that we have something to compare our model which includes our independent variable. Other than to give us a reference model, we do not have a huge amount of interest in this. It includes no predictors, but a random intercept by participant (pid) to take account of the fact we have three measurements per person.\n\n\nCode\nm0 &lt;- lmer(speed ~ 1 + (1 | pid), data = simRPT)\n\n\nNext, add a fixed effect of our predictor (music condition, music). First though, we’ll want to re-level it so that “speech” is the reference level (because that’s what we said we wanted).\n\n\nCode\nsimRPT &lt;-\n  simRPT %&gt;%\n  mutate(\n    music = fct_relevel(factor(music), \"speech\")\n  )\n\nm1 &lt;- lmer(speed ~ 1 + music + (1 | pid), data = simRPT)\nsummary(m1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: speed ~ 1 + music + (1 | pid)\n   Data: simRPT\n\nREML criterion at convergence: 895.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.47567 -0.56300 -0.01549  0.54691  2.42387 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n pid      (Intercept) 20.32    4.508   \n Residual             13.54    3.679   \nNumber of obs: 150, groups:  pid, 50\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)     29.0408     0.8229  35.290\nmusicclassical   5.2048     0.7358   7.073\nmusicrap        -0.5858     0.7358  -0.796\n\nCorrelation of Fixed Effects:\n            (Intr) msccls\nmusicclsscl -0.447       \nmusicrap    -0.447  0.500\n\n\nAnd we can compare our models. A Kenward-Rogers F ratio suggests that we appear to have a significant differences in speeds between conditions.\n\n\nCode\nlibrary(pbkrtest)\nKRmodcomp(m1, m0)\n\n\nlarge : speed ~ 1 + music + (1 | pid)\nsmall : speed ~ 1 + (1 | pid)\n       stat   ndf   ddf F.scaling   p.value    \nFtest 37.53  2.00 98.00         1 7.913e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "example_01_repeated_measures.html#check-model",
    "href": "example_01_repeated_measures.html#check-model",
    "title": "Analysis Walkthrough 1",
    "section": "Check model",
    "text": "Check model\nThe residuals look reasonably normally distributed, and there seems to be fairly constant variance across the linear predictor. We might be a little concerned about the potential tails of the plot below, at which residuals don’t appear to have a mean of zero\n\n\nCode\nplot(m1, type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\nCode\nlibrary(lattice)\nqqmath(m1)\n\n\n\n\n\n\n\n\n\nRandom effects are (roughly) normally distributed:\n\n\nCode\nrans &lt;- as.data.frame(ranef(m1)$pid)\nggplot(rans, aes(sample = `(Intercept)`)) + \n  stat_qq() + stat_qq_line() +\n  labs(title=\"random intercept\")"
  },
  {
    "objectID": "example_02_intervention.html",
    "href": "example_02_intervention.html",
    "title": "Analysis Walkthrough 2",
    "section": "",
    "text": "Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:\n\nFirst the data and research context is introduced. For the purpose of these tutorials, we will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated.\nSecond, we go through any tidying of the data that is required, before creating some brief descriptives and visualizations of the raw data.\nThen, we conduct an analysis. Where possible, we translate the research questions into formal equations prior to fitting the models in lme4. Model comparisons are conducted, along with checks of distributional assumptions on our model residuals.\nFinally, we visualize and interpret our analysis.\n\nPlease note that there will be only minimal explanation of the steps undertaken here, as these pages are intended as example analyses rather than additional labs readings. Please also be aware that there are many decisions to be made throughout conducting analyses, and it may be the case that you disagree with some of the choices we make here. As always with these things, it is how we justify our choices that is important. We warmly welcome any feedback and suggestions to improve these examples: please email ppls.psych.stats@ed.ac.uk."
  },
  {
    "objectID": "example_02_intervention.html#equations",
    "href": "example_02_intervention.html#equations",
    "title": "Analysis Walkthrough 2",
    "section": "Equations",
    "text": "Equations\nAs we want to assess how the change in stress differs between two groups, we’re looking at an interaction of time * group. But this interaction is across levels (i.e. group is a participant level variable, and time is observation level).\nAt the observation level, we model stress as a function of time. Time is a 3-level categorical, so we have two coefficients.\nWe are allowing the intercept to vary across participants (i.e. participants differ in how stressed they are), and we also model this intercept as differing depending on whether someone is in the treatment or control group.\nSimilarly, the two coefficients for time are also going to be modelled as a function of the group that the participant is in.\n\\[\n\\begin{aligned}\n&\\text{for timepoint }j \\text{ from participant } i \\\\\n  \\operatorname{stress}_{i[j]}  &= \\beta_{0i} + \\beta_{1i}(\\operatorname{timeDuring}_j) + \\beta_{2i}(\\operatorname{timePost}_j) + \\varepsilon_{i[j]} \\\\\n  \\beta_{0i} &= \\gamma_{00} + \\gamma_{01}(\\operatorname{groupTreatment}_i) + \\zeta_{0i} \\\\\n  \\beta_{1i} &= \\gamma_{10} + \\gamma_{11}(\\operatorname{groupTreatment}_i) \\\\\n  \\beta_{2i} &= \\gamma_{20} + \\gamma_{21}(\\operatorname{groupTreatment}_i) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "example_02_intervention.html#fitting-the-models",
    "href": "example_02_intervention.html#fitting-the-models",
    "title": "Analysis Walkthrough 2",
    "section": "Fitting the models",
    "text": "Fitting the models\n\n\nCode\nlibrary(lme4)\n\n\nBase model:\n\n\nCode\nm0 &lt;- lmer(stress ~ 1 +\n             (1 | ppt), data = simMIX)\nsummary(m0)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1286.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.02099 -0.49670  0.01511  0.49445  1.96548 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 179.7    13.40   \n Residual             209.7    14.48   \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   52.220      2.234   23.37\n\n\nMain effects:\n\n\nCode\nm1 &lt;- lmer(stress ~ 1 + time + group +\n             (1 | ppt), data = simMIX)\nsummary(m1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + time + group + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1185.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.69666 -0.62649  0.01574  0.59245  1.92387 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 166.57   12.91   \n Residual              98.01    9.90   \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)      71.100      3.046  23.344\ntimeDuring      -13.760      1.980  -6.950\ntimePost        -20.980      1.980 -10.596\ngroupTreatment  -14.600      3.992  -3.657\n\nCorrelation of Fixed Effects:\n            (Intr) tmDrng timPst\ntimeDuring  -0.325              \ntimePost    -0.325  0.500       \ngroupTrtmnt -0.655  0.000  0.000\n\n\nInteraction:\n\n\nCode\nm2 &lt;- lmer(stress ~ 1 + time + group + time*group +\n             (1 | ppt), data = simMIX)\nsummary(m2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + time + group + time * group + (1 | ppt)\n   Data: simMIX\n\nREML criterion at convergence: 1096.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.83946 -0.53326 -0.05639  0.53766  2.31546 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 184.82   13.595  \n Residual              43.26    6.577  \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)                 62.840      3.020  20.805\ntimeDuring                  -3.200      1.860  -1.720\ntimePost                    -6.760      1.860  -3.634\ngroupTreatment               1.920      4.272   0.449\ntimeDuring:groupTreatment  -21.120      2.631  -8.028\ntimePost:groupTreatment    -28.440      2.631 -10.810\n\nCorrelation of Fixed Effects:\n            (Intr) tmDrng timPst grpTrt tmDr:T\ntimeDuring  -0.308                            \ntimePost    -0.308  0.500                     \ngroupTrtmnt -0.707  0.218  0.218              \ntmDrng:grpT  0.218 -0.707 -0.354 -0.308       \ntmPst:grpTr  0.218 -0.354 -0.707 -0.308  0.500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compare all models with a parametric bootstrap likelihood ratio test:\n\n\nCode\nlibrary(pbkrtest)\nPBmodcomp(m1, m0)\nPBmodcomp(m2, m1)\n\n\n\n\nBootstrap test; time: 34.26 sec; samples: 1000; extremes: 0;\nlarge : stress ~ 1 + time + group + (1 | ppt)\nstress ~ 1 + (1 | ppt)\n         stat df   p.value    \nLRT    90.348  3 &lt; 2.2e-16 ***\nPBtest 90.348     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBootstrap test; time: 33.27 sec; samples: 1000; extremes: 0;\nlarge : stress ~ 1 + time + group + time * group + (1 | ppt)\nstress ~ 1 + time + group + (1 | ppt)\n         stat df   p.value    \nLRT    83.846  2 &lt; 2.2e-16 ***\nPBtest 83.846     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd extract some bootstrap 95% CIs\n\n\nCode\nconfint(m2,method=\"boot\")\n\n\n\n\n                                  2.5 % 97.5 %\n                            0.00  10.85  16.59\n                            0.00   5.62   7.39\n(Intercept)                62.84  56.96  68.90\ntimeDuring                 -3.20  -6.60  -0.12\ntimePost                   -6.76 -10.47  -3.13\ngroupTreatment              1.92  -7.00  10.05\ntimeDuring:groupTreatment -21.12 -26.40 -16.11\ntimePost:groupTreatment   -28.44 -33.67 -23.26"
  },
  {
    "objectID": "example_02_intervention.html#check-model",
    "href": "example_02_intervention.html#check-model",
    "title": "Analysis Walkthrough 2",
    "section": "Check Model",
    "text": "Check Model\nThe residuals look reasonably normally distributed, and there seems to be fairly constant variance across the linear predictor. We might be a little concerned about the potential tails of the plot below, at which residuals don’t appear to have a mean of zero\n\n\nCode\nplot(m2, type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\nCode\nplot(m2, sqrt(abs(resid(.)))~fitted(.))\n\n\n\n\n\n\n\n\n\nCode\nlibrary(lattice)\nqqmath(m2)\n\n\n\n\n\n\n\n\n\nRandom effects are (roughly) normally distributed:\n\n\nCode\nrans &lt;- as.data.frame(ranef(m2)$ppt)\nggplot(rans, aes(sample = `(Intercept)`)) + \n  stat_qq() + stat_qq_line() +\n  labs(title=\"random intercept\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DAPR3",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 3 (DAPR3) lab workbook. Using the menu above, you can find lab materials for each week. These include sets of exercises along with walkthrough readings in which we introduce some of the more important R code. It is strongly recommended that students have taken Data Analysis for Psychology in R 1 and 2 (DAPR1 & DAPR2)."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Welcome to DAPR3",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Welcome to DAPR3",
    "section": "Tips on googling statistics and R",
    "text": "Tips on googling statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Welcome to DAPR3",
    "section": "Feedback on labs",
    "text": "Feedback on labs\nIf you wish to make suggestions for improvements to these workbooks, please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin toss. For a fair coin, the chance of getting a heads/tails for any given toss is 0.5.\nWe can simulate the number of “heads” in a single fair coin toss with the following code (because it is a single toss, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 0\n\n\nWe can simulate the number of “heads” in 8 fair coin tosses with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 4\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin tosses? Answer: 4! Doing another 8 tosses:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 3\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin tosses, but to do 1000 sets of 8 tosses:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  3  26  99 253 283 214  96  24   2"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nWe can get to the probability of observing \\(k\\) heads in 8 tosses of a fair coin using dbinom().\nLet’s calculate the probability of observing 2 heads in 8 tosses.\nAs coin tosses are independent, we can calculate probability using the product rule (“\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent). So the probability of observing 2 heads in 2 tosses is \\(0.5 \\cdot 0.5 = 0.25\\):\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 tosses, those two heads could occur in various ways:\n\n\n# A tibble: 10 × 1\n   `Ways to get 2 heads in 8 tosses`\n   &lt;chr&gt;                            \n 1 HTTHTTTT                         \n 2 TTTTTTHH                         \n 3 TTTTHTTH                         \n 4 TTTTHHTT                         \n 5 TTTHTHTT                         \n 6 THTTHTTT                         \n 7 TTHTTHTT                         \n 8 THTTTTHT                         \n 9 HTTTHTTT                         \n10 ...                              \n\n\nIn fact there are 28 different ways this could happen:\n\ndim(combn(8, 2))\n\n[1]  2 28\n\n\nThe probability of getting 2 heads in 8 tosses of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin tosses (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given toss)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin tosses is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nFor likelihood, we are interested in hypotheses about our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?).\nTo consider these hypotheses, we need to observe some data, and so we need to have a given number of tosses, and a given number of heads. Whereas above we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given toss, for the likelihood we fix the number of heads observed, and can make statements about different possible parameters that might govern the coins behaviour.\nFor example, if we did observe 2 heads in 8 tosses, what is the likelihood of this data given various parameters?\nOur parameter can take any real number between from 0 to 1, but let’s do it for a selection:\n\nposs_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, poss_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin tosses, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). You can visualise this as below:\n\n\n\n\n\n\n\n\n\nFormalizing the intuition We have a stochastic process that takes discrete values (i.e. outcomes of tossing a coin 10 times). We calculated the probability of observing a particular set of outcomes (8 correct predictions) by making assumptions about the underlying stochastic process, that is, the probability that our test subject can correctly predict the outcome of the coin toss is (p) (e.g. 0.8). We also assumed implicitly that the coin tosses are independent."
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\). Given that we know that if we have a specific value for \\(\\theta\\), then \\(P(d | \\theta)\\) gives us the probability of observing \\(d\\), it follows that we would like to figure out what values of \\(\\theta\\) maximise \\(\\mathcal{L}(\\theta \\vert d) = P(d \\vert \\theta)\\), where \\(\\mathcal{L}(\\theta \\vert d)\\) is the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. In Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎"
  }
]