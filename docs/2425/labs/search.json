[
  {
    "objectID": "01a_clustered.html",
    "href": "01a_clustered.html",
    "title": "1. Clustered Data",
    "section": "",
    "text": "This reading:\n\nA refresher on the linear regression model\n\nAn introduction to clustered data\nWorking with clustered data (sample sizes, ICC, visualisations)"
  },
  {
    "objectID": "01a_clustered.html#clusters-clusters-everywhere",
    "href": "01a_clustered.html#clusters-clusters-everywhere",
    "title": "1. Clustered Data",
    "section": "Clusters clusters everywhere",
    "text": "Clusters clusters everywhere\nThe idea of observing “children in schools” is just one such example of clustering that we might come across. This same hierarchical data structure can be found in other settings, such as patients within medical practices, employees within departments, people within towns etc. These sort of groups are higher level observations that we might sample (i.e. I randomly sample 20 schools, and then from each school randomly sample 30 children). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a Repeated Measures study we have individual experimental trials clustered within participants. Longitudinal studies exhibit the same data structure but have time-ordered observations clustered within people.\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters4. Table 1 shows just a few examples of different levels of clustering that may arise from different types of study.\n\n\n\n\n\n\n\nTable 1:  Various different study designs will give rise to clustered data. \n  \n    \n       \n      Cross Sectional\n      Repeated Measures\n      Longitudinal\n    \n  \n  \n    Level n\n...\n...\n...\n    ...\n...\n...\n...\n    Level 3\nSchool\n...\nFamilies\n    Level 2\nClassroom\nParticipants\nPeople\n    Level 1 (Observations)\nChildren\nExperimental Stimuli\nTime\n  \n  \n  \n\n\n\n\n\nThe common thread throughout all these designs is the hierarchy. At the lowest level of our hierarchy is the individual observed thing. For some designs, individual people might be the lowest observation level, for others, people might be the clusters (i.e. we have multiple data points per person)."
  },
  {
    "objectID": "01a_clustered.html#what-are-clusters",
    "href": "01a_clustered.html#what-are-clusters",
    "title": "1. Clustered Data",
    "section": "What are ‘clusters’?",
    "text": "What are ‘clusters’?\nAt the fundamental level, we are using the term ‘cluster’ here to refer to a grouping of observations. In fact, we will probably start using the terms “clusters” and “groups” interchangeably, so it’s worth taking a bit of time to try and understand the kind of groupings that we’re talking about (and how we think about them).\n\n\n“Clusters” are just “groups”.\n\nWhen we talk about clustered data, the groups we are discussing can be thought of as a random sample of higher level units.\n\nMore often than not, the specific group-differences are not of interest.\n\n\nContrast the idea of ‘clusters’ with how we think about other sorts of groupings. In a study that looks at “how do drugs placebo/aspirin/beta-blockers influence people’s heart rate?” (Figure 7 LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we’ll use the same drugs (they’re not just a random sample of drugs - the x-axis of our LH plot in Figure 7 will be the same).\nIf we are interested in “what is the average grade at GCSE?”, and we have children grouped into different schools (Figure 7 RH plot), we are probably not interested in all the specific differences between grades in Broughton High School vs Gryffe High School etc. If we were to run our study again, we don’t collect data from the same set of schools. We can view these schools as ‘clusters’ - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).\n\n\n\n\n\nFigure 7: Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\n\n\n\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the type of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as “mixed effects modelling”), which is a regression modelling technique that allows us to explore questions such as these (and many more).5\n\n\n\n\n\n\noptional “univariate”and “multivariate”\n\n\n\n\n\nIn “univariate” statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.\nIn “multivariate” statistics, there are more sources of variation. For the “children in schools” example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to illicit different responses from one another.\n\n\n\n\n\n\n\n\n\noptional: “Panel data”\n\n\n\n\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as ‘panel data’. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:\n\n\n\n\n\nFigure 8: Panels of data\n\n\n\n\n\n\n\n\n\nFigure 9: Panels of panels of data"
  },
  {
    "objectID": "01a_clustered.html#determining-sample-sizes",
    "href": "01a_clustered.html#determining-sample-sizes",
    "title": "1. Clustered Data",
    "section": "Determining Sample Sizes",
    "text": "Determining Sample Sizes\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different levels.\nIn the description of the SchoolMot data above we are told the relevant numbers:\n\n\n\n\n\n\n  \n    \n       \n      Unit\n      Sample Size\n    \n  \n  \n    Level 2\nSchool\n30\n    Level 1 (Observations)\nChildren\n900\n  \n  \n  \n\n\n\n\nWe can check this in our data:\n\nschoolmot &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n\n[1] 900\n\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n\n[1] 30\n\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from Calderglen High School and only 10 from Broughton High School, or do we have the same number in each?\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the summary() function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):\n\nschoolmot |&gt;\n  count(schoolid) |&gt;\n  summary()\n\n   schoolid               n     \n Length:30          Min.   :30  \n Class :character   1st Qu.:30  \n Mode  :character   Median :30  \n                    Mean   :30  \n                    3rd Qu.:30  \n                    Max.   :30"
  },
  {
    "objectID": "01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "href": "01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "title": "1. Clustered Data",
    "section": "ICC - Quantifying clustering in an outcome variable",
    "text": "ICC - Quantifying clustering in an outcome variable\nThe IntraClass Correlation Coefficient (ICC) is a measure of how much variation in a variable is attributable to the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol \\(\\rho\\):7\n\\[\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n\\]\nThis is illustrated in the Figure 10 below, in which our continuous outcome variable (children’s grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the “between-group variance” as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the “within-group variance” as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\n\nCode\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n\n\n\n\n\nFigure 10: Variance in grades between schools. Data from https://uoepsy.github.io/data/schoolmot.csv\n\n\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can extract it from a fitted model.\nIn the school motivation data (visualised above), it’s estimated that 22% of the variance in grades is due to school-related differences:\n\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n\n[1] 0.2191859\n\n\n\n\n\n\n\n\noptional: calculating ICC manually\n\n\n\n\n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC:\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n= & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\]\nSo we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a couple of new columns. The first is the overall mean of \\(y\\), and the second is the mean of \\(y\\) for each group. Note that we calculate this by first using group_by to make the subsequent operation (the mutate) be applied to each group. To ensure that the grouping does not persist after this, we’ve passed it to ungroup at the end.\n\nschoolmot &lt;- \n  schoolmot |&gt; \n  mutate(\n    grand_mean = mean(grade)\n  ) |&gt;\n  group_by(schoolid) |&gt;\n  mutate(\n    group_mean = mean(grade)\n  ) |&gt;\n  ungroup()\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nschoolmot &lt;- schoolmot |&gt; \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n\nFinally, we divide them by the degrees of freedom. Our degrees of freedom for our between group variance \\(30 \\text{ groups} - 1 \\text{ grand mean}=29\\)\nOur degrees of freedom for our within group variance is \\(900 \\text{ observations} - 30 \\text{ groups}=870\\)\n\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n\nAnd calculate the ICC!!!\nThe 29 here is the \\(k-1\\) in the formula above, where \\(k\\) is the number of observations within each group.\n\n# ICC\n(msb-mse) /(msb + (29*mse))\n\n[1] 0.2191859\n\n\n\n\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\nWe can think of it as the average correlation between all possible pairs of observations from the same group. Suppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (Figure 11). The ICC is the correlation between such pairs.\n\n\n\n\n\nFigure 11: ICC is the correlation of randomly drawn pairs from the same group\n\n\n\n\n\n\n\n\n\n\noptional: a little simulation\n\n\n\n\n\nWe can actually do the “randomly drawn pair of observations from the same group” via simulation.\nThe code below creates a function for us to use. Can you figure out how it works?\n\nget_random_pair &lt;- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n\nTry it out, by running it several times.\n\nget_random_pair()\n\n[1] 28.35 50.84\n\n\nNow let’s make our computer do it loads and loads of times:\n\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims &lt;- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims &lt;- t(sims)\ncor(sims[,1], sims[,2])\n\n[1] 0.2097805\n\n\n\n\n\n\n\n\n\n\n\noptional: correlations from group-structured data\n\n\n\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n# A tibble: 7 × 3\n  cluster observation y    \n* &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 group_1 1           4    \n2 group_1 2           2    \n3 group_2 1           4    \n4 group_2 2           2    \n5 group_3 1           7    \n6 group_3 2           5    \n7 ...     ...         ...  \n\n\nThe ICC for this data is 0.18.\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, they’re just random draws from a group.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen any of these:\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 5     7    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group.\n\n\n\n\nWhy ICC?\nThe ICC tells us the proportion of the total variability in an outcome variable that is attributable to the differences between groups/clusters. It ranges from 0 to 1.\nThis helps us to assess the appropriateness of using a multilevel approach. If the ICC is high, it suggests that a large amount of the variance is at the cluster level (justifying the use of multilevel modeling to account for this structure).\nThere are no cut-offs - the interpretation of ICC values is inherently field-specific, as what constitutes a high or low ICC depends on the nature of the outcome variable, and the hierarchical structure within a particular research context."
  },
  {
    "objectID": "01a_clustered.html#visualisations",
    "href": "01a_clustered.html#visualisations",
    "title": "1. Clustered Data",
    "section": "Visualisations",
    "text": "Visualisations\nWhen we’re visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. For instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens within a given school (see e.g. Figure 5 from earlier).\nSome useful ggplot tools here are:\n\nfacet_wrap() - make a separate little plot for each level of a grouping variable\nthe group aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n\n\nfacets\n\nggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point() +\n  facet_wrap(~schoolid)\n\n\n\n\n\n\n\n\n\n\ngroup\n\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)"
  },
  {
    "objectID": "01a_clustered.html#footnotes",
    "href": "01a_clustered.html#footnotes",
    "title": "1. Clustered Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nWith the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we can actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).↩︎\nor “mean squares residual”↩︎\nIt’s “turtles all the way down”↩︎\nDepending on the research question and design of the study, we may be only interested in things that occur at “level 1” (the lowest observation level). While not the focus of this course, there are alternative methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply “account for the nuisance clustering”.↩︎\nNote, this is not true for a set of analytical methods called “cluster analysis”, which attempts to identify clusters that haven’t been measured/observed (or may not even ‘exist’ in any real sense of the word).↩︎\nalthough this symbol get used for lots of other correlation-y things too!↩︎"
  },
  {
    "objectID": "01b_lmm.html",
    "href": "01b_lmm.html",
    "title": "2. Linear Mixed Models/Multi-level Models",
    "section": "",
    "text": "This reading:\n\nIntroducing the multilevel model (MLM)\nHow the MLM achieves partial pooling\nFitting multilevel models in R\nModel estimation and convergence\n\n\n\n\n\n\n\ndifferent names for the same thing\n\n\n\n\n\nThe methods we’re going to start to look at are known by lots of different names (see Figure 1). The core idea is that model parameters vary at more than one level..\n\n\n\n\n\nFigure 1: size weighted by hits on google scholar search (sept 2020)"
  },
  {
    "objectID": "01b_lmm.html#random-intercepts",
    "href": "01b_lmm.html#random-intercepts",
    "title": "2. Linear Mixed Models/Multi-level Models",
    "section": "random intercepts",
    "text": "random intercepts\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.1 Then, we can take a coefficient \\(b_?\\) and allow it to be different for each cluster \\(i\\) by adding the suffix \\(b_{?i}\\). Below, we have done this for our intercept \\(b_0\\), which has become \\(b_{0i}\\).\nHowever, we also need to define these differences in some way, and the multilevel model does this by expressing each cluster’s intercept as a deviation (\\(\\zeta_{0i}\\) for cluster \\(i\\), below) from a fixed number (\\(\\gamma_{00}\\), below). Because these differences are to do with the clusters (and not the individual observations within them), we often write these as a “level 2 equation”:\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\n\\color{red}y_{ij} &\\color{black}= \\color{green}b_{0i} \\color{blue} + b_1 \\cdot x_{ij} \\color{black}+ \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\n\\color{green}b_{0i} &\\color{black}= \\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nmixed-effects notation\n\n\n\n\n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:\n\\[\n\\color{red}y_{ij} \\color{black}= \\underbrace{(\\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i}\\color{black})}_{\\color{green}b_{0i}} \\cdot 1 + \\color{blue}b_{1} \\cdot x_{ij} \\color{black}+  \\varepsilon_{ij}\n\\]\nThis notation typically corresponds with the “mixed effects” terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:\n\\[\ny_{ij} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\underbrace{\\zeta_{0i}}_{\\textrm{random}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_1}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n\\]\n\n\n\nReturning to our school children’s grade example, we can fit a model with “random intercepts for schools”, which would account for some schools having higher grades, some having lower grades, etc.\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n\\] If we consider one of our schools (e.g. “Beeslack Community High School”) we can see that our model predicts that this school has higher grades than most other schools (Figure 3). We can see how this is modelled as a deviation \\(\\zeta_{0\\text{B}}\\) (B for Beeslack) from some fixed value \\(\\gamma_{00}\\).\n\n\n\n\n\nFigure 3: Fitted values from a multilevel model with random intercepts for schools\n\n\n\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of lm(grade ~ motiv + schoolid)), which would also estimate a difference for each cluster?\n\nThe key to the multilevel model is that we are not actually estimating the cluster-specific lines themselves (although we can get these out). We are estimating a distribution of deviations.\n\nSpecifically, the parameters of the multilevel model that are estimated are the mean and the variance of a normal distribution of clusters.\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n\n\n\na fixed intercept \\(\\gamma_{00}\\)\n\nthe variance with which schools deviate from the fixed intercept \\(\\sigma^2_0\\)\n\na fixed slope for motiv \\(b_1\\)\n\nand we also need the residual variance too \\(\\sigma^2_\\varepsilon\\)\n\n\n\n\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\nRemember, \\(\\sim N(m,s)\\) is a way of writing “are normally distributed with a mean of \\(m\\) and a standard deviation of \\(s\\)”. So the \\(\\zeta_{0i} \\sim N(0,\\sigma_0)\\) bit is saying that the school deviations from the fixed intercept are modelled as a normal distribution, with a mean of 0, and a standard deviation of \\(\\sigma_0\\) (which gets estimated by our model).\nThis can be seen in Figure 4 - the model is actually estimating a fixed intercept; a fixed slope; and the spread of a normal distribution of school-level deviations from the fixed intercept.\n\n\n\n\n\nFigure 4: grade predicted by motivation, with a by-school random intercept. The school-level intercepts are modelled as a normal distribution. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)."
  },
  {
    "objectID": "01b_lmm.html#random-slopes",
    "href": "01b_lmm.html#random-slopes",
    "title": "2. Linear Mixed Models/Multi-level Models",
    "section": "random slopes",
    "text": "random slopes\nIt is not just the intercept that we can allow to vary by-schools. We can also model cluster-level deviations from other coefficients (i.e. slopes). For instance, we can allow the slope of \\(x\\) on \\(y\\) to be different for each cluster, by specifying in our model that \\(b_{1i}\\) is a distribution of cluster deviations \\(\\zeta_{1i}\\) around the fixed slope \\(\\gamma_{10}\\).\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\end{align}\n\\]\nWhen we have random intercepts and random slopes, our assumption is that both of intercepts and slopes are normally distributed. However, we also typically allow these to be correlated, so the complicated looking bit at the bottom of the equation above is really just saying “random intercepts and slopes are normally distributed with mean of 0 and standard deviations of \\(\\sigma_0\\) and \\(\\sigma_1\\) respectively, and with a correlation of \\(\\rho \\sigma_0 \\sigma_1\\)”. We’ll see more on this in future weeks, so don’t worry too much right now.\nIn Figure 5, we can see now that both the intercept and the slope of grades across motivation are varying by-school.\n\n\n\n\n\nFigure 5: predicted values from the multilevel model that includes by-school random intercepts and by-school random slopes of motivation.\n\n\n\n\nMuch like for the random intercepts, we are modelling the random slopes as the distribution of school-level deviations \\(\\zeta_{1i}\\) around a fixed estimate \\(\\gamma_{10}\\).\nSo each group (school) now has, as visualised in Figure 6:\n\na deviation from the fixed intercept\na deviation from the fixed slope\n\n\n\n\n\n\nFigure 6: random intercepts and random slopes\n\n\n\n\nWhile it’s possible to show the distribution of intercepts on the left hand side of our grade ~ motiv plot, it’s hard to put the distribution of slopes on the same plot, so I have placed these in the bottom panel in Figure 7. We can see, for instance, that “Hutcheson’s Grammar School” has a higher intercept, but a lower slope.\n\n\n\n\n\nFigure 7: grade predicted by motivation, with by-school random intercepts and by-school random slopes of motivation. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)\n\n\n\n\n\n\n\n\n\n\noptional: joint distribution of intercept and slopes\n\n\n\n\n\nWhen we have random intercepts and slopes in our model, we don’t just estimate two separate distributions of intercept deviations and slope deviations. We estimate them as related. This comes back to the part of the equation we mentioned briefly above, where we used:\n\n\\(\\sigma_0\\) to represent the standard deviation of intercept deviations\n\\(\\sigma_1\\) to represent the standard deviation of slope deviations\n\\(\\rho \\sigma_0 \\sigma_1\\) to represent the correlation between intercept deviations and slope deviations\n\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\] For a visual intuition about this, see Figure 8, in which the x-axis is the intercept deviations, and the y-axis is the slope deviations. We can see that these are each distributed normally, but are negatively related (schools with higher intercepts tend to have slightly lower slopes).\n\n\n\n\n\nFigure 8: Intercept deviations (x axis) and slope deviations (y axis). One school is highlighted for comparison with previous plot of fitted values"
  },
  {
    "objectID": "01b_lmm.html#extracting-model-parameters",
    "href": "01b_lmm.html#extracting-model-parameters",
    "title": "2. Linear Mixed Models/Multi-level Models",
    "section": "Extracting model parameters",
    "text": "Extracting model parameters\nAlongside summary(), there are some useful functions in R that allow us to extract the parameters estimated by the model:\n\nfixed effects\nThe fixed effects represent the estimated average relationship within the entire sample of clusters.\n\nfixef(smod2)\n\n(Intercept)       motiv \n  29.233320    4.475717 \n\n\n\n\nrandom effect variances\nThe random effect variances represent the estimated spread with which clusters vary around the fixed effects\n\nVarCorr(smod2)\n\n Groups   Name        Std.Dev. Corr  \n schoolid (Intercept) 12.5745        \n          motiv        2.0708  -0.698\n Residual             11.8036"
  },
  {
    "objectID": "01b_lmm.html#making-model-predictions",
    "href": "01b_lmm.html#making-model-predictions",
    "title": "2. Linear Mixed Models/Multi-level Models",
    "section": "Making model predictions",
    "text": "Making model predictions\nWhile they are not computed directly in the estimation of the model, the cluster-specific deviations from fixed effects can be extracted from our models\n\nrandom effects\nOften referred to as the “random effects”, the deviations for each cluster from the fixed effects can be obtained using ranef().\nNote that each row is a cluster (a school, in this example), and the columns show the distance from the fixed effects. We can see that “Anderson High School” has an estimated intercept that is 7.07 higher than average, and an estimate slope of motivation that is 0.47 lower than average.\n\nranef(smod2)\n\n$schoolid\n                                        (Intercept)       motiv\nAnderson High School                     7.07164826 -0.46505592\nArdnamurchan High School                -7.26417838  0.70012536\nBalwearie High School                  -20.53626558  2.31397177\nBeeslack Community High School          18.63574795 -1.45057126\n...                                     ...          ...\nWe can also visualise all these using a handy function. This sort of visualisation is great for checking for peculiar clusters.\n\ndotplot.ranef.mer(ranef(smod2))\n\n$schoolid\n\n\n\n\n\n\n\n\n\n\n\ncluster coefficients\nRather than looking at deviations from fixed effects, we can calculate the intercept and slope for each cluster.\nFor example, if we are estimating that “Anderson High School” has an intercept that is 7.07 higher than average, and the average is 29.23, then we know that this has an intercept of 29.23 + 7.07 = 36.3.\nWe can get these out using coef()\n\ncoef(smod2)\n\n$schoolid\n                                    (Intercept)  motiv\nAnderson High School                36.304968    4.010661\nArdnamurchan High School            21.969141    5.175842\nBalwearie High School                8.697054    6.789689\nBeeslack Community High School      47.869068    3.025146\n...                                 ...          ...\n\n\nfixef() + ranef() = coef()"
  },
  {
    "objectID": "01b_lmm.html#a-more-complex-model",
    "href": "01b_lmm.html#a-more-complex-model",
    "title": "2. Linear Mixed Models/Multi-level Models",
    "section": "A more complex model",
    "text": "A more complex model\nThe models fitted in the reading thus far are fairly simple in that they only really have one predictor (a measure of a child’s education motivation, motiv), and our observations (children) happen to be clustered into groups (schools).\nHowever, the multilevel model can also allow us to study questions that we might have about features of those groups (i.e., things about the schools) and how those relate to observation-level variables (things about the children).\nFor instance, we might have questions that take the form:\n\n“does [Level-2 variable] predict [Level-1 outcome]?”\n\n“does [Level-2 variable] influence the relationship between [Level-1 predictor] and [Level-1 outcome]?\n\n(in our example, Level-1 = children, Level-2 = Schools).\nConsider, for example, if we want to investigate whether the relationship between children’s motivation levels and their grades is different depending upon the source of school funding (private vs state).\nAddressing such questions requires a different fixed effect structure in order to allow us to test the relevant estimate of interest. Specifically here we need the interaction between motiv and funding (private vs state).\nNote that this interaction is ‘cross-level’! It allows us to ask whether something about children (the grade~motiv relationship) depends upon something about the school they’re in (funding type).\n\nsmod3 &lt;- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), \n              data = schoolmot)\n\nNote, we cannot include funding in the random effects part of our model, because “the effect of funding on school grades” is something we assess by comparing between schools. We cannot think of that effect varying by-school because every school is either “private” or “state” funded. We never observe “Ardnamurchan High School” as anything other than “state” funded, so “the effect on grades of being state/private funded” does not exist for Ardnamurchan High School (and hence it is illogical to try and say that this effect varies between schools).\nOur additions to the fixed effects part here simply add in a couple of fixed terms to our model (the funding coefficient and the motiv:funding interaction coefficient). This means that in terms of our model structure, it is simply moving from the single line we had in Figure 7, to having two lines (one for “private” schools and one for “state” schools). The random effects are, as before, the variance in deviations of individual schools around these fixed estimates.\n\n\n\n\n\n\nModel equation\n\n\n\n\n\nThis model is not too much of an extension on our previous equation, but when we move to models with more than 2 levels (e.g., children in schools in districts), these equations can become very cumbersome.\nAdditionally, as you become more practiced at fitting multilevel models, you may well begin to think of these models in terms of the lmer() syntax in R, rather than in terms of the mathematical expressions.\nThis is absolutely fine, and you should feel free to ignore these equations if they are of no help to your understanding!\nBecause the funding variable is something we measure at Level 2 (schools), in most notations it gets placed in the level 2 equations:\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_{1i} \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} + \\gamma_{01} \\cdot \\text{Funding}_i\\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} + \\gamma_{11} \\cdot \\text{Funding}_i\\\\\n\\end{align}\n\\]\nIt is sometimes easier to think of this in the “mixed effects notation” we saw above, where we substitute the level 2 equations into the level 1 equation, and rearrange to get:\n\\[\n\\begin{align}\n&\\text{For Child }j\\text{ in School }i \\\\\n&\\text{grade}_{ij} = (\\gamma_{00} + \\zeta_{0i}) + \\gamma_{01} \\cdot \\text{Funding}_i + (\\gamma_{10} + \\zeta_{1i})\\cdot \\text{motiv}_{ij} + \\gamma_{11} \\cdot \\text{Funding}_i \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\noptional: an attempted visual explanation\n\n\n\n\n\nFigure 12 shows an attempted visual intuition of how the different parts of the model work:\n\n\n\n\n\nFigure 12: visual explanation of a model with a cross-level interaction\n\n\n\n\n\n\n\n\n\n\n\n\nmodel summary\n\nsummary(smod3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: grade ~ motiv * funding + (1 + motiv | schoolid)\n   Data: schoolmot\n\nREML criterion at convergence: 7083.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.08250 -0.67269  0.03043  0.63562  3.13012 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolid (Intercept) 105.126  10.253        \n          motiv         2.595   1.611   -0.48\n Residual             139.030  11.791        \nNumber of obs: 900, groups:  schoolid, 30\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         40.3143     4.6414   8.686\nmotiv                2.6294     0.8652   3.039\nfundingstate       -17.2531     5.7347  -3.009\nmotiv:fundingstate   2.8485     1.0591   2.689\n\nCorrelation of Fixed Effects:\n            (Intr) motiv  fndngs\nmotiv       -0.782              \nfundingstat -0.809  0.633       \nmtv:fndngst  0.639 -0.817 -0.773\n\n\n\n\nplot\nFor plotting the fixed effect estimates (which are often the bit we’re most interested in) from multilevel models, we can’t rely on using predict(), fitted() or augment(), as these return to us the cluster-specific predicted values.\nInstead, we need to use tools like the effects package that we saw at the end of the USMR course, that takes a fixed effect and averages over the other terms in the model:\n\nlibrary(effects)\neffect(term=\"motiv*funding\",mod=smod3,xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=motiv,y=fit,col=funding,fill=funding))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper),alpha=.3)"
  },
  {
    "objectID": "01b_lmm.html#convergence-warnings-singular-fits",
    "href": "01b_lmm.html#convergence-warnings-singular-fits",
    "title": "2. Linear Mixed Models/Multi-level Models",
    "section": "convergence warnings & singular fits",
    "text": "convergence warnings & singular fits\nThere are different algorithms that we can use to actually undertake the iterative estimation procedure, which we can apply by using different ‘optimisers’.\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nTechnical problems to do with model convergence and ‘singular fit’ come into play when the optimiser we are using either can’t find a suitable maximum, or gets stuck in a plateau, or gets stuck trying to move towards a number that we know isn’t possible.\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning when trying to fit a model, and in the coming weeks you will see plenty of warnings such as:\n\nA typical convergence warning:\n\n\nwarning(s): Model failed to converge with max|grad| = 0.0071877 (tol = 0.002, component 1)\n\nA singular fit:\n\n\nboundary (singular) fit: see ?isSingular\n\n\n\nDo not trust the results of a model that does not converge\n\nThere are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them), but for the time being, if lmer() gives you convergence errors or singular fits, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model.\n\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))"
  },
  {
    "objectID": "01b_lmm.html#footnotes",
    "href": "01b_lmm.html#footnotes",
    "title": "2. Linear Mixed Models/Multi-level Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome books use “cluster \\(j\\) &gt;&gt; observation \\(i\\)”, others use “cluster \\(i\\) &gt;&gt; observation \\(j\\)”. We use the latter here↩︎\nthis exact formula applies to the model with random intercepts, but the logic scales up when random slopes are added↩︎\nremember, variance = standard deviation squared↩︎\nit’s a bit like n-1 being in the denominator of the formula for standard deviation↩︎"
  },
  {
    "objectID": "02ex.html",
    "href": "02ex.html",
    "title": "Week 2 Exercises: Intro to MLM",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "02ex.html#footnotes",
    "href": "02ex.html#footnotes",
    "title": "Week 2 Exercises: Intro to MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "04a_ranef.html",
    "href": "04a_ranef.html",
    "title": "Random Effect Structures",
    "section": "",
    "text": "This reading:\n\nextending the multilevel model to encompass more complex random effect structures\nmodel building and common issues"
  },
  {
    "objectID": "04a_ranef.html#example-1-two-levels",
    "href": "04a_ranef.html#example-1-two-levels",
    "title": "Random Effect Structures",
    "section": "Example 1: Two levels",
    "text": "Example 1: Two levels\nBelow is an example of a study that has a similar structure to those that we’ve seen thus far, in which we have just two levels (observations that are grouped in some way).\n\n\nStudy Design\nSuppose, for instance, that we conducted an experiment on a sample of 20 staff members from the Psychology department to investigate effects of CBD consumption on stress over the course of the working week. Participants were randomly allocated to one of two conditions: the control group continued as normal, and the CBD group were given one CBD drink every day. Over the course of the working week (5 days) participants stress levels were measured using a self-report questionnaire.\nWe can see our data here:\n\npsychstress &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek1.csv\")\nhead(psychstress)\n\n# A tibble: 6 × 6\n  dept  pid   CBD   measure       day stress\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 Psych Holly N     Self-report     1 -0.417\n2 Psych Holly N     Self-report     2  0.924\n3 Psych Holly N     Self-report     3  0.634\n4 Psych Holly N     Self-report     4  1.21 \n5 Psych Holly N     Self-report     5  0.506\n6 Psych Tom   Y     Self-report     1 -0.557\n\n\n\n\nPlot\n\n\nCode\n# take the dataset, and make the x axis of our plot the 'day' variable, \n# and the y axis the 'stress' variable: \n# color everything by the CBD groups\nggplot(psychstress, aes(x = day, y = stress, col=CBD)) + \n  geom_point() + # add points to the plot\n  geom_line() + # add lines to the plot\n  facet_wrap(~pid) # split it by participant\n\n\n\n\n\n\n\n\n\n\n\nModel\nWe might fit a model that looks something like this:\n\n\nCode\nlibrary(lme4)\n# re-center 'day' so the intercept is day 1\npsychstress$day &lt;- psychstress$day-1 \n\n# fit a model of stress over time: stress~day\n# estimate differences between the groups in their stress change: day*CBD\n# people vary in their overall stress levels: 1|pid\n# people vary in their how stress changes over the week: day|pid\nm2level &lt;- lmer(stress ~ 1 + day * CBD + \n                  (1 + day | pid), data = psychstress)\n\n\nNote that there is a line in the model summary output just below the random effects that shows us the information about the groups, telling us that we have 100 observations that are grouped into 20 different participants’.\n\nsummary(m2level)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + day * CBD + (1 + day | pid)\n   Data: psychstress\n\nREML criterion at convergence: 127.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.17535 -0.65204 -0.02667  0.64622  1.81574 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n pid      (Intercept) 0.199441 0.44659      \n          day         0.004328 0.06579  0.02\n Residual             0.112462 0.33535      \nNumber of obs: 100, groups:  pid, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.13178    0.14329   0.920\nday          0.07567    0.03461   2.186\nCBDY        -0.08516    0.24221  -0.352\nday:CBDY    -0.19128    0.05851  -3.270\n\nCorrelation of Fixed Effects:\n         (Intr) day    CBDY  \nday      -0.339              \nCBDY     -0.592  0.201       \nday:CBDY  0.201 -0.592 -0.339"
  },
  {
    "objectID": "04a_ranef.html#example-2-three-level-nested",
    "href": "04a_ranef.html#example-2-three-level-nested",
    "title": "Random Effect Structures",
    "section": "Example 2: Three level Nested",
    "text": "Example 2: Three level Nested\nLet’s suppose that instead of simply sampling 20 staff members from the Psychology department, we instead went out and sampled lots of people from different departments across the University. The dataset below contains not just our 20 Psychology staff members, but also data from 220 other people from departments such as History, Philosophy, Art, etc..\n\nneststress &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_nested.csv\")\nhead(neststress)\n\n# A tibble: 6 × 6\n  dept  pid      CBD   measure       day stress\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 CMVM  Ryan     Y     Self-report     1  0.933\n2 CMVM  Ryan     Y     Self-report     2  0.997\n3 CMVM  Ryan     Y     Self-report     3  0.408\n4 CMVM  Ryan     Y     Self-report     4  0.581\n5 CMVM  Ryan     Y     Self-report     5  0.442\n6 CMVM  Nicholas Y     Self-report     1  0.138\n\n\nIn this case, we have observations that are grouped by participants, and those participants can be grouped into the department in which they work. Three levels of nesting!\nYou can see in the Figure 6 below that there is variation between departments (i.e. people working in Art are a bit more relaxed, Political Science and CMVM is stressful, etc), and then within each of those, there is variation between participants (i.e. some people working in Art are more stressed than other people in Art).\n\n\nCode\nggplot(neststress, aes(x=day, y=stress,col=CBD))+\n  # plot points\n  geom_point()+\n  # split by departments\n  facet_wrap(~dept)+\n  # make a line for each participant\n  geom_line(aes(group=pid),alpha=.3)+ \n  # plot the mean and SE for each day.\n  stat_summary(geom=\"pointrange\",col=\"black\")\n\n\n\n\n\nFigure 6: A longitudinal study in which participants are nested within department\n\n\n\n\nTo account for these multiple sources of variation, we can fit a model that says both ( ... | dept) (“things vary by department”) and ( ... | dept:pid) (“things vary by participants within departments”).\nSo a model might look something like this:\n\n# re-center 'day' so the intercept is day 1\nneststress$day &lt;- neststress$day-1\n\nmnest &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day * CBD | dept) +\n                (1 + day | dept:pid), data = neststress)\n\nNote that we can have different random slopes for departments vs those for participants. Our model above includes all random slopes that are feasible given the study design.\n\n\n\n\n\n\nexplanations of each random slope\n\n\n\n\n\n\nparticipants can vary in their baseline stress levels.\n\n(1 | dept:pid)\n\nparticipants can vary in how stress changes over the week. e.g., some participants might get more stressed over the week, some might get less stressed\n\n(days | dept:pid)\n\n\nparticipants cannot vary in how CBD changes their stress level. because each participant is either CBD or control, “the effect of CBD on stress” doesn’t exist for a single participant (and so can’t very between participants)\n\n(CBD | dept:pid)\n\n\nparticipants cannot vary in how CBD affects their changes in stress over the week. For the same reason as above.\n\n( day*CBD | dept:pid)\n\ndepartments can vary in their baseline stress levels.\n\n(1 | dept)\n\n\ndepartments can vary in how stress changes over the week.\n\n(days | dept)\n\ndepartments can vary in how CBD changes stress levels. because each department contains some participants in the CBD group and some in the control group, “the effect of CBD on stress” does exist for a given department, and so could vary between departments. e.g. Philosophers taking CBD get really relaxed, but CBD doesn’t affect Mathematicians that much.\n\n(CBD | dept)\n\n\ndepartments can vary in how CBD affects changes in stress over the week\n\n( day*CBD | dept)\n\n\n\n\n\nNote that the above model is a singular fit, but it gives us a better place to start simplifying from. If we remove the day*CBD interaction in the by-department random effects, we get a model that converges:\n\nmnest2 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | dept) +\n                (1 + day | dept:pid), data = neststress)\n\nAnd plot our fitted values\n\n\nCode\nlibrary(broom.mixed)\naugment(mnest2) |&gt; \n  ggplot(aes(x=day, y=.fitted, col=CBD))+\n    # split by departments\n    facet_wrap(~dept) + \n    # make a line for each participant\n    geom_line(aes(group=pid),alpha=.3)+\n    # average fitted value for CBD vs control:  \n    stat_summary(geom=\"line\",aes(col=CBD),lwd=1)\n\n\n\n\n\nFigure 7: Plot of fitted values of the model. Individual lines for each participant, facetted by department. Thicker lines represent the department average fitted values split by CBD group\n\n\n\n\nAnd we can see in our summary that there is a lot of by-department variation - departments vary in their baseline stress levels with a standard deviation of 0.81, and within departments, participants vary in baseline stress scores with a standard deviation of 0.38.\n\nsummary(mnest2)\n\n...\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n dept:pid (Intercept) 0.147661 0.38427             \n          day         0.012142 0.11019  -0.03      \n dept     (Intercept) 0.648410 0.80524             \n          day         0.001979 0.04449  -0.18      \n          CBDY        0.055388 0.23535   0.40 -0.22\n Residual             0.129765 0.36023             \nNumber of obs: 1200, groups:  dept:pid, 240; dept, 12\n...\nExamining ranef(mnest2) now gives us a list of dept:pid random effects, and then of dept random effects. We can plot them using dotplot.ranef.mer(), as seen below. From these, we can see for instance, that the effect of CBD is more negative for Theology, and Sociology and Maths have higher slopes of day. These map with the plot of fitted values we saw in Figure 7 - the department lines are going up more Math and Sociology than in other departments, and in Theology the blue CBD line is much lower relative to the red control line than in other departments.\n\ndotplot.ranef.mer(ranef(mnest2))$dept"
  },
  {
    "objectID": "04a_ranef.html#example-3-crossed",
    "href": "04a_ranef.html#example-3-crossed",
    "title": "Random Effect Structures",
    "section": "Example 3: Crossed",
    "text": "Example 3: Crossed\nForgetting about participants nested in departments, let’s return to our sample of 20 staff members from the Psychology department. In our initial study design, we had just one self report measure of stress each day for each person.\nHowever, we might just as easily have taken more measurements. i.e. on Day 1, we could have recorded Martin’s stress levels 10 times. Furthermore, we could have used 10 different measurements of stress, rather than just a self-report measure. We could measure his cortisol levels, blood pressure, heart rate variability, give him different questionnaires, ask an informant like his son to report his stress, and so on. And we could have done the same for everybody.\n\nstresscross &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_crossed.csv\")\nhead(stresscross)\n\n# A tibble: 6 × 6\n  dept  pid   CBD   measure          day stress\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 Psych Aja   N     Alpha-Amylase      1  0.269\n2 Psych Aja   N     Blood Pressure     1  0.855\n3 Psych Aja   N     Cortisol           1  0.278\n4 Psych Aja   N     EEQ                1  0.470\n5 Psych Aja   N     HRV                1 -0.404\n6 Psych Aja   N     Informant          1  0.774\n\n\nIn this case, we can group our participants in two different ways. For each participant we have 5 datapoints for each of 10 different measures of stress. So we have 5x10 = 50 observations for each participant. But if we group them by measure instead, then we have each measure 5 times for 20 participants, so 5x20 = 100 observations of each measure. And there is no hierarchy here - the “blood pressure” measure is the same measure for Martin as it is for Dan and Aja etc. It makes sense to think of by-measure variability as not being ‘within-participants’.\nThis means we can choose when plotting whether to split the plots by participants, with a different line for each measure (Figure 8), or split by measure with a different line for each participant (Figure 9)\n\n\nfacet = participant, line = measure\n\n\nCode\nggplot(stresscross, aes(x=day, y=stress, col=CBD))+\n  geom_point()+\n  #make a line for each measure\n  geom_line(aes(group=measure))+\n  facet_wrap(~pid)\n\n\n\n\n\nFigure 8: crossed designs with participants and measures. we can facet by participant and plot a line for each measure\n\n\n\n\n\n\nfacet = measure, line = participant\n\n\nCode\nggplot(stresscross, aes(x=day, y=stress, col=CBD))+\n  geom_point()+\n  # make a line for each ppt\n  geom_line(aes(group=pid))+\n  facet_wrap(~measure)\n\n\n\n\n\nFigure 9: crossed designs with participants and measures. we can facet by measure and plot a line for each participant\n\n\n\n\n\n\nWe can fit a model that therefore accounts for the by-participant variation (“things vary between participants”) and the by-measure variation (“things vary between measures”).\nSo a model might look something like this:\n\n# re-center 'day' so the intercept is day 1\nstresscross$day &lt;- stresscross$day-1\n\nmcross &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day * CBD | measure) +\n                (1 + day | pid), data = stresscross)\n\nNote that just as with the nested example above, we can have different random slopes for measures vs those for participants, depending upon what effects can vary given the study design.\nAs before, removing the interaction in the random effects achieves model convergence:\n\nmcross2 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | measure) +\n                (1 + day | pid), data = stresscross)\n\nAnd again we might plot our fitted values either of the ways we plotted our initial data in Figure 8 above, only with the .fitted values obtained from the augment() function:\n\n\nCode\naugment(mcross2) |&gt;\n  ggplot(aes(x=day, y=.fitted, col=CBD))+\n    geom_point()+\n    geom_line(aes(group=pid))+\n    facet_wrap(~measure)\n\n\n\n\n\n\n\n\n\nOur random effect variances show the estimated variance in different terms (the intercept, slopes of day, effect of CBD) between participants, and between measures.\nFrom the below it is possible to see, for instance, that there is considerable variability between how measures respond to CBD (they vary in the effect of CBD on stress with a standard deviation of 0.53)\n\nsummary(mcross2)\n\n...\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n pid      (Intercept) 0.316578 0.56265             \n          day         0.014693 0.12121  -0.51      \n measure  (Intercept) 0.087111 0.29515             \n          day         0.008542 0.09242   0.88      \n          CBDY        0.283635 0.53257  -0.10  0.11\n Residual             0.088073 0.29677             \nNumber of obs: 1000, groups:  pid, 20; measure, 10\n...\nAgain, our dotplots of random effects help to also show this picture. We can see that the measures of “blood pressure”, “alpha-amylase”, “cortisol”, and “HRV” all have more effects of CBD that are more negative. We can see this in our plot of fitted values - these measures look like CBD vs control differnce is greater than in other measures.\n\ndotplot.ranef.mer(ranef(mcross2))$measure"
  },
  {
    "objectID": "04ex.html",
    "href": "04ex.html",
    "title": "Week 4 Exercises: Nested and Crossed",
    "section": "",
    "text": "Data: gadeduc.csv\nThis is synthetic data from a randomised controlled trial, in which 30 therapists randomly assigned patients (each therapist saw between 4 and 30 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nThe data are available at https://uoepsy.github.io/data/msmr_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\nTable 1: Data Dictionary: msmr_gadeduc.csv\n\n\nvariable\ndescription\n\n\n\n\npatient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n\n\nvisit_0\nScore on the GAD7 at baseline\n\n\nvisit_1\nGAD7 at 1 month assessment\n\n\nvisit_2\nGAD7 at 2 month assessment\n\n\nvisit_3\nGAD7 at 3 month assessment\n\n\nvisit_4\nGAD7 at 4 month assessment\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nUh-oh… these data aren’t in the same shape as the other datasets we’ve been giving you..\nCan you get it into a format that is ready for modelling?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nIt’s wide, and we want it long.\n\nOnce it’s long. “visit_0”, “visit_1”,.. needs to become the numbers 0, 1, …\nOne variable (patient) contains lots of information that we want to separate out. There’s a handy function in the tidyverse called separate(), check out the help docs!\n\n\n\n\n\n\n\n\n\n1 - reshaping\n\n\n\nHere’s the data. We have one row per patient, but we have multiple observations for each patient across the columns..\n\ngeduc = read_csv(\"../../data/msmr_gadeduc.csv\")\nhead(geduc)\n\n# A tibble: 6 × 6\n  patient        visit_0 visit_1 visit_2 visit_3 visit_4\n  &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 OT_Control_1        26      27      27      27      27\n2 OT_Control_2        25      24      27      26      25\n3 OT_Treatment_3      26      27      24      23      22\n4 OT_Treatment_4      26      26      26      25      25\n5 ND_Control_1        25      25      25      25      25\n6 ND_Control_2        26      24      25      24      23\n\n\nWe can make it long by taking the all the columns from visit_0 to visit_4 and shoving their values into one variable, and keeping the name of the column they come from as another variable:\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\")\n\n# A tibble: 2,600 × 3\n   patient      visit     GAD\n   &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt;\n 1 OT_Control_1 visit_0    26\n 2 OT_Control_1 visit_1    27\n 3 OT_Control_1 visit_2    27\n 4 OT_Control_1 visit_3    27\n 5 OT_Control_1 visit_4    27\n 6 OT_Control_2 visit_0    25\n 7 OT_Control_2 visit_1    24\n 8 OT_Control_2 visit_2    27\n 9 OT_Control_2 visit_3    26\n10 OT_Control_2 visit_4    25\n# ℹ 2,590 more rows\n\n\n\n\n\n\n\n2 - time is numeric\n\n\n\nNow we know how to get our data long, we need to sort out our time variable (visit) and make it into numbers.\nWe can replace all occurrences of the string \"visit_\" with nothingness \"\", and then convert them to numeric.\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) \n\n# A tibble: 2,600 × 3\n   patient      visit   GAD\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 OT_Control_1     0    26\n 2 OT_Control_1     1    27\n 3 OT_Control_1     2    27\n 4 OT_Control_1     3    27\n 5 OT_Control_1     4    27\n 6 OT_Control_2     0    25\n 7 OT_Control_2     1    24\n 8 OT_Control_2     2    27\n 9 OT_Control_2     3    26\n10 OT_Control_2     4    25\n# ℹ 2,590 more rows\n\n\n\n\n\n\n\n3 - splitting up the patient variable\n\n\n\nFinally, we need to sort out the patient variable. It contains 3 bits of information that we will want to have separated out. It has the therapist (their initials), then the group (treatment or control), and then the patient number. These are all separated by an underscore “_“.\nThe separate() function takes a column and separates it into several things (as many things as we give it), splitting them by some user defined separator such as an underscore:\n\ngeduc_long &lt;- geduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |&gt;\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n\nAnd we’re ready to go!\n\ngeduc_long\n\n# A tibble: 2,600 × 5\n   therapist group   patient visit   GAD\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 OT        Control 1           0    26\n 2 OT        Control 1           1    27\n 3 OT        Control 1           2    27\n 4 OT        Control 1           3    27\n 5 OT        Control 1           4    27\n 6 OT        Control 2           0    25\n 7 OT        Control 2           1    24\n 8 OT        Control 2           2    27\n 9 OT        Control 2           3    26\n10 OT        Control 2           4    25\n# ℹ 2,590 more rows\n\n\n\n\n\n\nQuestion 2\n\n\nVisualise the data. Does it look like the treatment had an effect?\nDoes it look like it worked for every therapist?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nremember, stat_summary() is very useful for aggregating data inside a plot.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere’s the overall picture. The average score on the GAD7 at each visit gets more and more different between the two groups. The treatment looks effective..\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nLet’s split this up by therapist, so we can see the averages across each therapist’s set of patients.\nThere’s clear variability between therapists in how well the treatment worked. For instance, the therapists PT and GI don’t seem to have much difference between their groups of patients.\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\") +\n  facet_wrap(~therapist)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFit a model to test if the psychoeducational treatment is associated with greater improvement in anxiety over time.\n\n\n\n\n\n1 - fixed effects\n\n\n\nWe want to know if how anxiety (GAD) changes over time (visit) is different between treatment and control (group).\nHopefully this should hopefully come as no surprise1 - it’s an interaction!\n\nlmer(GAD ~ visit * group + ...\n       ...\n     data = geduc_long)\n\n\n\n\n\n\n2 - grouping structure\n\n\n\nWe have multiple observations for each of the 520 patients, and those patients are nested within 30 therapists.\nNote that in our data, the patient variable does not uniquely specify the individual patients. i.e. patient “1” from therapist “OT” is a different person from patient “1” from therapist “ND”. To correctly group the observations into different patients (and not ‘patient numbers’), we need to have therapist:patient.\nSo we capture therapist-level differences in ( ... | therapist) and the patients-within-therapist-level differences in ( ... | therapist:patient):\n\nlmer(GAD ~ visit * group + ...\n       ( ... | therapist) + \n       ( ... | therapist:patient),\n     data = geduc_long)\n\n\n\n\n\n\n3 - random effects\n\n\n\nNote that each patient can change differently in their anxiety levels over time - i.e. the slope of visit could vary by participant.\nLikewise, some therapists could have patients who change differently from patients from another therapist, so visit|therapist can be included.\nEach patient is in one of the two groups - they’re either treatment or control. So we can’t say that “differences in anxiety due to treatment varies between patients”, because for any one patient the “difference in anxiety due to treatment” is not defined in our study design.\nHowever, therapists see multiple different patients, some of which are in the treatment group, and some of which are in the control group. So the treatment effect could be different for different therapists!\n\nmod1 &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|therapist:patient),\n             geduc_long)\n\n\n\n\n\nQuestion 4\n\n\nFor each of the models below, what is wrong with the random effect structure?\n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\n\n\n\n\n\nSolution\n\n\n\n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\nThe patient variable doesn’t capture the different patients within therapists, so this actually fits crossed random effects and treats all data where patient==1 as from the same group (even if this includes several different patients’ worth of data from different therapists!)\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\nUsing the / here means we have the same random slopes fitted for therapists and for patients-within-therapists. but the effect of group can’t vary by patient, so this doesn’t work. hence why we need to split them up into (...|therapist)+(...|therapist:patient).\n\n\n\n\nQuestion 5\n\n\nLet’s suppose that I don’t want the psychoeducation treatment, I just want the standard therapy sessions that the ‘Control’ group received. Which therapist should I go to?\n\n\n\n\n\n\nHints\n\n\n\n\n\ndotplot.ranef.mer() might help here!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nIt would be best to go to one of the therapists WG, EQ, or EI…\nWhy? These therapists all have the most negative slope of visit:\n\ndotplot.ranef.mer(ranef(mod1))$therapist\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nRecreate this plot.\nThe faint lines represent the model estimated lines for each patient. The points and ranges represent our fixed effect estimates and their uncertainty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nyou can get the patient-specific lines using augment() from the broom.mixed package, and the fixed effects estimates using the effects package.\nremember you can pull multiple datasets into ggplot:\n\n\nggplot(data = dataset1, aes(x=x,y=y)) + \n  geom_point() + # points from dataset1\n  geom_line(data = dataset2) # lines from dataset2\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient)\n  ) |&gt;\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")"
  },
  {
    "objectID": "04ex.html#footnotes",
    "href": "04ex.html#footnotes",
    "title": "Week 4 Exercises: Nested and Crossed",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif it does, head back to where we learned about interactions in the single level regressions lm(). It’s just the same here.↩︎"
  },
  {
    "objectID": "05a_assump.html",
    "href": "05a_assump.html",
    "title": "3. Model Assumptions",
    "section": "",
    "text": "This reading:\n\nMultilevel model assumptions: random effects can be thought of as another level of residual!\nInfluence in multilevel models: influential observations and influential groups."
  },
  {
    "objectID": "05a_assump.html#level-1-residuals",
    "href": "05a_assump.html#level-1-residuals",
    "title": "3. Model Assumptions",
    "section": "Level 1 residuals",
    "text": "Level 1 residuals\nWe can get the level 1 (observation-level) residuals the same way we used to do for lm() - by just using resid() or residuals(). Additionally, there are a few useful techniques for plotting these which we have listed below:\n\n\nresid vs fitted\nWe can plot the residuals vs fitted model (just like we used to for lm()), and assess the extend to which the assumption holds that the residuals are zero mean. (we want the blue smoothed line to be fairly close to zero across the plot)\n\n# \"p\" below is for points and \"smooth\" for the smoothed line\nplot(jsmod, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\n\nscale-location\nAgain, like we can for lm(), we can also look at a scale-location plot. This is where the square-root of the absolute value of the residuals is plotted against the fitted values, and allows us to more easily assess the assumption of constant variance.\n(we want the blue smoothed line to be close to horizontal across the plot)\n\nplot(jsmod,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\n\nfacetted plots\nWe can also plot these “resid v fitted” and “scale-location” plots for each cluster, to check that our residual mean and variance is not related to the clusters:\n\nplot(jsmod,\n         form = resid(.) ~ fitted(.) | dept,\n         type = c(\"p\"))\n\n\n\n\n\n\n\n\n\nplot(jsmod,\n         form = sqrt(abs(resid(.))) ~ fitted(.) | dept,\n         type = c(\"p\"))\n\n\n\n\n\n\n\n\n\n\nresidual normality\nWe can also examine the normality the level 1 residuals, using things such as histograms and QQplots:\n(we want the datapoints to follow close to the diagonal line)\n\nqqnorm(resid(jsmod)); qqline(resid(jsmod))\n\n\n\n\n\n\n\n\n\nhist(resid(jsmod))"
  },
  {
    "objectID": "05a_assump.html#level-2-residuals",
    "href": "05a_assump.html#level-2-residuals",
    "title": "3. Model Assumptions",
    "section": "Level 2+ residuals",
    "text": "Level 2+ residuals\nThe second level of residuals in the multilevel model are actually just our random effects! We’ve seen them already whenever we use ranef()!\nTo get out these we often need to do a bit of indexing. ranef(model) will give us a list with an item for each grouping. In each item we have a set of columns, one for each thing which is varying by that grouping.\nBelow, we see that ranef(jsmod) gives us something with one entry, $dept, which contains 2 columns (the random intercepts and random slopes of payscale):\n\nranef(jsmod)\n\n$dept\n                                        (Intercept)    payscale\nAccounting                              -0.03045458 -0.19259376\nArchitecture and Landscape Architecture  0.29419381 -0.35855884\nArt                                     -0.29094345  0.15293285\nBusiness Studies                        -0.27858102  0.18008149\n...                                      ...         ... \nSo we can extract the random intercepts using ranef(jsmod)$dept[,1].\nAgain, we want normality of the random effects, so we can make more histograms or qqplots, for both the random intercepts and the random slopes:\ne.g., for the random intercepts:\n\nqqnorm(ranef(jsmod)$dept[,1]);qqline(ranef(jsmod)$dept[,1])\n\n\n\n\n\n\n\n\nand for the random slopes:\n\nqqnorm(ranef(jsmod)$dept[,2]);qqline(ranef(jsmod)$dept[,2])"
  },
  {
    "objectID": "05a_assump.html#model-simulations",
    "href": "05a_assump.html#model-simulations",
    "title": "3. Model Assumptions",
    "section": "model simulations",
    "text": "model simulations\nSometimes, a good global assessment of your model comes from how good a representation of the observed data it is. We can look at this in a cool way by simulating from our model a new set of values for the outcome. If we do this a few times over, and plot each ‘draw’ (i.e. set of simulated values), we can look at how well it maps to the observed set of values:\nOne quick way to do this is with the check_predictions() function from the performance package:\n\nlibrary(performance)\ncheck_predictions(jsmod, iterations = 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptional: doing it yourself\n\n\n\n\n\nDoing this ourself gives us a lot more scope to query differences between our observed vs model-predicted data.\nThe simulate() function will simulate response variable values for us.\nThe re.form = NULL bit is saying to include the random effects when making simulations (i.e. use the information about the specific clusters we have in our data). If we said re.form = NA it would base simulations on a randomly generated set of clusters with the associated intercept and slope variances estimated by our model.\n\nmodsim &lt;- simulate(jsmod, nsim = 200, re.form=NULL)\n\nTo get this plotted, we’ll have to do a bit of reworking, because it gives us a separate column for each draw. So if we pivot them longer we can make a density plot for each draw, and then add on top of that our observed scores:\n\n# take the simulations\nmodsim |&gt; \n  # pivot \"everything()\" (useful function to capture all columns),\n  # put column names into \"sim\", and the values into \"value\"\n  pivot_longer(everything(), names_to=\"sim\",values_to=\"value\") |&gt;\n  # plot them! \n  ggplot(aes(x=value))+\n  # plot a different line for each sim. \n  # to make the alpha transparency work, i need to use\n  # geom_line(stat=\"density\") rather than \n  # geom_density() (for some reason alpha applies to fill here)\n  geom_line(aes(group=sim), stat=\"density\", alpha=.1,\n            col=\"darkorange\") +\n  # finally, add the observed scores!  \n  geom_density(data = jsuni, aes(x=jobsat), lwd=1)\n\n\n\n\n\n\n\n\nHowever, we can also go further! We can pick a statistic, let’s use the IQR, and see how different our observed IQR is from the IQRs of a series of simulated draws.\nHere are 1000 simulations. This time I don’t care about simulating for these specific clusters, I just want to compare to random draws of clusters:\n\nsims &lt;- simulate(jsmod, nsim=1000, re.form=NA)\n\nThe apply() function (see also lapply, sapply ,vapply, tapply) is a really nice way to take an object, and apply a function to it. The number 2 here is to say “do it on each column”. If we had 1 it would be saying “do it on each row”.\nThis gives us the IQR of each simulation:\n\nsimsIQR &lt;- apply(sims, 2, IQR)\n\nWe can then ask what proportion of our simulated draws have an IQR smaller than our observed IQR? If the answer is very big or very small it indicates our model does not very represent this part of reality very well.\n\nmean(IQR(jsuni$jobsat)&gt;simsIQR)\n\n[1] 0.451"
  },
  {
    "objectID": "05b_writing.html",
    "href": "05b_writing.html",
    "title": "5. Writing",
    "section": "",
    "text": "This reading:\n\nA (non-exhaustive) checklist of things to think about/include when writing up analyses with multilevel models"
  },
  {
    "objectID": "05b_writing.html#the-sample-data",
    "href": "05b_writing.html#the-sample-data",
    "title": "5. Writing",
    "section": "The sample data",
    "text": "The sample data\nDescriptives of hierarchical data are sometimes a bit more difficult than when we don’t have any ‘levels’. Typically, what we are wanting to do is provide our readers with a picture of the characteristics of our sample. “Our sample” now refers to multiple levels, so we want to describe each of these. More often than not, one of these levels will be a bit more interesting to us as a population we are hoping to generalise to. In psychology we are usually interested in “people”, so if we have data that is multiple trials per participant, we would probably want to focus on describing the participants (the clusters) as the individual trials are something we exert control over as the experimenter. If each datapoint was a child and they were nested in schools, we would probably want to describe both the children and the schools that are in our sample.\nThe aim here is to provide a picture of our sample so that a reader can get a sense of how ‘transportable’ the findings are to different contexts. For instance, if participants in our study are all university students, then we want to be careful about thinking that the findings will apply in other populations (see e.g. “most people aren’t WEIRD”).\n\nA checklist\n\n\nwhat is the hierarchical data structure (how many levels, what is each level?)\nDescribe any data cleaning outlier/data removal prior to calculating descriptive statistics (these tend to be the impossible values - i.e. observations that you would never want in your data anyway)\nsample sizes: how many at each level?\n\nhow many lower-level within each higher level unit? (if this varies, provide an average, and possibly a min and a max)\n\nscales of measured variables\ndescriptive statistics of relevant variables that characterise your sample.\n\nthese should be computed at the level at which they were measured. For instance, if you have observations grouped by participant, mean(data$age) would give the average age of your observations (which isn’t meaningful, and would differ from the average age of your participants if you have a different number of observations for each participant).\n\nHow much of the variability in the outcome variable is attributable to the clustering? (i.e. ICC)"
  },
  {
    "objectID": "05b_writing.html#the-methods",
    "href": "05b_writing.html#the-methods",
    "title": "5. Writing",
    "section": "The methods",
    "text": "The methods\nWhen writing up any statistical analysis, one important thing to keep in mind is transparency in the decisions and actions taken in the analysis process. The aim is to avoid a reader wondering “how did they end up with these results?”. Ideally, another researcher would be able to reproduce your analysis based on your explanation of what you have done.\nWith multilevel models, there’s a lot of choices that we make - the scaling and centering of variables, models being fitted with ML vs REML, the method used to conduct inference, and so on. In addition, in the event that we arrived at our final model after a series of non-converging models that were then simplified, we would ideally explain this process.\n\nA checklist\n\n\nDescribe any transformations to the data that are made prior to conducting the analysis (e.g., you’ll often re-center a time variable)\nDescribe the process that led to your final model(s)\n\nClearly explain the structure of your initial model (e.g. this might be the ‘maximal model’), and if this failed to converge, explain what random effects were removed and in what order? if possible, explain why.\n\nState the software packages and versions used to fit models, along with the estimation method (ML/REML) and optimiser used.\n\nWhat is the structure of your final model(s)?\n\nYou don’t need to write a complicated mathematical equation for your model. Describing it in words is fine provided you’re clear. e.g. “the outcome variable Y was modelled using mixed effects regression with afixed effects including a main effect of A and B as well as their interaction. The random effects include a random intercept by participant”\nLinear/binomial/poisson/… - if not linear, what link function (e.g., logit, log) was used?\nSpecify all fixed effects.\nSpecify all random effects according to the sampling units (e.g. schools/children etc) with which they interact. Be careful to make sure it’s clear what slopes are for which groupings!.\n\n\nIt’s often useful to state clearly the relevant test/comparison/parameter estimate of interest, and link this explicitly to the research questions/hypotheses.\n\nAny model comparisons should be clearly stated so that the reader understands the structure of both models being compared.\nSpecify the methods used to conduct inference (e.g. LRT, bootstrap), and if relevant, explain why (e.g. Kenward Rogers might be used due to a small number of level 2 units)."
  },
  {
    "objectID": "05b_writing.html#the-results",
    "href": "05b_writing.html#the-results",
    "title": "5. Writing",
    "section": "The results",
    "text": "The results\nWriting up results will vary depending on the strategies employed. The important part is to highlight the relevant test/comparison that addresses the research aims, and explain what the result means with respect to the question at hand.\nAdditionally, be sure to take some time to understand what the estimate actually means (\\(p&lt;.05\\) is just a small part of the story). With models like these we are almost always just looking at outcome “differences” between levels of a categorical predictor or “change” across some continuous predictor. Does the estimated difference/change, and its direction, make sense to you? What does it mean practically? Asking yourself questions like this is also a good way of sense checking your analysis (i.e. a strong counter-intuitive finding could mean you have a variable coded back to front!).\nFor reporting parameter estimates, ideally we would include both the estimate and the precision (i.e. the standard error or a confidence interval). When reporting statistical tests, make sure to include the test statistic (\\(t\\), \\(F\\), \\(\\chi^2\\), etc.), the relevant degrees of freedom, and the p-value.\n\nA checklist\n\n\nresults of model comparisons and what they mean in the context of the research question\n\nparameter estimates and precision for relevant fixed effects.\n\nvariance components\n\nhow does the effect of interest vary between groups?\n\nis it related to other group level variance (i.e. the random effect correlations if modelled)\n\nif relevant - sensitivity to influential observations and clusters."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DAPR3",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 3 (DAPR3) lab workbook. Using the menu above, you can find lab materials for each week. These include sets of exercises along with walkthrough readings in which we introduce some of the more important R code. It is strongly recommended that students have taken Data Analysis for Psychology in R 1 and 2 (DAPR1 & DAPR2)."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Welcome to DAPR3",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Welcome to DAPR3",
    "section": "Tips on googling statistics and R",
    "text": "Tips on googling statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Welcome to DAPR3",
    "section": "Feedback on labs",
    "text": "Feedback on labs\nIf you wish to make suggestions for improvements to these workbooks, please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  }
]