<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>WEEK 2 Linear Mixed Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josiah King" />
    <script src="jk_libs/libs/header-attrs/header-attrs.js"></script>
    <link href="jk_libs/libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="jk_libs/libs/tile-view/tile-view.js"></script>
    <link href="jk_libs/libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="jk_libs/libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="jk_libs/libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="jk_libs/libs/clipboard/clipboard.min.js"></script>
    <link href="jk_libs/libs/shareon/shareon.min.css" rel="stylesheet" />
    <script src="jk_libs/libs/shareon/shareon.min.js"></script>
    <link href="jk_libs/libs/xaringanExtra-shareagain/shareagain.css" rel="stylesheet" />
    <script src="jk_libs/libs/xaringanExtra-shareagain/shareagain.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="jk_libs/tweaks.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <b>WEEK 2<br>Linear Mixed Models</b>
## Data Analysis for Psychology in R 3
### Josiah King
### Department of Psychology<br/>The University of Edinburgh
### AY 2021-2022

---







---
class: inverse, center, middle

&lt;h2&gt;Part 1: LM to LMM&lt;/h2&gt;
&lt;h2 style="text-align: left;opacity:0.3;"&gt;Part 2: Inference in LMM&lt;/h2&gt;
&lt;h2 style="text-align: left;opacity:0.3;"&gt;Part 3: Examples&lt;/h2&gt;


---
# Terminology



&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="jk_img_sandbox/mlmname.png" alt="(size weighted by hits on google scholar)" width="827" /&gt;
&lt;p class="caption"&gt;(size weighted by hits on google scholar)&lt;/p&gt;
&lt;/div&gt;



---
# Notation 
&lt;!-- $$ --&gt;
&lt;!-- \begin{align} --&gt;
&lt;!-- &amp; \text{for observation }i \\ --&gt;
&lt;!-- \quad \\ --&gt;
&lt;!-- &amp; \color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 \; + \; \beta_1 \cdot{} x_{i} } + \varepsilon_i \\ --&gt;
&lt;!-- \end{align} --&gt;
&lt;!-- $$ --&gt;
**Simple regression**  
.pull-left[
`\(\begin{align} &amp; \text{for observation }i \\ \quad \\ &amp; \color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 \; + \; \beta_1 \cdot{} x_{i} } + \varepsilon_i \\ \end{align}\)`
]


---
# Notation 

&lt;!-- $$ --&gt;
&lt;!-- \begin{align} --&gt;
&lt;!-- &amp; \text{for observation }j\text{ in group }i \\ --&gt;
&lt;!-- \quad \\ --&gt;
&lt;!-- &amp; \text{Level 1:} \\ --&gt;
&lt;!-- &amp; \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ --&gt;
&lt;!-- &amp; \text{Level 2:} \\ --&gt;
&lt;!-- &amp; \color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}} \\ --&gt;
&lt;!-- &amp; \color{blue}{\beta_{1i}} = \gamma_{10} + \color{orange}{\zeta_{1i}} \\ --&gt;
&lt;!-- \quad \\ --&gt;
&lt;!-- &amp; \text{Where:} \\ --&gt;
&lt;!-- &amp; \gamma_{00}\text{ is the population intercept, and }\color{orange}{\zeta_{0i}}\text{ is the deviation of group }i\text{ from }\gamma_{00} \\ --&gt;
&lt;!-- &amp; \gamma_{10}\text{ is the population slope, and }\color{orange}{\zeta_{1i}}\text{ is the deviation of group }i\text{ from }\gamma_{10} \\ --&gt;
&lt;!-- \end{align} --&gt;
&lt;!-- $$ --&gt;
**Multi-level**  
.pull-left[
`\(\begin{align} &amp; \text{for observation }j\text{ in group }i \\ \quad \\ &amp; \text{Level 1:} \\ &amp; \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ &amp; \text{Level 2:} \\ &amp; \color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}} \\ &amp; \color{blue}{\beta_{1i}} = \gamma_{10} + \color{orange}{\zeta_{1i}} \\ \quad \\ \end{align}\)`
]

--

.pull-right[
`\(\begin{align} &amp; \text{Where:} \\ &amp; \gamma_{00}\text{ is the population intercept}\\ &amp; \text{and  }\color{orange}{\zeta_{0i}}\text{ is the deviation of group }i\text{ from }\gamma_{00} \\ \qquad \\ &amp; \gamma_{10}\text{ is the population slope,}\\ &amp; \text{and }\color{orange}{\zeta_{1i}}\text{ is the deviation of group }i\text{ from }\gamma_{10} \\ \end{align}\)`
]

--

We are now assuming `\(\color{orange}{\zeta_0}\)`, `\(\color{orange}{\zeta_1}\)`, and `\(\varepsilon\)` to be normally distributed with a mean of 0, and we denote their variances as `\(\sigma_{\color{orange}{\zeta_0}}^2\)`, `\(\sigma_{\color{orange}{\zeta_1}}^2\)`, `\(\sigma_\varepsilon^2\)` respectively.   

The `\(\color{orange}{\zeta}\)` components also get termed the "random effects" part of the model, Hence names like "random effects model", etc.

---
# Notation 

**Multi-level/Mixed-effects**

Sometimes, you will see the levels collapsed into one equation, as it might make for more intuitive reading:

`\(\color{red}{y_{ij}} = \underbrace{(\gamma_{00} + \color{orange}{\zeta_{0i}})}_{\color{blue}{\beta_{0i}}} \cdot 1 + \underbrace{(\gamma_{10} + \color{orange}{\zeta_{1i}})}_{\color{blue}{\beta_{1i}}} \cdot x_{ij}  +  \varepsilon_{ij} \\\)`


--

.footnote[
**other notation to be aware of**  

- Many people use the symbol `\(u\)` in place of `\(\zeta\)`  

- Sometimes people use `\(\beta_{00}\)` instead of `\(\gamma_{00}\)`  

- In various resources, you are likely to see `\(\alpha\)` used to denote the intercept instead of `\(\beta_0\)`  

]

---
# Notation 

__Matrix form__

And then we also have the condensed matrix form of the model, in which the Z matrix represents the grouping structure of the data, and `\(\zeta\)` contains the estimated random deviations. 


`\(\begin{align} \color{red}{\begin{bmatrix} y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\ y_{32} \\ \end{bmatrix}} &amp; = \color{blue}{\begin{bmatrix} 1 &amp; x_{11} \\ 1 &amp; x_{12} \\ 1 &amp; x_{21} \\ 1 &amp; x_{22} \\1 &amp; x_{31} \\ 1 &amp; x_{32} \\ \end{bmatrix} \begin{bmatrix} \gamma_{00} \\ \beta_1 \\  \end{bmatrix}} &amp; + &amp; \color{orange}{ \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix} \begin{bmatrix}\zeta_{01} \\ \zeta_{02} \\ \zeta_{03} \end{bmatrix}} &amp; + &amp; \begin{bmatrix} \varepsilon_{11} \\ \varepsilon_{12} \\ \varepsilon_{21} \\ \varepsilon_{22} \\ \varepsilon_{31} \\ \varepsilon_{32} \end{bmatrix} \\ \qquad \\ \\ \color{red}{\boldsymbol y}\;\;\;\;\; &amp; = \qquad \mathbf{\color{blue}{X \qquad \;\;\boldsymbol \beta}} &amp; + &amp; \qquad \; \mathbf{\color{orange}{Z \qquad \;\;\;\;\; \boldsymbol \zeta}} &amp; + &amp; \;\;\;\varepsilon \\ \end{align}\)`

&lt;!-- $$ --&gt;
&lt;!-- \begin{align}  --&gt;
&lt;!-- \color{red}{ --&gt;
&lt;!-- \begin{bmatrix} --&gt;
&lt;!-- y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\ y_{32} \\ --&gt;
&lt;!-- \end{bmatrix} --&gt;
&lt;!-- } &amp; =  --&gt;
&lt;!-- \color{blue}{ --&gt;
&lt;!-- \begin{bmatrix} --&gt;
&lt;!-- 1 &amp; x_{11} \\ --&gt;
&lt;!-- 1 &amp; x_{12} \\ --&gt;
&lt;!-- 1 &amp; x_{21} \\ --&gt;
&lt;!-- 1 &amp; x_{22} \\ --&gt;
&lt;!-- 1 &amp; x_{31} \\ --&gt;
&lt;!-- 1 &amp; x_{32} \\ --&gt;
&lt;!-- \end{bmatrix}  --&gt;
&lt;!-- \begin{bmatrix}  --&gt;
&lt;!-- \gamma_{00} \\ \beta_1 \\   --&gt;
&lt;!-- \end{bmatrix} --&gt;
&lt;!-- }  --&gt;
&lt;!-- &amp; --&gt;
&lt;!-- + &amp; --&gt;
&lt;!-- \color{orange}{ --&gt;
&lt;!-- \begin{bmatrix}  --&gt;
&lt;!-- 1 &amp; 0 &amp; 0 \\  --&gt;
&lt;!-- 1 &amp; 0 &amp; 0 \\ --&gt;
&lt;!-- 0 &amp; 1 &amp; 0 \\ --&gt;
&lt;!-- 0 &amp; 1 &amp; 0 \\ --&gt;
&lt;!-- 0 &amp; 0 &amp; 1 \\ --&gt;
&lt;!-- 0 &amp; 0 &amp; 1 \\ --&gt;
&lt;!-- \end{bmatrix} --&gt;
&lt;!-- \begin{bmatrix}  --&gt;
&lt;!-- \zeta_{01} \\ \zeta_{02} \\ \zeta_{03}  --&gt;
&lt;!-- \end{bmatrix} --&gt;
&lt;!-- } --&gt;
&lt;!-- &amp; + &amp; --&gt;
&lt;!-- \begin{bmatrix}  --&gt;
&lt;!-- \varepsilon_{11} \\ \varepsilon_{12} \\ \varepsilon_{21} \\ \varepsilon_{22} \\ \varepsilon_{31} \\ \varepsilon_{32}  --&gt;
&lt;!-- \end{bmatrix} \\  --&gt;
&lt;!-- \qquad \\  --&gt;
&lt;!-- \\ --&gt;
&lt;!-- \color{red}{\boldsymbol y}\;\;\;\;\; &amp; = \qquad \mathbf{\color{blue}{X \qquad \;\;\boldsymbol \beta}} &amp; + &amp; \qquad \; \mathbf{\color{orange}{Z \qquad \;\;\;\;\; \boldsymbol \zeta}} &amp; + &amp; \;\;\;\varepsilon \\  --&gt;
&lt;!-- \end{align} --&gt;
&lt;!-- $$ --&gt;


---
# Some Data

.pull-left[

&gt; 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire. Eight of the schools were taking part in an initiative to specifically teach emotion regulation as part of the curriculum. Data were also gathered regarding the average hours each child slept per night.

]
.pull-right[

```r
crq &lt;- read_csv("https://uoepsy.github.io/data/crqdata.csv")
head(crq)
```

```
## # A tibble: 6 x 5
##   emot_dysreg   crq int     schoolid sleep
##         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;
## 1        0.73  2.28 Control school1  &lt;8hr 
## 2       -0.04  2.61 Control school1  &lt;8hr 
## 3       -1.06  3.21 Control school1  &lt;8hr 
## 4        0.84  2.14 Control school1  &lt;8hr 
## 5        0.09  1.44 Control school1  8hr+ 
## 6        1.6   1.38 Control school1  &lt;8hr
```
]

---
count: false
# Some Data

.pull-left[

&gt; 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire. Eight of the schools were taking part in an initiative to specifically teach emotion regulation as part of the curriculum. Data were also gathered regarding the average hours each child slept per night.


```r
schoolplots &lt;- 
  ggplot(crq, aes(x = crq, y = emot_dysreg, 
                  col = schoolid)) +
  geom_point()+
  facet_wrap(~schoolid) + 
  guides(col = FALSE) +
  labs(x = "Child Routines Questionnaire (CRQ)", 
       y = "Emotion Dysregulation Scale (EDS)") +
  themedapr3()

schoolplots
```
]
.pull-right[

![](dapr3_lec2_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

]

---
# ICC

.pull-left[


```r
library(ggridges)
ggplot(crq, aes(x = emot_dysreg, y = schoolid, 
                fill = schoolid)) +
  geom_density_ridges(jittered_points = TRUE, 
                      position = "raincloud", alpha = .4) +
  guides(fill=FALSE) + 
  themedapr3()
```

![](dapr3_lec2_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]
.pull-right[


```r
library(ICC)
ICCbare(schoolid, emot_dysreg, data = crq)
```

```
## [1] 0.3396
```

__Reminder:__ the Intraclass Correlation Coefficient is ratio of variance between clusters to the total variance (variance within + variance between).

]


---
# Linear Mixed Models in R

- **lme4** package (many others are available, but **lme4** is most popular).  

- `lmer()` function.  

- syntax is similar to `lm()`, in that we specify:   

    __*[outcome variable]*__ ~ __*[explanatory variables]*__, data = __*[name of dataframe]*__
    
- in `lmer()`, we add to this the random effect structure in parentheses:  

    __*[outcome variable]*__ ~ __*[explanatory variables]*__ + (__*[vary this]*__ | __*[by this grouping variable]*__), data = __*[name of dataframe]*__

---
# R: lm

.pull-left[

```r
lm_mod &lt;- lm(emot_dysreg ~ crq, data = crq)
summary(lm_mod)
```

```
## 
## Call:
## lm(formula = emot_dysreg ~ crq, data = crq)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.0023 -0.6013  0.0614  0.5749  2.3390 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
*## (Intercept)   1.7419     0.1719   10.14  &lt; 2e-16 ***
*## crq          -0.5921     0.0664   -8.92  7.9e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.823 on 168 degrees of freedom
## Multiple R-squared:  0.321,	Adjusted R-squared:  0.317 
## F-statistic: 79.5 on 1 and 168 DF,  p-value: 7.89e-16
```

]

--

.pull-right[

```r
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1)
```

![](dapr3_lec2_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

]

---
# R: Adding a random intercept

.pull-left[
vary the intercept by schools.

```r
library(lme4)
ri_mod &lt;- lmer(emot_dysreg ~ crq + 
                 (1 | schoolid), data = crq)
summary(ri_mod)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: emot_dysreg ~ crq + (1 | schoolid)
##    Data: crq
## 
## REML criterion at convergence: 353.2
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.3428 -0.7225  0.0009  0.6430  2.2893 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
*##  schoolid (Intercept) 0.334    0.578   
##  Residual             0.353    0.594   
## Number of obs: 170, groups:  schoolid, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
*## (Intercept)    1.776      0.185    9.61
## crq           -0.594      0.051  -11.64
## 
## Correlation of Fixed Effects:
##     (Intr)
## crq -0.669
```

]

--

.pull-right[

```r
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1) + 
  geom_line(aes(y=fitted(ri_mod)), col = "red", lwd=1)
```

![](dapr3_lec2_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
] 

---
# R: Adding a random slope

.pull-left[
vary the intercept and the effect (slope) of crq by schools

```r
rs_mod &lt;- lmer(emot_dysreg ~ crq + 
                 (1 + crq | schoolid), data = crq)
summary(rs_mod)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: emot_dysreg ~ crq + (1 + crq | schoolid)
##    Data: crq
## 
## REML criterion at convergence: 334.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.1279 -0.7009  0.0414  0.6645  2.1010 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
*##  schoolid (Intercept) 1.326    1.152         
*##           crq         0.152    0.390    -0.88
##  Residual             0.262    0.512         
## Number of obs: 170, groups:  schoolid, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
*## (Intercept)   1.7890     0.2858    6.26
*## crq          -0.6250     0.0996   -6.27
## 
## Correlation of Fixed Effects:
##     (Intr)
## crq -0.894
```

]

--

.pull-right[

```r
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1) + 
  geom_line(aes(y=fitted(ri_mod)), col = "red", lwd=1) + 
  geom_line(aes(y=fitted(rs_mod)), col = "orange", lwd=1)
```

![](dapr3_lec2_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
]

---
# Partial Pooling vs No Pooling

.pull-left[
Why not fit a fixed effect adjustment to the slope of x for each group?  
`lm(y ~ x * group)`?


```r
fe_mod &lt;- lm(emot_dysreg ~ crq * schoolid, data = crq)
```
]

.pull-right[

```r
schoolplots + 
  geom_line(aes(y=fitted(fe_mod)), col = "black", lwd=1)
```

![](dapr3_lec2_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;
]


---
# Partial Pooling vs No Pooling

.pull-left[
- We talked last week about how this results in a lot of output. With 20 schools, we get: intercept at reference school, adjustment for every other school, the effect of x at reference school, adjustment to effect of x for every other school. 
    
    ```r
    length(coef(fe_mod))
    ```
    
    ```
    ## [1] 40
    ```
- information is not combined in anyway (data from school `\(i\)` contributes to differences from reference school to school `\(i\)`, but nothing else. No overall estimates)

]

--

.pull-right[
![](dapr3_lec2_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]

---
# Understanding LMM output

.pull-left[

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: y ~ x + (1 + x | group)
##    Data: my_data
## 
## REML criterion at convergence: 334.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.1279 -0.7009  0.0414  0.6645  2.1010 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  group    (Intercept) 1.326    1.152         
##           x           0.152    0.390    -0.88
##  Residual             0.262    0.512         
## Number of obs: 170, groups:  group, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
*## (Intercept)   1.7890     0.2858    6.26
*## x            -0.6250     0.0996   -6.27
```
]
.pull-right[
&lt;img src="jk_img_sandbox/lmer2.png" width="1391" /&gt;
]

---
count: false
# Understanding LMM output

.pull-left[

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: y ~ x + (1 + x | group)
##    Data: my_data
## 
## REML criterion at convergence: 334.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.1279 -0.7009  0.0414  0.6645  2.1010 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
*##  group    (Intercept) 1.326    1.152         
*##           x           0.152    0.390    -0.88
##  Residual             0.262    0.512         
## Number of obs: 170, groups:  group, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   1.7890     0.2858    6.26
## x            -0.6250     0.0996   -6.27
```
]
.pull-right[
&lt;img src="jk_img_sandbox/lmer2a.png" width="1391" /&gt;
]

---
count: false
# Understanding LMM output

.pull-left[

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: y ~ x + (1 + x | group)
##    Data: my_data
## 
## REML criterion at convergence: 334.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.1279 -0.7009  0.0414  0.6645  2.1010 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
*##  group    (Intercept) 1.326    1.152         
*##           x           0.152    0.390    -0.88
##  Residual             0.262    0.512         
## Number of obs: 170, groups:  group, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
*## (Intercept)   1.7890     0.2858    6.26
*## x            -0.6250     0.0996   -6.27
```
]
.pull-right[
&lt;img src="jk_img_sandbox/lmer3.png" width="1391" /&gt;
]

---
count: false
# Understanding LMM output

.pull-left[

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: y ~ x + (1 + x | group)
##    Data: my_data
## 
## REML criterion at convergence: 334.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.1279 -0.7009  0.0414  0.6645  2.1010 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  group    (Intercept) 1.326    1.152         
##           x           0.152    0.390    -0.88
*##  Residual             0.262    0.512         
## Number of obs: 170, groups:  group, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   1.7890     0.2858    6.26
## x            -0.6250     0.0996   -6.27
```
]
.pull-right[
&lt;img src="jk_img_sandbox/lmer4.png" width="1391" /&gt;
]
---
# ICC in lmer

.pull-left[

```r
base_mod &lt;- lmer(emot_dysreg ~ 1 + (1 | schoolid), data = crq) 
summary(base_mod)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: emot_dysreg ~ 1 + (1 | schoolid)
##    Data: crq
## 
## REML criterion at convergence: 446.9
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -2.319 -0.664  0.048  0.580  2.514 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
*##  schoolid (Intercept) 0.334    0.578   
*##  Residual             0.664    0.815   
## Number of obs: 170, groups:  schoolid, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)    0.334      0.144    2.32
```
]
.pull-right[

```r
0.334 / (0.334 + 0.664)
```

```
## [1] 0.3347
```

Note: ICC is conditional on zero values of random-effects covariates.
In other words, it has computed the ICC based on a value of zero for the random slope variable(s), so any interpretation of the ICC is also based on a value of zero for the slope variable(s).


&lt;!-- Once we introduce random slopes/coefficients, things get more complicated. The ICC is no longer the same as the VPC, because the ICC will be a function of the variable(s) for which random slopes are specified. Therefore there can be an infinite number of values for the ICC is the variable in question is continuous, and as many as the number of levels if it is categorical or a count. Thus any interpretation of the ICC in a random slopes model becomes more difficult. Stata, for example, will calculate a single value for the ICC but in a random slopes model, this is accompanied by the warning: --&gt;



]

---
# Fixed vs Random

What is the difference? 

When specifying a random effects model, think about the data you have and how they fit in the following table:

| Criterion: | Repetition: &lt;br&gt; _If the experiment were repeated:_ | Desired inference: &lt;br&gt; _The conclusions refer to:_ |
|----------------|--------------------------------------------------|----------------------------------------------------|
| Fixed effects  | &lt;center&gt;Same levels would be used&lt;/center&gt;     |    &lt;center&gt;The levels used &lt;/center&gt;                                   |
| Random effects | &lt;center&gt;Different levels would be used&lt;/center&gt;   | &lt;center&gt;A population from which the levels used&lt;br&gt; are just a (random) sample&lt;/center&gt; |

--

- Sometimes, there isn't much variability in a specific random effect and to allow your model to fit it is common to just model that variable as a fixed effect. 

- Other times, you don't have sufficient data or levels to estimate the random effect variance, and you are forced to model it as a fixed effect. 




---
class: inverse, center, middle, animated, rotateInDownLeft

# A quick break

---
# LMM in a longitudinal setting

Researchers are interested in how cognition changes over time. 

.pull-left[

```r
cogtime &lt;- read_csv("https://uoepsy.github.io/data/cogtimerpm.csv")
cogtime &lt;- cogtime %&gt;% 
  mutate(across(c(participant, sexFemale, alc), factor))
head(cogtime, 12L)
```

```
## # A tibble: 12 x 6
##    visit_n sexFemale   cog y_bin participant alc  
##      &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;
##  1       1 1          56.1     1 1           1    
##  2       2 1          71.5     1 1           1    
##  3       3 1          68.9     1 1           0    
##  4       4 1          73.0     1 1           0    
##  5       5 1          59.4     1 1           0    
##  6       6 1          76.4     1 1           1    
##  7       7 1          72.1     1 1           1    
##  8       8 1          64.2     1 1           1    
##  9       9 1          74.3     1 1           0    
## 10      10 1          69.7     1 1           1    
## 11       1 1          82.2     1 2           1    
## 12       2 1          65.1     1 2           0
```

]

--

.pull-right[

```r
ggplot(cogtime, aes(x=visit_n, y = cog, col=participant))+
  geom_line(alpha = 0.5)+
  guides(col=FALSE)+
  scale_x_continuous(breaks=1:10)+
  themedapr3()
```

![](dapr3_lec2_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;
]

---
# LMM

__determining random effect structure__

- multiple data-points per participant: **1 | participant**

--

- explanatory variable of interest (`visit_n`) varies *within* participants: **visit_n | participant**

--

- allow by-participant intercepts to correlate with by-participant slopes: **1 + visit_n | participant**  
(more on this in future weeks)

--

__fitting the model__


```r
cogtime_model &lt;- lmer(cog ~ visit_n + (1 + visit_n | participant), data = cogtime)
```

---
# LMM

.pull-left[
__model output__


```r
summary(cogtime_model)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: cog ~ visit_n + (1 + visit_n | participant)
##    Data: cogtime
## 
## REML criterion at convergence: 1357
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -2.274 -0.663 -0.091  0.577  3.227 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev. Corr
##  participant (Intercept) 10.06    3.17         
##              visit_n      1.22    1.11     0.69
##  Residual                37.93    6.16         
## Number of obs: 200, groups:  participant, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)    68.56       1.18    58.2
## visit_n        -1.22       0.29    -4.2
## 
## Correlation of Fixed Effects:
##         (Intr)
## visit_n -0.019
```
]
.pull-right[
__raw data__  


```r
ggplot(cogtime, aes(x=visit_n, y = cog, col=participant))+
  geom_path(alpha = 0.5)+
  guides(col=FALSE)+
  scale_x_continuous(breaks=1:10)+
  themedapr3()
```

![](dapr3_lec2_files/figure-html/unnamed-chunk-32-1.png)&lt;!-- --&gt;
]

---
# Plotting the model
#### **sjPlot::plot_model()**

.pull-left[

```r
library(sjPlot)
plot_model(cogtime_model, type="pred")
```
]
--
.pull-right[

```
## $visit_n
```

![](dapr3_lec2_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;
]


---
# Plotting the model 
#### **effects::effect()**

.pull-left[

```r
library(effects)
as.data.frame(effect("visit_n",cogtime_model))
```

```
##   visit_n   fit    se lower upper
## 1       1 67.34 1.208 64.96 69.72
## 2       3 64.90 1.452 62.04 67.77
## 3       6 61.25 2.083 57.14 65.35
## 4       8 58.81 2.582 53.72 63.90
## 5      10 56.37 3.110 50.24 62.50
```


```r
as.data.frame(effect("visit_n",cogtime_model)) %&gt;%
  ggplot(.,aes(x=visit_n, y=fit))+
  geom_line()+
  geom_ribbon(aes(ymin=lower,ymax=upper), alpha=.3)+
  themedapr3()
```
]

--

.pull-right[
![](dapr3_lec2_files/figure-html/unnamed-chunk-37-1.png)&lt;!-- --&gt;
]

---
# Plotting the model
#### **broom.mixed::augment()** for subject fits

.pull-left[

```r
library(broom.mixed)
augment(cogtime_model)
```

```
## # A tibble: 200 x 14
##      cog visit_n participant .fitted  .resid   .hat .cooksd .fixed   .mu .offset
##    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1  56.1       1 1              69.7 -13.6   0.0782 2.25e-1   67.3  69.7       0
##  2  71.5       2 1              69.6   1.93  0.0684 3.86e-3   66.1  69.6       0
##  3  68.9       3 1              69.5  -0.611 0.0653 3.69e-4   64.9  69.5       0
##  4  73.0       4 1              69.3   3.62  0.0690 1.38e-2   63.7  69.3       0
##  5  59.4       5 1              69.2  -9.79  0.0793 1.18e-1   62.5  69.2       0
##  6  76.4       6 1              69.1   7.32  0.0964 8.33e-2   61.2  69.1       0
##  7  72.1       7 1              69.0   3.16  0.120  2.04e-2   60.0  69.0       0
##  8  64.2       8 1              68.8  -4.64  0.151  5.94e-2   58.8  68.8       0
##  9  74.3       9 1              68.7   5.65  0.188  1.20e-1   57.6  68.7       0
## 10  69.7      10 1              68.6   1.16  0.232  6.95e-3   56.4  68.6       0
## # … with 190 more rows, and 4 more variables: .sqrtXwt &lt;dbl&gt;, .sqrtrwt &lt;dbl&gt;,
## #   .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;
```

```r
ggplot(augment(cogtime_model), 
       aes(x=visit_n, y=.fitted,
           col=participant))+
  geom_line() +
  guides(col=FALSE)+
  themedapr3()
```
]

--

.pull-right[
![](dapr3_lec2_files/figure-html/unnamed-chunk-40-1.png)&lt;!-- --&gt;
]

---
# Summary

LM to LMM
notation
Random intercepts
Random slopes



---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

&lt;h2 style="text-align: left;opacity:0.3;"&gt;Part 1: LM to LMM&lt;/h2&gt;
&lt;h2&gt;Part 2: Inference in LMM&lt;/h2&gt;
&lt;h2 style="text-align: left;opacity:0.3;"&gt;Part 3: Examples&lt;/h2&gt;

---
# &lt;p&gt;&lt;/p&gt;

.pull-left[
you might have noticed...


```r
summary(cogtime_model)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: cog ~ visit_n + (1 + visit_n | participant)
##    Data: cogtime
## 
## REML criterion at convergence: 1357
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -2.274 -0.663 -0.091  0.577  3.227 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev. Corr
##  participant (Intercept) 10.06    3.17         
##              visit_n      1.22    1.11     0.69
##  Residual                37.93    6.16         
## Number of obs: 200, groups:  participant, 20
## 
## Fixed effects:
##             Estimate Std. Error t value
*## (Intercept)    68.56       1.18    58.2
*## visit_n        -1.22       0.29    -4.2
## 
## Correlation of Fixed Effects:
##         (Intr)
## visit_n -0.019
```
]
.pull-right[
![](jk_img_sandbox/wotnop.png)
]




---
# Why no p-values?

**Extensive debate about how best to test parameters from LMMs.**  

--

In simple LM, we test the reduction in residual SS (sums of squares), which follows an `\(F\)` distribution with a known `\(df\)`.
$$
`\begin{align}
F \qquad = \qquad \frac{MS_{model}}{MS_{residual}} \qquad = \qquad \frac{SS_{model}/df_{model}}{SS_{residual}/df_{residual}} \\
\quad \\
df_{model} = k \\
df_{residual} = n-k-1 \\
\end{align}`
$$

--

The `\(t\)`-statistic for a coefficient in a simple regression model is the square root of `\(F\)` ratio between models with and without that parameter. 

- Such `\(F\)` will have 1 numerator degree of freedom (and `\(n-k-1\)` denominator degrees of freedom).
- The analogous `\(t\)`-distribution has `\(n-k-1\)` degrees of freedom

---
# Why no p-values?

In LMM, the distribution of a test statistic when the null hypothesis is true is **unknown.**

--

&lt;!-- - Parameter estimates are not based on observed and expected mean squaresm but they are the ML estimates (which is good as it allows us to study much more broad scope of study designs) --&gt;

Under very specific conditions (normally distributed outcome variable, perfectly balanced designs), we can use an `\(F\)` distribution and correctly determine the denominator `\(df\)`.  

--

But for most situations:
  - unclear how to calculate denominator `\(df\)`
  - unclear whether the test statistics even follow an `\(F\)` distribution

---
# 1. `\(df\)` approximations: Satterthwaite

.pull-left[
- There are some suggested approaches to approximating the denominator `\(df\)`. 
&lt;br&gt;&lt;br&gt;
- Loading the package **lmerTest** will fit your models and print the summary with p-values approximated by the Satterthwaite method.

```r
library(lmerTest)
full_model &lt;- lmer(cog ~  1 + visit_n + (1 + visit_n | participant), data = cogtime)
summary(full_model)
```
]
.pull-right[

```
## Linear mixed model fit by REML. t-tests use Satterthwaite's method [
## lmerModLmerTest]
## Formula: cog ~ 1 + visit_n + (1 + visit_n | participant)
##    Data: cogtime
## 
## REML criterion at convergence: 1357
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -2.274 -0.663 -0.091  0.577  3.227 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev. Corr
##  participant (Intercept) 10.06    3.17         
##              visit_n      1.22    1.11     0.69
##  Residual                37.93    6.16         
## Number of obs: 200, groups:  participant, 20
## 
## Fixed effects:
##             Estimate Std. Error    df t value Pr(&gt;|t|)    
## (Intercept)    68.56       1.18 19.00    58.2  &lt; 2e-16 ***
## visit_n        -1.22       0.29 19.00    -4.2  0.00048 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##         (Intr)
## visit_n -0.019
```
]



---
# 1. `\(df\)` approximations: Kenward-Rogers

- The **pbkrtest** package implements the slightly more reliable Kenward-Rogers method. 


```r
library(pbkrtest)
restricted_model &lt;- lmer(cog ~ 1 + (1 + visit_n | participant), data = cogtime)
full_model &lt;- lmer(cog ~ 1 + visit_n + (1 + visit_n | participant), data = cogtime)
KRmodcomp(full_model, restricted_model)
```

```
## large : cog ~ 1 + visit_n + (1 + visit_n | participant)
## small : cog ~ 1 + (1 + visit_n | participant)
##       stat  ndf  ddf F.scaling p.value    
## Ftest 17.7  1.0 19.0         1 0.00048 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---
# 2. Likelihood ratio tests

.pull-left[
We can also conduct a Likelihood Ratio Test (LRT). 


```r
anova(restricted_model, full_model, test = "LRT")
```

```
## Data: cogtime
## Models:
## restricted_model: cog ~ 1 + (1 + visit_n | participant)
## full_model: cog ~ 1 + visit_n + (1 + visit_n | participant)
##                  npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## restricted_model    5 1382 1398   -686     1372                        
## full_model          6 1370 1390   -679     1358  13.2  1    0.00029 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

]

--

.pull-right[
- Compares the log-likelihood of two competing models.  
&lt;br&gt;&lt;br&gt;
{{content}}
]

--

- what is the "likelihood"?  
{{content}}

--

    - a function that associates to a parameter the probability (or probability density) of observing the given sample data.  
&lt;br&gt;&lt;br&gt;
{{content}}

&lt;!-- likelihood of theta given data. probability of data given theta --&gt;

--

- ratio of two likelihoods is asymptotically `\(\chi^2\)`-square distributed.
{{content}}

--

    - this means for finite samples it may be unreliable 


---
# 3. Parametric Bootstrap

.pull-left[
The idea here is that in order to do inference on the effect of (a) predictor(s), you 

1. fit the reduced model (without the predictors) to the data. 
{{content}}
]
--

2. 
  - a) simulate data from the reduced model
  - b) fit both the reduced and the full model to the simulated (null) data
  - c) compute some statistic(s), e.g. t-statistic of the relevant parameter.
{{content}}

--

3. Compare the parameter estimates obtained from fitting the full model to the data, to the "null distribution" constructed in step 2. 

--

.pull-right[
More robust.  
Easy to do with `PBmodcomp()` in the **pbkrtest** package.


```r
library(pbkrtest)
PBmodcomp(full_model, restricted_model)
```

```
## Bootstrap test; time: 25.39 sec; samples: 1000; extremes: 1;
## Requested samples: 1000 Used samples: 983 Extremes: 1
## large : cog ~ 1 + visit_n + (1 + visit_n | participant)
## cog ~ 1 + (1 + visit_n | participant)
##        stat df p.value    
## LRT    13.1  1 0.00029 ***
## PBtest 13.1    0.00203 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]
---
# Summary

Inference, df etc.


---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 2

---
class: inverse, center, middle

&lt;h2 style="text-align: left;opacity:0.3;"&gt;Part 1: LM to LMM&lt;/h2&gt;
&lt;h2 style="text-align: left;opacity:0.3;"&gt;Part 2: Inference in LMM&lt;/h2&gt;
&lt;h2&gt;Part 3: Examples&lt;/h2&gt;

---
# Data


```r
nursedf &lt;- read_csv("https://uoepsy.github.io/data/nurse_stress.csv")
nursedf &lt;- nursedf %&gt;% 
  mutate(across(c(hospital, expcon, gender, wardtype, hospsize), factor))
head(nursedf)
```

```
## # A tibble: 6 x 10
##   hospital expcon nurse   age gender experien stress Zstress wardtype   hospsize
##   &lt;fct&gt;    &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;   
## 1 1        1          1    36 0            11      7    2.07 general c… large   
## 2 1        1          2    45 0            20      7    2.07 general c… large   
## 3 1        1          3    32 0             7      7    2.07 general c… large   
## 4 1        1          4    57 1            25      6    1.04 general c… large   
## 5 1        1          5    46 1            22      6    1.04 general c… large   
## 6 1        1          6    60 1            22      6    1.04 general c… large
```


_The files nurses.csv contains three-level simulated data from a hypothetical study on stress in hospitals. The data are from nurses working in wards nested within hospitals. It is a cluster-randomized experiment. In each of 25 hospitals, four wards are selected and randomly assigned to an experimental and a control condition. In the experimental condition, a training program is offered to all nurses to cope with job-related stress. After the program is completed, a sample of about 10 nurses from each ward is given a test that measures job-related stress. Additional variables are: nurse age (years), nurse experience (years), nurse gender (0 = male, 1 = female), type of ward (0 = general care, 1 = special care), and hospital size (0 = small, 1 = medium, 2 = large)._  
(From https://multilevel-analysis.sites.uu.nl/datasets/ )


---
# test of a single parameter

&gt; After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much?

--


```r
mod1 &lt;- lmer(Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital), data = nursedf)
summary(mod1)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital)
##    Data: nursedf
## 
## REML criterion at convergence: 2218
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -2.950 -0.672  0.026  0.645  3.192 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  hospital (Intercept) 0.296    0.544   
##  Residual             0.484    0.696   
## Number of obs: 1000, groups:  hospital, 25
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  0.90568    0.14386    6.30
## experien    -0.05912    0.00638   -9.26
## age          0.01965    0.00320    6.15
## gender1     -0.46214    0.05045   -9.16
## expcon1     -0.76007    0.04403  -17.26
## 
## Correlation of Fixed Effects:
##          (Intr) expern age    gendr1
## experien  0.018                     
## age      -0.338 -0.817              
## gender1  -0.269  0.026 -0.006       
## expcon1  -0.165  0.000  0.015 -0.015
```

---
count:false
# test of a single parameter

&gt; After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much?

__Likelihood Ratio Test:__

```r
mod0 &lt;- lmer(Zstress ~ 1 + experien + age + gender + (1 | hospital), data = nursedf)
mod1 &lt;- lmer(Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital), data = nursedf)
anova(mod0, mod1, test="Chisq")
```

```
## Data: nursedf
## Models:
## mod0: Zstress ~ 1 + experien + age + gender + (1 | hospital)
## mod1: Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital)
##      npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## mod0    6 2461 2490  -1224     2449                        
## mod1    7 2202 2236  -1094     2188   261  1     &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---
count:false
# test of a single parameter

&gt; After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much?

__Parametric Bootstrap__ 

```r
mod0 &lt;- lmer(Zstress ~ 1 + experien + age + gender + (1 | hospital), data = nursedf)
mod1 &lt;- lmer(Zstress ~ 1 + experien + age + gender +  expcon + (1 | hospital), data = nursedf)
PBmodcomp(mod1, mod0)
```

```
## Bootstrap test; time: 22.55 sec; samples: 1000; extremes: 0;
## large : Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital)
## Zstress ~ 1 + experien + age + gender + (1 | hospital)
##        stat df p.value    
## LRT     261  1  &lt;2e-16 ***
## PBtest  261      0.001 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---
count:false
# test of a single parameter

&gt; After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, **by how much?**

__Bootstrap Confidence Intervals__

```r
confint(mod1, method="boot")
```

```r
cis = confint(mod1, method="boot")
cis
```

```
##                2.5 %   97.5 %
## .sig01       0.38048  0.70304
## .sigma       0.66348  0.72520
## (Intercept)  0.63698  1.19378
## experien    -0.07124 -0.04642
## age          0.01300  0.02635
## gender1     -0.56199 -0.36866
*## expcon1     -0.84796 -0.68266
```

---
count:false
# test of a single parameter

&gt; After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much?

Attendance of training programs on job-related stress was found to predict stress levels of nurses in 25 hospitals, beyond individual nurses' years of experience, age and gender (Parametric Bootstrap Likelihood Ratio Test statistic = 260.919, p&lt;.001). Having attended the training program was associated with a decrease in -0.7601 (Bootstrap 95% CI [-0.85, -0.68] ) standard deviations on the measure of job-related stress. 

---
# testing that several parameters are simultaneously zero

&gt; Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience? 

__Likelihood Ratio Test__

```r
mod0 &lt;- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf)
mod1 &lt;- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf)
anova(mod0, mod1, test="Chisq")
```

```
## Data: nursedf
## Models:
## mod0: Zstress ~ experien + age + gender + expcon + (1 | hospital)
## mod1: Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital)
##      npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)   
## mod0    7 2202 2236  -1094     2188                       
## mod1   10 2194 2243  -1087     2174    14  3     0.0029 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---
count:false
# testing that several parameters are simultaneously zero

&gt; Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience? 

__Kenward-Rogers `\(df\)`-approximation__

```r
mod0 &lt;- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf)
mod1 &lt;- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf)
anova(mod0, mod1, test="Chisq")
```

```
## Data: nursedf
## Models:
## mod0: Zstress ~ experien + age + gender + expcon + (1 | hospital)
## mod1: Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital)
##      npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)   
## mod0    7 2202 2236  -1094     2188                       
## mod1   10 2194 2243  -1087     2174    14  3     0.0029 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---
count:false
# testing that several parameters are simultaneously zero

&gt; Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience? 

__Parametric Bootstrap__

```r
mod0 &lt;- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf)
mod1 &lt;- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf)
PBmodcomp(mod1, mod0)
```

```
## Bootstrap test; time: 22.80 sec; samples: 1000; extremes: 4;
## large : Zstress ~ experien + age + gender + expcon + wardtype + hospsize + 
##     (1 | hospital)
## Zstress ~ experien + age + gender + expcon + (1 | hospital)
##        stat df p.value   
## LRT    13.8  3  0.0032 **
## PBtest 13.8     0.0050 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---
# testing random effects 

__are you sure you want to?__

- Justify the random effect structure based on study design, theory, and practicalities more than tests of significance.

- If needed, the __RLRsim__ package can test a single random effect (e.g. `lm()` vs `lmer()`).



```r
library(RLRsim)
mod0 &lt;- lm(stress ~ expcon + experien + age + gender + wardtype + hospsize, data = nursedf)
mod1 &lt;- lmer(stress ~ expcon + experien + age + gender + wardtype + hospsize + 
               (1 | hospital), data = nursedf)
exactLRT(m = mod1, m0 = mod0)
```

```
## 
## 	simulated finite sample distribution of LRT. (p-value based on 10000
## 	simulated values)
## 
## data:  
## LRT = 240, p-value &lt;2e-16
```


---
# Summary


---
class: inverse, center, middle, animated, rotateInDownLeft

# End 

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="jk_libs/macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
