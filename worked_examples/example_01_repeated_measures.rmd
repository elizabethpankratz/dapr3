---
title: 'Analysis Example: Repeated-measures'
output:
  html_document:
    code_folding: show
    theme: flatly
    toc: yes
    toc_depth: 3
    toc_float: yes
    css: assets/style-labs.css
---
```{r setup, include=F}
knitr::opts_chunk$set(message = F, warning = F, fig.align = 'center')
```

:::frame

Each of these pages provides an analysis run through for a different type of design. Each document is structured in the same way:  

  - First the data is introduced. For the purpose of these tutorials, I will only use examples where the data can be shared - either because it is from an open access publication, or because it is unpublished or simulated. 
  - Second, the structure of the data is discussed so that we can more easily see what data structure the design creates, and how this aligns to the variables in the data.
  - Third, we translate the research questions into formal equations, and then `lmer()` code. 
  - Finally, we will follow those through for our example data.

:::

# Overview

The idea behind "repeated measures" is that the same variable is measured on the same set of subjects over two or more time periods or under different conditions.  

The data used for this worked example are simulated to represent data from 50 participants, each measured at 3 different time-points on an outcome variable of interest. This is a fairly simple design, leading from a question such as "how does [dependent variable] change over time?"


This is a very simple way to simulate repeated measures data structure (with long data). There are a good number of other approaches, but this will do for now as you may well be familiar with all the functions involved: 
```{r}
set.seed(347)
library(tidyverse)
simRPT <- tibble(
  pid = factor(rep(paste("ID", 1:50, sep=""),each=3)),
  dv = rnorm(150,c(40,50,70),sd=5),
  iv = factor(rep(c("T1", "T2", "T3"), each=1, 50))
)
```

:::rtip
If you are unclear about any section of the code above, why not try running small bits of it in your console to see what it is doing?   
For instance, try running:

- `paste("ID", 1:50, sep="")`  
- `rep(paste("ID", 1:50, sep=""),each=3)`  
- `factor(rep(paste("ID", 1:50, sep=""),each=3))`  

:::


# Data structure

Because we simulated our data, it is already nice and tidy. Each observation is a row, and we have variable indicating participant id (`pid`).  
```{r}
head(simRPT)
```




# Equations

# Analysis

```{r}
library(lme4)
library(kableExtra)
```

## Describe
Let' see our summaries per time-point:

```{r}
sumRPT <- 
  simRPT %>%
  group_by(iv) %>%
  summarise(
    n = n_distinct(pid),
    mean.dv = round(mean(dv, na.rm=T),2),
    sd.dv = round(sd(dv, na.rm=T),2)
    )
sumRPT
```
We can make this a little prettier:
```{r}
library(knitr)
library(kableExtra)
kable(sumRPT) %>%
  kable_styling("striped")
```

Well...we knew what the answer was going to be, but there we have it, our scores improve across the three administrations of our test.

## Visualize Data
We can construct some simple plots showing distribution of the outcome variable at each level of the independent variable (iv): 

```{r}
simRPT %>% 
  ggplot(aes(x = iv, y = dv)) + 
  geom_violin() + 
  geom_jitter(alpha=.5,width=.1,height=0) + 
  labs(x="Time", y = "Test Score", 
       title="Scores across trials", 
       subtitle = "Violin Plots with (jittered) Observations")+
  theme_minimal()
```

So what does this show? Essentially we are plotting all responses at each point in time. The points are `jittered` so that they are not all overlaid on one another. The areas marked at each time point are mirrored density plots (i.e. they show the distribution of the scores at each point in time). 

If you want to get an intuitive sense of these plotted areas, look at them against the mean's and sd's per time point calculated above.

```{r}
simRPT %>% 
  ggplot(aes(as.numeric(iv), dv)) +  
  stat_summary(fun.data = mean_cl_boot, geom="ribbon", alpha=.3) + 
  stat_summary(fun.y = mean, geom="line") + 
  labs(x="Time", y = "Test Score", 
       title="Scores across trials", 
       subtitle = "Mean and Boostrapped 95% Confidence Intervals")
```

We can also show each participants' trajectory over time, by using the `group` aesthetic mapping. 
```{r}
simRPT %>%
  ggplot(aes(x = iv, y = dv)) +
  geom_point(size=3, alpha=.4)+
  geom_line(aes(group=pid), alpha = .2) +
  theme_minimal()
```



## Run models
Here we run an empty model so that we have something to compare our model which includes our iv. Other than to give us a reference model, we do not have a huge amount of interest in this.

```{r}
m0 <- lmer(dv ~ 1 +
             (1 | pid), 
           data = simRPT
           )
```

Next, we specify our model. Here we include a fixed effect of our predictor (group membership, `iv`), and a random effect of participant (`iv`) to take account of the fact we have three measurements per person.

```{r}
m1 <- lmer(dv ~ 1 + iv +
             (1 | pid), 
           data = simRPT
           )
summary(m1)
```

And we can compare our models:

```{r}
library(pbkrtest)
PBmodcomp(m1, m0)
```

```{r}
anova(m0,m1)
```

OK, so we can see that we appear to have a significant effect of our repeated factor here. But this does not look like a typical ANOVA output. For piece of mind, it can be useful to compare models.

## Comparison to `aov()`
```{r}
m2 <- aov(dv ~ iv + Error(pid), data = simRPT)
```

Here the term `Error(pid)` is specifying the within person error, or residual. This is what we are doing with our random effect (`(1 | pid)`) in `lmer()`

And we can compare the model sums of squares from both approaches to see the equivalence:

```{r}
summary(m2)
```

```{r}
anova(m1)
```

## Check model

```{r}
plot(m1)
qqnorm(ranef(m1)$pid[,1])
```

## Visualise Model


```{r}
library(sjPlot)
plot_model(m1, type="pred")
plot_model(m1, type="re")
dotplot.ranef.mer(ranef(m1))
```

## Interpret model

```{r}
confint(m1, method = "boot")
```


